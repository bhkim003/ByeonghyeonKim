{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4771/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True and trace_on == True\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.25 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "# const2 = False # True # False\n",
    "\n",
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "# if const2 == True:\n",
    "#     const2 = decay\n",
    "# else:\n",
    "#     const2 = 0.0\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                 BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 128, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.720291189014991,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                 lif_layer_sg_width = 3.555718888923306, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = ['M', 'M', 200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 learning_rate = 0.0001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 10000,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 5, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 100_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "#                 # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "#                 # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = False, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용\n",
    "\n",
    "#                 exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "#                 merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 0, \n",
    "\n",
    "#                 num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = True, # True # False \n",
    "\n",
    "#                 last_lif = False,\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoid와 BN이 있어야 잘된다.\n",
    "# # average pooling  \n",
    "# # 이 낫다. \n",
    "\n",
    "# # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: vt95hgaf\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x8xsky3c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250429_230531-x8xsky3c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x8xsky3c' target=\"_blank\">sage-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x8xsky3c' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x8xsky3c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.303353/  2.303047, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.303309/  2.302968, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.303253/  2.302913, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.303180/  2.302890, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  2.303385/  2.302858, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  2.302999/  2.302758, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  2.302884/  2.302676, val:  10.42%, val_best:  10.42%, tr:  10.52%, tr_best:  10.52%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  2.302937/  2.302622, val:  10.83%, val_best:  10.83%, tr:  10.32%, tr_best:  10.52%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  2.302510/  2.302466, val:  11.25%, val_best:  11.25%, tr:  11.54%, tr_best:  11.54%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  2.302069/  2.302081, val:  12.50%, val_best:  12.50%, tr:  12.46%, tr_best:  12.46%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  2.302145/  2.301618, val:  10.00%, val_best:  12.50%, tr:  11.75%, tr_best:  12.46%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  2.301182/  2.300749, val:  10.42%, val_best:  12.50%, tr:  11.85%, tr_best:  12.46%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  2.299289/  2.299141, val:  10.00%, val_best:  12.50%, tr:  13.89%, tr_best:  13.89%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  2.296365/  2.296639, val:  12.50%, val_best:  12.50%, tr:  13.99%, tr_best:  13.99%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  2.292319/  2.292144, val:  12.08%, val_best:  12.50%, tr:  14.50%, tr_best:  14.50%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  2.284315/  2.285668, val:  14.17%, val_best:  14.17%, tr:  14.91%, tr_best:  14.91%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  2.274568/  2.274913, val:  14.17%, val_best:  14.17%, tr:  13.59%, tr_best:  14.91%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  2.257969/  2.260733, val:  17.08%, val_best:  17.08%, tr:  15.22%, tr_best:  15.22%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  2.236376/  2.239287, val:  20.00%, val_best:  20.00%, tr:  19.00%, tr_best:  19.00%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  2.207176/  2.216041, val:  24.17%, val_best:  24.17%, tr:  22.98%, tr_best:  22.98%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  2.175920/  2.185832, val:  23.33%, val_best:  24.17%, tr:  24.72%, tr_best:  24.72%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  2.137046/  2.153112, val:  30.42%, val_best:  30.42%, tr:  28.60%, tr_best:  28.60%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  2.100350/  2.122704, val:  31.67%, val_best:  31.67%, tr:  33.71%, tr_best:  33.71%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  2.065196/  2.090460, val:  33.33%, val_best:  33.33%, tr:  35.65%, tr_best:  35.65%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  2.027109/  2.058760, val:  38.33%, val_best:  38.33%, tr:  37.28%, tr_best:  37.28%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  1.991935/  2.025528, val:  41.25%, val_best:  41.25%, tr:  40.55%, tr_best:  40.55%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  1.950905/  1.994202, val:  42.08%, val_best:  42.08%, tr:  44.02%, tr_best:  44.02%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  1.911328/  1.961835, val:  42.50%, val_best:  42.50%, tr:  44.64%, tr_best:  44.64%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  1.878763/  1.928056, val:  43.75%, val_best:  43.75%, tr:  46.68%, tr_best:  46.68%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  1.838106/  1.894669, val:  42.50%, val_best:  43.75%, tr:  48.01%, tr_best:  48.01%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  1.804025/  1.865112, val:  43.75%, val_best:  43.75%, tr:  48.93%, tr_best:  48.93%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  1.763242/  1.836773, val:  42.08%, val_best:  43.75%, tr:  49.95%, tr_best:  49.95%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  1.727479/  1.809641, val:  42.50%, val_best:  43.75%, tr:  50.46%, tr_best:  50.46%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  1.703662/  1.786343, val:  45.83%, val_best:  45.83%, tr:  51.17%, tr_best:  51.17%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  1.679972/  1.763433, val:  43.75%, val_best:  45.83%, tr:  51.79%, tr_best:  51.79%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  1.646387/  1.744075, val:  45.83%, val_best:  45.83%, tr:  53.32%, tr_best:  53.32%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  1.626353/  1.724657, val:  44.17%, val_best:  45.83%, tr:  53.93%, tr_best:  53.93%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  1.606478/  1.706433, val:  45.00%, val_best:  45.83%, tr:  53.52%, tr_best:  53.93%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  1.588147/  1.691136, val:  44.17%, val_best:  45.83%, tr:  54.55%, tr_best:  54.55%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  1.572647/  1.677410, val:  47.08%, val_best:  47.08%, tr:  56.28%, tr_best:  56.28%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  1.549720/  1.664349, val:  44.17%, val_best:  47.08%, tr:  57.20%, tr_best:  57.20%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  1.535283/  1.648630, val:  45.00%, val_best:  47.08%, tr:  58.12%, tr_best:  58.12%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  1.519009/  1.637509, val:  46.25%, val_best:  47.08%, tr:  58.73%, tr_best:  58.73%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  1.509553/  1.625227, val:  47.92%, val_best:  47.92%, tr:  58.94%, tr_best:  58.94%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  1.492961/  1.614434, val:  48.75%, val_best:  48.75%, tr:  57.30%, tr_best:  58.94%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  1.484063/  1.605855, val:  48.75%, val_best:  48.75%, tr:  57.81%, tr_best:  58.94%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  1.465103/  1.592090, val:  49.58%, val_best:  49.58%, tr:  58.73%, tr_best:  58.94%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  1.457727/  1.584098, val:  50.00%, val_best:  50.00%, tr:  59.45%, tr_best:  59.45%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  1.445830/  1.571961, val:  52.50%, val_best:  52.50%, tr:  59.96%, tr_best:  59.96%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  1.438472/  1.562812, val:  52.92%, val_best:  52.92%, tr:  58.02%, tr_best:  59.96%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  1.427315/  1.553786, val:  50.42%, val_best:  52.92%, tr:  60.67%, tr_best:  60.67%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  1.416676/  1.550790, val:  52.92%, val_best:  52.92%, tr:  60.16%, tr_best:  60.67%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  1.404887/  1.542407, val:  53.75%, val_best:  53.75%, tr:  59.96%, tr_best:  60.67%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  1.398384/  1.536589, val:  54.17%, val_best:  54.17%, tr:  61.59%, tr_best:  61.59%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  1.391913/  1.530380, val:  55.83%, val_best:  55.83%, tr:  60.88%, tr_best:  61.59%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  1.381779/  1.523501, val:  56.25%, val_best:  56.25%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  1.369507/  1.517961, val:  55.00%, val_best:  56.25%, tr:  62.51%, tr_best:  62.72%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  1.369595/  1.514389, val:  58.75%, val_best:  58.75%, tr:  62.41%, tr_best:  62.72%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  1.353787/  1.509668, val:  55.83%, val_best:  58.75%, tr:  60.88%, tr_best:  62.72%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  1.354373/  1.504657, val:  57.92%, val_best:  58.75%, tr:  61.39%, tr_best:  62.72%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  1.346911/  1.499307, val:  56.25%, val_best:  58.75%, tr:  61.39%, tr_best:  62.72%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  1.345516/  1.493930, val:  57.92%, val_best:  58.75%, tr:  63.43%, tr_best:  63.43%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  1.338707/  1.489630, val:  57.08%, val_best:  58.75%, tr:  62.72%, tr_best:  63.43%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  1.326416/  1.487149, val:  56.25%, val_best:  58.75%, tr:  62.82%, tr_best:  63.43%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  1.316465/  1.481256, val:  57.50%, val_best:  58.75%, tr:  63.13%, tr_best:  63.43%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  1.318961/  1.476951, val:  57.50%, val_best:  58.75%, tr:  63.13%, tr_best:  63.43%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  1.313264/  1.476341, val:  56.67%, val_best:  58.75%, tr:  63.43%, tr_best:  63.43%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  1.308202/  1.473343, val:  57.92%, val_best:  58.75%, tr:  63.43%, tr_best:  63.43%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  1.296274/  1.466686, val:  56.25%, val_best:  58.75%, tr:  62.41%, tr_best:  63.43%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  1.297400/  1.459043, val:  57.92%, val_best:  58.75%, tr:  65.17%, tr_best:  65.17%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  1.290336/  1.454976, val:  58.33%, val_best:  58.75%, tr:  64.04%, tr_best:  65.17%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  1.284018/  1.452057, val:  57.92%, val_best:  58.75%, tr:  63.23%, tr_best:  65.17%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  1.277087/  1.448907, val:  58.75%, val_best:  58.75%, tr:  62.92%, tr_best:  65.17%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  1.271181/  1.444814, val:  58.75%, val_best:  58.75%, tr:  65.47%, tr_best:  65.47%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  1.265088/  1.443850, val:  58.33%, val_best:  58.75%, tr:  62.51%, tr_best:  65.47%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  1.265444/  1.440282, val:  59.58%, val_best:  59.58%, tr:  63.94%, tr_best:  65.47%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  1.261984/  1.434208, val:  58.75%, val_best:  59.58%, tr:  63.94%, tr_best:  65.47%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  1.260119/  1.433017, val:  59.58%, val_best:  59.58%, tr:  63.94%, tr_best:  65.47%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  1.254975/  1.429463, val:  59.17%, val_best:  59.58%, tr:  64.45%, tr_best:  65.47%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  1.255700/  1.426156, val:  57.50%, val_best:  59.58%, tr:  64.66%, tr_best:  65.47%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  1.244677/  1.422965, val:  60.83%, val_best:  60.83%, tr:  64.25%, tr_best:  65.47%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  1.248790/  1.421648, val:  61.25%, val_best:  61.25%, tr:  65.27%, tr_best:  65.47%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  1.240613/  1.419371, val:  60.42%, val_best:  61.25%, tr:  65.58%, tr_best:  65.58%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  1.241135/  1.417874, val:  60.00%, val_best:  61.25%, tr:  66.50%, tr_best:  66.50%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  1.240056/  1.415753, val:  60.83%, val_best:  61.25%, tr:  64.45%, tr_best:  66.50%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  1.238025/  1.412904, val:  60.42%, val_best:  61.25%, tr:  64.86%, tr_best:  66.50%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  1.225889/  1.410465, val:  59.17%, val_best:  61.25%, tr:  64.66%, tr_best:  66.50%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  1.228369/  1.410762, val:  60.83%, val_best:  61.25%, tr:  67.62%, tr_best:  67.62%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  1.223306/  1.409378, val:  60.00%, val_best:  61.25%, tr:  66.80%, tr_best:  67.62%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  1.216402/  1.409564, val:  60.00%, val_best:  61.25%, tr:  66.29%, tr_best:  67.62%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  1.217195/  1.407600, val:  60.83%, val_best:  61.25%, tr:  67.31%, tr_best:  67.62%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  1.203808/  1.405875, val:  61.25%, val_best:  61.25%, tr:  66.29%, tr_best:  67.62%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  1.209737/  1.401118, val:  60.42%, val_best:  61.25%, tr:  66.70%, tr_best:  67.62%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  1.212175/  1.403679, val:  60.83%, val_best:  61.25%, tr:  67.93%, tr_best:  67.93%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  1.198833/  1.397734, val:  61.25%, val_best:  61.25%, tr:  67.11%, tr_best:  67.93%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  1.200905/  1.395111, val:  62.08%, val_best:  62.08%, tr:  65.88%, tr_best:  67.93%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  1.191531/  1.394050, val:  61.25%, val_best:  62.08%, tr:  67.93%, tr_best:  67.93%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  1.192198/  1.388378, val:  61.25%, val_best:  62.08%, tr:  67.31%, tr_best:  67.93%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  1.193458/  1.390504, val:  61.25%, val_best:  62.08%, tr:  66.39%, tr_best:  67.93%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  1.183497/  1.386864, val:  61.25%, val_best:  62.08%, tr:  68.44%, tr_best:  68.44%\n",
      "epoch-100 lr=['0.0001000'], tr/val_loss:  1.178301/  1.382349, val:  61.25%, val_best:  62.08%, tr:  66.19%, tr_best:  68.44%\n",
      "epoch-101 lr=['0.0001000'], tr/val_loss:  1.178781/  1.382492, val:  61.25%, val_best:  62.08%, tr:  69.66%, tr_best:  69.66%\n",
      "epoch-102 lr=['0.0001000'], tr/val_loss:  1.180720/  1.380333, val:  61.67%, val_best:  62.08%, tr:  67.93%, tr_best:  69.66%\n",
      "epoch-103 lr=['0.0001000'], tr/val_loss:  1.177317/  1.374965, val:  62.08%, val_best:  62.08%, tr:  67.31%, tr_best:  69.66%\n",
      "epoch-104 lr=['0.0001000'], tr/val_loss:  1.168690/  1.375508, val:  62.08%, val_best:  62.08%, tr:  69.46%, tr_best:  69.66%\n",
      "epoch-105 lr=['0.0001000'], tr/val_loss:  1.176096/  1.374774, val:  61.67%, val_best:  62.08%, tr:  67.62%, tr_best:  69.66%\n",
      "epoch-106 lr=['0.0001000'], tr/val_loss:  1.172410/  1.373699, val:  63.33%, val_best:  63.33%, tr:  68.74%, tr_best:  69.66%\n",
      "epoch-107 lr=['0.0001000'], tr/val_loss:  1.169890/  1.375928, val:  62.92%, val_best:  63.33%, tr:  68.34%, tr_best:  69.66%\n",
      "epoch-108 lr=['0.0001000'], tr/val_loss:  1.155669/  1.372627, val:  60.83%, val_best:  63.33%, tr:  70.07%, tr_best:  70.07%\n",
      "epoch-109 lr=['0.0001000'], tr/val_loss:  1.161465/  1.373353, val:  63.75%, val_best:  63.75%, tr:  69.05%, tr_best:  70.07%\n",
      "epoch-110 lr=['0.0001000'], tr/val_loss:  1.150182/  1.367711, val:  65.00%, val_best:  65.00%, tr:  69.87%, tr_best:  70.07%\n",
      "epoch-111 lr=['0.0001000'], tr/val_loss:  1.153038/  1.366161, val:  64.58%, val_best:  65.00%, tr:  69.77%, tr_best:  70.07%\n",
      "epoch-112 lr=['0.0001000'], tr/val_loss:  1.145608/  1.363422, val:  63.75%, val_best:  65.00%, tr:  70.28%, tr_best:  70.28%\n",
      "epoch-113 lr=['0.0001000'], tr/val_loss:  1.151779/  1.363166, val:  64.58%, val_best:  65.00%, tr:  69.56%, tr_best:  70.28%\n",
      "epoch-114 lr=['0.0001000'], tr/val_loss:  1.152143/  1.360148, val:  62.08%, val_best:  65.00%, tr:  69.77%, tr_best:  70.28%\n",
      "epoch-115 lr=['0.0001000'], tr/val_loss:  1.141943/  1.358133, val:  65.83%, val_best:  65.83%, tr:  68.95%, tr_best:  70.28%\n",
      "epoch-116 lr=['0.0001000'], tr/val_loss:  1.136975/  1.359021, val:  63.75%, val_best:  65.83%, tr:  68.85%, tr_best:  70.28%\n",
      "epoch-117 lr=['0.0001000'], tr/val_loss:  1.138532/  1.359076, val:  63.75%, val_best:  65.83%, tr:  69.15%, tr_best:  70.28%\n",
      "epoch-118 lr=['0.0001000'], tr/val_loss:  1.137962/  1.360851, val:  65.00%, val_best:  65.83%, tr:  69.15%, tr_best:  70.28%\n",
      "epoch-119 lr=['0.0001000'], tr/val_loss:  1.130830/  1.356774, val:  65.00%, val_best:  65.83%, tr:  69.56%, tr_best:  70.28%\n",
      "epoch-120 lr=['0.0001000'], tr/val_loss:  1.129454/  1.355089, val:  64.58%, val_best:  65.83%, tr:  69.87%, tr_best:  70.28%\n",
      "epoch-121 lr=['0.0001000'], tr/val_loss:  1.130666/  1.352265, val:  62.92%, val_best:  65.83%, tr:  70.58%, tr_best:  70.58%\n",
      "epoch-122 lr=['0.0001000'], tr/val_loss:  1.135765/  1.354257, val:  64.17%, val_best:  65.83%, tr:  69.97%, tr_best:  70.58%\n",
      "epoch-123 lr=['0.0001000'], tr/val_loss:  1.133216/  1.349532, val:  60.83%, val_best:  65.83%, tr:  69.66%, tr_best:  70.58%\n",
      "epoch-124 lr=['0.0001000'], tr/val_loss:  1.117613/  1.351529, val:  62.08%, val_best:  65.83%, tr:  70.99%, tr_best:  70.99%\n",
      "epoch-125 lr=['0.0001000'], tr/val_loss:  1.113118/  1.347363, val:  64.17%, val_best:  65.83%, tr:  71.09%, tr_best:  71.09%\n",
      "epoch-126 lr=['0.0001000'], tr/val_loss:  1.126052/  1.347907, val:  65.00%, val_best:  65.83%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-127 lr=['0.0001000'], tr/val_loss:  1.122297/  1.347484, val:  63.75%, val_best:  65.83%, tr:  71.50%, tr_best:  71.50%\n",
      "epoch-128 lr=['0.0001000'], tr/val_loss:  1.113805/  1.348751, val:  64.17%, val_best:  65.83%, tr:  70.58%, tr_best:  71.50%\n",
      "epoch-129 lr=['0.0001000'], tr/val_loss:  1.114721/  1.342713, val:  64.58%, val_best:  65.83%, tr:  72.63%, tr_best:  72.63%\n",
      "epoch-130 lr=['0.0001000'], tr/val_loss:  1.117513/  1.340897, val:  65.42%, val_best:  65.83%, tr:  71.60%, tr_best:  72.63%\n",
      "epoch-131 lr=['0.0001000'], tr/val_loss:  1.102278/  1.337405, val:  66.25%, val_best:  66.25%, tr:  72.93%, tr_best:  72.93%\n",
      "epoch-132 lr=['0.0001000'], tr/val_loss:  1.099504/  1.334320, val:  65.00%, val_best:  66.25%, tr:  69.46%, tr_best:  72.93%\n",
      "epoch-133 lr=['0.0001000'], tr/val_loss:  1.099479/  1.335608, val:  65.83%, val_best:  66.25%, tr:  72.01%, tr_best:  72.93%\n",
      "epoch-134 lr=['0.0001000'], tr/val_loss:  1.096795/  1.333802, val:  66.25%, val_best:  66.25%, tr:  72.01%, tr_best:  72.93%\n",
      "epoch-135 lr=['0.0001000'], tr/val_loss:  1.094121/  1.334877, val:  66.25%, val_best:  66.25%, tr:  71.81%, tr_best:  72.93%\n",
      "epoch-136 lr=['0.0001000'], tr/val_loss:  1.099723/  1.330453, val:  65.83%, val_best:  66.25%, tr:  72.32%, tr_best:  72.93%\n",
      "epoch-137 lr=['0.0001000'], tr/val_loss:  1.101690/  1.328499, val:  67.08%, val_best:  67.08%, tr:  72.22%, tr_best:  72.93%\n",
      "epoch-138 lr=['0.0001000'], tr/val_loss:  1.094382/  1.327267, val:  65.42%, val_best:  67.08%, tr:  71.71%, tr_best:  72.93%\n",
      "epoch-139 lr=['0.0001000'], tr/val_loss:  1.091104/  1.326132, val:  66.67%, val_best:  67.08%, tr:  72.42%, tr_best:  72.93%\n",
      "epoch-140 lr=['0.0001000'], tr/val_loss:  1.090698/  1.323904, val:  67.08%, val_best:  67.08%, tr:  71.09%, tr_best:  72.93%\n",
      "epoch-141 lr=['0.0001000'], tr/val_loss:  1.077954/  1.324875, val:  67.50%, val_best:  67.50%, tr:  72.83%, tr_best:  72.93%\n",
      "epoch-142 lr=['0.0001000'], tr/val_loss:  1.079211/  1.320949, val:  66.25%, val_best:  67.50%, tr:  71.09%, tr_best:  72.93%\n",
      "epoch-143 lr=['0.0001000'], tr/val_loss:  1.079115/  1.321221, val:  66.25%, val_best:  67.50%, tr:  72.93%, tr_best:  72.93%\n",
      "epoch-144 lr=['0.0001000'], tr/val_loss:  1.079847/  1.321191, val:  67.50%, val_best:  67.50%, tr:  73.54%, tr_best:  73.54%\n",
      "epoch-145 lr=['0.0001000'], tr/val_loss:  1.075492/  1.319746, val:  67.92%, val_best:  67.92%, tr:  71.40%, tr_best:  73.54%\n",
      "epoch-146 lr=['0.0001000'], tr/val_loss:  1.070112/  1.319290, val:  65.83%, val_best:  67.92%, tr:  72.42%, tr_best:  73.54%\n",
      "epoch-147 lr=['0.0001000'], tr/val_loss:  1.086233/  1.316770, val:  67.08%, val_best:  67.92%, tr:  72.93%, tr_best:  73.54%\n",
      "epoch-148 lr=['0.0001000'], tr/val_loss:  1.069008/  1.313817, val:  65.83%, val_best:  67.92%, tr:  73.24%, tr_best:  73.54%\n",
      "epoch-149 lr=['0.0001000'], tr/val_loss:  1.066252/  1.315996, val:  67.92%, val_best:  67.92%, tr:  74.57%, tr_best:  74.57%\n",
      "epoch-150 lr=['0.0001000'], tr/val_loss:  1.079857/  1.314180, val:  67.92%, val_best:  67.92%, tr:  73.54%, tr_best:  74.57%\n",
      "epoch-151 lr=['0.0001000'], tr/val_loss:  1.065213/  1.312805, val:  67.50%, val_best:  67.92%, tr:  73.75%, tr_best:  74.57%\n",
      "epoch-152 lr=['0.0001000'], tr/val_loss:  1.058008/  1.311980, val:  68.33%, val_best:  68.33%, tr:  73.65%, tr_best:  74.57%\n",
      "epoch-153 lr=['0.0001000'], tr/val_loss:  1.074112/  1.310884, val:  67.50%, val_best:  68.33%, tr:  74.36%, tr_best:  74.57%\n",
      "epoch-154 lr=['0.0001000'], tr/val_loss:  1.058458/  1.309846, val:  67.08%, val_best:  68.33%, tr:  73.44%, tr_best:  74.57%\n",
      "epoch-155 lr=['0.0001000'], tr/val_loss:  1.060412/  1.311859, val:  67.08%, val_best:  68.33%, tr:  73.44%, tr_best:  74.57%\n",
      "epoch-156 lr=['0.0001000'], tr/val_loss:  1.058227/  1.308354, val:  67.92%, val_best:  68.33%, tr:  72.83%, tr_best:  74.57%\n",
      "epoch-157 lr=['0.0001000'], tr/val_loss:  1.047424/  1.308625, val:  67.92%, val_best:  68.33%, tr:  73.65%, tr_best:  74.57%\n",
      "epoch-158 lr=['0.0001000'], tr/val_loss:  1.059704/  1.311400, val:  67.50%, val_best:  68.33%, tr:  74.06%, tr_best:  74.57%\n",
      "epoch-159 lr=['0.0001000'], tr/val_loss:  1.051444/  1.309884, val:  66.67%, val_best:  68.33%, tr:  74.16%, tr_best:  74.57%\n",
      "epoch-160 lr=['0.0001000'], tr/val_loss:  1.048540/  1.307123, val:  68.75%, val_best:  68.75%, tr:  74.06%, tr_best:  74.57%\n",
      "epoch-161 lr=['0.0001000'], tr/val_loss:  1.042638/  1.307643, val:  66.67%, val_best:  68.75%, tr:  75.28%, tr_best:  75.28%\n",
      "epoch-162 lr=['0.0001000'], tr/val_loss:  1.048777/  1.307669, val:  66.67%, val_best:  68.75%, tr:  75.18%, tr_best:  75.28%\n",
      "epoch-163 lr=['0.0001000'], tr/val_loss:  1.045015/  1.304959, val:  67.08%, val_best:  68.75%, tr:  74.36%, tr_best:  75.28%\n",
      "epoch-164 lr=['0.0001000'], tr/val_loss:  1.042409/  1.306468, val:  67.50%, val_best:  68.75%, tr:  74.06%, tr_best:  75.28%\n",
      "epoch-165 lr=['0.0001000'], tr/val_loss:  1.042598/  1.303831, val:  68.33%, val_best:  68.75%, tr:  73.03%, tr_best:  75.28%\n",
      "epoch-166 lr=['0.0001000'], tr/val_loss:  1.035709/  1.304744, val:  67.50%, val_best:  68.75%, tr:  73.14%, tr_best:  75.28%\n",
      "epoch-167 lr=['0.0001000'], tr/val_loss:  1.036229/  1.303208, val:  67.50%, val_best:  68.75%, tr:  75.49%, tr_best:  75.49%\n",
      "epoch-168 lr=['0.0001000'], tr/val_loss:  1.033532/  1.299236, val:  66.67%, val_best:  68.75%, tr:  74.67%, tr_best:  75.49%\n",
      "epoch-169 lr=['0.0001000'], tr/val_loss:  1.026508/  1.299890, val:  67.50%, val_best:  68.75%, tr:  74.97%, tr_best:  75.49%\n",
      "epoch-170 lr=['0.0001000'], tr/val_loss:  1.034427/  1.298198, val:  67.92%, val_best:  68.75%, tr:  73.95%, tr_best:  75.49%\n",
      "epoch-171 lr=['0.0001000'], tr/val_loss:  1.031492/  1.297934, val:  66.25%, val_best:  68.75%, tr:  75.28%, tr_best:  75.49%\n",
      "epoch-172 lr=['0.0001000'], tr/val_loss:  1.027404/  1.296085, val:  67.08%, val_best:  68.75%, tr:  75.59%, tr_best:  75.59%\n",
      "epoch-173 lr=['0.0001000'], tr/val_loss:  1.027403/  1.297278, val:  68.75%, val_best:  68.75%, tr:  74.67%, tr_best:  75.59%\n",
      "epoch-174 lr=['0.0001000'], tr/val_loss:  1.021261/  1.294233, val:  67.08%, val_best:  68.75%, tr:  74.87%, tr_best:  75.59%\n",
      "epoch-175 lr=['0.0001000'], tr/val_loss:  1.019883/  1.292647, val:  67.92%, val_best:  68.75%, tr:  74.46%, tr_best:  75.59%\n",
      "epoch-176 lr=['0.0001000'], tr/val_loss:  1.017357/  1.288864, val:  67.50%, val_best:  68.75%, tr:  76.00%, tr_best:  76.00%\n",
      "epoch-177 lr=['0.0001000'], tr/val_loss:  1.027516/  1.288814, val:  67.08%, val_best:  68.75%, tr:  74.87%, tr_best:  76.00%\n",
      "epoch-178 lr=['0.0001000'], tr/val_loss:  1.023523/  1.287764, val:  68.33%, val_best:  68.75%, tr:  74.67%, tr_best:  76.00%\n",
      "epoch-179 lr=['0.0001000'], tr/val_loss:  1.024068/  1.289154, val:  67.50%, val_best:  68.75%, tr:  74.46%, tr_best:  76.00%\n",
      "epoch-180 lr=['0.0001000'], tr/val_loss:  1.007493/  1.288381, val:  67.92%, val_best:  68.75%, tr:  75.28%, tr_best:  76.00%\n",
      "epoch-181 lr=['0.0001000'], tr/val_loss:  1.015774/  1.287142, val:  68.75%, val_best:  68.75%, tr:  75.38%, tr_best:  76.00%\n",
      "epoch-182 lr=['0.0001000'], tr/val_loss:  1.006098/  1.288572, val:  66.67%, val_best:  68.75%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-183 lr=['0.0001000'], tr/val_loss:  1.013399/  1.287910, val:  68.75%, val_best:  68.75%, tr:  76.20%, tr_best:  76.30%\n",
      "epoch-184 lr=['0.0001000'], tr/val_loss:  1.009853/  1.290684, val:  67.50%, val_best:  68.75%, tr:  74.87%, tr_best:  76.30%\n",
      "epoch-185 lr=['0.0001000'], tr/val_loss:  1.010754/  1.290276, val:  68.75%, val_best:  68.75%, tr:  76.40%, tr_best:  76.40%\n",
      "epoch-186 lr=['0.0001000'], tr/val_loss:  1.004523/  1.293368, val:  67.92%, val_best:  68.75%, tr:  76.81%, tr_best:  76.81%\n",
      "epoch-187 lr=['0.0001000'], tr/val_loss:  1.016042/  1.289001, val:  66.67%, val_best:  68.75%, tr:  76.61%, tr_best:  76.81%\n",
      "epoch-188 lr=['0.0001000'], tr/val_loss:  0.995975/  1.291098, val:  68.75%, val_best:  68.75%, tr:  76.20%, tr_best:  76.81%\n",
      "epoch-189 lr=['0.0001000'], tr/val_loss:  0.996025/  1.289149, val:  67.08%, val_best:  68.75%, tr:  76.71%, tr_best:  76.81%\n",
      "epoch-190 lr=['0.0001000'], tr/val_loss:  0.996196/  1.288311, val:  66.67%, val_best:  68.75%, tr:  75.59%, tr_best:  76.81%\n",
      "epoch-191 lr=['0.0001000'], tr/val_loss:  1.006363/  1.288226, val:  66.67%, val_best:  68.75%, tr:  77.53%, tr_best:  77.53%\n",
      "epoch-192 lr=['0.0001000'], tr/val_loss:  0.991843/  1.289558, val:  67.92%, val_best:  68.75%, tr:  75.49%, tr_best:  77.53%\n",
      "epoch-193 lr=['0.0001000'], tr/val_loss:  0.990926/  1.288571, val:  68.33%, val_best:  68.75%, tr:  76.40%, tr_best:  77.53%\n",
      "epoch-194 lr=['0.0001000'], tr/val_loss:  0.992843/  1.288991, val:  67.08%, val_best:  68.75%, tr:  76.10%, tr_best:  77.53%\n",
      "epoch-195 lr=['0.0001000'], tr/val_loss:  0.996207/  1.287118, val:  67.08%, val_best:  68.75%, tr:  74.57%, tr_best:  77.53%\n",
      "epoch-196 lr=['0.0001000'], tr/val_loss:  0.992267/  1.283093, val:  68.33%, val_best:  68.75%, tr:  77.12%, tr_best:  77.53%\n",
      "epoch-197 lr=['0.0001000'], tr/val_loss:  0.983894/  1.285584, val:  67.92%, val_best:  68.75%, tr:  76.92%, tr_best:  77.53%\n",
      "epoch-198 lr=['0.0001000'], tr/val_loss:  0.983362/  1.281843, val:  67.92%, val_best:  68.75%, tr:  77.02%, tr_best:  77.53%\n",
      "epoch-199 lr=['0.0001000'], tr/val_loss:  0.976906/  1.282537, val:  67.50%, val_best:  68.75%, tr:  78.45%, tr_best:  78.45%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d80c4174604ab6bc9bbf408b569264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▁▂▂▄▄▅▇▆▆▄▆▆▅▆▆▆▄▇▇▇▇▅▆▆▆▆▆▇▅▇▇▇████▁█</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▃▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇█▇█▇██████████████</td></tr><tr><td>tr_acc</td><td>▁▁▁▂▂▄▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇██████████</td></tr><tr><td>tr_epoch_loss</td><td>████▇▆▆▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▃▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▃▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇█▇█▇██████████████</td></tr><tr><td>val_loss</td><td>████▇▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.78447</td></tr><tr><td>tr_epoch_loss</td><td>0.97691</td></tr><tr><td>val_acc_best</td><td>0.6875</td></tr><tr><td>val_acc_now</td><td>0.675</td></tr><tr><td>val_loss</td><td>1.28254</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sage-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x8xsky3c' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x8xsky3c</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250429_230531-x8xsky3c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ozt1nw0v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250429_231843-ozt1nw0v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ozt1nw0v' target=\"_blank\">autumn-sweep-2</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ozt1nw0v' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ozt1nw0v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325702/  2.309367, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:   8.89%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.323408/  2.309339, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:   9.40%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.319913/  2.316170, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:   9.40%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.322621/  2.318868, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.40%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.323630/  2.316624, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:   9.40%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.314643/  2.312522, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  2.322383/  2.317189, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  2.323500/  2.311199, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.91%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  2.318820/  2.316814, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:   9.91%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.325358/  2.328855, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:   9.91%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  2.318062/  2.313319, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:   9.91%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  2.321499/  2.314362, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  2.318739/  2.304731, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.91%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  2.323381/  2.312831, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  10.52%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  2.318632/  2.314031, val:  10.00%, val_best:  10.00%, tr:  11.24%, tr_best:  11.24%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  2.334141/  2.312476, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  2.325246/  2.309235, val:  10.00%, val_best:  10.00%, tr:  10.73%, tr_best:  11.24%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  2.313498/  2.304715, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  11.24%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  2.317416/  2.315703, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  11.24%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  2.328925/  2.309338, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  11.24%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  2.325712/  2.311423, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  2.317473/  2.314049, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  11.24%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  2.321084/  2.316247, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  2.323519/  2.311760, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  11.24%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  2.315132/  2.309812, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  11.24%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  2.321132/  2.308563, val:  10.00%, val_best:  10.00%, tr:   7.56%, tr_best:  11.24%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  2.324926/  2.309820, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  11.24%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  2.315943/  2.313346, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  11.24%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  2.320931/  2.314629, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  11.24%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  2.316856/  2.309039, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  11.24%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  2.317557/  2.312356, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  11.24%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  2.285669/  2.185875, val:  18.33%, val_best:  18.33%, tr:  11.95%, tr_best:  11.95%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  1.890752/  1.754429, val:  42.92%, val_best:  42.92%, tr:  32.28%, tr_best:  32.28%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  1.517961/  1.475993, val:  56.25%, val_best:  56.25%, tr:  48.72%, tr_best:  48.72%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  1.305305/  1.657177, val:  45.00%, val_best:  56.25%, tr:  56.08%, tr_best:  56.08%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  1.193971/  1.439764, val:  56.25%, val_best:  56.25%, tr:  61.59%, tr_best:  61.59%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  1.064710/  1.406465, val:  65.00%, val_best:  65.00%, tr:  65.17%, tr_best:  65.17%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  1.015308/  1.433348, val:  56.25%, val_best:  65.00%, tr:  67.93%, tr_best:  67.93%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.913985/  1.466741, val:  59.17%, val_best:  65.00%, tr:  69.15%, tr_best:  69.15%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.812126/  1.448129, val:  60.42%, val_best:  65.00%, tr:  74.06%, tr_best:  74.06%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.770694/  1.559213, val:  54.58%, val_best:  65.00%, tr:  74.87%, tr_best:  74.87%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.742183/  1.609424, val:  59.58%, val_best:  65.00%, tr:  76.71%, tr_best:  76.71%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.634664/  1.553419, val:  59.17%, val_best:  65.00%, tr:  78.65%, tr_best:  78.65%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.627182/  1.554873, val:  61.25%, val_best:  65.00%, tr:  77.43%, tr_best:  78.65%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.519652/  1.608618, val:  66.25%, val_best:  66.25%, tr:  81.41%, tr_best:  81.41%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.488055/  1.605652, val:  65.42%, val_best:  66.25%, tr:  85.70%, tr_best:  85.70%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.486002/  1.640077, val:  65.00%, val_best:  66.25%, tr:  85.70%, tr_best:  85.70%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.423122/  1.804963, val:  66.67%, val_best:  66.67%, tr:  89.48%, tr_best:  89.48%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.439059/  1.741451, val:  68.75%, val_best:  68.75%, tr:  90.60%, tr_best:  90.60%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.377964/  1.759156, val:  72.92%, val_best:  72.92%, tr:  93.56%, tr_best:  93.56%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.325454/  1.905649, val:  65.83%, val_best:  72.92%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.298875/  1.832445, val:  71.67%, val_best:  72.92%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.234958/  2.006532, val:  69.58%, val_best:  72.92%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.223064/  1.932011, val:  74.58%, val_best:  74.58%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.189094/  1.940802, val:  72.92%, val_best:  74.58%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.174836/  1.965054, val:  77.08%, val_best:  77.08%, tr:  99.28%, tr_best:  99.59%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.152092/  2.049807, val:  75.42%, val_best:  77.08%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.101200/  2.076707, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.107258/  2.060333, val:  78.33%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.087759/  2.153847, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.066556/  2.190986, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.067264/  2.190288, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.077668/  2.133889, val:  77.08%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.058010/  2.195139, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.042956/  2.181031, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.033591/  2.186554, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.049055/  2.277934, val:  76.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.041022/  2.190682, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.028925/  2.264973, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.017539/  2.425088, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.015820/  2.385879, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.012687/  2.344019, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.014744/  2.413901, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.014711/  2.404495, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.010482/  2.367594, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.006067/  2.422406, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.003815/  2.448516, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.002325/  2.488131, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.003298/  2.505614, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.003815/  2.521605, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.004824/  2.525585, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.007316/  2.514380, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.007120/  2.603476, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.009565/  2.558939, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.009543/  2.619449, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.010667/  2.636659, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.015043/  2.606387, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.063697/  2.612732, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.037501/  2.495685, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.018492/  2.578315, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.018727/  2.515392, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.014921/  2.572905, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.016262/  2.569275, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.015302/  2.621622, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.016537/  2.595874, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.011598/  2.627140, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.010827/  2.668920, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.010747/  2.578698, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.016208/  2.609841, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.028808/  2.685028, val:  76.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.022057/  2.734057, val:  73.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.044234/  2.717262, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.058579/  2.674900, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.060987/  2.663826, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.038886/  2.638976, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.037904/  2.697163, val:  75.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.092125/  2.569755, val:  76.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.045341/  2.631493, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.058004/  2.725737, val:  72.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.037598/  2.645051, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.030996/  2.646441, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.025008/  2.712647, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.026386/  2.751117, val:  75.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.016956/  2.631363, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.018321/  2.650596, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.015384/  2.637683, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.009640/  2.701369, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.004587/  2.702442, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.005157/  2.733791, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.014633/  2.665590, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.007838/  2.631052, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.010300/  2.705751, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.012356/  2.659417, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.012469/  2.592097, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.005769/  2.766325, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.003141/  2.695191, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.002091/  2.665546, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.001179/  2.670817, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.001050/  2.677858, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.001641/  2.765759, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.001676/  2.755904, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.001311/  2.743860, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.000865/  2.772599, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.001332/  2.773874, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.000616/  2.762848, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.000530/  2.772669, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.000482/  2.769133, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.000444/  2.765732, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.000387/  2.765155, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.000362/  2.774956, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.000331/  2.772918, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.000327/  2.759598, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.000314/  2.770576, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.000298/  2.785919, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.000286/  2.789469, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.000281/  2.795392, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.000275/  2.798190, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.000280/  2.796026, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.000255/  2.785323, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.000256/  2.787434, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.000249/  2.787270, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.000243/  2.802255, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.000234/  2.806582, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.000240/  2.798910, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.000227/  2.796018, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.000221/  2.804891, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.000213/  2.809911, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.000210/  2.811945, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.000210/  2.806966, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.000212/  2.794672, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.000202/  2.798397, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.000203/  2.800357, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.000200/  2.798359, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.000192/  2.799168, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.000187/  2.809397, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.000189/  2.816840, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.000184/  2.817760, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.000189/  2.813930, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.000183/  2.810785, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.000180/  2.816931, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.000181/  2.815378, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.000177/  2.816843, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.000170/  2.822483, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.000172/  2.826492, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.000169/  2.827769, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.000173/  2.826782, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.000169/  2.834486, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.000164/  2.835734, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.000161/  2.835798, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.000163/  2.835604, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.000160/  2.836977, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.000162/  2.840573, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.000159/  2.834832, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.000160/  2.834587, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.000156/  2.840317, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.000154/  2.847488, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.000153/  2.857527, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.000150/  2.863741, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.000147/  2.854799, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.000147/  2.856102, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.000146/  2.858265, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.000149/  2.861959, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.000145/  2.868550, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.000146/  2.870723, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.000145/  2.867881, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.000143/  2.870159, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.000145/  2.866669, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.000142/  2.871137, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.000143/  2.871414, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.000139/  2.875157, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56436b3f10643129a4e2ef8aacac13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▁▁▁▁▂▂▆▇▇▅█████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▁▁▆▅▆▆▇████████▇▇██████████████████</td></tr><tr><td>tr_acc</td><td>▁▁▁▁▁▁▁▅▆▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>███████▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▁▁▆▆▆▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▁▁▆▅▆▆▇████████▇▇██████████████████</td></tr><tr><td>val_loss</td><td>▅▅▅▅▅▅▅▁▂▂▃▄▅▅▅▆▆▇▇▇▇▇▇▇▇█▇█▇███████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00014</td></tr><tr><td>val_acc_best</td><td>0.81667</td></tr><tr><td>val_acc_now</td><td>0.80417</td></tr><tr><td>val_loss</td><td>2.87516</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">autumn-sweep-2</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ozt1nw0v' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ozt1nw0v</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250429_231843-ozt1nw0v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cewrv4ug with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250429_233030-cewrv4ug</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cewrv4ug' target=\"_blank\">polar-sweep-3</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cewrv4ug' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cewrv4ug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.099158/  2.101242, val:  49.17%, val_best:  49.17%, tr:  27.17%, tr_best:  27.17%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.826459/  2.853147, val:  46.67%, val_best:  49.17%, tr:  51.38%, tr_best:  51.38%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.446761/  2.563767, val:  50.00%, val_best:  50.00%, tr:  55.77%, tr_best:  55.77%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.563375/  2.428992, val:  54.17%, val_best:  54.17%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.602624/  2.094844, val:  58.33%, val_best:  58.33%, tr:  65.68%, tr_best:  65.68%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.631392/  2.502775, val:  50.00%, val_best:  58.33%, tr:  70.07%, tr_best:  70.07%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.450199/  2.300280, val:  58.33%, val_best:  58.33%, tr:  73.44%, tr_best:  73.44%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.327860/  2.443680, val:  62.92%, val_best:  62.92%, tr:  75.69%, tr_best:  75.69%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.025610/  2.462610, val:  62.50%, val_best:  62.92%, tr:  81.92%, tr_best:  81.92%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.006208/  2.931801, val:  61.25%, val_best:  62.92%, tr:  85.19%, tr_best:  85.19%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.942194/  2.643322, val:  67.50%, val_best:  67.50%, tr:  86.31%, tr_best:  86.31%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.737616/  2.678744, val:  69.17%, val_best:  69.17%, tr:  91.52%, tr_best:  91.52%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.558110/  2.636996, val:  74.58%, val_best:  74.58%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.460456/  2.844511, val:  73.33%, val_best:  74.58%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.406840/  3.108489, val:  71.25%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.361385/  3.320201, val:  70.83%, val_best:  74.58%, tr:  97.65%, tr_best:  97.75%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.260825/  3.416022, val:  73.75%, val_best:  74.58%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.248959/  3.411290, val:  77.92%, val_best:  77.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.213048/  3.722826, val:  71.25%, val_best:  77.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.176750/  3.587035, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.159871/  3.698340, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.131568/  3.796420, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.106505/  3.933098, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.108298/  4.027157, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.080848/  4.071619, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.080876/  4.175393, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.083765/  4.224347, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.076134/  4.280438, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.064802/  4.203683, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.067370/  4.387602, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.055367/  4.553826, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.053203/  4.418664, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.037335/  4.480360, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.047017/  4.421072, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.041260/  4.512586, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.031643/  4.584931, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.039493/  4.527942, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.032766/  4.756074, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.044635/  4.654333, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.045563/  4.752289, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.040425/  4.725685, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.033106/  4.777292, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.032652/  4.918391, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.026178/  4.934171, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.024529/  4.766272, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.030755/  4.891180, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.026897/  4.887289, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.026404/  5.058212, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.036465/  4.978759, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.027422/  5.147470, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.024965/  5.057787, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.018838/  5.117118, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.020543/  5.189607, val:  76.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.022280/  5.215025, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.027199/  5.244824, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.024758/  5.246288, val:  76.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.017539/  5.300371, val:  75.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.018701/  5.214971, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.023343/  5.280485, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.017239/  5.200712, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.022120/  5.210549, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.016390/  5.300327, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.030742/  5.278739, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.013998/  5.468502, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.014018/  5.370606, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.011799/  5.398010, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.009983/  5.428972, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.015104/  5.384973, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.015894/  5.535203, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.013409/  5.420325, val:  75.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.014032/  5.525440, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.014090/  5.433284, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.013105/  5.433581, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.012675/  5.299798, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.006606/  5.535796, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.007965/  5.599808, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.006919/  5.539249, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.008862/  5.522953, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.009958/  5.599457, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.012373/  5.576940, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.007390/  5.434172, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.011059/  5.603341, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.005842/  5.600822, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.008983/  5.620021, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.009201/  5.639697, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.012737/  5.601735, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.010377/  5.596240, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.007242/  5.630605, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.009580/  5.856675, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.009294/  5.574144, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.008757/  5.633227, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.007526/  5.633436, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.006371/  5.589679, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.010515/  5.650331, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.011969/  5.782412, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.011647/  5.667858, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.008932/  5.763807, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.011877/  5.781220, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.012842/  5.796974, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.013074/  5.693725, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.010036/  5.731477, val:  76.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.005894/  5.666288, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.008885/  5.776805, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.009088/  5.764265, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.006717/  5.765067, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.005493/  5.753680, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.005432/  5.806322, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.009495/  5.875660, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.005669/  5.785023, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.010941/  5.606164, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.004671/  5.790107, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.006129/  5.758298, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.006085/  5.837014, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.005778/  5.733932, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.005781/  5.797405, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.010030/  5.858714, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.009773/  5.873743, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.006650/  5.950727, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.006095/  5.817922, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.006635/  5.754041, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.004598/  5.825514, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.005416/  5.813128, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.004642/  5.813951, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.003549/  5.886604, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.004216/  5.870558, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.005491/  5.816228, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.004797/  5.904501, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.003368/  5.780783, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.006632/  5.794197, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.002696/  5.875767, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.004296/  5.824181, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.004387/  5.872257, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.010501/  5.976466, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.008083/  5.933280, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.009125/  5.978192, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.012940/  5.974922, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.006849/  5.900082, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.005797/  5.957459, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.004457/  5.999338, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.004664/  6.023899, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.003033/  5.865406, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.003135/  5.931873, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.002989/  5.890352, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.003184/  5.859719, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.005120/  5.916811, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.002711/  5.985971, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.002166/  5.993088, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.003838/  6.030819, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.001748/  6.012091, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.003879/  6.066010, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.002824/  6.087425, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.002026/  6.055626, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.002350/  6.150193, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.002366/  6.109835, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.002343/  6.063729, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.002045/  6.079382, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.001670/  6.151986, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.002995/  6.112964, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.002896/  6.113753, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.003886/  6.167374, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.003578/  6.135291, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.000992/  6.027692, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.001940/  6.172112, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.004036/  6.116829, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.003772/  6.150991, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.002056/  6.182588, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.003695/  6.161651, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.002939/  6.147885, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.002793/  6.153823, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.003791/  6.060559, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.004235/  6.053575, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.002375/  6.089475, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.005226/  6.074348, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.015506/  6.128105, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.008307/  6.090564, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.005616/  6.112813, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.003350/  6.084305, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.005354/  6.085530, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.005454/  6.230404, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.007376/  6.096161, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.004584/  6.081945, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.001211/  6.040050, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.001231/  6.125368, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.000981/  6.101789, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.000563/  6.097891, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.001528/  6.106710, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.002831/  6.140317, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.002074/  6.167663, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.002029/  6.158438, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.001072/  6.205370, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.002145/  6.248883, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.003719/  6.293360, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.005445/  6.185605, val:  77.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.004205/  6.187843, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.002974/  6.151477, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.001709/  6.128605, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.004763/  6.128962, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.004392/  6.066301, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.003023/  6.199403, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.003764/  6.245645, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe991d4b1474ef785eba30cf73ce004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▇▃█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▄▆▇▇██▇█▇▇██████▇▇▇██████▇██▇█▇▇██████</td></tr><tr><td>tr_acc</td><td>▁▄▆█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▆▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▄▆▇▇██▇█▇▇██████▇▇▇██████▇██▇█▇▇██████</td></tr><tr><td>val_loss</td><td>▂▁▂▃▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00376</td></tr><tr><td>val_acc_best</td><td>0.81667</td></tr><tr><td>val_acc_now</td><td>0.79583</td></tr><tr><td>val_loss</td><td>6.24564</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-sweep-3</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cewrv4ug' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cewrv4ug</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250429_233030-cewrv4ug/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8dsxbymt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250429_234349-8dsxbymt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8dsxbymt' target=\"_blank\">comic-sweep-4</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8dsxbymt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8dsxbymt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.060629/  1.679069, val:  52.08%, val_best:  52.08%, tr:  27.58%, tr_best:  27.58%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.430566/  1.481912, val:  57.08%, val_best:  57.08%, tr:  56.08%, tr_best:  56.08%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  1.247546/  1.386219, val:  59.17%, val_best:  59.17%, tr:  61.49%, tr_best:  61.49%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  1.087887/  1.520179, val:  58.75%, val_best:  59.17%, tr:  66.39%, tr_best:  66.39%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  0.993297/  1.475701, val:  58.33%, val_best:  59.17%, tr:  70.68%, tr_best:  70.68%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  0.954488/  1.404588, val:  66.25%, val_best:  66.25%, tr:  70.99%, tr_best:  70.99%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  0.858299/  1.398571, val:  62.50%, val_best:  66.25%, tr:  75.79%, tr_best:  75.79%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  0.803276/  1.489702, val:  62.92%, val_best:  66.25%, tr:  76.71%, tr_best:  76.71%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  0.759205/  1.515086, val:  65.00%, val_best:  66.25%, tr:  78.55%, tr_best:  78.55%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.696871/  1.599966, val:  67.92%, val_best:  67.92%, tr:  84.98%, tr_best:  84.98%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.663912/  1.654228, val:  66.67%, val_best:  67.92%, tr:  85.80%, tr_best:  85.80%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.605999/  1.709758, val:  74.17%, val_best:  74.17%, tr:  87.33%, tr_best:  87.33%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.583955/  1.686770, val:  68.75%, val_best:  74.17%, tr:  90.40%, tr_best:  90.40%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.518462/  1.795502, val:  73.75%, val_best:  74.17%, tr:  91.52%, tr_best:  91.52%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.439055/  2.259461, val:  66.25%, val_best:  74.17%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.429415/  2.086715, val:  68.33%, val_best:  74.17%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.377435/  2.156818, val:  73.33%, val_best:  74.17%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.370347/  2.122242, val:  75.42%, val_best:  75.42%, tr:  96.22%, tr_best:  97.55%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.323428/  2.296088, val:  67.92%, val_best:  75.42%, tr:  96.32%, tr_best:  97.55%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.304189/  2.366753, val:  72.92%, val_best:  75.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.268390/  2.314691, val:  74.58%, val_best:  75.42%, tr:  97.96%, tr_best:  98.16%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.241368/  2.516396, val:  67.50%, val_best:  75.42%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.192926/  2.468761, val:  75.83%, val_best:  75.83%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.286472/  2.497247, val:  76.25%, val_best:  76.25%, tr:  97.85%, tr_best:  99.80%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.180535/  2.554672, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.139435/  2.551605, val:  75.83%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.158872/  2.636175, val:  77.50%, val_best:  77.50%, tr:  99.39%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.149316/  2.692245, val:  79.17%, val_best:  79.17%, tr:  99.49%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.113046/  2.715733, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.075672/  2.892610, val:  75.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.072648/  2.874541, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.058813/  3.033061, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.056109/  3.094202, val:  76.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.050927/  3.039777, val:  75.42%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.039025/  3.248237, val:  73.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.047744/  3.122020, val:  75.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.036543/  3.131391, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.031634/  3.212302, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.026369/  3.293380, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.027552/  3.286091, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.026962/  3.279810, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.018657/  3.325139, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.012598/  3.382587, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.015043/  3.383414, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.014956/  3.349200, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.012062/  3.486905, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.008683/  3.357147, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.009804/  3.379526, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.009466/  3.435631, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.006019/  3.488192, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.005703/  3.452363, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.006238/  3.476765, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.008102/  3.487957, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.005164/  3.538590, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.006160/  3.614767, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.003797/  3.629519, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.003511/  3.591570, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.004160/  3.641281, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.006857/  3.583557, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.004247/  3.594861, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.004083/  3.632077, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.003154/  3.637581, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.003780/  3.611901, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.002289/  3.625989, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.002258/  3.627726, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.002044/  3.640879, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.001986/  3.664173, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.001870/  3.657732, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.001639/  3.655009, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.001591/  3.692255, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.001537/  3.644223, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.001569/  3.659027, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.001351/  3.668327, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.001339/  3.659644, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.001235/  3.678963, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.001150/  3.667636, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.001102/  3.667462, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.001115/  3.682498, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.001493/  3.706462, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.001748/  3.735568, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.001691/  3.740037, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.002256/  3.764322, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.001961/  3.711734, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.002507/  3.725888, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.001387/  3.694194, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.001190/  3.708414, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.000975/  3.717273, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.001267/  3.717355, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.001178/  3.731985, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.001096/  3.734346, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.001133/  3.712806, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.001434/  3.725451, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.000943/  3.726154, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.001350/  3.773880, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.002364/  3.742359, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.001501/  3.736881, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.001116/  3.778324, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.001181/  3.812017, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.000853/  3.795150, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.000931/  3.821196, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.001194/  3.784974, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.000762/  3.790703, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.000708/  3.795638, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.000772/  3.799021, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.000665/  3.772412, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.000715/  3.786115, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.000975/  3.823418, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.000837/  3.806842, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.000743/  3.798434, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.000752/  3.810892, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.000762/  3.812314, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.000675/  3.803962, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.000748/  3.789832, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.000965/  3.823438, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.000714/  3.808701, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.000977/  3.849674, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.001123/  3.794391, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.000639/  3.787372, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.000649/  3.820709, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.000648/  3.827323, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.000612/  3.810407, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.000594/  3.827479, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.000861/  3.799301, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.000782/  3.835930, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.000588/  3.819609, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.000547/  3.835976, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.000722/  3.829676, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.000808/  3.846114, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.000867/  3.862858, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.000751/  3.853639, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.000757/  3.872535, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.000719/  3.855123, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.000605/  3.855429, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.000520/  3.865397, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.000517/  3.861522, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.000722/  3.878982, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.000506/  3.895532, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.000462/  3.895630, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.000478/  3.881721, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.000460/  3.890136, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.000414/  3.894478, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.000458/  3.899340, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.000405/  3.899158, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.000392/  3.907988, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.000382/  3.900229, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.000355/  3.908529, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.000391/  3.904945, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.000370/  3.914406, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.000419/  3.906921, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.000398/  3.911721, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.000397/  3.905454, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.000504/  3.907668, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.000399/  3.931695, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.000421/  3.935393, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.000371/  3.922695, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.000362/  3.908462, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.000361/  3.905788, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.000357/  3.915290, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.000353/  3.921112, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.000363/  3.911729, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.000394/  3.914687, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.000411/  3.919672, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.000342/  3.905974, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.000404/  3.918029, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.000394/  3.897744, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.000363/  3.912373, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.000349/  3.917100, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.000443/  3.918165, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.000391/  3.925870, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.000346/  3.918513, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.001075/  3.925710, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.000953/  3.950434, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.000934/  3.927640, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.000730/  3.924101, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.000839/  3.964981, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.000768/  3.953959, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.000949/  3.973304, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.000566/  3.964466, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.000527/  3.959982, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.000592/  3.958743, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.000418/  3.965327, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.000361/  3.957874, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.000359/  3.973700, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.000334/  3.979137, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.000377/  3.967909, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.000491/  3.956068, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.000400/  3.949471, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.000338/  3.954689, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.000323/  3.961749, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.000326/  3.973019, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.000319/  3.964444, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.000306/  3.966955, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.000319/  3.968744, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.000293/  3.967716, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.000327/  3.981321, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.000305/  3.969918, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.000287/  3.969296, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.000286/  3.960714, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.000291/  3.956765, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.000276/  3.960056, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3db2ff01db54e60b75c0610131e5e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▄█▇▇██████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▄▄▆▆▆█▇▇▇▇▇▇▇█▇▇▇▇██████▇█▇█▇███▇▇▇▇▇▇</td></tr><tr><td>tr_acc</td><td>▁▃▆▇████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▆▆▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▄▄▆▆▆█▇▇▇▇▇▇▇█▇▇▇▇██████▇█▇█▇███▇▇▇▇▇▇</td></tr><tr><td>val_loss</td><td>▁▁▂▃▄▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00028</td></tr><tr><td>val_acc_best</td><td>0.81667</td></tr><tr><td>val_acc_now</td><td>0.77917</td></tr><tr><td>val_loss</td><td>3.96006</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comic-sweep-4</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8dsxbymt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8dsxbymt</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250429_234349-8dsxbymt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u1kc35sa with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250429_235604-u1kc35sa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/u1kc35sa' target=\"_blank\">dry-sweep-5</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/u1kc35sa' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/u1kc35sa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.305365/  2.302841, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:   8.17%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.305089/  2.302072, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:   8.17%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.252978/  2.178869, val:  22.08%, val_best:  22.08%, tr:  15.73%, tr_best:  15.73%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.002720/  1.871363, val:  47.50%, val_best:  47.50%, tr:  37.59%, tr_best:  37.59%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  1.639050/  1.628767, val:  52.08%, val_best:  52.08%, tr:  53.01%, tr_best:  53.01%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  1.409929/  1.502154, val:  60.00%, val_best:  60.00%, tr:  60.78%, tr_best:  60.78%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  1.294261/  1.468562, val:  60.00%, val_best:  60.00%, tr:  63.02%, tr_best:  63.02%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  1.220485/  1.404702, val:  59.17%, val_best:  60.00%, tr:  64.15%, tr_best:  64.15%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  1.163677/  1.373053, val:  61.67%, val_best:  61.67%, tr:  64.56%, tr_best:  64.56%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  1.121073/  1.383700, val:  62.08%, val_best:  62.08%, tr:  68.13%, tr_best:  68.13%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  1.091132/  1.388209, val:  58.33%, val_best:  62.08%, tr:  68.13%, tr_best:  68.13%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  1.064667/  1.350815, val:  61.25%, val_best:  62.08%, tr:  68.95%, tr_best:  68.95%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  1.042811/  1.345117, val:  60.42%, val_best:  62.08%, tr:  70.79%, tr_best:  70.79%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  1.022344/  1.323482, val:  62.92%, val_best:  62.92%, tr:  71.30%, tr_best:  71.30%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.970931/  1.388894, val:  60.00%, val_best:  62.92%, tr:  72.42%, tr_best:  72.42%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.963258/  1.341403, val:  65.00%, val_best:  65.00%, tr:  74.46%, tr_best:  74.46%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.948283/  1.324726, val:  64.58%, val_best:  65.00%, tr:  73.95%, tr_best:  74.46%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.912530/  1.305997, val:  70.83%, val_best:  70.83%, tr:  78.75%, tr_best:  78.75%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.893616/  1.346317, val:  66.25%, val_best:  70.83%, tr:  79.67%, tr_best:  79.67%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.874414/  1.351056, val:  60.42%, val_best:  70.83%, tr:  76.30%, tr_best:  79.67%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.842865/  1.304476, val:  67.08%, val_best:  70.83%, tr:  82.53%, tr_best:  82.53%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.822803/  1.291887, val:  70.83%, val_best:  70.83%, tr:  84.17%, tr_best:  84.17%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.811725/  1.297137, val:  70.83%, val_best:  70.83%, tr:  82.64%, tr_best:  84.17%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.773771/  1.295759, val:  70.42%, val_best:  70.83%, tr:  87.03%, tr_best:  87.03%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.751037/  1.317079, val:  69.58%, val_best:  70.83%, tr:  88.87%, tr_best:  88.87%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.728905/  1.290858, val:  73.75%, val_best:  73.75%, tr:  89.58%, tr_best:  89.58%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.709497/  1.289541, val:  72.50%, val_best:  73.75%, tr:  90.40%, tr_best:  90.40%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.678929/  1.335268, val:  69.17%, val_best:  73.75%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.668339/  1.299115, val:  75.83%, val_best:  75.83%, tr:  93.05%, tr_best:  93.05%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.641568/  1.332414, val:  72.50%, val_best:  75.83%, tr:  94.59%, tr_best:  94.59%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.628566/  1.326949, val:  73.75%, val_best:  75.83%, tr:  94.69%, tr_best:  94.69%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.623744/  1.360081, val:  73.33%, val_best:  75.83%, tr:  92.75%, tr_best:  94.69%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.607039/  1.341544, val:  74.58%, val_best:  75.83%, tr:  93.46%, tr_best:  94.69%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.590133/  1.340707, val:  74.17%, val_best:  75.83%, tr:  95.30%, tr_best:  95.30%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.574580/  1.359458, val:  69.58%, val_best:  75.83%, tr:  95.10%, tr_best:  95.30%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.563932/  1.378430, val:  70.00%, val_best:  75.83%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.534237/  1.360773, val:  75.42%, val_best:  75.83%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.533269/  1.378122, val:  74.17%, val_best:  75.83%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.515889/  1.377369, val:  76.67%, val_best:  76.67%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.502081/  1.371508, val:  77.08%, val_best:  77.08%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.482354/  1.394032, val:  75.00%, val_best:  77.08%, tr:  97.65%, tr_best:  97.96%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.476699/  1.419138, val:  72.50%, val_best:  77.08%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.471411/  1.405033, val:  72.92%, val_best:  77.08%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.442458/  1.418372, val:  77.50%, val_best:  77.50%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.434379/  1.407782, val:  76.67%, val_best:  77.50%, tr:  97.96%, tr_best:  98.37%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.423180/  1.428805, val:  76.25%, val_best:  77.50%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.415010/  1.435583, val:  77.08%, val_best:  77.50%, tr:  98.57%, tr_best:  98.67%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.403908/  1.458728, val:  76.25%, val_best:  77.50%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.402402/  1.449531, val:  77.08%, val_best:  77.50%, tr:  98.67%, tr_best:  98.88%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.394513/  1.465867, val:  77.50%, val_best:  77.50%, tr:  98.77%, tr_best:  98.88%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.379408/  1.475329, val:  75.42%, val_best:  77.50%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.373925/  1.509219, val:  76.25%, val_best:  77.50%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.364068/  1.478633, val:  77.08%, val_best:  77.50%, tr:  99.08%, tr_best:  99.18%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.350833/  1.489711, val:  78.33%, val_best:  78.33%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.346011/  1.538976, val:  78.33%, val_best:  78.33%, tr:  99.18%, tr_best:  99.28%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.329837/  1.529103, val:  77.92%, val_best:  78.33%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.328189/  1.554611, val:  78.75%, val_best:  78.75%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.331023/  1.537284, val:  78.33%, val_best:  78.75%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.316806/  1.548477, val:  77.50%, val_best:  78.75%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.307954/  1.569440, val:  77.50%, val_best:  78.75%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.311739/  1.559707, val:  78.75%, val_best:  78.75%, tr:  99.18%, tr_best:  99.59%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.309978/  1.593645, val:  78.33%, val_best:  78.75%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.302495/  1.588440, val:  78.75%, val_best:  78.75%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.288752/  1.614576, val:  77.92%, val_best:  78.75%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.287303/  1.645617, val:  76.67%, val_best:  78.75%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.278174/  1.615646, val:  78.33%, val_best:  78.75%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.278988/  1.612653, val:  78.75%, val_best:  78.75%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.276671/  1.628645, val:  78.75%, val_best:  78.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.272476/  1.651209, val:  77.08%, val_best:  78.75%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.259041/  1.653335, val:  77.92%, val_best:  78.75%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.254389/  1.673941, val:  76.25%, val_best:  78.75%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.260940/  1.702798, val:  75.83%, val_best:  78.75%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.248247/  1.708169, val:  76.25%, val_best:  78.75%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.247143/  1.705455, val:  77.92%, val_best:  78.75%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.240173/  1.696256, val:  78.33%, val_best:  78.75%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.231608/  1.718375, val:  77.50%, val_best:  78.75%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.231554/  1.732904, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.228040/  1.747649, val:  76.67%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.228830/  1.746833, val:  78.33%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.221629/  1.756747, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.213352/  1.768548, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.216596/  1.747296, val:  77.92%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.208818/  1.811753, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.208440/  1.790210, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.200866/  1.801805, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.202416/  1.811792, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.197471/  1.836165, val:  76.25%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.193382/  1.817209, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.190987/  1.829709, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.188078/  1.853862, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.181396/  1.832179, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.180209/  1.857036, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.174925/  1.877571, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.174281/  1.884073, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.172942/  1.912182, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.169213/  1.898740, val:  77.08%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.168441/  1.937471, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.163555/  1.897131, val:  77.50%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.160182/  1.914550, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.164671/  1.921255, val:  77.50%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.159752/  1.917746, val:  77.50%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.151094/  1.922933, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.146536/  1.950440, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.150290/  1.944809, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.145682/  1.973861, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.146814/  1.958579, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.145539/  1.993645, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.139771/  1.987155, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.138635/  1.990729, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.134677/  2.001517, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.134596/  1.998487, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.130738/  2.002125, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.127193/  2.021126, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.127354/  2.019807, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.123924/  2.017389, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.123044/  2.025366, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.120616/  2.039658, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.123988/  2.028431, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.120211/  2.026790, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.120424/  2.092613, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.117014/  2.046578, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.115854/  2.029382, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.111016/  2.046317, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.113374/  2.059130, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.111133/  2.065412, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.109481/  2.084309, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.111945/  2.078954, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.108998/  2.091201, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.107883/  2.097689, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.106084/  2.115090, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.106551/  2.105783, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.104261/  2.114105, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.098658/  2.108130, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.096354/  2.117333, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.099193/  2.147891, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.101466/  2.128325, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.093100/  2.149866, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.098474/  2.130040, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.094382/  2.158908, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.094853/  2.151825, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.091995/  2.160403, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.093760/  2.155988, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.091712/  2.180392, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.090478/  2.160670, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.086361/  2.182627, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.088305/  2.197287, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.087672/  2.204972, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.086118/  2.226213, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.083757/  2.215000, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.082783/  2.221758, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.083612/  2.223742, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.083981/  2.233747, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.085517/  2.242302, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.085566/  2.239210, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.078405/  2.257831, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.082313/  2.263321, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.079901/  2.267119, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.078353/  2.281666, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.078476/  2.261105, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.076519/  2.291066, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.075558/  2.292867, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.071982/  2.288460, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.074239/  2.309828, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.075836/  2.306246, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.072348/  2.305584, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.073007/  2.302890, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.069153/  2.312427, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.070065/  2.338222, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.070992/  2.329260, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.066683/  2.329678, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.067094/  2.357427, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.067347/  2.355723, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.064935/  2.343397, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.065343/  2.348162, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.066542/  2.339230, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.066020/  2.348859, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.063779/  2.356315, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.063399/  2.344715, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.064584/  2.351071, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.064699/  2.357731, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.062098/  2.371268, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.063141/  2.380656, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.062557/  2.379457, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.064340/  2.405727, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.063188/  2.390952, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.057950/  2.367758, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.059028/  2.418471, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.058840/  2.417266, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.061019/  2.410412, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.059506/  2.410706, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.061418/  2.417775, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.059968/  2.439228, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.059566/  2.458569, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.057762/  2.441890, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.056998/  2.433675, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.056585/  2.459342, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.057198/  2.466031, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.056285/  2.449859, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.055370/  2.466882, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.052746/  2.469255, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "162f6021ff86437eb4e46863cec4d77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▅▆▇███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▆▆▆▇▇█▇███████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▆▆▆▇██████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▆▆▇▇██████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▆▆▆▇▇█▇███████████████████████████████</td></tr><tr><td>val_loss</td><td>▇▂▂▁▁▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.05275</td></tr><tr><td>val_acc_best</td><td>0.80417</td></tr><tr><td>val_acc_now</td><td>0.7875</td></tr><tr><td>val_loss</td><td>2.46925</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dry-sweep-5</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/u1kc35sa' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/u1kc35sa</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250429_235604-u1kc35sa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qcfubsq8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_000933-qcfubsq8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qcfubsq8' target=\"_blank\">swift-sweep-6</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qcfubsq8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qcfubsq8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  1.957073/  1.538933, val:  54.58%, val_best:  54.58%, tr:  31.15%, tr_best:  31.15%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.291305/  1.396871, val:  55.83%, val_best:  55.83%, tr:  58.63%, tr_best:  58.63%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  1.101060/  1.289411, val:  58.75%, val_best:  58.75%, tr:  62.92%, tr_best:  62.92%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  0.920764/  1.387030, val:  60.83%, val_best:  60.83%, tr:  69.77%, tr_best:  69.77%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  0.808189/  1.329958, val:  61.25%, val_best:  61.25%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  0.733569/  1.266379, val:  69.58%, val_best:  69.58%, tr:  77.22%, tr_best:  77.22%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  0.664230/  1.248816, val:  67.50%, val_best:  69.58%, tr:  82.64%, tr_best:  82.64%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  0.590696/  1.337065, val:  63.75%, val_best:  69.58%, tr:  83.04%, tr_best:  83.04%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  0.551449/  1.362797, val:  64.17%, val_best:  69.58%, tr:  84.78%, tr_best:  84.78%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.445037/  1.383757, val:  67.08%, val_best:  69.58%, tr:  90.30%, tr_best:  90.30%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.408753/  1.507142, val:  67.08%, val_best:  69.58%, tr:  92.13%, tr_best:  92.13%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.344364/  1.529292, val:  75.00%, val_best:  75.00%, tr:  95.20%, tr_best:  95.20%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.288097/  1.527572, val:  73.33%, val_best:  75.00%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.252045/  1.606242, val:  74.58%, val_best:  75.00%, tr:  97.04%, tr_best:  97.34%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.186839/  1.795442, val:  70.83%, val_best:  75.00%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.161989/  1.857511, val:  67.92%, val_best:  75.00%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.115854/  1.953218, val:  71.25%, val_best:  75.00%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.141981/  1.990165, val:  72.08%, val_best:  75.00%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.097326/  1.964939, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.087319/  1.982119, val:  75.42%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.061569/  2.013613, val:  75.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.043610/  2.136260, val:  72.50%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.033305/  2.083334, val:  74.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.025353/  2.117206, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.020841/  2.163277, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.021782/  2.188435, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.021976/  2.243493, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.013368/  2.260771, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.012100/  2.277853, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.008842/  2.303206, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.007924/  2.270687, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.007363/  2.288495, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.006937/  2.347878, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.005396/  2.352182, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.005570/  2.372597, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.004986/  2.362289, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.004246/  2.363674, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.003764/  2.363402, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.003686/  2.376322, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.004391/  2.375667, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.003993/  2.409364, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.003237/  2.401194, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.002814/  2.431450, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.002773/  2.457448, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.002912/  2.433091, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.002546/  2.441670, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.002368/  2.461489, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.002264/  2.449035, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.002513/  2.469102, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.002477/  2.464642, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.002158/  2.479714, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.002563/  2.493438, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.002200/  2.482085, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.002049/  2.475434, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.001813/  2.493677, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.001651/  2.485032, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.002425/  2.497836, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.001717/  2.481738, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.001511/  2.494472, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.001696/  2.486559, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.001551/  2.506783, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.001360/  2.513916, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.001326/  2.506241, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.001349/  2.493743, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.001306/  2.498175, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.001431/  2.522136, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.001188/  2.525834, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.001369/  2.531277, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.001499/  2.537018, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.001262/  2.536241, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.001130/  2.553225, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.001139/  2.542223, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.001075/  2.555832, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.001047/  2.554490, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.001110/  2.561623, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.001061/  2.561861, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.001309/  2.574905, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.001281/  2.577984, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.001159/  2.575677, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.001113/  2.583035, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.000960/  2.586264, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.000972/  2.590423, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.000951/  2.594494, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.000959/  2.573489, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.000928/  2.593845, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.000863/  2.582112, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.000866/  2.596251, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.001306/  2.585508, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.001432/  2.605535, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.001202/  2.612906, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.000950/  2.630672, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.000880/  2.623271, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.000863/  2.624882, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.000839/  2.611538, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.000881/  2.630229, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.000767/  2.619477, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.000805/  2.623837, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.001266/  2.640125, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.000922/  2.654451, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.000850/  2.633842, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.000785/  2.643204, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.000773/  2.643084, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.000686/  2.657412, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.000666/  2.657617, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.000667/  2.651525, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.000693/  2.661084, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.000692/  2.648119, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.000669/  2.654372, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.000702/  2.667193, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.000657/  2.677902, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.000631/  2.668792, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.000646/  2.680497, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.000635/  2.679312, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.000632/  2.681955, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.000604/  2.682520, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.000652/  2.679889, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.000627/  2.673934, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.000579/  2.686740, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.000545/  2.692019, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.000529/  2.706501, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.000536/  2.714277, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.000580/  2.712941, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.000588/  2.704996, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.000553/  2.710003, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.000557/  2.695770, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.000582/  2.708892, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.000543/  2.723102, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.000514/  2.730366, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.000521/  2.719584, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.000541/  2.725008, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.000510/  2.734236, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.000557/  2.729710, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.000514/  2.744137, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.000548/  2.733213, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.000521/  2.735941, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.000497/  2.743615, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.000540/  2.739241, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.000548/  2.745599, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.000483/  2.741285, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.000458/  2.744265, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.000467/  2.735373, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.000461/  2.755050, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.000438/  2.751193, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.000480/  2.746004, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.000435/  2.761835, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.000466/  2.756013, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.000442/  2.745410, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.000415/  2.758405, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.000406/  2.746565, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.000421/  2.744399, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.000405/  2.760814, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.000405/  2.750521, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.000397/  2.755278, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.000407/  2.760428, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.000406/  2.756909, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.000392/  2.763521, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.000411/  2.763780, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.000397/  2.774137, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.000402/  2.779808, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.000414/  2.778700, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.000423/  2.782132, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.000402/  2.791639, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.000416/  2.784019, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.000410/  2.782920, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.000422/  2.781220, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.000440/  2.788174, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.000390/  2.787146, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.000392/  2.794984, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.000393/  2.796401, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.000376/  2.788097, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.000364/  2.781761, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.000420/  2.800025, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.000386/  2.791282, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.000371/  2.790601, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.000361/  2.791127, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.000376/  2.787634, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.000368/  2.784856, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.000396/  2.788023, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.000378/  2.797923, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.000353/  2.793584, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.000367/  2.796515, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.000349/  2.791422, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.000356/  2.794699, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.000344/  2.800707, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.000365/  2.809437, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.000374/  2.806972, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.000345/  2.805583, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.000343/  2.791025, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.000339/  2.794211, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.000347/  2.808961, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.000342/  2.802847, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.000351/  2.804286, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.000331/  2.811045, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.000333/  2.796742, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.000397/  2.819448, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.000372/  2.814100, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.000340/  2.811155, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.000336/  2.810674, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.000326/  2.811359, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.000310/  2.816072, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e7909208a345cc86147ec1052028a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▅▅▇▆▆██▇▇▇▇▇█▇▇▇▇▇█▇██▇█▇█▇█▇█▇▇▇▇▇▇▇▇</td></tr><tr><td>tr_acc</td><td>▁▄▆█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▅▇▇▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▅▅▇▆▆██▇▇▇▇▇█▇▇▇▇▇█▇██▇█▇█▇█▇█▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss</td><td>▂▁▂▄▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00031</td></tr><tr><td>val_acc_best</td><td>0.7875</td></tr><tr><td>val_acc_now</td><td>0.75833</td></tr><tr><td>val_loss</td><td>2.81607</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swift-sweep-6</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qcfubsq8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qcfubsq8</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_000933-qcfubsq8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o11kh7ar with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_002100-o11kh7ar</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o11kh7ar' target=\"_blank\">whole-sweep-7</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o11kh7ar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o11kh7ar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.305365/  2.302841, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:   8.17%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.304994/  2.302639, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:   8.17%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.305079/  2.302667, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:   8.99%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.304731/  2.302708, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:   8.99%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  2.304833/  2.302682, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:   8.99%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  2.304569/  2.302663, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:   8.99%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  2.305086/  2.302687, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:   9.70%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  2.304632/  2.302694, val:  10.00%, val_best:  10.00%, tr:   7.46%, tr_best:   9.70%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  2.304545/  2.302613, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:   9.70%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  2.305080/  2.302782, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:   9.70%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  2.304688/  2.302772, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  2.304834/  2.302622, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.01%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  2.304284/  2.302666, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.01%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  2.304813/  2.302634, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.01%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  2.305231/  2.302722, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.01%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  2.305301/  2.302723, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.01%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  2.304772/  2.302634, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.01%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  2.303716/  2.302659, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  2.304624/  2.302669, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.01%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  2.305880/  2.302768, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.01%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  2.305089/  2.302728, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.01%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  2.304662/  2.302646, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.01%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  2.305019/  2.302714, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  10.32%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  2.304242/  2.302678, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.32%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  2.303980/  2.302631, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  10.32%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  2.304494/  2.302676, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.32%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  2.304318/  2.302692, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.32%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  2.304585/  2.302743, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  10.32%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  2.304604/  2.302687, val:  10.00%, val_best:  10.00%, tr:   7.46%, tr_best:  10.32%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  2.304065/  2.302622, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  10.32%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  2.304437/  2.302634, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  10.32%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  2.304662/  2.302706, val:  10.00%, val_best:  10.00%, tr:   7.25%, tr_best:  10.32%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  2.305145/  2.302731, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  10.32%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  2.304735/  2.302727, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  10.32%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  2.304953/  2.302632, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.32%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  2.304517/  2.302745, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.32%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  2.304946/  2.302701, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.32%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  2.305197/  2.302687, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.32%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  2.304106/  2.302715, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.32%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  2.304535/  2.302683, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.32%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  2.304653/  2.302674, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.32%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  2.304348/  2.302677, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.32%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  2.304514/  2.302729, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.32%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  2.304611/  2.302673, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.32%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  2.304545/  2.302760, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.32%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  2.304274/  2.302822, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.32%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  2.304114/  2.302626, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.32%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  2.304720/  2.302717, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.32%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  2.304052/  2.302806, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  10.32%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  2.304631/  2.302734, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.32%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  2.304856/  2.302790, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.32%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  2.303980/  2.302723, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.32%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  2.305447/  2.302761, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  10.32%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  2.304740/  2.302716, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.32%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  2.304604/  2.302668, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.32%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  2.304554/  2.302632, val:  10.00%, val_best:  10.00%, tr:   7.35%, tr_best:  10.32%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  2.305324/  2.302709, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.32%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  2.304349/  2.302676, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  10.32%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  2.304797/  2.302742, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.32%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  2.305519/  2.302842, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.32%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  2.304214/  2.302650, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  10.32%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  2.304307/  2.302675, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.32%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  2.304764/  2.302663, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.32%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  2.304623/  2.302703, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.32%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  2.303885/  2.302848, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.32%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  2.304417/  2.302755, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.32%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  2.303841/  2.302698, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.32%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  2.305081/  2.302692, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.32%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  2.305073/  2.302701, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.32%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  2.304717/  2.302645, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.32%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  2.305171/  2.302620, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.32%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  2.305242/  2.302688, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.32%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  2.304678/  2.302624, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  10.32%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  2.304676/  2.302652, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.32%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  2.304883/  2.302660, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.32%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  2.305337/  2.302700, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.32%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  2.305019/  2.302656, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.32%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  2.305215/  2.302638, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.32%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  2.304676/  2.302660, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.32%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  2.304999/  2.302665, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  10.32%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  2.304617/  2.302732, val:  10.00%, val_best:  10.00%, tr:   7.35%, tr_best:  10.32%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  2.304430/  2.302707, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  10.32%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  2.304436/  2.302796, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.32%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  2.305023/  2.302669, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  10.32%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  2.304577/  2.302758, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.32%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  2.304359/  2.302626, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  10.32%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  2.305443/  2.302771, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.32%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  2.304418/  2.302737, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.32%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  2.305364/  2.302662, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.32%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  2.304665/  2.302694, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.32%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  2.305019/  2.302693, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.32%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  2.304266/  2.302669, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  10.32%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  2.304705/  2.302684, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.32%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  2.304860/  2.302634, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.32%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  2.304419/  2.302675, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.32%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  2.304308/  2.302676, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.32%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  2.304734/  2.302685, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.32%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  2.304735/  2.302724, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.32%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  2.305681/  2.302760, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.32%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  2.304706/  2.302780, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.32%\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  2.305158/  2.302699, val:  10.00%, val_best:  10.00%, tr:   6.54%, tr_best:  10.32%\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  2.304867/  2.302778, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.32%\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  2.304375/  2.302702, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.32%\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  2.304655/  2.302752, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.32%\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  2.304761/  2.302665, val:  10.00%, val_best:  10.00%, tr:   7.76%, tr_best:  10.32%\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  2.304966/  2.302768, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.32%\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  2.304906/  2.302717, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  10.32%\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  2.305127/  2.302662, val:  10.00%, val_best:  10.00%, tr:   6.95%, tr_best:  10.32%\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  2.304595/  2.302719, val:  10.00%, val_best:  10.00%, tr:   7.25%, tr_best:  10.32%\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  2.305093/  2.302662, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.32%\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  2.304393/  2.302672, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.32%\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  2.304302/  2.302687, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.32%\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  2.304455/  2.302690, val:  10.00%, val_best:  10.00%, tr:   7.25%, tr_best:  10.32%\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  2.304937/  2.302734, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.32%\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  2.304642/  2.302700, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.32%\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  2.304222/  2.302634, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.32%\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  2.304668/  2.302686, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.32%\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  2.304756/  2.302751, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.32%\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  2.305106/  2.302714, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  10.32%\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  2.304076/  2.302617, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  10.32%\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  2.304539/  2.302695, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.32%\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  2.304581/  2.302675, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  10.32%\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  2.304841/  2.302795, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  10.42%\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  2.304517/  2.302639, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.42%\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  2.304891/  2.302676, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.42%\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  2.304248/  2.302704, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.42%\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  2.304295/  2.302717, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.42%\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  2.305186/  2.302700, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.42%\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  2.304085/  2.302706, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.42%\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  2.304705/  2.302714, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.42%\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  2.304660/  2.302701, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  10.42%\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  2.304396/  2.302722, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.42%\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  2.305037/  2.302761, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.42%\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  2.304597/  2.302761, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.42%\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  2.304707/  2.302688, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.42%\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  2.304858/  2.302674, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.42%\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  2.304539/  2.302634, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.42%\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  2.304361/  2.302673, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.42%\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  2.304319/  2.302663, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.42%\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  2.304672/  2.302737, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.42%\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  2.304782/  2.302743, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.42%\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  2.304064/  2.302693, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.42%\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  2.304764/  2.302696, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.42%\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  2.304937/  2.302727, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  10.42%\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  2.305282/  2.302684, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.42%\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  2.305234/  2.302700, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.42%\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  2.304279/  2.302776, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.42%\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  2.304681/  2.302688, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.42%\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  2.304300/  2.302703, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  10.42%\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  2.305141/  2.302741, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.42%\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  2.304828/  2.302743, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.42%\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  2.305272/  2.302764, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.42%\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  2.304450/  2.302664, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.42%\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  2.304113/  2.302652, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  10.42%\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  2.304611/  2.302716, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.42%\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  2.304296/  2.302705, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.42%\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  2.305102/  2.302738, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  10.42%\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  2.304333/  2.302636, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.42%\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  2.304590/  2.302721, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.42%\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  2.304259/  2.302684, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.42%\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  2.305023/  2.302698, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.42%\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  2.305002/  2.302706, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.42%\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  2.304321/  2.302701, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.42%\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  2.304691/  2.302666, val:  10.00%, val_best:  10.00%, tr:   7.25%, tr_best:  10.42%\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  2.304740/  2.302628, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.42%\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  2.305027/  2.302656, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  10.42%\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  2.304668/  2.302692, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.42%\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  2.304651/  2.302659, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.42%\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  2.304525/  2.302747, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.42%\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  2.304557/  2.302690, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.42%\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  2.304708/  2.302716, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.42%\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  2.304638/  2.302738, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.42%\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  2.304281/  2.302666, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.42%\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  2.304698/  2.302671, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  10.42%\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  2.304534/  2.302709, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.42%\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  2.304440/  2.302680, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.42%\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  2.304618/  2.302688, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.42%\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  2.304554/  2.302678, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.42%\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  2.304758/  2.302659, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.42%\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  2.304272/  2.302652, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.42%\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  2.304761/  2.302732, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.42%\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  2.304675/  2.302645, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  10.42%\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  2.305034/  2.302695, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.42%\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  2.304607/  2.302672, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  10.42%\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  2.304089/  2.302781, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.42%\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  2.304208/  2.302747, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  10.42%\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  2.304517/  2.302700, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.42%\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  2.304846/  2.302669, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  10.42%\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  2.304576/  2.302695, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.42%\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  2.304630/  2.302760, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.42%\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  2.304305/  2.302675, val:  10.00%, val_best:  10.00%, tr:   7.76%, tr_best:  10.42%\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  2.304057/  2.302670, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.42%\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  2.304518/  2.302674, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.42%\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  2.304333/  2.302781, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.42%\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  2.305202/  2.302675, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.42%\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  2.305531/  2.302742, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.42%\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  2.304623/  2.302709, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.42%\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  2.304986/  2.302699, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.42%\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  2.304866/  2.302685, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  10.42%\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  2.304235/  2.302706, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.42%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d73d5ab9fb437db189a0dbf38a0995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▆▂▃▂▂▂▁▅▅█▁▅▁▃▂▂▃▅▃▂▂▅▃▁▂▃▂▅▂▃▅▁▅▂▂▁▁▁▂</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tr_acc</td><td>▃▄▆▅▅▄▇▅▆▄█▇▄▆▅▆▇▄▆█▁▄▇▆▅▅▅▄▇▅▆█▄▆▇▄▄▇▄▇</td></tr><tr><td>tr_epoch_loss</td><td>▅▃▅▆█▃▁▄▃▃▄▃▂▂▆▆▅▂▃▄▅▄▂▃▃▄▂▄▁▆▆▂▅▃▃▃▄▃▁▇</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂▃█▅▇▃▁▄▃▇█▃▂▇▄▄▃▁▄▄▄▅▃▄▄▃▅▃▄▄▇▄▄▄▅▄▆▄▃▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.09091</td></tr><tr><td>tr_epoch_loss</td><td>2.30423</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.30271</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">whole-sweep-7</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o11kh7ar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o11kh7ar</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_002100-o11kh7ar/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i123rt5v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_003259-i123rt5v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i123rt5v' target=\"_blank\">youthful-sweep-8</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i123rt5v' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i123rt5v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.303353/  2.303047, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.303309/  2.302968, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.303253/  2.302913, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.303180/  2.302890, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  2.303389/  2.302870, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  2.303007/  2.302809, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  2.302946/  2.302785, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  2.302988/  2.302803, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  2.302760/  2.302755, val:  10.42%, val_best:  10.42%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  2.302509/  2.302651, val:  10.00%, val_best:  10.42%, tr:  10.11%, tr_best:  10.11%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  2.302878/  2.302444, val:  11.25%, val_best:  11.25%, tr:   9.50%, tr_best:  10.11%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  2.302515/  2.302329, val:  10.83%, val_best:  11.25%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  2.301688/  2.301977, val:  10.42%, val_best:  11.25%, tr:  11.75%, tr_best:  11.75%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  2.300838/  2.301564, val:  10.42%, val_best:  11.25%, tr:  13.18%, tr_best:  13.18%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  2.299665/  2.300274, val:  10.00%, val_best:  11.25%, tr:  13.69%, tr_best:  13.69%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  2.297041/  2.298498, val:  10.83%, val_best:  11.25%, tr:  15.02%, tr_best:  15.02%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  2.293200/  2.295453, val:  10.83%, val_best:  11.25%, tr:  15.93%, tr_best:  15.93%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  2.287126/  2.291477, val:  13.75%, val_best:  13.75%, tr:  16.45%, tr_best:  16.45%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  2.276569/  2.285278, val:  15.42%, val_best:  15.42%, tr:  16.55%, tr_best:  16.55%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  2.264480/  2.275257, val:  15.00%, val_best:  15.42%, tr:  16.24%, tr_best:  16.55%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  2.249542/  2.264838, val:  16.25%, val_best:  16.25%, tr:  16.14%, tr_best:  16.55%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  2.234258/  2.254094, val:  16.25%, val_best:  16.25%, tr:  16.96%, tr_best:  16.96%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  2.214931/  2.241273, val:  17.08%, val_best:  17.08%, tr:  18.59%, tr_best:  18.59%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  2.203075/  2.230237, val:  18.75%, val_best:  18.75%, tr:  21.04%, tr_best:  21.04%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  2.187423/  2.213153, val:  21.67%, val_best:  21.67%, tr:  22.78%, tr_best:  22.78%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  2.168899/  2.194858, val:  27.08%, val_best:  27.08%, tr:  26.35%, tr_best:  26.35%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  2.137033/  2.171431, val:  36.67%, val_best:  36.67%, tr:  32.99%, tr_best:  32.99%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  2.107804/  2.140156, val:  38.33%, val_best:  38.33%, tr:  39.02%, tr_best:  39.02%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  2.068620/  2.101121, val:  36.25%, val_best:  38.33%, tr:  40.96%, tr_best:  40.96%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  2.013347/  2.055259, val:  39.17%, val_best:  39.17%, tr:  42.80%, tr_best:  42.80%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  1.967863/  2.012510, val:  42.92%, val_best:  42.92%, tr:  42.70%, tr_best:  42.80%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  1.911144/  1.962941, val:  43.75%, val_best:  43.75%, tr:  46.88%, tr_best:  46.88%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  1.857522/  1.914594, val:  45.83%, val_best:  45.83%, tr:  49.95%, tr_best:  49.95%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  1.810839/  1.874553, val:  45.83%, val_best:  45.83%, tr:  52.50%, tr_best:  52.50%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  1.769647/  1.840027, val:  46.67%, val_best:  46.67%, tr:  53.12%, tr_best:  53.12%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  1.718336/  1.803674, val:  50.42%, val_best:  50.42%, tr:  54.55%, tr_best:  54.55%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  1.684075/  1.777817, val:  51.25%, val_best:  51.25%, tr:  55.06%, tr_best:  55.06%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  1.650167/  1.753219, val:  52.08%, val_best:  52.08%, tr:  56.59%, tr_best:  56.59%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  1.617644/  1.730561, val:  54.17%, val_best:  54.17%, tr:  58.32%, tr_best:  58.32%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  1.593649/  1.705751, val:  57.50%, val_best:  57.50%, tr:  59.96%, tr_best:  59.96%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  1.562517/  1.691699, val:  55.83%, val_best:  57.50%, tr:  58.94%, tr_best:  59.96%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  1.541539/  1.678638, val:  57.92%, val_best:  57.92%, tr:  59.65%, tr_best:  59.96%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  1.514872/  1.666548, val:  57.50%, val_best:  57.92%, tr:  60.57%, tr_best:  60.57%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  1.498486/  1.648406, val:  57.08%, val_best:  57.92%, tr:  60.27%, tr_best:  60.57%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  1.483814/  1.639573, val:  55.83%, val_best:  57.92%, tr:  60.47%, tr_best:  60.57%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  1.462570/  1.626548, val:  62.08%, val_best:  62.08%, tr:  60.57%, tr_best:  60.57%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  1.444965/  1.619410, val:  59.58%, val_best:  62.08%, tr:  61.39%, tr_best:  61.39%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  1.431715/  1.603662, val:  58.75%, val_best:  62.08%, tr:  61.18%, tr_best:  61.39%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  1.418024/  1.593713, val:  60.00%, val_best:  62.08%, tr:  62.41%, tr_best:  62.41%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  1.409521/  1.585723, val:  60.00%, val_best:  62.08%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  1.394540/  1.585412, val:  60.00%, val_best:  62.08%, tr:  62.82%, tr_best:  62.82%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  1.375067/  1.577594, val:  59.17%, val_best:  62.08%, tr:  63.53%, tr_best:  63.53%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  1.359203/  1.574434, val:  61.25%, val_best:  62.08%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  1.351757/  1.567596, val:  62.50%, val_best:  62.50%, tr:  63.84%, tr_best:  64.04%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  1.346530/  1.568430, val:  58.33%, val_best:  62.50%, tr:  64.86%, tr_best:  64.86%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  1.333343/  1.561318, val:  57.92%, val_best:  62.50%, tr:  66.29%, tr_best:  66.29%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  1.315303/  1.554865, val:  56.67%, val_best:  62.50%, tr:  64.45%, tr_best:  66.29%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  1.313904/  1.550417, val:  59.58%, val_best:  62.50%, tr:  64.45%, tr_best:  66.29%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  1.293170/  1.551587, val:  60.00%, val_best:  62.50%, tr:  64.56%, tr_best:  66.29%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  1.295251/  1.547327, val:  60.42%, val_best:  62.50%, tr:  66.29%, tr_best:  66.29%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  1.281555/  1.545348, val:  60.83%, val_best:  62.50%, tr:  64.15%, tr_best:  66.29%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  1.278809/  1.546935, val:  60.42%, val_best:  62.50%, tr:  65.17%, tr_best:  66.29%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  1.265763/  1.547633, val:  58.33%, val_best:  62.50%, tr:  65.17%, tr_best:  66.29%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  1.248846/  1.534839, val:  58.33%, val_best:  62.50%, tr:  66.70%, tr_best:  66.70%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  1.242738/  1.538170, val:  61.67%, val_best:  62.50%, tr:  67.72%, tr_best:  67.72%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  1.239228/  1.532622, val:  57.08%, val_best:  62.50%, tr:  66.80%, tr_best:  67.72%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  1.235170/  1.530059, val:  61.25%, val_best:  62.50%, tr:  68.13%, tr_best:  68.13%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  1.232898/  1.524882, val:  61.25%, val_best:  62.50%, tr:  68.03%, tr_best:  68.13%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  1.217768/  1.530451, val:  57.92%, val_best:  62.50%, tr:  67.52%, tr_best:  68.13%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  1.220690/  1.523270, val:  61.25%, val_best:  62.50%, tr:  67.82%, tr_best:  68.13%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  1.208170/  1.517274, val:  58.75%, val_best:  62.50%, tr:  69.25%, tr_best:  69.25%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  1.204989/  1.518639, val:  60.83%, val_best:  62.50%, tr:  68.03%, tr_best:  69.25%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  1.188234/  1.519339, val:  60.00%, val_best:  62.50%, tr:  67.31%, tr_best:  69.25%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  1.191576/  1.518863, val:  61.67%, val_best:  62.50%, tr:  68.64%, tr_best:  69.25%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  1.180225/  1.519065, val:  59.58%, val_best:  62.50%, tr:  68.44%, tr_best:  69.25%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  1.184653/  1.512813, val:  59.58%, val_best:  62.50%, tr:  67.52%, tr_best:  69.25%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  1.169357/  1.517695, val:  60.00%, val_best:  62.50%, tr:  68.44%, tr_best:  69.25%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  1.170990/  1.512687, val:  58.33%, val_best:  62.50%, tr:  69.77%, tr_best:  69.77%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  1.159416/  1.507617, val:  60.83%, val_best:  62.50%, tr:  69.97%, tr_best:  69.97%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  1.160923/  1.517882, val:  60.00%, val_best:  62.50%, tr:  71.30%, tr_best:  71.30%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  1.147937/  1.509559, val:  59.17%, val_best:  62.50%, tr:  68.34%, tr_best:  71.30%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  1.145273/  1.510379, val:  59.17%, val_best:  62.50%, tr:  70.17%, tr_best:  71.30%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  1.147620/  1.503095, val:  60.00%, val_best:  62.50%, tr:  70.48%, tr_best:  71.30%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  1.139368/  1.503595, val:  59.17%, val_best:  62.50%, tr:  70.68%, tr_best:  71.30%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  1.143005/  1.504977, val:  58.33%, val_best:  62.50%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  1.137899/  1.508388, val:  57.08%, val_best:  62.50%, tr:  70.28%, tr_best:  71.40%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  1.128412/  1.503804, val:  60.00%, val_best:  62.50%, tr:  71.20%, tr_best:  71.40%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  1.132173/  1.506243, val:  62.50%, val_best:  62.50%, tr:  72.42%, tr_best:  72.42%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  1.122161/  1.509856, val:  57.08%, val_best:  62.50%, tr:  71.30%, tr_best:  72.42%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  1.112343/  1.523401, val:  57.92%, val_best:  62.50%, tr:  70.89%, tr_best:  72.42%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  1.114506/  1.510836, val:  59.58%, val_best:  62.50%, tr:  70.68%, tr_best:  72.42%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  1.099671/  1.503997, val:  58.75%, val_best:  62.50%, tr:  72.22%, tr_best:  72.42%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  1.106900/  1.510365, val:  56.67%, val_best:  62.50%, tr:  70.17%, tr_best:  72.42%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  1.104143/  1.506382, val:  61.25%, val_best:  62.50%, tr:  71.40%, tr_best:  72.42%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  1.100845/  1.509277, val:  59.58%, val_best:  62.50%, tr:  72.63%, tr_best:  72.63%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  1.095230/  1.512213, val:  58.33%, val_best:  62.50%, tr:  71.30%, tr_best:  72.63%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  1.085379/  1.520828, val:  59.58%, val_best:  62.50%, tr:  71.91%, tr_best:  72.63%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  1.091475/  1.502674, val:  60.42%, val_best:  62.50%, tr:  71.91%, tr_best:  72.63%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  1.087255/  1.522985, val:  60.83%, val_best:  62.50%, tr:  71.91%, tr_best:  72.63%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  1.088042/  1.534629, val:  58.33%, val_best:  62.50%, tr:  71.60%, tr_best:  72.63%\n",
      "epoch-100 lr=['0.0001000'], tr/val_loss:  1.072659/  1.524762, val:  60.83%, val_best:  62.50%, tr:  70.89%, tr_best:  72.63%\n",
      "epoch-101 lr=['0.0001000'], tr/val_loss:  1.065828/  1.524511, val:  60.00%, val_best:  62.50%, tr:  73.24%, tr_best:  73.24%\n",
      "epoch-102 lr=['0.0001000'], tr/val_loss:  1.070730/  1.521337, val:  61.25%, val_best:  62.50%, tr:  73.54%, tr_best:  73.54%\n",
      "epoch-103 lr=['0.0001000'], tr/val_loss:  1.063900/  1.538661, val:  62.08%, val_best:  62.50%, tr:  71.40%, tr_best:  73.54%\n",
      "epoch-104 lr=['0.0001000'], tr/val_loss:  1.059235/  1.527563, val:  60.42%, val_best:  62.50%, tr:  71.91%, tr_best:  73.54%\n",
      "epoch-105 lr=['0.0001000'], tr/val_loss:  1.072801/  1.530105, val:  63.33%, val_best:  63.33%, tr:  70.48%, tr_best:  73.54%\n",
      "epoch-106 lr=['0.0001000'], tr/val_loss:  1.057615/  1.532369, val:  59.17%, val_best:  63.33%, tr:  73.34%, tr_best:  73.54%\n",
      "epoch-107 lr=['0.0001000'], tr/val_loss:  1.053774/  1.546634, val:  58.75%, val_best:  63.33%, tr:  71.91%, tr_best:  73.54%\n",
      "epoch-108 lr=['0.0001000'], tr/val_loss:  1.053514/  1.542002, val:  61.67%, val_best:  63.33%, tr:  72.93%, tr_best:  73.54%\n",
      "epoch-109 lr=['0.0001000'], tr/val_loss:  1.045350/  1.551201, val:  59.58%, val_best:  63.33%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-110 lr=['0.0001000'], tr/val_loss:  1.037860/  1.549987, val:  60.83%, val_best:  63.33%, tr:  73.24%, tr_best:  74.26%\n",
      "epoch-111 lr=['0.0001000'], tr/val_loss:  1.043224/  1.534180, val:  60.42%, val_best:  63.33%, tr:  73.34%, tr_best:  74.26%\n",
      "epoch-112 lr=['0.0001000'], tr/val_loss:  1.032954/  1.548035, val:  58.33%, val_best:  63.33%, tr:  73.85%, tr_best:  74.26%\n",
      "epoch-113 lr=['0.0001000'], tr/val_loss:  1.042717/  1.564583, val:  58.33%, val_best:  63.33%, tr:  73.85%, tr_best:  74.26%\n",
      "epoch-114 lr=['0.0001000'], tr/val_loss:  1.042820/  1.558403, val:  61.67%, val_best:  63.33%, tr:  73.85%, tr_best:  74.26%\n",
      "epoch-115 lr=['0.0001000'], tr/val_loss:  1.030081/  1.569237, val:  60.00%, val_best:  63.33%, tr:  75.28%, tr_best:  75.28%\n",
      "epoch-116 lr=['0.0001000'], tr/val_loss:  1.022818/  1.567766, val:  58.33%, val_best:  63.33%, tr:  72.93%, tr_best:  75.28%\n",
      "epoch-117 lr=['0.0001000'], tr/val_loss:  1.028124/  1.565653, val:  60.42%, val_best:  63.33%, tr:  73.54%, tr_best:  75.28%\n",
      "epoch-118 lr=['0.0001000'], tr/val_loss:  1.015544/  1.588356, val:  58.75%, val_best:  63.33%, tr:  74.26%, tr_best:  75.28%\n",
      "epoch-119 lr=['0.0001000'], tr/val_loss:  1.015936/  1.576892, val:  58.33%, val_best:  63.33%, tr:  75.69%, tr_best:  75.69%\n",
      "epoch-120 lr=['0.0001000'], tr/val_loss:  1.010194/  1.583401, val:  59.58%, val_best:  63.33%, tr:  76.51%, tr_best:  76.51%\n",
      "epoch-121 lr=['0.0001000'], tr/val_loss:  1.014489/  1.580212, val:  61.67%, val_best:  63.33%, tr:  76.00%, tr_best:  76.51%\n",
      "epoch-122 lr=['0.0001000'], tr/val_loss:  1.027196/  1.567584, val:  60.00%, val_best:  63.33%, tr:  75.79%, tr_best:  76.51%\n",
      "epoch-123 lr=['0.0001000'], tr/val_loss:  1.016794/  1.587971, val:  60.42%, val_best:  63.33%, tr:  74.97%, tr_best:  76.51%\n",
      "epoch-124 lr=['0.0001000'], tr/val_loss:  1.003495/  1.589797, val:  59.17%, val_best:  63.33%, tr:  76.00%, tr_best:  76.51%\n",
      "epoch-125 lr=['0.0001000'], tr/val_loss:  1.005280/  1.571147, val:  59.58%, val_best:  63.33%, tr:  76.30%, tr_best:  76.51%\n",
      "epoch-126 lr=['0.0001000'], tr/val_loss:  1.015357/  1.577265, val:  63.33%, val_best:  63.33%, tr:  77.32%, tr_best:  77.32%\n",
      "epoch-127 lr=['0.0001000'], tr/val_loss:  1.003622/  1.599152, val:  58.33%, val_best:  63.33%, tr:  76.51%, tr_best:  77.32%\n",
      "epoch-128 lr=['0.0001000'], tr/val_loss:  0.996178/  1.566036, val:  63.33%, val_best:  63.33%, tr:  75.79%, tr_best:  77.32%\n",
      "epoch-129 lr=['0.0001000'], tr/val_loss:  0.993291/  1.580578, val:  60.83%, val_best:  63.33%, tr:  77.22%, tr_best:  77.32%\n",
      "epoch-130 lr=['0.0001000'], tr/val_loss:  1.005045/  1.574630, val:  63.33%, val_best:  63.33%, tr:  75.28%, tr_best:  77.32%\n",
      "epoch-131 lr=['0.0001000'], tr/val_loss:  0.981752/  1.602070, val:  61.25%, val_best:  63.33%, tr:  76.92%, tr_best:  77.32%\n",
      "epoch-132 lr=['0.0001000'], tr/val_loss:  0.982319/  1.598535, val:  62.50%, val_best:  63.33%, tr:  75.89%, tr_best:  77.32%\n",
      "epoch-133 lr=['0.0001000'], tr/val_loss:  0.978537/  1.600165, val:  62.08%, val_best:  63.33%, tr:  78.55%, tr_best:  78.55%\n",
      "epoch-134 lr=['0.0001000'], tr/val_loss:  0.974911/  1.600076, val:  64.17%, val_best:  64.17%, tr:  79.16%, tr_best:  79.16%\n",
      "epoch-135 lr=['0.0001000'], tr/val_loss:  0.965853/  1.589797, val:  65.00%, val_best:  65.00%, tr:  76.71%, tr_best:  79.16%\n",
      "epoch-136 lr=['0.0001000'], tr/val_loss:  0.976763/  1.609440, val:  60.83%, val_best:  65.00%, tr:  78.86%, tr_best:  79.16%\n",
      "epoch-137 lr=['0.0001000'], tr/val_loss:  0.976724/  1.603257, val:  61.25%, val_best:  65.00%, tr:  76.30%, tr_best:  79.16%\n",
      "epoch-138 lr=['0.0001000'], tr/val_loss:  0.971460/  1.602701, val:  62.92%, val_best:  65.00%, tr:  76.51%, tr_best:  79.16%\n",
      "epoch-139 lr=['0.0001000'], tr/val_loss:  0.975007/  1.595810, val:  64.17%, val_best:  65.00%, tr:  76.51%, tr_best:  79.16%\n",
      "epoch-140 lr=['0.0001000'], tr/val_loss:  0.959345/  1.600384, val:  60.00%, val_best:  65.00%, tr:  77.53%, tr_best:  79.16%\n",
      "epoch-141 lr=['0.0001000'], tr/val_loss:  0.955328/  1.603699, val:  63.75%, val_best:  65.00%, tr:  78.04%, tr_best:  79.16%\n",
      "epoch-142 lr=['0.0001000'], tr/val_loss:  0.953706/  1.611864, val:  63.75%, val_best:  65.00%, tr:  77.53%, tr_best:  79.16%\n",
      "epoch-143 lr=['0.0001000'], tr/val_loss:  0.958520/  1.590494, val:  65.42%, val_best:  65.42%, tr:  78.35%, tr_best:  79.16%\n",
      "epoch-144 lr=['0.0001000'], tr/val_loss:  0.953304/  1.617169, val:  62.92%, val_best:  65.42%, tr:  79.16%, tr_best:  79.16%\n",
      "epoch-145 lr=['0.0001000'], tr/val_loss:  0.950407/  1.620935, val:  65.00%, val_best:  65.42%, tr:  80.39%, tr_best:  80.39%\n",
      "epoch-146 lr=['0.0001000'], tr/val_loss:  0.942810/  1.604933, val:  66.25%, val_best:  66.25%, tr:  79.57%, tr_best:  80.39%\n",
      "epoch-147 lr=['0.0001000'], tr/val_loss:  0.950087/  1.622066, val:  63.33%, val_best:  66.25%, tr:  79.06%, tr_best:  80.39%\n",
      "epoch-148 lr=['0.0001000'], tr/val_loss:  0.933690/  1.625487, val:  65.00%, val_best:  66.25%, tr:  79.06%, tr_best:  80.39%\n",
      "epoch-149 lr=['0.0001000'], tr/val_loss:  0.928533/  1.627036, val:  66.67%, val_best:  66.67%, tr:  79.57%, tr_best:  80.39%\n",
      "epoch-150 lr=['0.0001000'], tr/val_loss:  0.947667/  1.647497, val:  62.50%, val_best:  66.67%, tr:  79.57%, tr_best:  80.39%\n",
      "epoch-151 lr=['0.0001000'], tr/val_loss:  0.922190/  1.641478, val:  62.92%, val_best:  66.67%, tr:  79.26%, tr_best:  80.39%\n",
      "epoch-152 lr=['0.0001000'], tr/val_loss:  0.925484/  1.655059, val:  62.92%, val_best:  66.67%, tr:  78.35%, tr_best:  80.39%\n",
      "epoch-153 lr=['0.0001000'], tr/val_loss:  0.924994/  1.666202, val:  60.42%, val_best:  66.67%, tr:  79.88%, tr_best:  80.39%\n",
      "epoch-154 lr=['0.0001000'], tr/val_loss:  0.923323/  1.638839, val:  62.92%, val_best:  66.67%, tr:  78.14%, tr_best:  80.39%\n",
      "epoch-155 lr=['0.0001000'], tr/val_loss:  0.920134/  1.638547, val:  63.75%, val_best:  66.67%, tr:  81.21%, tr_best:  81.21%\n",
      "epoch-156 lr=['0.0001000'], tr/val_loss:  0.923303/  1.641589, val:  65.42%, val_best:  66.67%, tr:  78.96%, tr_best:  81.21%\n",
      "epoch-157 lr=['0.0001000'], tr/val_loss:  0.904941/  1.644480, val:  63.75%, val_best:  66.67%, tr:  79.47%, tr_best:  81.21%\n",
      "epoch-158 lr=['0.0001000'], tr/val_loss:  0.920604/  1.636671, val:  62.92%, val_best:  66.67%, tr:  79.78%, tr_best:  81.21%\n",
      "epoch-159 lr=['0.0001000'], tr/val_loss:  0.903752/  1.629384, val:  62.50%, val_best:  66.67%, tr:  81.72%, tr_best:  81.72%\n",
      "epoch-160 lr=['0.0001000'], tr/val_loss:  0.909414/  1.634420, val:  64.17%, val_best:  66.67%, tr:  82.43%, tr_best:  82.43%\n",
      "epoch-161 lr=['0.0001000'], tr/val_loss:  0.898924/  1.651989, val:  62.50%, val_best:  66.67%, tr:  83.15%, tr_best:  83.15%\n",
      "epoch-162 lr=['0.0001000'], tr/val_loss:  0.915531/  1.644844, val:  63.75%, val_best:  66.67%, tr:  81.10%, tr_best:  83.15%\n",
      "epoch-163 lr=['0.0001000'], tr/val_loss:  0.906265/  1.630226, val:  67.08%, val_best:  67.08%, tr:  79.26%, tr_best:  83.15%\n",
      "epoch-164 lr=['0.0001000'], tr/val_loss:  0.901087/  1.658368, val:  65.00%, val_best:  67.08%, tr:  82.33%, tr_best:  83.15%\n",
      "epoch-165 lr=['0.0001000'], tr/val_loss:  0.914287/  1.664145, val:  66.25%, val_best:  67.08%, tr:  79.88%, tr_best:  83.15%\n",
      "epoch-166 lr=['0.0001000'], tr/val_loss:  0.899187/  1.672001, val:  62.50%, val_best:  67.08%, tr:  81.82%, tr_best:  83.15%\n",
      "epoch-167 lr=['0.0001000'], tr/val_loss:  0.900536/  1.652578, val:  64.17%, val_best:  67.08%, tr:  81.31%, tr_best:  83.15%\n",
      "epoch-168 lr=['0.0001000'], tr/val_loss:  0.893704/  1.635472, val:  63.75%, val_best:  67.08%, tr:  80.18%, tr_best:  83.15%\n",
      "epoch-169 lr=['0.0001000'], tr/val_loss:  0.883228/  1.655293, val:  63.33%, val_best:  67.08%, tr:  82.33%, tr_best:  83.15%\n",
      "epoch-170 lr=['0.0001000'], tr/val_loss:  0.881027/  1.672555, val:  62.50%, val_best:  67.08%, tr:  80.90%, tr_best:  83.15%\n",
      "epoch-171 lr=['0.0001000'], tr/val_loss:  0.889694/  1.636147, val:  67.92%, val_best:  67.92%, tr:  80.18%, tr_best:  83.15%\n",
      "epoch-172 lr=['0.0001000'], tr/val_loss:  0.870482/  1.664130, val:  66.67%, val_best:  67.92%, tr:  83.25%, tr_best:  83.25%\n",
      "epoch-173 lr=['0.0001000'], tr/val_loss:  0.876576/  1.679266, val:  62.50%, val_best:  67.92%, tr:  81.31%, tr_best:  83.25%\n",
      "epoch-174 lr=['0.0001000'], tr/val_loss:  0.875220/  1.675244, val:  63.75%, val_best:  67.92%, tr:  80.90%, tr_best:  83.25%\n",
      "epoch-175 lr=['0.0001000'], tr/val_loss:  0.874777/  1.668767, val:  64.17%, val_best:  67.92%, tr:  82.64%, tr_best:  83.25%\n",
      "epoch-176 lr=['0.0001000'], tr/val_loss:  0.866544/  1.671921, val:  66.25%, val_best:  67.92%, tr:  82.33%, tr_best:  83.25%\n",
      "epoch-177 lr=['0.0001000'], tr/val_loss:  0.883265/  1.671718, val:  69.17%, val_best:  69.17%, tr:  82.43%, tr_best:  83.25%\n",
      "epoch-178 lr=['0.0001000'], tr/val_loss:  0.871371/  1.697727, val:  66.67%, val_best:  69.17%, tr:  81.10%, tr_best:  83.25%\n",
      "epoch-179 lr=['0.0001000'], tr/val_loss:  0.876198/  1.700997, val:  63.75%, val_best:  69.17%, tr:  82.02%, tr_best:  83.25%\n",
      "epoch-180 lr=['0.0001000'], tr/val_loss:  0.860336/  1.702048, val:  65.00%, val_best:  69.17%, tr:  80.80%, tr_best:  83.25%\n",
      "epoch-181 lr=['0.0001000'], tr/val_loss:  0.866531/  1.723210, val:  64.58%, val_best:  69.17%, tr:  81.41%, tr_best:  83.25%\n",
      "epoch-182 lr=['0.0001000'], tr/val_loss:  0.859467/  1.710818, val:  64.58%, val_best:  69.17%, tr:  83.55%, tr_best:  83.55%\n",
      "epoch-183 lr=['0.0001000'], tr/val_loss:  0.867111/  1.725434, val:  67.92%, val_best:  69.17%, tr:  80.59%, tr_best:  83.55%\n",
      "epoch-184 lr=['0.0001000'], tr/val_loss:  0.863422/  1.722613, val:  65.42%, val_best:  69.17%, tr:  80.69%, tr_best:  83.55%\n",
      "epoch-185 lr=['0.0001000'], tr/val_loss:  0.862010/  1.752472, val:  62.08%, val_best:  69.17%, tr:  83.86%, tr_best:  83.86%\n",
      "epoch-186 lr=['0.0001000'], tr/val_loss:  0.859279/  1.750616, val:  64.58%, val_best:  69.17%, tr:  84.78%, tr_best:  84.78%\n",
      "epoch-187 lr=['0.0001000'], tr/val_loss:  0.859331/  1.750469, val:  65.42%, val_best:  69.17%, tr:  83.04%, tr_best:  84.78%\n",
      "epoch-188 lr=['0.0001000'], tr/val_loss:  0.847094/  1.743155, val:  66.25%, val_best:  69.17%, tr:  82.33%, tr_best:  84.78%\n",
      "epoch-189 lr=['0.0001000'], tr/val_loss:  0.844863/  1.764789, val:  66.25%, val_best:  69.17%, tr:  84.58%, tr_best:  84.78%\n",
      "epoch-190 lr=['0.0001000'], tr/val_loss:  0.844645/  1.771614, val:  64.58%, val_best:  69.17%, tr:  83.04%, tr_best:  84.78%\n",
      "epoch-191 lr=['0.0001000'], tr/val_loss:  0.856808/  1.766666, val:  64.58%, val_best:  69.17%, tr:  81.31%, tr_best:  84.78%\n",
      "epoch-192 lr=['0.0001000'], tr/val_loss:  0.839786/  1.764893, val:  64.58%, val_best:  69.17%, tr:  84.17%, tr_best:  84.78%\n",
      "epoch-193 lr=['0.0001000'], tr/val_loss:  0.831453/  1.758118, val:  64.58%, val_best:  69.17%, tr:  85.39%, tr_best:  85.39%\n",
      "epoch-194 lr=['0.0001000'], tr/val_loss:  0.830980/  1.780935, val:  66.25%, val_best:  69.17%, tr:  82.43%, tr_best:  85.39%\n",
      "epoch-195 lr=['0.0001000'], tr/val_loss:  0.829147/  1.771177, val:  67.50%, val_best:  69.17%, tr:  81.92%, tr_best:  85.39%\n",
      "epoch-196 lr=['0.0001000'], tr/val_loss:  0.834662/  1.758769, val:  68.33%, val_best:  69.17%, tr:  84.88%, tr_best:  85.39%\n",
      "epoch-197 lr=['0.0001000'], tr/val_loss:  0.827623/  1.770064, val:  65.83%, val_best:  69.17%, tr:  84.07%, tr_best:  85.39%\n",
      "epoch-198 lr=['0.0001000'], tr/val_loss:  0.829159/  1.745021, val:  68.33%, val_best:  69.17%, tr:  82.02%, tr_best:  85.39%\n",
      "epoch-199 lr=['0.0001000'], tr/val_loss:  0.818333/  1.765296, val:  66.67%, val_best:  69.17%, tr:  85.90%, tr_best:  85.90%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0eae4afda546868a625140a082cf7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▁▃▂▅▄▅▇▆▇▅▅▆▆▆▅▆▄▆▆▅▆▄▆▇▇███▆▆▇██▇█▆▆▇</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▂▃▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇██▇▇█████</td></tr><tr><td>tr_acc</td><td>▁▁▁▁▂▃▄▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█████████</td></tr><tr><td>tr_epoch_loss</td><td>█████▇▇▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▂▃▄▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▂▃▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇██▇▇█████</td></tr><tr><td>val_loss</td><td>█████▇▆▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.85904</td></tr><tr><td>tr_epoch_loss</td><td>0.81833</td></tr><tr><td>val_acc_best</td><td>0.69167</td></tr><tr><td>val_acc_now</td><td>0.66667</td></tr><tr><td>val_loss</td><td>1.7653</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">youthful-sweep-8</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i123rt5v' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i123rt5v</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_003259-i123rt5v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o8zmklpo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_004442-o8zmklpo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o8zmklpo' target=\"_blank\">snowy-sweep-9</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o8zmklpo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o8zmklpo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.443700/  3.759110, val:  42.08%, val_best:  42.08%, tr:  34.63%, tr_best:  34.63%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.838712/  3.833223, val:  41.67%, val_best:  42.08%, tr:  50.66%, tr_best:  50.66%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.928153/  4.244479, val:  49.17%, val_best:  49.17%, tr:  55.77%, tr_best:  55.77%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.578548/  4.071402, val:  50.00%, val_best:  50.00%, tr:  61.70%, tr_best:  61.70%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.371674/  2.039979, val:  64.17%, val_best:  64.17%, tr:  65.37%, tr_best:  65.37%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.541156/  2.887727, val:  50.83%, val_best:  64.17%, tr:  69.66%, tr_best:  69.66%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.201802/  3.127130, val:  58.33%, val_best:  64.17%, tr:  75.59%, tr_best:  75.59%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.103250/  3.223701, val:  51.67%, val_best:  64.17%, tr:  79.16%, tr_best:  79.16%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.977078/  2.407423, val:  62.92%, val_best:  64.17%, tr:  83.66%, tr_best:  83.66%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.133456/  3.689099, val:  56.67%, val_best:  64.17%, tr:  84.68%, tr_best:  84.68%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.767656/  2.703539, val:  69.17%, val_best:  69.17%, tr:  91.73%, tr_best:  91.73%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.693199/  3.270592, val:  67.50%, val_best:  69.17%, tr:  92.03%, tr_best:  92.03%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.589108/  2.817807, val:  75.83%, val_best:  75.83%, tr:  94.59%, tr_best:  94.59%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.431412/  3.395913, val:  72.08%, val_best:  75.83%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.341067/  2.975091, val:  76.25%, val_best:  76.25%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.235714/  3.237986, val:  72.08%, val_best:  76.25%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.206160/  3.368796, val:  74.17%, val_best:  76.25%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.174541/  3.495549, val:  77.08%, val_best:  77.08%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.150862/  3.740272, val:  70.83%, val_best:  77.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.086582/  3.686306, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.087283/  3.583341, val:  77.50%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.069407/  3.771095, val:  74.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.068511/  3.802570, val:  79.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.039260/  3.903610, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.029229/  3.938364, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.022671/  3.762516, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.020612/  3.891619, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.019045/  4.003141, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.014087/  3.937691, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.018327/  4.073914, val:  73.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.015318/  4.021255, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.015275/  4.044879, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.010926/  4.040794, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.013659/  4.117518, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.010137/  4.110371, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.011451/  4.100543, val:  76.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.008789/  4.166442, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.009368/  4.115799, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.008136/  4.155250, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.007997/  4.179325, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.006618/  4.197518, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.007090/  4.184069, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.005424/  4.206751, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.005950/  4.140443, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.005286/  4.195718, val:  76.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.005125/  4.148941, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.005615/  4.179548, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.005474/  4.120051, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.004661/  4.124496, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.005542/  4.168606, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.003538/  4.247077, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.003462/  4.116002, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.002638/  4.228700, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.003309/  4.158201, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.004369/  4.245676, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.004086/  4.253573, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.002503/  4.247866, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.002489/  4.302923, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.003075/  4.246509, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.002001/  4.326397, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.001703/  4.291141, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.001395/  4.306822, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.002099/  4.310888, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.002022/  4.339138, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.003365/  4.231590, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.002602/  4.268498, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.002415/  4.276540, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.001461/  4.311284, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.001337/  4.377041, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.002118/  4.389793, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.002613/  4.339881, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.002593/  4.314187, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.004591/  4.342736, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.002419/  4.309607, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.002105/  4.332304, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.001544/  4.342653, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.002097/  4.331601, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.001745/  4.341642, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.002251/  4.290915, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.001500/  4.415998, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.001064/  4.361730, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.000722/  4.395036, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.000693/  4.403596, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.000495/  4.344305, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.000556/  4.371794, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.000422/  4.366826, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.000390/  4.407089, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.000392/  4.378240, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.000326/  4.369543, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.000294/  4.395342, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.000432/  4.414379, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000507/  4.376746, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000561/  4.405581, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000341/  4.400969, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000524/  4.391744, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.001157/  4.372167, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000876/  4.396383, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000510/  4.437981, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000289/  4.427369, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000395/  4.434061, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.000880/  4.389751, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.000666/  4.382498, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.000372/  4.409364, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.000473/  4.415936, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.000437/  4.414485, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.000285/  4.390553, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.000313/  4.399789, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.000316/  4.412404, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.000272/  4.427488, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.000245/  4.413752, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.000250/  4.432815, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.000199/  4.449280, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.000176/  4.434643, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.000179/  4.430892, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.000198/  4.438448, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.000215/  4.454504, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.003130/  4.446832, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.001849/  4.366008, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.001486/  4.428790, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.001487/  4.484495, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.002060/  4.392318, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.001016/  4.441184, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.001358/  4.381744, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.001411/  4.405780, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.001422/  4.484207, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.002630/  4.518995, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.002014/  4.428933, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.002588/  4.517241, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.003325/  4.496436, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.000919/  4.515290, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.001381/  4.544101, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.000595/  4.571272, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.000601/  4.548548, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.000849/  4.537887, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.001152/  4.515630, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.001024/  4.484341, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.000702/  4.469509, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.000540/  4.474726, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.000649/  4.467994, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.000574/  4.493690, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.000246/  4.487622, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.000216/  4.506089, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.000204/  4.510682, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.000201/  4.510920, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.000192/  4.519468, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.000257/  4.558783, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.000502/  4.573846, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.000419/  4.537197, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.000285/  4.571623, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.000242/  4.575951, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.000164/  4.547139, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.000143/  4.555144, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.000351/  4.594379, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.000318/  4.573931, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.000265/  4.577233, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.000192/  4.565205, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.000165/  4.553851, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.000142/  4.555392, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.000125/  4.565016, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.000120/  4.566659, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.000140/  4.552726, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.000158/  4.550585, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.000372/  4.548777, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.000384/  4.532355, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.000233/  4.550170, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.000553/  4.570456, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.000200/  4.555583, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.000153/  4.566127, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.000153/  4.548765, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.000149/  4.569280, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.000143/  4.553015, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.000158/  4.554686, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.000147/  4.536703, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.000171/  4.559890, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.000141/  4.566969, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.000139/  4.591325, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.000141/  4.595468, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.000135/  4.579286, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.000132/  4.591628, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.000126/  4.584686, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.000248/  4.566124, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.000265/  4.572407, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.000795/  4.578161, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.001056/  4.580479, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.000578/  4.625041, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.000673/  4.651138, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.000755/  4.630586, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.000451/  4.619681, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.000263/  4.625463, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.000196/  4.626214, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.000344/  4.662753, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.000328/  4.624214, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.000709/  4.611001, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.000374/  4.621461, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.000305/  4.661033, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.000898/  4.609021, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.000333/  4.649953, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.001479/  4.660898, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.000597/  4.632522, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.000619/  4.665694, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f0cbc736254d04aeab601cbe5b8727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▆▇▇▇▇█▇███████████████▇▇█████████████</td></tr><tr><td>tr_acc</td><td>▁▄▆█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▅▇▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▆▇▇▇▇█▇███████████████▇▇█████████████</td></tr><tr><td>val_loss</td><td>▅▁▄▂▄▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇███████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00062</td></tr><tr><td>val_acc_best</td><td>0.80833</td></tr><tr><td>val_acc_now</td><td>0.8</td></tr><tr><td>val_loss</td><td>4.66569</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snowy-sweep-9</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o8zmklpo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o8zmklpo</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_004442-o8zmklpo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: su4xeuh7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_005753-su4xeuh7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/su4xeuh7' target=\"_blank\">daily-sweep-10</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/su4xeuh7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/su4xeuh7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.296865/  2.236556, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:   8.78%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.874541/  1.662358, val:  55.42%, val_best:  55.42%, tr:  37.49%, tr_best:  37.49%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  1.463419/  1.557704, val:  55.83%, val_best:  55.83%, tr:  57.81%, tr_best:  57.81%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  1.298406/  1.499862, val:  57.92%, val_best:  57.92%, tr:  64.66%, tr_best:  64.66%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  1.228890/  1.452645, val:  60.42%, val_best:  60.42%, tr:  64.86%, tr_best:  64.86%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  1.168455/  1.399279, val:  64.17%, val_best:  64.17%, tr:  66.09%, tr_best:  66.09%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  1.110704/  1.384134, val:  62.92%, val_best:  64.17%, tr:  70.89%, tr_best:  70.89%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  1.077227/  1.347596, val:  62.08%, val_best:  64.17%, tr:  69.46%, tr_best:  70.89%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  1.042533/  1.349630, val:  67.50%, val_best:  67.50%, tr:  70.48%, tr_best:  70.89%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  1.008625/  1.363574, val:  64.17%, val_best:  67.50%, tr:  74.46%, tr_best:  74.46%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.985024/  1.389233, val:  62.50%, val_best:  67.50%, tr:  73.54%, tr_best:  74.46%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.940821/  1.317487, val:  65.42%, val_best:  67.50%, tr:  77.12%, tr_best:  77.12%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.917289/  1.325646, val:  65.42%, val_best:  67.50%, tr:  80.59%, tr_best:  80.59%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.890666/  1.329599, val:  65.42%, val_best:  67.50%, tr:  81.41%, tr_best:  81.41%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.835860/  1.462698, val:  60.83%, val_best:  67.50%, tr:  83.15%, tr_best:  83.15%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.815778/  1.346977, val:  67.50%, val_best:  67.50%, tr:  86.72%, tr_best:  86.72%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.805901/  1.310035, val:  71.25%, val_best:  71.25%, tr:  83.86%, tr_best:  86.72%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.743655/  1.303868, val:  71.67%, val_best:  71.67%, tr:  90.30%, tr_best:  90.30%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.733230/  1.371329, val:  69.17%, val_best:  71.67%, tr:  89.68%, tr_best:  90.30%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.705532/  1.340017, val:  74.17%, val_best:  74.17%, tr:  88.15%, tr_best:  90.30%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.678280/  1.342089, val:  69.17%, val_best:  74.17%, tr:  92.34%, tr_best:  92.34%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.656517/  1.354603, val:  72.92%, val_best:  74.17%, tr:  92.75%, tr_best:  92.75%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.644810/  1.343390, val:  75.42%, val_best:  75.42%, tr:  93.67%, tr_best:  93.67%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.618588/  1.374325, val:  76.25%, val_best:  76.25%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.580944/  1.382521, val:  73.75%, val_best:  76.25%, tr:  95.20%, tr_best:  95.20%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.572506/  1.368191, val:  76.25%, val_best:  76.25%, tr:  94.99%, tr_best:  95.20%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.563482/  1.383180, val:  78.75%, val_best:  78.75%, tr:  94.48%, tr_best:  95.20%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.533099/  1.418693, val:  72.92%, val_best:  78.75%, tr:  95.81%, tr_best:  95.81%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.516726/  1.390352, val:  79.17%, val_best:  79.17%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.511037/  1.448192, val:  73.75%, val_best:  79.17%, tr:  96.83%, tr_best:  97.34%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.482567/  1.432166, val:  76.25%, val_best:  79.17%, tr:  97.24%, tr_best:  97.34%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.477196/  1.463871, val:  75.83%, val_best:  79.17%, tr:  96.12%, tr_best:  97.34%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.468192/  1.480660, val:  76.25%, val_best:  79.17%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.459061/  1.475697, val:  76.67%, val_best:  79.17%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.443432/  1.507985, val:  75.00%, val_best:  79.17%, tr:  97.65%, tr_best:  97.75%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.431417/  1.524130, val:  75.83%, val_best:  79.17%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.399434/  1.497355, val:  77.50%, val_best:  79.17%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.403779/  1.553091, val:  77.92%, val_best:  79.17%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.388915/  1.541225, val:  77.50%, val_best:  79.17%, tr:  98.16%, tr_best:  98.77%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.380584/  1.546930, val:  79.17%, val_best:  79.17%, tr:  98.37%, tr_best:  98.77%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.373297/  1.596007, val:  77.08%, val_best:  79.17%, tr:  98.47%, tr_best:  98.77%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.359962/  1.589684, val:  76.67%, val_best:  79.17%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.354921/  1.564190, val:  81.25%, val_best:  81.25%, tr:  98.88%, tr_best:  98.98%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.330931/  1.596582, val:  78.33%, val_best:  81.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.327936/  1.584628, val:  79.58%, val_best:  81.25%, tr:  99.08%, tr_best:  99.28%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.312913/  1.622727, val:  79.58%, val_best:  81.25%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.306681/  1.623375, val:  80.42%, val_best:  81.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.296039/  1.657290, val:  79.58%, val_best:  81.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.293386/  1.633437, val:  81.67%, val_best:  81.67%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.286338/  1.666612, val:  81.67%, val_best:  81.67%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.279020/  1.712053, val:  82.50%, val_best:  82.50%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.268550/  1.718600, val:  79.58%, val_best:  82.50%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.261994/  1.721570, val:  80.00%, val_best:  82.50%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.260163/  1.752943, val:  80.83%, val_best:  82.50%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.255940/  1.751336, val:  81.67%, val_best:  82.50%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.238653/  1.788869, val:  80.42%, val_best:  82.50%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.228817/  1.805831, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.237788/  1.800289, val:  80.42%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.220879/  1.827814, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.222246/  1.834605, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.212448/  1.829992, val:  81.67%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.213349/  1.851746, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.205461/  1.850290, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.200869/  1.871160, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.194090/  1.894805, val:  81.25%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.185558/  1.888241, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.185427/  1.910474, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.182613/  1.940005, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.180389/  1.928059, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.168311/  1.959444, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.166046/  1.990721, val:  81.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.171124/  1.978571, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.159309/  2.023595, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.159395/  2.009304, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.155222/  2.022102, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.151204/  2.046901, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.146269/  2.054862, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.146598/  2.070654, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.142158/  2.091094, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.144248/  2.086977, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.137779/  2.097517, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.134391/  2.095987, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.132981/  2.125561, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.132305/  2.169937, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.128851/  2.140849, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.134262/  2.195688, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.126616/  2.170353, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.122086/  2.202959, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.116344/  2.198207, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.116480/  2.199645, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.112048/  2.202270, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.108442/  2.226799, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.107860/  2.257619, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.106012/  2.221844, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.104685/  2.247047, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.100731/  2.260942, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.096040/  2.310864, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.099364/  2.294726, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.095172/  2.303420, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.100411/  2.311126, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.095248/  2.330555, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.088376/  2.337430, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.085901/  2.369524, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.087179/  2.365901, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.087886/  2.379114, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.087393/  2.356181, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.084311/  2.368539, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.083350/  2.379118, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.078122/  2.375844, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.076883/  2.400954, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.073929/  2.428525, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.073749/  2.423546, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.074915/  2.441298, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.076393/  2.467613, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.076438/  2.470143, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.073125/  2.481918, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.072852/  2.476099, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.071364/  2.474314, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.069787/  2.484619, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.071805/  2.484172, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.072666/  2.491806, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.070250/  2.483953, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.071779/  2.501446, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.072743/  2.513190, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.070519/  2.517901, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.067373/  2.527569, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.066690/  2.518035, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.066543/  2.537164, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.064449/  2.565725, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.069276/  2.553889, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.067289/  2.539090, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.062510/  2.549746, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.062337/  2.586080, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.062235/  2.578480, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.060065/  2.592280, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.058605/  2.574369, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.057517/  2.588029, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.060770/  2.573296, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.060116/  2.600696, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.058593/  2.584622, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.054546/  2.650334, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.055787/  2.634109, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.054183/  2.637276, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.053049/  2.636490, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.052802/  2.649223, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.050812/  2.633592, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.050591/  2.674180, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.053402/  2.666422, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.050199/  2.682640, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.050316/  2.684133, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.051253/  2.722394, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.050055/  2.691343, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.048329/  2.684232, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.051994/  2.694045, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.049700/  2.712435, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.047799/  2.719870, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.051055/  2.712225, val:  79.58%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.047974/  2.736902, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.047632/  2.741711, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.048486/  2.762819, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.044334/  2.776310, val:  79.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.044185/  2.758244, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.046750/  2.782269, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.043880/  2.806945, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.042329/  2.811834, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.043151/  2.825865, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.042425/  2.827057, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.042285/  2.855903, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.039198/  2.835584, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.040257/  2.839642, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.038438/  2.836627, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.038840/  2.847799, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.037339/  2.859803, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.036567/  2.860952, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.037478/  2.854371, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.036922/  2.887357, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.038873/  2.877236, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.038228/  2.913859, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.040224/  2.901771, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.038687/  2.896022, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.038578/  2.913531, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.034605/  2.911462, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.034327/  2.913151, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.034426/  2.887630, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.035667/  2.903422, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.038412/  2.906843, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.035284/  2.913274, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.036597/  2.916744, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.036009/  2.938342, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.033551/  2.929520, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.034657/  2.926442, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.031651/  2.950989, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.031381/  2.962396, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.032490/  2.942449, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.031750/  2.980622, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.032008/  2.982308, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.030834/  2.991208, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.029773/  2.985962, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.032169/  3.013621, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.032136/  3.012643, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9c501c8307464a8ce05e0ddca4bbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄▆▅█▇█▇███████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▃▄▆▆▆▇▆▇████▇▇▇▇████▇▇▇▇██▇▇▇█▇███████</td></tr><tr><td>tr_acc</td><td>▁▄▅▇▇▇██████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▅▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▆▆▇▇▇▇██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▃▄▆▆▆▇▆▇████▇▇▇▇████▇▇▇▇██▇▇▇█▇███████</td></tr><tr><td>val_loss</td><td>▂▁▁▁▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03214</td></tr><tr><td>val_acc_best</td><td>0.83333</td></tr><tr><td>val_acc_now</td><td>0.8125</td></tr><tr><td>val_loss</td><td>3.01264</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">daily-sweep-10</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/su4xeuh7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/su4xeuh7</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_005753-su4xeuh7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i2hlmkqd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6704bf4d1f2a4e5a990eb925b30c3abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114056832674477, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_011118-i2hlmkqd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i2hlmkqd' target=\"_blank\">cosmic-sweep-11</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i2hlmkqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i2hlmkqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.294283/  2.278136, val:  20.42%, val_best:  20.42%, tr:  14.30%, tr_best:  14.30%\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.250021/  2.231585, val:  27.92%, val_best:  27.92%, tr:  22.47%, tr_best:  22.47%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.172328/  2.152942, val:  30.42%, val_best:  30.42%, tr:  28.91%, tr_best:  28.91%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.057534/  2.039476, val:  34.17%, val_best:  34.17%, tr:  34.73%, tr_best:  34.73%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  1.912960/  1.918901, val:  40.83%, val_best:  40.83%, tr:  42.59%, tr_best:  42.59%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  1.763015/  1.802168, val:  53.33%, val_best:  53.33%, tr:  49.13%, tr_best:  49.13%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  1.650923/  1.713262, val:  55.00%, val_best:  55.00%, tr:  52.81%, tr_best:  52.81%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  1.543937/  1.650108, val:  55.42%, val_best:  55.42%, tr:  57.00%, tr_best:  57.00%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  1.469849/  1.593996, val:  57.08%, val_best:  57.08%, tr:  58.94%, tr_best:  58.94%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  1.405006/  1.553342, val:  55.42%, val_best:  57.08%, tr:  60.27%, tr_best:  60.27%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  1.350285/  1.514239, val:  59.58%, val_best:  59.58%, tr:  63.02%, tr_best:  63.02%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  1.305724/  1.479523, val:  59.58%, val_best:  59.58%, tr:  63.33%, tr_best:  63.33%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  1.274132/  1.456217, val:  65.00%, val_best:  65.00%, tr:  63.94%, tr_best:  63.94%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  1.233605/  1.431832, val:  66.67%, val_best:  66.67%, tr:  66.80%, tr_best:  66.80%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  1.191705/  1.417802, val:  62.08%, val_best:  66.67%, tr:  66.19%, tr_best:  66.80%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  1.162215/  1.407095, val:  59.17%, val_best:  66.67%, tr:  66.80%, tr_best:  66.80%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  1.126788/  1.382362, val:  60.42%, val_best:  66.67%, tr:  67.11%, tr_best:  67.11%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  1.105483/  1.368863, val:  61.67%, val_best:  66.67%, tr:  68.64%, tr_best:  68.64%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  1.082800/  1.361298, val:  59.17%, val_best:  66.67%, tr:  70.79%, tr_best:  70.79%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  1.052518/  1.353919, val:  60.00%, val_best:  66.67%, tr:  68.03%, tr_best:  70.79%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  1.032591/  1.348650, val:  57.92%, val_best:  66.67%, tr:  70.89%, tr_best:  70.89%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  1.012867/  1.339646, val:  63.75%, val_best:  66.67%, tr:  72.52%, tr_best:  72.52%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  0.995120/  1.328390, val:  61.25%, val_best:  66.67%, tr:  69.97%, tr_best:  72.52%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  0.977611/  1.321506, val:  60.42%, val_best:  66.67%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  0.957855/  1.310052, val:  62.08%, val_best:  66.67%, tr:  74.16%, tr_best:  74.16%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  0.941055/  1.312379, val:  60.83%, val_best:  66.67%, tr:  75.49%, tr_best:  75.49%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  0.924863/  1.300268, val:  60.42%, val_best:  66.67%, tr:  73.03%, tr_best:  75.49%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  0.906146/  1.305149, val:  60.83%, val_best:  66.67%, tr:  76.51%, tr_best:  76.51%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  0.907552/  1.288968, val:  67.50%, val_best:  67.50%, tr:  77.12%, tr_best:  77.12%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  0.884942/  1.282568, val:  65.00%, val_best:  67.50%, tr:  78.04%, tr_best:  78.04%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  0.874839/  1.283223, val:  65.00%, val_best:  67.50%, tr:  77.53%, tr_best:  78.04%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  0.861032/  1.288480, val:  65.83%, val_best:  67.50%, tr:  76.92%, tr_best:  78.04%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  0.847517/  1.297099, val:  62.08%, val_best:  67.50%, tr:  78.24%, tr_best:  78.24%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  0.842659/  1.300708, val:  60.83%, val_best:  67.50%, tr:  77.43%, tr_best:  78.24%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  0.829635/  1.263542, val:  67.08%, val_best:  67.50%, tr:  79.37%, tr_best:  79.37%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  0.811530/  1.272797, val:  63.75%, val_best:  67.50%, tr:  79.67%, tr_best:  79.67%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  0.801168/  1.273617, val:  61.25%, val_best:  67.50%, tr:  79.16%, tr_best:  79.67%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  0.789573/  1.265015, val:  68.33%, val_best:  68.33%, tr:  80.49%, tr_best:  80.49%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  0.787347/  1.252924, val:  65.00%, val_best:  68.33%, tr:  82.02%, tr_best:  82.02%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  0.773684/  1.249518, val:  65.42%, val_best:  68.33%, tr:  82.43%, tr_best:  82.43%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  0.756737/  1.252157, val:  68.75%, val_best:  68.75%, tr:  81.82%, tr_best:  82.43%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  0.758832/  1.253620, val:  65.83%, val_best:  68.75%, tr:  84.88%, tr_best:  84.88%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  0.743184/  1.242407, val:  67.50%, val_best:  68.75%, tr:  83.96%, tr_best:  84.88%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  0.728895/  1.264938, val:  67.08%, val_best:  68.75%, tr:  84.78%, tr_best:  84.88%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  0.722531/  1.243768, val:  68.33%, val_best:  68.75%, tr:  83.55%, tr_best:  84.88%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  0.707790/  1.244256, val:  68.33%, val_best:  68.75%, tr:  84.98%, tr_best:  84.98%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  0.704199/  1.262428, val:  66.25%, val_best:  68.75%, tr:  86.21%, tr_best:  86.21%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  0.701393/  1.252601, val:  67.50%, val_best:  68.75%, tr:  83.96%, tr_best:  86.21%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  0.690568/  1.242092, val:  69.58%, val_best:  69.58%, tr:  88.25%, tr_best:  88.25%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  0.678558/  1.237885, val:  68.33%, val_best:  69.58%, tr:  86.52%, tr_best:  88.25%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  0.669266/  1.243241, val:  69.17%, val_best:  69.58%, tr:  89.07%, tr_best:  89.07%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  0.655499/  1.249120, val:  69.58%, val_best:  69.58%, tr:  86.62%, tr_best:  89.07%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  0.649137/  1.251965, val:  66.67%, val_best:  69.58%, tr:  87.44%, tr_best:  89.07%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  0.638551/  1.231333, val:  65.42%, val_best:  69.58%, tr:  90.50%, tr_best:  90.50%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  0.631106/  1.252097, val:  67.08%, val_best:  69.58%, tr:  88.97%, tr_best:  90.50%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  0.621633/  1.242382, val:  68.75%, val_best:  69.58%, tr:  90.19%, tr_best:  90.50%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  0.608187/  1.251624, val:  69.58%, val_best:  69.58%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  0.605099/  1.229726, val:  69.17%, val_best:  69.58%, tr:  89.17%, tr_best:  91.62%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  0.595069/  1.254437, val:  69.17%, val_best:  69.58%, tr:  90.81%, tr_best:  91.62%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  0.593494/  1.243595, val:  70.00%, val_best:  70.00%, tr:  89.99%, tr_best:  91.62%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  0.585590/  1.254282, val:  70.00%, val_best:  70.00%, tr:  89.17%, tr_best:  91.62%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  0.576232/  1.264403, val:  69.58%, val_best:  70.00%, tr:  91.32%, tr_best:  91.62%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  0.575880/  1.259787, val:  72.08%, val_best:  72.08%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  0.563055/  1.254392, val:  70.42%, val_best:  72.08%, tr:  91.93%, tr_best:  91.93%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  0.553826/  1.277241, val:  69.58%, val_best:  72.08%, tr:  92.54%, tr_best:  92.54%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  0.549108/  1.266422, val:  70.42%, val_best:  72.08%, tr:  92.34%, tr_best:  92.54%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  0.542033/  1.260035, val:  74.17%, val_best:  74.17%, tr:  92.75%, tr_best:  92.75%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  0.541812/  1.262225, val:  70.83%, val_best:  74.17%, tr:  93.56%, tr_best:  93.56%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  0.528509/  1.287283, val:  69.58%, val_best:  74.17%, tr:  91.52%, tr_best:  93.56%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  0.519658/  1.310338, val:  71.25%, val_best:  74.17%, tr:  94.69%, tr_best:  94.69%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  0.511516/  1.310168, val:  71.67%, val_best:  74.17%, tr:  94.38%, tr_best:  94.69%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  0.505022/  1.323645, val:  71.25%, val_best:  74.17%, tr:  94.59%, tr_best:  94.69%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  0.494651/  1.333416, val:  69.58%, val_best:  74.17%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  0.496176/  1.341860, val:  68.75%, val_best:  74.17%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  0.485966/  1.353797, val:  70.42%, val_best:  74.17%, tr:  94.48%, tr_best:  95.10%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  0.479524/  1.370026, val:  72.08%, val_best:  74.17%, tr:  93.97%, tr_best:  95.10%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  0.470632/  1.358053, val:  71.67%, val_best:  74.17%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  0.475531/  1.375938, val:  70.42%, val_best:  74.17%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  0.468039/  1.367650, val:  72.92%, val_best:  74.17%, tr:  95.20%, tr_best:  95.91%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  0.450935/  1.366381, val:  71.67%, val_best:  74.17%, tr:  95.20%, tr_best:  95.91%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  0.441983/  1.409509, val:  70.83%, val_best:  74.17%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  0.453896/  1.391891, val:  71.25%, val_best:  74.17%, tr:  95.40%, tr_best:  96.02%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  0.433544/  1.422947, val:  72.08%, val_best:  74.17%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  0.434678/  1.424720, val:  72.08%, val_best:  74.17%, tr:  96.63%, tr_best:  96.83%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  0.426296/  1.411699, val:  71.67%, val_best:  74.17%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  0.426144/  1.433097, val:  72.92%, val_best:  74.17%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  0.420469/  1.453077, val:  72.92%, val_best:  74.17%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  0.415379/  1.446127, val:  72.92%, val_best:  74.17%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  0.399526/  1.461169, val:  72.08%, val_best:  74.17%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  0.394347/  1.477656, val:  72.92%, val_best:  74.17%, tr:  97.96%, tr_best:  98.77%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  0.399608/  1.457147, val:  74.58%, val_best:  74.58%, tr:  98.26%, tr_best:  98.77%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  0.385592/  1.476402, val:  72.50%, val_best:  74.58%, tr:  98.16%, tr_best:  98.77%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  0.386646/  1.500330, val:  71.25%, val_best:  74.58%, tr:  97.96%, tr_best:  98.77%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  0.379940/  1.480611, val:  71.67%, val_best:  74.58%, tr:  98.26%, tr_best:  98.77%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  0.375973/  1.483957, val:  75.00%, val_best:  75.00%, tr:  98.47%, tr_best:  98.77%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  0.368690/  1.486494, val:  74.58%, val_best:  75.00%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  0.359113/  1.542402, val:  70.42%, val_best:  75.00%, tr:  98.88%, tr_best:  99.08%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  0.359347/  1.511612, val:  72.08%, val_best:  75.00%, tr:  98.06%, tr_best:  99.08%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  0.353898/  1.534720, val:  71.25%, val_best:  75.00%, tr:  98.98%, tr_best:  99.08%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  0.356792/  1.505097, val:  76.25%, val_best:  76.25%, tr:  98.37%, tr_best:  99.08%\n",
      "epoch-100 lr=['0.0001000'], tr/val_loss:  0.344684/  1.545586, val:  71.25%, val_best:  76.25%, tr:  98.57%, tr_best:  99.08%\n",
      "epoch-101 lr=['0.0001000'], tr/val_loss:  0.324184/  1.533448, val:  75.00%, val_best:  76.25%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-102 lr=['0.0001000'], tr/val_loss:  0.326772/  1.531223, val:  72.08%, val_best:  76.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-103 lr=['0.0001000'], tr/val_loss:  0.332233/  1.546085, val:  74.17%, val_best:  76.25%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-104 lr=['0.0001000'], tr/val_loss:  0.319833/  1.554396, val:  73.33%, val_best:  76.25%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-105 lr=['0.0001000'], tr/val_loss:  0.325764/  1.564504, val:  73.33%, val_best:  76.25%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-106 lr=['0.0001000'], tr/val_loss:  0.314976/  1.549187, val:  74.58%, val_best:  76.25%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-107 lr=['0.0001000'], tr/val_loss:  0.310564/  1.568508, val:  73.75%, val_best:  76.25%, tr:  98.77%, tr_best:  99.49%\n",
      "epoch-108 lr=['0.0001000'], tr/val_loss:  0.305801/  1.573705, val:  74.17%, val_best:  76.25%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-109 lr=['0.0001000'], tr/val_loss:  0.300266/  1.586251, val:  73.33%, val_best:  76.25%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-110 lr=['0.0001000'], tr/val_loss:  0.297747/  1.596505, val:  74.17%, val_best:  76.25%, tr:  98.88%, tr_best:  99.49%\n",
      "epoch-111 lr=['0.0001000'], tr/val_loss:  0.288130/  1.594389, val:  75.83%, val_best:  76.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-112 lr=['0.0001000'], tr/val_loss:  0.281024/  1.628000, val:  72.92%, val_best:  76.25%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-113 lr=['0.0001000'], tr/val_loss:  0.281840/  1.670775, val:  73.33%, val_best:  76.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-114 lr=['0.0001000'], tr/val_loss:  0.280645/  1.648507, val:  74.17%, val_best:  76.25%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-115 lr=['0.0001000'], tr/val_loss:  0.269784/  1.656764, val:  72.92%, val_best:  76.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-116 lr=['0.0001000'], tr/val_loss:  0.264200/  1.643998, val:  74.17%, val_best:  76.25%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-117 lr=['0.0001000'], tr/val_loss:  0.262128/  1.664029, val:  73.33%, val_best:  76.25%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-118 lr=['0.0001000'], tr/val_loss:  0.256888/  1.684496, val:  74.17%, val_best:  76.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-119 lr=['0.0001000'], tr/val_loss:  0.260238/  1.691316, val:  72.50%, val_best:  76.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-120 lr=['0.0001000'], tr/val_loss:  0.249837/  1.700655, val:  72.08%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0001000'], tr/val_loss:  0.246449/  1.715634, val:  73.33%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0001000'], tr/val_loss:  0.249267/  1.728603, val:  70.83%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0001000'], tr/val_loss:  0.242555/  1.725293, val:  74.17%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0001000'], tr/val_loss:  0.235417/  1.731629, val:  73.75%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0001000'], tr/val_loss:  0.231802/  1.746894, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0001000'], tr/val_loss:  0.235272/  1.796244, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0001000'], tr/val_loss:  0.229277/  1.779590, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0001000'], tr/val_loss:  0.228001/  1.790088, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0001000'], tr/val_loss:  0.223920/  1.803499, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0001000'], tr/val_loss:  0.223753/  1.792064, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0001000'], tr/val_loss:  0.216045/  1.820298, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0001000'], tr/val_loss:  0.211011/  1.815480, val:  74.17%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0001000'], tr/val_loss:  0.210670/  1.831303, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0001000'], tr/val_loss:  0.208219/  1.856035, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0001000'], tr/val_loss:  0.208819/  1.834591, val:  75.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0001000'], tr/val_loss:  0.203391/  1.836507, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0001000'], tr/val_loss:  0.206724/  1.858338, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0001000'], tr/val_loss:  0.201538/  1.865296, val:  75.83%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0001000'], tr/val_loss:  0.200667/  1.868693, val:  73.33%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0001000'], tr/val_loss:  0.192181/  1.882887, val:  74.17%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0001000'], tr/val_loss:  0.186131/  1.871239, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0001000'], tr/val_loss:  0.181608/  1.919216, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0001000'], tr/val_loss:  0.189238/  1.915694, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0001000'], tr/val_loss:  0.184050/  1.913245, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0001000'], tr/val_loss:  0.178217/  1.923120, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0001000'], tr/val_loss:  0.177306/  1.950352, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0001000'], tr/val_loss:  0.176720/  1.936793, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0001000'], tr/val_loss:  0.182079/  1.987964, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0001000'], tr/val_loss:  0.167134/  1.936216, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0001000'], tr/val_loss:  0.174375/  1.955906, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0001000'], tr/val_loss:  0.166391/  1.967440, val:  73.75%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0001000'], tr/val_loss:  0.161550/  1.955105, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0001000'], tr/val_loss:  0.164327/  1.988930, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0001000'], tr/val_loss:  0.159995/  2.003918, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0001000'], tr/val_loss:  0.157113/  2.002445, val:  72.50%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0001000'], tr/val_loss:  0.158201/  2.013625, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0001000'], tr/val_loss:  0.154640/  2.000132, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0001000'], tr/val_loss:  0.153042/  2.022795, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0001000'], tr/val_loss:  0.150204/  2.000446, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0001000'], tr/val_loss:  0.144074/  2.053850, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0001000'], tr/val_loss:  0.147998/  2.042785, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0001000'], tr/val_loss:  0.147634/  2.091706, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0001000'], tr/val_loss:  0.141920/  2.076642, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0001000'], tr/val_loss:  0.138944/  2.078293, val:  75.00%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0001000'], tr/val_loss:  0.140882/  2.108008, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0001000'], tr/val_loss:  0.142756/  2.111353, val:  73.75%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0001000'], tr/val_loss:  0.137086/  2.110464, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0001000'], tr/val_loss:  0.142426/  2.123882, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0001000'], tr/val_loss:  0.139428/  2.143229, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0001000'], tr/val_loss:  0.131827/  2.102161, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0001000'], tr/val_loss:  0.138753/  2.150049, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0001000'], tr/val_loss:  0.129423/  2.148576, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0001000'], tr/val_loss:  0.126456/  2.167031, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0001000'], tr/val_loss:  0.127965/  2.171956, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0001000'], tr/val_loss:  0.121967/  2.155337, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0001000'], tr/val_loss:  0.124189/  2.178039, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0001000'], tr/val_loss:  0.127920/  2.168228, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0001000'], tr/val_loss:  0.134932/  2.173318, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0001000'], tr/val_loss:  0.115613/  2.209573, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0001000'], tr/val_loss:  0.119006/  2.188723, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0001000'], tr/val_loss:  0.118691/  2.206523, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0001000'], tr/val_loss:  0.114938/  2.208511, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0001000'], tr/val_loss:  0.113178/  2.199783, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0001000'], tr/val_loss:  0.114315/  2.192607, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0001000'], tr/val_loss:  0.118516/  2.234833, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0001000'], tr/val_loss:  0.108354/  2.249589, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0001000'], tr/val_loss:  0.110687/  2.254552, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0001000'], tr/val_loss:  0.115531/  2.242303, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0001000'], tr/val_loss:  0.110759/  2.233461, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0001000'], tr/val_loss:  0.106793/  2.240613, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0001000'], tr/val_loss:  0.106025/  2.238864, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0001000'], tr/val_loss:  0.110850/  2.252732, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0001000'], tr/val_loss:  0.103375/  2.274615, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0001000'], tr/val_loss:  0.098478/  2.284441, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0001000'], tr/val_loss:  0.097975/  2.300166, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0001000'], tr/val_loss:  0.095462/  2.287582, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0001000'], tr/val_loss:  0.093102/  2.268101, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0001000'], tr/val_loss:  0.100673/  2.291358, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0001000'], tr/val_loss:  0.094968/  2.278901, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd411306a2fd41e8895384b67d202b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄▅▄▇▇▅▇█▅▅▇▇▇█▇▇██████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇██▇▇███▇██████▇████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▅▆▆▆▆▇▇▇▇▇█▇████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▅▇▇▇▇▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇██▇▇███▇██████▇████████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.09497</td></tr><tr><td>val_acc_best</td><td>0.77083</td></tr><tr><td>val_acc_now</td><td>0.76667</td></tr><tr><td>val_loss</td><td>2.2789</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cosmic-sweep-11</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i2hlmkqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i2hlmkqd</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_011118-i2hlmkqd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tgzexf3q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_012308-tgzexf3q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tgzexf3q' target=\"_blank\">young-sweep-12</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tgzexf3q' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tgzexf3q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  3.078041/  4.595693, val:  40.83%, val_best:  40.83%, tr:  33.61%, tr_best:  33.61%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  3.582163/  5.354991, val:  41.67%, val_best:  41.67%, tr:  49.64%, tr_best:  49.64%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  3.932248/  6.662419, val:  47.92%, val_best:  47.92%, tr:  54.24%, tr_best:  54.24%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  3.130392/  5.341043, val:  45.42%, val_best:  47.92%, tr:  62.51%, tr_best:  62.51%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.828908/  4.270485, val:  54.58%, val_best:  54.58%, tr:  64.86%, tr_best:  64.86%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  3.129392/  6.500168, val:  45.42%, val_best:  54.58%, tr:  68.44%, tr_best:  68.44%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.456627/  4.492008, val:  53.75%, val_best:  54.58%, tr:  79.98%, tr_best:  79.98%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.332773/  3.818031, val:  62.08%, val_best:  62.08%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.094579/  3.722522, val:  63.75%, val_best:  63.75%, tr:  85.50%, tr_best:  85.50%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.396978/  4.882776, val:  57.08%, val_best:  63.75%, tr:  85.70%, tr_best:  85.70%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.704528/  4.181053, val:  64.58%, val_best:  64.58%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.285076/  4.722267, val:  65.00%, val_best:  65.00%, tr:  90.09%, tr_best:  93.36%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.575544/  3.979649, val:  72.50%, val_best:  72.50%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.349389/  4.724475, val:  71.25%, val_best:  72.50%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.232711/  4.890580, val:  69.17%, val_best:  72.50%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.135041/  4.190631, val:  75.00%, val_best:  75.00%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.190062/  4.538607, val:  75.00%, val_best:  75.00%, tr:  98.98%, tr_best:  99.69%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.099124/  4.499447, val:  75.00%, val_best:  75.00%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.084957/  4.811436, val:  72.50%, val_best:  75.00%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.099325/  4.505254, val:  72.92%, val_best:  75.00%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.045477/  4.511773, val:  72.08%, val_best:  75.00%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.039204/  4.658678, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.024398/  4.627961, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.034467/  4.752848, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.015278/  4.810307, val:  74.58%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.008793/  4.914933, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.006298/  4.830704, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.005362/  4.964178, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.008106/  4.991485, val:  71.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.003505/  4.872732, val:  75.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.003032/  4.970393, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.001328/  4.964255, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.001052/  5.010026, val:  74.58%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.001094/  4.957547, val:  74.58%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.001095/  4.979417, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.000789/  5.002647, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.001559/  4.968293, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.001666/  5.053623, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.001343/  5.057839, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.000793/  5.092213, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.000535/  5.102648, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.000816/  5.044898, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.000532/  5.043043, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.000574/  4.999577, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.000876/  4.971161, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.001130/  4.956433, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.000496/  4.957989, val:  74.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.000462/  4.983023, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.000393/  4.976095, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.000376/  4.981300, val:  74.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.000350/  5.005304, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.000459/  4.986225, val:  74.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.000429/  4.961061, val:  74.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.000340/  4.943995, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.000300/  4.985874, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.000235/  4.976566, val:  75.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.000190/  4.983138, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.000185/  4.998833, val:  75.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.000192/  5.008559, val:  75.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.000192/  4.999708, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.000175/  4.991749, val:  75.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.000173/  5.003803, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.000426/  5.013259, val:  75.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.000378/  5.022130, val:  74.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.000463/  5.011432, val:  75.42%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.000302/  5.038320, val:  74.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.000490/  5.102368, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.000259/  5.064632, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.000218/  5.064078, val:  75.42%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.000206/  5.065996, val:  75.42%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.000167/  5.054855, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.000186/  5.067789, val:  75.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.000274/  5.062084, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.000163/  5.069648, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.000164/  5.078016, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.000155/  5.065637, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.000150/  5.074703, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.000143/  5.075257, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.000145/  5.051988, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.000139/  5.053549, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.000203/  5.073609, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.000216/  5.070008, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.000158/  5.076672, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.000168/  5.071533, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.000149/  5.069477, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.000191/  5.064929, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.000146/  5.057406, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.000139/  5.045019, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.000152/  5.044548, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.000122/  5.043256, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.000121/  5.046277, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000131/  5.048915, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000350/  5.051538, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000932/  5.043615, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000283/  5.086624, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000283/  5.045742, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000235/  5.058569, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000242/  5.068705, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000202/  5.038306, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000138/  5.036434, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.000134/  5.027815, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.000249/  5.044356, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.000170/  5.013786, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.000175/  5.019782, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.000137/  5.025761, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.000156/  5.036607, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.000121/  5.051033, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.000135/  5.045610, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.000127/  5.051723, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.000127/  5.048400, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.000118/  5.049937, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.000161/  5.074468, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.000163/  5.069777, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.000103/  5.076575, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.000150/  5.104386, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.000143/  5.092135, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.000151/  5.090153, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.000137/  5.042805, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.000136/  5.070271, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.000164/  5.068363, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.000117/  5.072593, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.000103/  5.079875, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.000121/  5.077677, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.000120/  5.086574, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.000120/  5.089784, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.000110/  5.095767, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.000111/  5.095444, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.000100/  5.099601, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.000115/  5.096415, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.000103/  5.092541, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.000096/  5.098899, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.000082/  5.086182, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.000086/  5.091930, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.000092/  5.108915, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.000176/  5.117381, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.000088/  5.105836, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.000117/  5.105817, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.000084/  5.102911, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.000080/  5.094093, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.000087/  5.091116, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.000079/  5.077229, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.000429/  5.117050, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.000272/  5.074373, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.000164/  5.109402, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.000109/  5.103681, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.000557/  5.086380, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.000096/  5.071181, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.000089/  5.077360, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.000087/  5.078171, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.000083/  5.070437, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.000079/  5.070395, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.000085/  5.070779, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.000085/  5.064705, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.000074/  5.076847, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.000083/  5.087447, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.000078/  5.096276, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.000079/  5.090893, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.000071/  5.089830, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.000078/  5.079840, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.000073/  5.072957, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.000096/  5.078020, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.000109/  5.080953, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.000085/  5.078717, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.000094/  5.083361, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.000104/  5.073705, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.000092/  5.069912, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.000086/  5.063755, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.000079/  5.070738, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.000092/  5.070122, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.000094/  5.074045, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.000085/  5.093291, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.000087/  5.098177, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.000091/  5.101612, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.000071/  5.111710, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.000086/  5.124899, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.000069/  5.117280, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.000085/  5.114259, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.000113/  5.121661, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.000099/  5.107321, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.000071/  5.087797, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.000074/  5.089568, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.000070/  5.075946, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.000079/  5.072485, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.000071/  5.075113, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.000074/  5.077389, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.000072/  5.081837, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.000076/  5.092192, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.000074/  5.085702, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.000075/  5.084771, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.000091/  5.084927, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.000258/  5.097662, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.000121/  5.095875, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.000085/  5.089183, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.000075/  5.105255, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.000077/  5.109978, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.000087/  5.098897, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.000091/  5.114183, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.000075/  5.109300, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.000069/  5.114182, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.000072/  5.113349, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ca95e7dc9a44029eb05e3fe38ed1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▄█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▄█▇███▇█▇▇████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▆█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▅█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▄█▇███▇█▇▇████████████████████████████</td></tr><tr><td>val_loss</td><td>▅█▃▁▂▃▃▃▄▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>7e-05</td></tr><tr><td>val_acc_best</td><td>0.77083</td></tr><tr><td>val_acc_now</td><td>0.75833</td></tr><tr><td>val_loss</td><td>5.11335</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">young-sweep-12</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tgzexf3q' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tgzexf3q</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_012308-tgzexf3q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o0570zav with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_013632-o0570zav</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o0570zav' target=\"_blank\">glamorous-sweep-13</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o0570zav' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o0570zav</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.296237/  2.276956, val:  20.42%, val_best:  20.42%, tr:  14.20%, tr_best:  14.20%\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.256141/  2.240716, val:  25.00%, val_best:  25.00%, tr:  23.60%, tr_best:  23.60%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.211246/  2.197888, val:  27.50%, val_best:  27.50%, tr:  28.19%, tr_best:  28.19%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.150449/  2.137187, val:  31.25%, val_best:  31.25%, tr:  32.38%, tr_best:  32.38%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  2.075456/  2.067997, val:  32.50%, val_best:  32.50%, tr:  35.34%, tr_best:  35.34%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  1.985543/  1.986416, val:  41.67%, val_best:  41.67%, tr:  41.68%, tr_best:  41.68%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  1.896733/  1.902687, val:  48.33%, val_best:  48.33%, tr:  47.50%, tr_best:  47.50%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  1.795009/  1.825896, val:  47.08%, val_best:  48.33%, tr:  52.20%, tr_best:  52.20%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  1.713530/  1.755731, val:  51.25%, val_best:  51.25%, tr:  54.75%, tr_best:  54.75%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  1.635979/  1.697541, val:  49.58%, val_best:  51.25%, tr:  54.95%, tr_best:  54.95%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  1.576602/  1.649126, val:  50.42%, val_best:  51.25%, tr:  57.41%, tr_best:  57.41%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  1.523986/  1.607644, val:  51.25%, val_best:  51.25%, tr:  58.32%, tr_best:  58.32%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  1.480505/  1.569276, val:  55.42%, val_best:  55.42%, tr:  60.27%, tr_best:  60.27%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  1.442345/  1.536660, val:  53.75%, val_best:  55.42%, tr:  61.80%, tr_best:  61.80%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  1.403050/  1.510211, val:  55.00%, val_best:  55.42%, tr:  61.59%, tr_best:  61.80%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  1.365176/  1.486763, val:  57.08%, val_best:  57.08%, tr:  62.41%, tr_best:  62.41%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  1.335704/  1.461843, val:  57.08%, val_best:  57.08%, tr:  62.51%, tr_best:  62.51%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  1.315090/  1.440239, val:  62.08%, val_best:  62.08%, tr:  65.78%, tr_best:  65.78%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  1.285806/  1.426497, val:  58.75%, val_best:  62.08%, tr:  64.15%, tr_best:  65.78%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  1.261214/  1.417154, val:  59.58%, val_best:  62.08%, tr:  64.15%, tr_best:  65.78%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  1.236499/  1.400781, val:  59.17%, val_best:  62.08%, tr:  64.45%, tr_best:  65.78%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  1.220245/  1.390003, val:  62.08%, val_best:  62.08%, tr:  66.60%, tr_best:  66.60%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  1.199935/  1.375063, val:  63.33%, val_best:  63.33%, tr:  65.07%, tr_best:  66.60%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  1.178748/  1.364379, val:  63.75%, val_best:  63.75%, tr:  67.52%, tr_best:  67.52%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  1.164248/  1.356623, val:  65.42%, val_best:  65.42%, tr:  68.13%, tr_best:  68.13%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  1.146839/  1.346968, val:  65.00%, val_best:  65.42%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  1.134107/  1.331161, val:  67.50%, val_best:  67.50%, tr:  68.34%, tr_best:  69.36%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  1.117829/  1.318979, val:  67.50%, val_best:  67.50%, tr:  70.58%, tr_best:  70.58%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  1.114895/  1.317476, val:  67.50%, val_best:  67.50%, tr:  68.44%, tr_best:  70.58%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  1.088375/  1.314530, val:  67.92%, val_best:  67.92%, tr:  69.66%, tr_best:  70.58%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  1.080805/  1.312318, val:  66.25%, val_best:  67.92%, tr:  70.99%, tr_best:  70.99%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  1.061738/  1.304375, val:  70.42%, val_best:  70.42%, tr:  69.66%, tr_best:  70.99%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  1.052786/  1.301050, val:  66.67%, val_best:  70.42%, tr:  70.89%, tr_best:  70.99%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  1.051723/  1.290796, val:  67.08%, val_best:  70.42%, tr:  71.30%, tr_best:  71.30%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  1.031903/  1.288062, val:  69.17%, val_best:  70.42%, tr:  72.01%, tr_best:  72.01%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  1.018153/  1.278000, val:  71.67%, val_best:  71.67%, tr:  73.34%, tr_best:  73.34%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  1.011324/  1.279128, val:  67.08%, val_best:  71.67%, tr:  72.93%, tr_best:  73.34%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  1.003722/  1.274775, val:  72.08%, val_best:  72.08%, tr:  72.73%, tr_best:  73.34%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  0.996701/  1.273019, val:  68.75%, val_best:  72.08%, tr:  72.63%, tr_best:  73.34%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  0.988977/  1.268471, val:  72.92%, val_best:  72.92%, tr:  74.67%, tr_best:  74.67%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  0.969566/  1.266646, val:  70.00%, val_best:  72.92%, tr:  73.44%, tr_best:  74.67%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  0.977939/  1.253464, val:  70.42%, val_best:  72.92%, tr:  75.28%, tr_best:  75.28%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  0.953912/  1.249381, val:  72.08%, val_best:  72.92%, tr:  77.02%, tr_best:  77.02%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  0.948039/  1.246401, val:  67.08%, val_best:  72.92%, tr:  76.81%, tr_best:  77.02%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  0.939590/  1.243747, val:  66.25%, val_best:  72.92%, tr:  75.69%, tr_best:  77.02%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  0.932991/  1.235165, val:  71.25%, val_best:  72.92%, tr:  74.97%, tr_best:  77.02%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  0.921949/  1.234610, val:  70.00%, val_best:  72.92%, tr:  76.81%, tr_best:  77.02%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  0.925189/  1.236155, val:  68.75%, val_best:  72.92%, tr:  74.57%, tr_best:  77.02%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  0.913221/  1.232003, val:  71.25%, val_best:  72.92%, tr:  77.12%, tr_best:  77.12%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  0.905344/  1.226900, val:  68.33%, val_best:  72.92%, tr:  76.00%, tr_best:  77.12%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  0.901373/  1.223835, val:  68.33%, val_best:  72.92%, tr:  77.32%, tr_best:  77.32%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  0.892340/  1.222181, val:  68.75%, val_best:  72.92%, tr:  77.73%, tr_best:  77.73%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  0.884859/  1.217515, val:  67.50%, val_best:  72.92%, tr:  76.51%, tr_best:  77.73%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  0.876587/  1.218513, val:  69.17%, val_best:  72.92%, tr:  79.78%, tr_best:  79.78%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  0.871061/  1.227200, val:  65.00%, val_best:  72.92%, tr:  79.78%, tr_best:  79.78%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  0.865994/  1.219256, val:  68.33%, val_best:  72.92%, tr:  79.88%, tr_best:  79.88%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  0.849900/  1.216973, val:  66.25%, val_best:  72.92%, tr:  81.61%, tr_best:  81.61%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  0.847596/  1.220241, val:  69.58%, val_best:  72.92%, tr:  80.18%, tr_best:  81.61%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  0.839512/  1.214408, val:  69.17%, val_best:  72.92%, tr:  78.55%, tr_best:  81.61%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  0.838407/  1.211132, val:  69.17%, val_best:  72.92%, tr:  81.51%, tr_best:  81.61%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  0.832431/  1.215039, val:  66.25%, val_best:  72.92%, tr:  78.65%, tr_best:  81.61%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  0.831468/  1.210750, val:  67.08%, val_best:  72.92%, tr:  81.10%, tr_best:  81.61%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  0.823514/  1.207944, val:  67.50%, val_best:  72.92%, tr:  81.82%, tr_best:  81.82%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  0.812744/  1.206914, val:  68.33%, val_best:  72.92%, tr:  82.74%, tr_best:  82.74%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  0.803110/  1.210454, val:  65.42%, val_best:  72.92%, tr:  83.35%, tr_best:  83.35%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  0.805393/  1.206814, val:  67.92%, val_best:  72.92%, tr:  82.74%, tr_best:  83.35%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  0.798887/  1.195765, val:  70.00%, val_best:  72.92%, tr:  82.94%, tr_best:  83.35%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  0.800148/  1.199396, val:  67.08%, val_best:  72.92%, tr:  84.37%, tr_best:  84.37%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  0.782797/  1.199071, val:  68.33%, val_best:  72.92%, tr:  82.84%, tr_best:  84.37%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  0.779254/  1.194436, val:  69.17%, val_best:  72.92%, tr:  84.37%, tr_best:  84.37%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  0.774635/  1.196621, val:  67.08%, val_best:  72.92%, tr:  83.96%, tr_best:  84.37%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  0.770102/  1.197184, val:  69.17%, val_best:  72.92%, tr:  84.17%, tr_best:  84.37%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  0.759729/  1.196795, val:  69.17%, val_best:  72.92%, tr:  83.55%, tr_best:  84.37%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  0.758349/  1.188432, val:  66.25%, val_best:  72.92%, tr:  85.60%, tr_best:  85.60%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  0.748175/  1.191733, val:  68.75%, val_best:  72.92%, tr:  85.09%, tr_best:  85.60%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  0.747345/  1.187442, val:  67.08%, val_best:  72.92%, tr:  85.29%, tr_best:  85.60%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  0.735381/  1.193082, val:  69.58%, val_best:  72.92%, tr:  84.27%, tr_best:  85.60%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  0.740601/  1.197638, val:  69.17%, val_best:  72.92%, tr:  86.62%, tr_best:  86.62%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  0.732107/  1.189203, val:  68.75%, val_best:  72.92%, tr:  87.03%, tr_best:  87.03%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  0.729840/  1.188064, val:  67.08%, val_best:  72.92%, tr:  86.31%, tr_best:  87.03%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  0.720687/  1.189372, val:  70.00%, val_best:  72.92%, tr:  86.62%, tr_best:  87.03%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  0.720842/  1.188912, val:  70.83%, val_best:  72.92%, tr:  87.54%, tr_best:  87.54%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  0.709457/  1.186439, val:  71.25%, val_best:  72.92%, tr:  87.95%, tr_best:  87.95%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  0.708579/  1.183959, val:  70.00%, val_best:  72.92%, tr:  88.25%, tr_best:  88.25%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  0.705255/  1.190975, val:  69.58%, val_best:  72.92%, tr:  89.07%, tr_best:  89.07%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  0.703053/  1.187304, val:  71.25%, val_best:  72.92%, tr:  88.05%, tr_best:  89.07%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  0.690614/  1.190613, val:  70.83%, val_best:  72.92%, tr:  87.64%, tr_best:  89.07%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  0.688491/  1.184887, val:  71.25%, val_best:  72.92%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  0.682669/  1.194525, val:  67.50%, val_best:  72.92%, tr:  88.56%, tr_best:  89.99%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  0.674598/  1.195341, val:  69.17%, val_best:  72.92%, tr:  87.84%, tr_best:  89.99%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  0.678811/  1.193582, val:  70.00%, val_best:  72.92%, tr:  88.15%, tr_best:  89.99%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  0.663705/  1.191776, val:  72.50%, val_best:  72.92%, tr:  89.27%, tr_best:  89.99%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  0.664736/  1.193843, val:  70.42%, val_best:  72.92%, tr:  88.97%, tr_best:  89.99%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  0.660909/  1.189798, val:  71.25%, val_best:  72.92%, tr:  90.60%, tr_best:  90.60%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  0.658193/  1.191118, val:  71.25%, val_best:  72.92%, tr:  90.19%, tr_best:  90.60%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  0.645321/  1.199736, val:  72.92%, val_best:  72.92%, tr:  90.81%, tr_best:  90.81%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  0.643196/  1.200610, val:  70.42%, val_best:  72.92%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  0.636831/  1.205059, val:  70.00%, val_best:  72.92%, tr:  89.89%, tr_best:  91.83%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  0.636045/  1.199260, val:  68.33%, val_best:  72.92%, tr:  89.27%, tr_best:  91.83%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  0.632608/  1.206164, val:  71.25%, val_best:  72.92%, tr:  89.99%, tr_best:  91.83%\n",
      "epoch-100 lr=['0.0001000'], tr/val_loss:  0.623228/  1.200642, val:  70.83%, val_best:  72.92%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-101 lr=['0.0001000'], tr/val_loss:  0.616234/  1.204220, val:  71.25%, val_best:  72.92%, tr:  91.62%, tr_best:  91.83%\n",
      "epoch-102 lr=['0.0001000'], tr/val_loss:  0.618348/  1.204129, val:  70.83%, val_best:  72.92%, tr:  91.52%, tr_best:  91.83%\n",
      "epoch-103 lr=['0.0001000'], tr/val_loss:  0.611264/  1.206869, val:  70.00%, val_best:  72.92%, tr:  91.73%, tr_best:  91.83%\n",
      "epoch-104 lr=['0.0001000'], tr/val_loss:  0.603790/  1.202033, val:  70.00%, val_best:  72.92%, tr:  92.13%, tr_best:  92.13%\n",
      "epoch-105 lr=['0.0001000'], tr/val_loss:  0.609969/  1.207233, val:  70.83%, val_best:  72.92%, tr:  92.03%, tr_best:  92.13%\n",
      "epoch-106 lr=['0.0001000'], tr/val_loss:  0.601736/  1.195456, val:  70.00%, val_best:  72.92%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-107 lr=['0.0001000'], tr/val_loss:  0.595082/  1.207232, val:  69.58%, val_best:  72.92%, tr:  92.13%, tr_best:  92.85%\n",
      "epoch-108 lr=['0.0001000'], tr/val_loss:  0.589159/  1.209879, val:  70.42%, val_best:  72.92%, tr:  93.16%, tr_best:  93.16%\n",
      "epoch-109 lr=['0.0001000'], tr/val_loss:  0.584601/  1.206390, val:  71.25%, val_best:  72.92%, tr:  92.75%, tr_best:  93.16%\n",
      "epoch-110 lr=['0.0001000'], tr/val_loss:  0.579492/  1.208543, val:  70.42%, val_best:  72.92%, tr:  93.16%, tr_best:  93.16%\n",
      "epoch-111 lr=['0.0001000'], tr/val_loss:  0.572698/  1.214277, val:  70.00%, val_best:  72.92%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-112 lr=['0.0001000'], tr/val_loss:  0.569299/  1.209541, val:  69.17%, val_best:  72.92%, tr:  93.77%, tr_best:  94.18%\n",
      "epoch-113 lr=['0.0001000'], tr/val_loss:  0.571166/  1.211491, val:  70.00%, val_best:  72.92%, tr:  92.95%, tr_best:  94.18%\n",
      "epoch-114 lr=['0.0001000'], tr/val_loss:  0.570362/  1.218363, val:  71.67%, val_best:  72.92%, tr:  94.08%, tr_best:  94.18%\n",
      "epoch-115 lr=['0.0001000'], tr/val_loss:  0.559032/  1.216336, val:  70.83%, val_best:  72.92%, tr:  94.79%, tr_best:  94.79%\n",
      "epoch-116 lr=['0.0001000'], tr/val_loss:  0.555281/  1.222121, val:  68.75%, val_best:  72.92%, tr:  93.16%, tr_best:  94.79%\n",
      "epoch-117 lr=['0.0001000'], tr/val_loss:  0.551820/  1.214885, val:  70.83%, val_best:  72.92%, tr:  93.97%, tr_best:  94.79%\n",
      "epoch-118 lr=['0.0001000'], tr/val_loss:  0.550062/  1.219759, val:  69.17%, val_best:  72.92%, tr:  93.87%, tr_best:  94.79%\n",
      "epoch-119 lr=['0.0001000'], tr/val_loss:  0.542842/  1.220197, val:  70.83%, val_best:  72.92%, tr:  94.38%, tr_best:  94.79%\n",
      "epoch-120 lr=['0.0001000'], tr/val_loss:  0.536290/  1.217553, val:  71.25%, val_best:  72.92%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-121 lr=['0.0001000'], tr/val_loss:  0.536644/  1.222408, val:  72.08%, val_best:  72.92%, tr:  93.97%, tr_best:  95.10%\n",
      "epoch-122 lr=['0.0001000'], tr/val_loss:  0.541611/  1.221385, val:  73.75%, val_best:  73.75%, tr:  94.48%, tr_best:  95.10%\n",
      "epoch-123 lr=['0.0001000'], tr/val_loss:  0.532512/  1.227804, val:  71.25%, val_best:  73.75%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-124 lr=['0.0001000'], tr/val_loss:  0.519728/  1.227383, val:  71.67%, val_best:  73.75%, tr:  94.99%, tr_best:  95.10%\n",
      "epoch-125 lr=['0.0001000'], tr/val_loss:  0.518584/  1.221560, val:  72.92%, val_best:  73.75%, tr:  94.89%, tr_best:  95.10%\n",
      "epoch-126 lr=['0.0001000'], tr/val_loss:  0.525255/  1.232293, val:  70.42%, val_best:  73.75%, tr:  95.81%, tr_best:  95.81%\n",
      "epoch-127 lr=['0.0001000'], tr/val_loss:  0.512828/  1.233635, val:  70.00%, val_best:  73.75%, tr:  95.10%, tr_best:  95.81%\n",
      "epoch-128 lr=['0.0001000'], tr/val_loss:  0.510465/  1.232289, val:  71.25%, val_best:  73.75%, tr:  95.71%, tr_best:  95.81%\n",
      "epoch-129 lr=['0.0001000'], tr/val_loss:  0.504448/  1.235430, val:  72.08%, val_best:  73.75%, tr:  95.20%, tr_best:  95.81%\n",
      "epoch-130 lr=['0.0001000'], tr/val_loss:  0.506264/  1.234441, val:  70.00%, val_best:  73.75%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-131 lr=['0.0001000'], tr/val_loss:  0.496390/  1.248968, val:  71.67%, val_best:  73.75%, tr:  95.30%, tr_best:  96.02%\n",
      "epoch-132 lr=['0.0001000'], tr/val_loss:  0.493651/  1.255376, val:  68.33%, val_best:  73.75%, tr:  94.89%, tr_best:  96.02%\n",
      "epoch-133 lr=['0.0001000'], tr/val_loss:  0.490746/  1.243306, val:  70.00%, val_best:  73.75%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-134 lr=['0.0001000'], tr/val_loss:  0.484325/  1.241254, val:  73.75%, val_best:  73.75%, tr:  95.91%, tr_best:  96.02%\n",
      "epoch-135 lr=['0.0001000'], tr/val_loss:  0.481016/  1.247862, val:  72.08%, val_best:  73.75%, tr:  96.63%, tr_best:  96.63%\n",
      "epoch-136 lr=['0.0001000'], tr/val_loss:  0.483687/  1.259220, val:  70.00%, val_best:  73.75%, tr:  95.61%, tr_best:  96.63%\n",
      "epoch-137 lr=['0.0001000'], tr/val_loss:  0.487667/  1.258560, val:  72.08%, val_best:  73.75%, tr:  96.22%, tr_best:  96.63%\n",
      "epoch-138 lr=['0.0001000'], tr/val_loss:  0.469223/  1.267908, val:  70.42%, val_best:  73.75%, tr:  96.63%, tr_best:  96.63%\n",
      "epoch-139 lr=['0.0001000'], tr/val_loss:  0.465746/  1.264667, val:  72.50%, val_best:  73.75%, tr:  96.42%, tr_best:  96.63%\n",
      "epoch-140 lr=['0.0001000'], tr/val_loss:  0.471714/  1.274063, val:  70.83%, val_best:  73.75%, tr:  95.91%, tr_best:  96.63%\n",
      "epoch-141 lr=['0.0001000'], tr/val_loss:  0.457706/  1.276605, val:  71.25%, val_best:  73.75%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-142 lr=['0.0001000'], tr/val_loss:  0.453803/  1.284343, val:  70.83%, val_best:  73.75%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-143 lr=['0.0001000'], tr/val_loss:  0.452851/  1.275963, val:  70.42%, val_best:  73.75%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-144 lr=['0.0001000'], tr/val_loss:  0.448974/  1.287308, val:  70.00%, val_best:  73.75%, tr:  96.83%, tr_best:  97.45%\n",
      "epoch-145 lr=['0.0001000'], tr/val_loss:  0.446087/  1.293666, val:  70.83%, val_best:  73.75%, tr:  97.14%, tr_best:  97.45%\n",
      "epoch-146 lr=['0.0001000'], tr/val_loss:  0.446987/  1.297136, val:  71.25%, val_best:  73.75%, tr:  96.83%, tr_best:  97.45%\n",
      "epoch-147 lr=['0.0001000'], tr/val_loss:  0.448351/  1.298652, val:  71.25%, val_best:  73.75%, tr:  96.83%, tr_best:  97.45%\n",
      "epoch-148 lr=['0.0001000'], tr/val_loss:  0.436514/  1.300475, val:  70.83%, val_best:  73.75%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-149 lr=['0.0001000'], tr/val_loss:  0.431961/  1.295423, val:  71.67%, val_best:  73.75%, tr:  97.14%, tr_best:  97.55%\n",
      "epoch-150 lr=['0.0001000'], tr/val_loss:  0.430884/  1.304356, val:  71.25%, val_best:  73.75%, tr:  97.24%, tr_best:  97.55%\n",
      "epoch-151 lr=['0.0001000'], tr/val_loss:  0.427968/  1.304731, val:  72.08%, val_best:  73.75%, tr:  97.04%, tr_best:  97.55%\n",
      "epoch-152 lr=['0.0001000'], tr/val_loss:  0.423116/  1.315257, val:  71.25%, val_best:  73.75%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-153 lr=['0.0001000'], tr/val_loss:  0.422098/  1.320856, val:  71.67%, val_best:  73.75%, tr:  97.34%, tr_best:  97.55%\n",
      "epoch-154 lr=['0.0001000'], tr/val_loss:  0.417880/  1.305984, val:  72.08%, val_best:  73.75%, tr:  97.45%, tr_best:  97.55%\n",
      "epoch-155 lr=['0.0001000'], tr/val_loss:  0.411085/  1.320687, val:  72.50%, val_best:  73.75%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-156 lr=['0.0001000'], tr/val_loss:  0.412855/  1.314372, val:  72.92%, val_best:  73.75%, tr:  97.65%, tr_best:  98.26%\n",
      "epoch-157 lr=['0.0001000'], tr/val_loss:  0.406370/  1.337178, val:  71.25%, val_best:  73.75%, tr:  97.55%, tr_best:  98.26%\n",
      "epoch-158 lr=['0.0001000'], tr/val_loss:  0.411529/  1.329527, val:  71.67%, val_best:  73.75%, tr:  97.85%, tr_best:  98.26%\n",
      "epoch-159 lr=['0.0001000'], tr/val_loss:  0.400935/  1.331781, val:  70.83%, val_best:  73.75%, tr:  97.75%, tr_best:  98.26%\n",
      "epoch-160 lr=['0.0001000'], tr/val_loss:  0.397858/  1.337524, val:  72.08%, val_best:  73.75%, tr:  97.96%, tr_best:  98.26%\n",
      "epoch-161 lr=['0.0001000'], tr/val_loss:  0.392914/  1.344698, val:  70.00%, val_best:  73.75%, tr:  98.06%, tr_best:  98.26%\n",
      "epoch-162 lr=['0.0001000'], tr/val_loss:  0.397684/  1.349157, val:  70.83%, val_best:  73.75%, tr:  97.96%, tr_best:  98.26%\n",
      "epoch-163 lr=['0.0001000'], tr/val_loss:  0.390441/  1.350968, val:  72.92%, val_best:  73.75%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-164 lr=['0.0001000'], tr/val_loss:  0.391391/  1.343095, val:  70.42%, val_best:  73.75%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-165 lr=['0.0001000'], tr/val_loss:  0.386479/  1.366679, val:  71.67%, val_best:  73.75%, tr:  98.26%, tr_best:  98.57%\n",
      "epoch-166 lr=['0.0001000'], tr/val_loss:  0.381402/  1.365258, val:  71.67%, val_best:  73.75%, tr:  98.47%, tr_best:  98.57%\n",
      "epoch-167 lr=['0.0001000'], tr/val_loss:  0.382591/  1.359112, val:  72.50%, val_best:  73.75%, tr:  98.26%, tr_best:  98.57%\n",
      "epoch-168 lr=['0.0001000'], tr/val_loss:  0.381838/  1.363007, val:  70.42%, val_best:  73.75%, tr:  98.37%, tr_best:  98.57%\n",
      "epoch-169 lr=['0.0001000'], tr/val_loss:  0.372164/  1.363789, val:  71.67%, val_best:  73.75%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-170 lr=['0.0001000'], tr/val_loss:  0.375240/  1.363217, val:  72.08%, val_best:  73.75%, tr:  98.47%, tr_best:  98.57%\n",
      "epoch-171 lr=['0.0001000'], tr/val_loss:  0.370359/  1.368348, val:  72.50%, val_best:  73.75%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-172 lr=['0.0001000'], tr/val_loss:  0.365165/  1.370109, val:  72.50%, val_best:  73.75%, tr:  98.57%, tr_best:  98.88%\n",
      "epoch-173 lr=['0.0001000'], tr/val_loss:  0.362605/  1.385651, val:  72.92%, val_best:  73.75%, tr:  98.77%, tr_best:  98.88%\n",
      "epoch-174 lr=['0.0001000'], tr/val_loss:  0.359052/  1.380354, val:  71.67%, val_best:  73.75%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-175 lr=['0.0001000'], tr/val_loss:  0.357751/  1.379550, val:  72.50%, val_best:  73.75%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-176 lr=['0.0001000'], tr/val_loss:  0.353114/  1.380173, val:  70.83%, val_best:  73.75%, tr:  98.88%, tr_best:  99.08%\n",
      "epoch-177 lr=['0.0001000'], tr/val_loss:  0.360736/  1.382396, val:  73.75%, val_best:  73.75%, tr:  98.77%, tr_best:  99.08%\n",
      "epoch-178 lr=['0.0001000'], tr/val_loss:  0.355370/  1.380380, val:  72.50%, val_best:  73.75%, tr:  98.57%, tr_best:  99.08%\n",
      "epoch-179 lr=['0.0001000'], tr/val_loss:  0.350876/  1.395467, val:  71.67%, val_best:  73.75%, tr:  98.77%, tr_best:  99.08%\n",
      "epoch-180 lr=['0.0001000'], tr/val_loss:  0.340907/  1.382146, val:  71.67%, val_best:  73.75%, tr:  98.88%, tr_best:  99.08%\n",
      "epoch-181 lr=['0.0001000'], tr/val_loss:  0.344084/  1.406336, val:  72.92%, val_best:  73.75%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-182 lr=['0.0001000'], tr/val_loss:  0.338115/  1.411226, val:  72.92%, val_best:  73.75%, tr:  99.28%, tr_best:  99.39%\n",
      "epoch-183 lr=['0.0001000'], tr/val_loss:  0.339413/  1.396421, val:  72.50%, val_best:  73.75%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-184 lr=['0.0001000'], tr/val_loss:  0.339530/  1.392507, val:  72.50%, val_best:  73.75%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-185 lr=['0.0001000'], tr/val_loss:  0.330767/  1.396221, val:  72.92%, val_best:  73.75%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-186 lr=['0.0001000'], tr/val_loss:  0.331205/  1.402206, val:  72.92%, val_best:  73.75%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-187 lr=['0.0001000'], tr/val_loss:  0.326164/  1.406928, val:  71.67%, val_best:  73.75%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-188 lr=['0.0001000'], tr/val_loss:  0.323469/  1.403708, val:  71.67%, val_best:  73.75%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-189 lr=['0.0001000'], tr/val_loss:  0.321256/  1.412605, val:  73.33%, val_best:  73.75%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-190 lr=['0.0001000'], tr/val_loss:  0.318231/  1.423106, val:  72.08%, val_best:  73.75%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-191 lr=['0.0001000'], tr/val_loss:  0.323844/  1.420131, val:  72.92%, val_best:  73.75%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-192 lr=['0.0001000'], tr/val_loss:  0.311113/  1.414523, val:  73.33%, val_best:  73.75%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-193 lr=['0.0001000'], tr/val_loss:  0.314098/  1.418513, val:  72.92%, val_best:  73.75%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-194 lr=['0.0001000'], tr/val_loss:  0.310910/  1.433165, val:  70.00%, val_best:  73.75%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-195 lr=['0.0001000'], tr/val_loss:  0.310485/  1.425111, val:  73.33%, val_best:  73.75%, tr:  99.18%, tr_best:  99.59%\n",
      "epoch-196 lr=['0.0001000'], tr/val_loss:  0.307830/  1.436701, val:  72.92%, val_best:  73.75%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-197 lr=['0.0001000'], tr/val_loss:  0.302070/  1.425912, val:  72.08%, val_best:  73.75%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-198 lr=['0.0001000'], tr/val_loss:  0.302839/  1.430184, val:  73.33%, val_best:  73.75%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-199 lr=['0.0001000'], tr/val_loss:  0.294272/  1.441573, val:  73.33%, val_best:  73.75%, tr:  99.59%, tr_best:  99.59%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de701be284fa478a9c56f2699d84c3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄▆▃▅▅▆▇█▆▅▆█▇▇▇▇▆████▇██▇█████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▅▆▆▇▇▇█▇▇▇▇▇▇▇▇█▇████▇████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▆▅▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▅▆▆▇▇▇█▇▇▇▇▇▇▇▇█▇████▇████████████████</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99591</td></tr><tr><td>tr_epoch_loss</td><td>0.29427</td></tr><tr><td>val_acc_best</td><td>0.7375</td></tr><tr><td>val_acc_now</td><td>0.73333</td></tr><tr><td>val_loss</td><td>1.44157</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glamorous-sweep-13</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o0570zav' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/o0570zav</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_013632-o0570zav/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ksupvkdi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7807b97ff1444a59ad0aea7a6db4c708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113633520694242, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_014836-ksupvkdi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ksupvkdi' target=\"_blank\">blooming-sweep-14</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ksupvkdi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ksupvkdi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.185388/  1.953995, val:  39.58%, val_best:  39.58%, tr:  21.65%, tr_best:  21.65%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.631786/  1.548397, val:  55.83%, val_best:  55.83%, tr:  52.60%, tr_best:  52.60%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  1.365721/  1.449115, val:  57.08%, val_best:  57.08%, tr:  58.63%, tr_best:  58.63%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  1.225300/  1.430301, val:  58.75%, val_best:  58.75%, tr:  65.37%, tr_best:  65.37%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  1.147487/  1.343770, val:  63.75%, val_best:  63.75%, tr:  64.96%, tr_best:  65.37%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  1.080473/  1.293350, val:  65.00%, val_best:  65.00%, tr:  65.99%, tr_best:  65.99%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  1.006375/  1.280098, val:  66.67%, val_best:  66.67%, tr:  71.20%, tr_best:  71.20%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  0.966871/  1.266460, val:  63.33%, val_best:  66.67%, tr:  72.52%, tr_best:  72.52%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  0.923951/  1.276804, val:  66.67%, val_best:  66.67%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.895835/  1.292640, val:  66.67%, val_best:  66.67%, tr:  76.40%, tr_best:  76.40%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.865744/  1.346816, val:  62.08%, val_best:  66.67%, tr:  74.87%, tr_best:  76.40%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.811206/  1.249282, val:  65.42%, val_best:  66.67%, tr:  77.32%, tr_best:  77.32%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.803857/  1.265766, val:  66.25%, val_best:  66.67%, tr:  79.78%, tr_best:  79.78%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.770453/  1.301781, val:  65.42%, val_best:  66.67%, tr:  81.82%, tr_best:  81.82%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.731615/  1.391628, val:  60.83%, val_best:  66.67%, tr:  81.72%, tr_best:  81.82%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.703273/  1.358759, val:  68.75%, val_best:  68.75%, tr:  85.19%, tr_best:  85.19%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.710856/  1.284859, val:  69.17%, val_best:  69.17%, tr:  82.64%, tr_best:  85.19%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.651377/  1.316934, val:  70.42%, val_best:  70.42%, tr:  88.36%, tr_best:  88.36%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.626479/  1.365201, val:  66.67%, val_best:  70.42%, tr:  87.74%, tr_best:  88.36%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.616617/  1.365178, val:  70.42%, val_best:  70.42%, tr:  85.50%, tr_best:  88.36%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.575669/  1.384170, val:  65.42%, val_best:  70.42%, tr:  90.30%, tr_best:  90.30%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.553762/  1.407160, val:  66.67%, val_best:  70.42%, tr:  89.58%, tr_best:  90.30%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.550838/  1.392679, val:  70.00%, val_best:  70.42%, tr:  89.79%, tr_best:  90.30%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.510358/  1.435957, val:  70.00%, val_best:  70.42%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.472209/  1.476634, val:  68.75%, val_best:  70.42%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.460284/  1.485863, val:  71.25%, val_best:  71.25%, tr:  95.10%, tr_best:  95.40%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.460730/  1.485591, val:  72.92%, val_best:  72.92%, tr:  93.36%, tr_best:  95.40%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.425683/  1.530734, val:  69.58%, val_best:  72.92%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.410814/  1.501347, val:  72.92%, val_best:  72.92%, tr:  96.94%, tr_best:  96.94%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.394781/  1.601103, val:  68.75%, val_best:  72.92%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.368170/  1.593626, val:  71.25%, val_best:  72.92%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.370819/  1.632266, val:  71.25%, val_best:  72.92%, tr:  96.63%, tr_best:  98.26%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.359842/  1.655125, val:  71.67%, val_best:  72.92%, tr:  97.65%, tr_best:  98.26%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.344807/  1.643029, val:  72.92%, val_best:  72.92%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.328783/  1.722455, val:  68.33%, val_best:  72.92%, tr:  97.85%, tr_best:  98.57%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.325792/  1.701831, val:  72.08%, val_best:  72.92%, tr:  97.85%, tr_best:  98.57%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.294247/  1.677153, val:  74.58%, val_best:  74.58%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.279357/  1.730737, val:  71.67%, val_best:  74.58%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.275394/  1.735416, val:  71.25%, val_best:  74.58%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.254375/  1.732976, val:  72.50%, val_best:  74.58%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.254364/  1.819641, val:  71.67%, val_best:  74.58%, tr:  98.47%, tr_best:  99.69%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.236409/  1.825167, val:  72.50%, val_best:  74.58%, tr:  99.18%, tr_best:  99.69%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.215655/  1.825068, val:  73.75%, val_best:  74.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.216858/  1.905315, val:  70.83%, val_best:  74.58%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.211179/  1.873549, val:  73.75%, val_best:  74.58%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.193643/  1.864648, val:  75.42%, val_best:  75.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.180116/  1.927731, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.173874/  1.912330, val:  77.50%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.173420/  1.933530, val:  76.25%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.157793/  1.953565, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.141236/  2.030754, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.143288/  2.031038, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.139232/  1.982779, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.130939/  2.090399, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.118132/  2.107708, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.117554/  2.101778, val:  75.42%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.114696/  2.129805, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.124388/  2.107877, val:  75.00%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.113057/  2.147144, val:  75.83%, val_best:  77.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.098443/  2.172183, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.088535/  2.208841, val:  74.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.095382/  2.218567, val:  74.17%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.084061/  2.228013, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.079405/  2.252248, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.087201/  2.296426, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.074065/  2.313872, val:  74.17%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.069903/  2.326783, val:  74.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.064701/  2.344017, val:  74.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.063557/  2.385467, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.061966/  2.379107, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.055436/  2.394763, val:  74.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.063448/  2.406173, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.052629/  2.444074, val:  73.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.052617/  2.438284, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.050518/  2.451897, val:  74.17%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.049765/  2.489810, val:  73.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.048292/  2.535080, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.045604/  2.541323, val:  74.17%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.043395/  2.519506, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.042654/  2.498291, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.039626/  2.535225, val:  73.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.038009/  2.532586, val:  74.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.038500/  2.579136, val:  73.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.036031/  2.602077, val:  74.17%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.033328/  2.583726, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.033444/  2.622647, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.034348/  2.616670, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.032919/  2.638202, val:  74.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.029980/  2.642679, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.029969/  2.669199, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.031522/  2.683905, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.029393/  2.743812, val:  74.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.026151/  2.717625, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.026408/  2.708025, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.022883/  2.738179, val:  74.17%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.024513/  2.768614, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.022385/  2.776782, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.024087/  2.730427, val:  74.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.022285/  2.799921, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.021625/  2.763212, val:  73.33%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.020923/  2.790658, val:  73.33%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.018755/  2.799376, val:  73.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.020341/  2.816175, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.018935/  2.784645, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.020736/  2.849818, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.018921/  2.842882, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.017308/  2.834400, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.018651/  2.872228, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.017246/  2.842740, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.020956/  2.833797, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.017249/  2.836451, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.015919/  2.873077, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.015022/  2.874677, val:  73.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.016543/  2.907788, val:  74.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.015846/  2.900256, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.014677/  2.879260, val:  74.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.013139/  2.929108, val:  74.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.014024/  2.937160, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.012808/  2.938160, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.012856/  2.943640, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.012178/  2.957792, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.012359/  2.978344, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.012111/  2.972507, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.013574/  2.980535, val:  74.17%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.011531/  2.991956, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.011903/  3.012693, val:  74.17%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.009735/  3.015900, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.009627/  2.997993, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.008926/  3.012837, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.009209/  3.021085, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.009106/  3.012983, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.010460/  3.061056, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.008158/  3.034008, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.007719/  3.039703, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.007718/  3.033926, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.008194/  3.059871, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.008869/  3.081469, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.007545/  3.058914, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.007599/  3.085679, val:  73.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.007583/  3.094793, val:  73.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.007537/  3.104735, val:  73.33%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.007985/  3.105584, val:  74.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.008707/  3.098264, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.008949/  3.068200, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.009778/  3.114717, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.009403/  3.100581, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.010557/  3.107509, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.008329/  3.088936, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.007888/  3.116872, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.006523/  3.107496, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.007074/  3.123780, val:  74.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.006373/  3.132407, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.005712/  3.127584, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.005333/  3.135980, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.005344/  3.123384, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.005837/  3.161533, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.005885/  3.169178, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.005793/  3.167795, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.005855/  3.199850, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.006385/  3.177599, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.006902/  3.163868, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.006682/  3.173224, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.006528/  3.156887, val:  74.17%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.006845/  3.183827, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.006099/  3.198786, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.005793/  3.205743, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.005480/  3.194590, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.005381/  3.179156, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.005132/  3.196470, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.005774/  3.206468, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.005713/  3.232811, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.005398/  3.199976, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.005322/  3.206787, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.004850/  3.232563, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.004907/  3.221440, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.004842/  3.209578, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.004604/  3.226399, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.004490/  3.240693, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.004773/  3.222946, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.004466/  3.226584, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.003902/  3.235245, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.004619/  3.221647, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.004180/  3.234886, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.004197/  3.241222, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.004310/  3.247371, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.003955/  3.252318, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.004017/  3.261366, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.003876/  3.269433, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.003520/  3.270779, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.003876/  3.270720, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.003649/  3.278116, val:  75.42%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.003508/  3.276531, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.003752/  3.280403, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.003458/  3.276083, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.003153/  3.281419, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.003399/  3.295727, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.003172/  3.286900, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.002941/  3.277685, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.003154/  3.307754, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.003468/  3.312270, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec90e7a30159405baf15c22c84334dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▆▇█▇█████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▅▅▆▆▅▇▆▇█▇▇▇█▇███▇▇██▇███▇▇████▇▇▇███▇</td></tr><tr><td>tr_acc</td><td>▁▃▅▆▆▇██████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▅▆▆▆▇▇▇██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▅▅▆▆▅▇▆▇█▇▇▇█▇███▇▇██▇███▇▇████▇▇▇███▇</td></tr><tr><td>val_loss</td><td>▂▁▁▁▁▂▂▂▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00347</td></tr><tr><td>val_acc_best</td><td>0.77917</td></tr><tr><td>val_acc_now</td><td>0.75</td></tr><tr><td>val_loss</td><td>3.31227</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-sweep-14</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ksupvkdi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ksupvkdi</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_014836-ksupvkdi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h25ej9yy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_020040-h25ej9yy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h25ej9yy' target=\"_blank\">misty-sweep-15</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h25ej9yy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h25ej9yy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.048345/  1.608755, val:  45.83%, val_best:  45.83%, tr:  23.39%, tr_best:  23.39%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.307725/  1.387209, val:  54.17%, val_best:  54.17%, tr:  54.14%, tr_best:  54.14%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  1.096445/  1.329417, val:  57.08%, val_best:  57.08%, tr:  61.08%, tr_best:  61.08%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  0.981904/  1.287495, val:  60.00%, val_best:  60.00%, tr:  66.39%, tr_best:  66.39%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  0.932707/  1.221137, val:  61.67%, val_best:  61.67%, tr:  68.34%, tr_best:  68.34%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  0.873325/  1.176639, val:  67.50%, val_best:  67.50%, tr:  69.46%, tr_best:  69.46%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  0.810590/  1.190916, val:  60.00%, val_best:  67.50%, tr:  74.97%, tr_best:  74.97%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  0.788863/  1.147136, val:  67.92%, val_best:  67.92%, tr:  74.06%, tr_best:  74.97%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  0.734883/  1.163405, val:  69.58%, val_best:  69.58%, tr:  78.75%, tr_best:  78.75%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.697745/  1.179413, val:  69.17%, val_best:  69.58%, tr:  83.04%, tr_best:  83.04%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.668638/  1.233232, val:  66.67%, val_best:  69.58%, tr:  84.88%, tr_best:  84.88%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.598353/  1.126269, val:  72.50%, val_best:  72.50%, tr:  88.46%, tr_best:  88.46%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.592247/  1.143284, val:  69.58%, val_best:  72.50%, tr:  89.17%, tr_best:  89.17%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.554782/  1.136868, val:  72.50%, val_best:  72.50%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.509235/  1.325955, val:  65.42%, val_best:  72.50%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.481798/  1.249804, val:  65.83%, val_best:  72.50%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.471197/  1.175127, val:  72.50%, val_best:  72.50%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.416000/  1.189803, val:  73.33%, val_best:  73.33%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.402143/  1.262303, val:  71.67%, val_best:  73.33%, tr:  96.02%, tr_best:  96.22%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.377603/  1.239696, val:  74.17%, val_best:  74.17%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.357631/  1.267683, val:  69.58%, val_best:  74.17%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.339217/  1.246144, val:  72.50%, val_best:  74.17%, tr:  97.45%, tr_best:  97.75%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.332984/  1.249981, val:  75.00%, val_best:  75.00%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.326973/  1.283299, val:  71.67%, val_best:  75.00%, tr:  97.45%, tr_best:  98.37%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.287762/  1.260168, val:  73.33%, val_best:  75.00%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.270158/  1.282945, val:  73.75%, val_best:  75.00%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.279815/  1.293055, val:  73.75%, val_best:  75.00%, tr:  98.37%, tr_best:  99.28%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.251746/  1.315577, val:  70.42%, val_best:  75.00%, tr:  98.77%, tr_best:  99.28%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.239100/  1.300069, val:  75.00%, val_best:  75.00%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.226900/  1.404814, val:  67.92%, val_best:  75.00%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.220884/  1.349543, val:  72.50%, val_best:  75.00%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.217881/  1.382683, val:  72.92%, val_best:  75.00%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.212462/  1.396751, val:  71.67%, val_best:  75.00%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.205354/  1.410371, val:  73.33%, val_best:  75.00%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.193572/  1.389625, val:  70.83%, val_best:  75.00%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.185315/  1.472130, val:  70.00%, val_best:  75.00%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.177427/  1.417780, val:  75.42%, val_best:  75.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.168204/  1.466271, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.160245/  1.437948, val:  72.92%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.156830/  1.438232, val:  71.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.149065/  1.486640, val:  74.17%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.142371/  1.507364, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.149227/  1.482182, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.137314/  1.538015, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.136804/  1.502250, val:  73.75%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.121582/  1.538804, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.120031/  1.514312, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.114460/  1.559959, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.113846/  1.541656, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.111957/  1.595055, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.104611/  1.616339, val:  74.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.099392/  1.616029, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.100069/  1.610202, val:  71.67%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.096965/  1.635631, val:  72.08%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.098508/  1.659672, val:  72.50%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.090070/  1.656929, val:  72.08%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.090829/  1.657115, val:  72.92%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.090499/  1.651850, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.084844/  1.663037, val:  72.92%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.081469/  1.681296, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.078493/  1.669535, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.077148/  1.694669, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.073815/  1.692779, val:  73.33%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.071286/  1.705822, val:  74.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.068850/  1.758053, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.067317/  1.734873, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.066760/  1.733888, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.065511/  1.739171, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.065294/  1.745030, val:  75.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.063853/  1.754093, val:  75.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.063779/  1.767519, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.064753/  1.768116, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.062370/  1.820211, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.062651/  1.795196, val:  75.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.058213/  1.807759, val:  74.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.057975/  1.811745, val:  72.92%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.056382/  1.832831, val:  73.33%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.055772/  1.806891, val:  75.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.054211/  1.834644, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.054516/  1.837137, val:  75.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.050633/  1.817801, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.054537/  1.840632, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.051883/  1.893929, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.049078/  1.894982, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.048373/  1.863132, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.049553/  1.891761, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.048947/  1.866929, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.048424/  1.891955, val:  75.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.045230/  1.912381, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.044458/  1.880548, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.044464/  1.879753, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.045406/  1.884560, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.042533/  1.912040, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.042289/  1.906412, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.042897/  1.917522, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.041366/  1.917658, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.042158/  1.959086, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.040527/  1.932195, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.040154/  1.956122, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.041972/  1.949604, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.038349/  1.956659, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.036602/  1.962969, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.036808/  1.964967, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.035513/  1.981656, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.035802/  1.992085, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.035448/  1.961946, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.036875/  1.991852, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.034532/  2.002275, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.034825/  2.018369, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.033051/  2.008276, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.033571/  2.016078, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.034799/  2.018211, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.032090/  2.026493, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.031882/  2.036707, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.032010/  2.073549, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.030761/  2.041975, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.030476/  2.041116, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.029315/  2.056732, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.028617/  2.051757, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.028054/  2.066980, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.028734/  2.076261, val:  75.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.026382/  2.071971, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.027061/  2.093882, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.027160/  2.090960, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.029302/  2.083365, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.026880/  2.128036, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.025974/  2.130190, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.026168/  2.139762, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.026402/  2.129416, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.026063/  2.109266, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.026484/  2.113583, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.026499/  2.134778, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.024624/  2.133663, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.025228/  2.148859, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.025828/  2.148900, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.026661/  2.129770, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.023742/  2.181397, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.027504/  2.166795, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.023361/  2.171835, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.026168/  2.186317, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.026550/  2.181799, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.025193/  2.186285, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.024808/  2.195308, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.024197/  2.196752, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.026051/  2.205880, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.025051/  2.176727, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.024100/  2.213773, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.022786/  2.211259, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.023298/  2.205852, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.022250/  2.222307, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.022414/  2.214232, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.021289/  2.216731, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.021936/  2.221241, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.020510/  2.241758, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.022306/  2.227804, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.020707/  2.231567, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.022589/  2.271932, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.020377/  2.256579, val:  72.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.020342/  2.263317, val:  72.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.019342/  2.252419, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.019174/  2.269901, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.018205/  2.252507, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.017529/  2.261813, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.017791/  2.287498, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.017405/  2.278286, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.017249/  2.285064, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.017957/  2.301145, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.017271/  2.309158, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.018070/  2.312972, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.018923/  2.308345, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.016772/  2.314969, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.017271/  2.320654, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.016613/  2.327347, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.017186/  2.337095, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.017269/  2.316214, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.016362/  2.314616, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.015728/  2.331129, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.015899/  2.325350, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.017911/  2.336437, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.016372/  2.318815, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.015266/  2.321857, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.014476/  2.337818, val:  75.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.015635/  2.324217, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.015349/  2.335773, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.015906/  2.332241, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.015111/  2.360685, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.014440/  2.335910, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.014958/  2.335536, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.015244/  2.323018, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.014680/  2.352396, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.014689/  2.349827, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.015239/  2.355721, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.015293/  2.382639, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.015275/  2.362404, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.014563/  2.381873, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.015148/  2.384468, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.015291/  2.376858, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.014338/  2.344532, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.013432/  2.372353, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.014253/  2.362015, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7ba996adeb4b728af5838eb0d8caf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▇████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▅▇▇▅█▇▇▇▇█▇█▇█▇█▇▇█▇▇█▇▇▇██▇▇▇▇▇▇████</td></tr><tr><td>tr_acc</td><td>▁▃▅▇▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▅▇▇▅█▇▇▇▇█▇█▇█▇█▇▇█▇▇█▇▇▇██▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▂▁▁▁▁▂▂▂▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇███████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.01425</td></tr><tr><td>val_acc_best</td><td>0.7625</td></tr><tr><td>val_acc_now</td><td>0.75417</td></tr><tr><td>val_loss</td><td>2.36202</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misty-sweep-15</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h25ej9yy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/h25ej9yy</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_020040-h25ej9yy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iw6nrehe with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_021410-iw6nrehe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iw6nrehe' target=\"_blank\">rare-sweep-16</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iw6nrehe' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iw6nrehe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.092947/  2.733715, val:  45.83%, val_best:  45.83%, tr:  33.20%, tr_best:  33.20%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.029886/  2.517073, val:  43.33%, val_best:  45.83%, tr:  50.77%, tr_best:  50.77%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.336044/  2.524317, val:  50.00%, val_best:  50.00%, tr:  52.91%, tr_best:  52.91%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.982548/  3.089748, val:  47.08%, val_best:  50.00%, tr:  58.63%, tr_best:  58.63%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.813993/  1.968981, val:  62.50%, val_best:  62.50%, tr:  61.59%, tr_best:  61.59%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.766299/  2.025199, val:  54.17%, val_best:  62.50%, tr:  65.37%, tr_best:  65.37%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.208794/  2.170132, val:  63.33%, val_best:  63.33%, tr:  72.42%, tr_best:  72.42%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.976128/  2.409544, val:  47.08%, val_best:  63.33%, tr:  79.06%, tr_best:  79.06%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.838966/  1.527648, val:  69.58%, val_best:  69.58%, tr:  84.37%, tr_best:  84.37%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.186780/  2.625674, val:  60.42%, val_best:  69.58%, tr:  82.43%, tr_best:  84.37%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.025166/  1.690781, val:  77.08%, val_best:  77.08%, tr:  84.27%, tr_best:  84.37%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.846319/  2.144420, val:  68.33%, val_best:  77.08%, tr:  88.66%, tr_best:  88.66%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.670802/  2.046111, val:  69.58%, val_best:  77.08%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.553066/  2.199632, val:  71.25%, val_best:  77.08%, tr:  93.56%, tr_best:  93.56%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.486124/  2.430470, val:  67.92%, val_best:  77.08%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.413692/  2.209290, val:  69.58%, val_best:  77.08%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.365352/  2.324907, val:  70.42%, val_best:  77.08%, tr:  95.71%, tr_best:  96.22%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.272451/  2.053746, val:  77.50%, val_best:  77.50%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.238821/  2.411228, val:  71.25%, val_best:  77.50%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.187183/  2.056455, val:  79.58%, val_best:  79.58%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.164502/  2.352451, val:  73.33%, val_best:  79.58%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.155231/  2.239315, val:  76.67%, val_best:  79.58%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.108869/  2.278255, val:  79.58%, val_best:  79.58%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.105440/  2.325430, val:  77.08%, val_best:  79.58%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.069508/  2.374915, val:  77.92%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.068057/  2.371302, val:  79.17%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.086877/  2.448625, val:  77.08%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.075555/  2.468113, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.050824/  2.507307, val:  79.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.039587/  2.607471, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.034172/  2.504275, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.031908/  2.576477, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.026788/  2.587345, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.025139/  2.658201, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.023283/  2.714951, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.019250/  2.685200, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.015872/  2.683712, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.015245/  2.715208, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.015585/  2.716886, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.015695/  2.754947, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.016856/  2.726050, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.016057/  2.736388, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.011348/  2.766957, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.011768/  2.819757, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.008522/  2.803874, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.008271/  2.800312, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.007942/  2.828001, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.007345/  2.823452, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.006397/  2.811191, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.005807/  2.823591, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.006350/  2.893696, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.006774/  2.870012, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.006254/  2.846688, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.005853/  2.882762, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.005467/  2.851332, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.004526/  2.866423, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.004564/  2.901260, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.004454/  2.919800, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.004316/  2.942875, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.003660/  2.947444, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.003669/  2.953462, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.003386/  2.928792, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.003645/  2.943565, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.003804/  2.988230, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.003656/  2.988150, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.002945/  3.017932, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.003120/  3.040861, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.003721/  3.029005, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.003132/  3.054041, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.003326/  3.056125, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.003093/  3.036803, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.004062/  3.042173, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.005834/  3.057696, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.002969/  3.034021, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.003451/  3.052841, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.002892/  3.040810, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.002943/  3.036888, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.002954/  3.055851, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.002862/  3.093729, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.002623/  3.080340, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.002632/  3.107404, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.002487/  3.112414, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.002689/  3.143488, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.002069/  3.126570, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.002089/  3.123857, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.002199/  3.126404, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.002215/  3.150444, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.002166/  3.127923, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.002566/  3.165792, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.002340/  3.130772, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.002170/  3.102413, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.002147/  3.133240, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.001768/  3.149837, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.001845/  3.158576, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.001914/  3.139368, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.002444/  3.141449, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.002365/  3.155213, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.003644/  3.175553, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.008264/  3.133837, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.005041/  3.147357, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.003724/  3.134700, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.002498/  3.128119, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.002515/  3.179111, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.001903/  3.174576, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.002211/  3.181753, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.002065/  3.179792, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.001771/  3.164938, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.001596/  3.158551, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.001644/  3.169545, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.001645/  3.182532, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.001884/  3.231166, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.001874/  3.236422, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.002212/  3.220812, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.002016/  3.225959, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.001706/  3.224939, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.001679/  3.248044, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.001438/  3.234048, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.001563/  3.233637, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.001537/  3.246550, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.002598/  3.229456, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.002003/  3.285655, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.001645/  3.269987, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.001828/  3.284500, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.001723/  3.268558, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.002087/  3.272053, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.001682/  3.291702, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.002007/  3.297256, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.001909/  3.304297, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.001556/  3.302458, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.001271/  3.294584, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.001235/  3.291787, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.001099/  3.282350, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.001414/  3.293132, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.001101/  3.301543, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.001322/  3.292177, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.001011/  3.305477, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.001000/  3.319944, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.000871/  3.310073, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.000822/  3.310007, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.000938/  3.302336, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.000861/  3.311510, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.001108/  3.336667, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.000904/  3.302689, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.000862/  3.306647, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.000837/  3.301654, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.000901/  3.325162, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.000991/  3.303868, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.000853/  3.326979, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.000830/  3.340565, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.000749/  3.347617, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.000759/  3.355844, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.000907/  3.360182, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.000956/  3.351731, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.000924/  3.369335, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.000880/  3.341790, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.000791/  3.358913, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.000922/  3.357731, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.000871/  3.360193, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.000795/  3.373630, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.000825/  3.368597, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.000743/  3.378497, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.000822/  3.382666, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.000893/  3.360426, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.000881/  3.379271, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.000842/  3.380231, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.000875/  3.375616, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.000867/  3.399689, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.001060/  3.383720, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.000937/  3.384560, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.001129/  3.402265, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.000771/  3.406659, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.001039/  3.390743, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.000745/  3.396448, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.001005/  3.394392, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.000779/  3.401240, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.000733/  3.414743, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.000832/  3.408391, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.000687/  3.404922, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.000726/  3.405639, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.000905/  3.425586, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.001295/  3.422924, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.001179/  3.420405, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.000826/  3.413925, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.000791/  3.422860, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.000721/  3.409823, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.000676/  3.418312, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.000781/  3.414889, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.001301/  3.398035, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.000749/  3.404329, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.000655/  3.401324, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.000670/  3.404239, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.000649/  3.422487, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.000624/  3.426197, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.000634/  3.425128, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.000644/  3.432947, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.000627/  3.442522, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.000584/  3.437385, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.000622/  3.429330, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.000606/  3.429141, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.000617/  3.420565, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1b6d02a6074a2b8974c5a1b029afb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▅▇████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▆▇▇▇████████████▇████████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▆▇████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▅▇▇▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▆▇▇▇████████████▇████████████████████</td></tr><tr><td>val_loss</td><td>▃▁▄▂▁▃▄▄▄▅▅▅▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇██████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00062</td></tr><tr><td>val_acc_best</td><td>0.8375</td></tr><tr><td>val_acc_now</td><td>0.82083</td></tr><tr><td>val_loss</td><td>3.42057</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rare-sweep-16</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iw6nrehe' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iw6nrehe</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_021410-iw6nrehe/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 47c0zkt2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_022744-47c0zkt2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/47c0zkt2' target=\"_blank\">vocal-sweep-17</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/47c0zkt2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/47c0zkt2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.302698/  2.292768, val:  17.08%, val_best:  17.08%, tr:  12.46%, tr_best:  12.46%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.274752/  2.258935, val:  15.42%, val_best:  17.08%, tr:  17.47%, tr_best:  17.47%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.204992/  2.174848, val:  20.00%, val_best:  20.00%, tr:  21.25%, tr_best:  21.25%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.085248/  2.052755, val:  36.67%, val_best:  36.67%, tr:  36.67%, tr_best:  36.67%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  1.932786/  1.907182, val:  44.17%, val_best:  44.17%, tr:  44.74%, tr_best:  44.74%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  1.774296/  1.786407, val:  41.25%, val_best:  44.17%, tr:  46.68%, tr_best:  46.68%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  1.659975/  1.691252, val:  42.50%, val_best:  44.17%, tr:  51.07%, tr_best:  51.07%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  1.557328/  1.623603, val:  46.25%, val_best:  46.25%, tr:  55.77%, tr_best:  55.77%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  1.489562/  1.574490, val:  49.58%, val_best:  49.58%, tr:  56.28%, tr_best:  56.28%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  1.433274/  1.536899, val:  47.92%, val_best:  49.58%, tr:  57.30%, tr_best:  57.30%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  1.385136/  1.509164, val:  52.08%, val_best:  52.08%, tr:  59.55%, tr_best:  59.55%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  1.355187/  1.487221, val:  52.50%, val_best:  52.50%, tr:  60.67%, tr_best:  60.67%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  1.330998/  1.467402, val:  52.08%, val_best:  52.50%, tr:  60.98%, tr_best:  60.98%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  1.303529/  1.456255, val:  55.00%, val_best:  55.00%, tr:  64.25%, tr_best:  64.25%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  1.270272/  1.438672, val:  55.00%, val_best:  55.00%, tr:  62.82%, tr_best:  64.25%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  1.249672/  1.429348, val:  55.42%, val_best:  55.42%, tr:  63.33%, tr_best:  64.25%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  1.218952/  1.416096, val:  57.50%, val_best:  57.50%, tr:  65.07%, tr_best:  65.07%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  1.208867/  1.400289, val:  56.67%, val_best:  57.50%, tr:  65.37%, tr_best:  65.37%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  1.190412/  1.393940, val:  58.75%, val_best:  58.75%, tr:  67.31%, tr_best:  67.31%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  1.170019/  1.384895, val:  61.25%, val_best:  61.25%, tr:  65.17%, tr_best:  67.31%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  1.153712/  1.379607, val:  59.17%, val_best:  61.25%, tr:  66.39%, tr_best:  67.31%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  1.136266/  1.371371, val:  60.00%, val_best:  61.25%, tr:  68.13%, tr_best:  68.13%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  1.118979/  1.368522, val:  59.58%, val_best:  61.25%, tr:  67.62%, tr_best:  68.13%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  1.101579/  1.357459, val:  61.67%, val_best:  61.67%, tr:  69.56%, tr_best:  69.56%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  1.091408/  1.349270, val:  62.08%, val_best:  62.08%, tr:  69.87%, tr_best:  69.87%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  1.079010/  1.341327, val:  60.83%, val_best:  62.08%, tr:  72.01%, tr_best:  72.01%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  1.067978/  1.329979, val:  61.25%, val_best:  62.08%, tr:  68.85%, tr_best:  72.01%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  1.053607/  1.319202, val:  63.33%, val_best:  63.33%, tr:  71.40%, tr_best:  72.01%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  1.056343/  1.308584, val:  63.33%, val_best:  63.33%, tr:  70.68%, tr_best:  72.01%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  1.039373/  1.312725, val:  62.50%, val_best:  63.33%, tr:  71.20%, tr_best:  72.01%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  1.026930/  1.310829, val:  64.17%, val_best:  64.17%, tr:  72.11%, tr_best:  72.11%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  1.020496/  1.306484, val:  64.58%, val_best:  64.58%, tr:  70.99%, tr_best:  72.11%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  1.012640/  1.304612, val:  64.17%, val_best:  64.58%, tr:  73.44%, tr_best:  73.44%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  1.010994/  1.296196, val:  65.00%, val_best:  65.00%, tr:  72.63%, tr_best:  73.44%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  0.995898/  1.289801, val:  66.25%, val_best:  66.25%, tr:  73.65%, tr_best:  73.65%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  0.987727/  1.293414, val:  65.83%, val_best:  66.25%, tr:  74.36%, tr_best:  74.36%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  0.986436/  1.287196, val:  66.25%, val_best:  66.25%, tr:  73.65%, tr_best:  74.36%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  0.974112/  1.281370, val:  63.75%, val_best:  66.25%, tr:  74.97%, tr_best:  74.97%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  0.973239/  1.280752, val:  65.00%, val_best:  66.25%, tr:  74.57%, tr_best:  74.97%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  0.963944/  1.275829, val:  65.00%, val_best:  66.25%, tr:  75.08%, tr_best:  75.08%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  0.952409/  1.275715, val:  65.83%, val_best:  66.25%, tr:  74.67%, tr_best:  75.08%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  0.955548/  1.272587, val:  67.50%, val_best:  67.50%, tr:  77.83%, tr_best:  77.83%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  0.935081/  1.272249, val:  68.75%, val_best:  68.75%, tr:  79.16%, tr_best:  79.16%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  0.936234/  1.270575, val:  66.25%, val_best:  68.75%, tr:  77.83%, tr_best:  79.16%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  0.929215/  1.260638, val:  65.42%, val_best:  68.75%, tr:  76.92%, tr_best:  79.16%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  0.921339/  1.262255, val:  69.17%, val_best:  69.17%, tr:  78.65%, tr_best:  79.16%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  0.915414/  1.262135, val:  68.33%, val_best:  69.17%, tr:  79.78%, tr_best:  79.78%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  0.918281/  1.257876, val:  68.33%, val_best:  69.17%, tr:  78.04%, tr_best:  79.78%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  0.908801/  1.261870, val:  68.33%, val_best:  69.17%, tr:  80.69%, tr_best:  80.69%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  0.899446/  1.256992, val:  65.83%, val_best:  69.17%, tr:  79.57%, tr_best:  80.69%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  0.893979/  1.256333, val:  65.83%, val_best:  69.17%, tr:  80.59%, tr_best:  80.69%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  0.892416/  1.253569, val:  71.25%, val_best:  71.25%, tr:  79.98%, tr_best:  80.69%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  0.883906/  1.256265, val:  67.92%, val_best:  71.25%, tr:  80.80%, tr_best:  80.80%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  0.875784/  1.251568, val:  70.00%, val_best:  71.25%, tr:  82.53%, tr_best:  82.53%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  0.870680/  1.255640, val:  69.58%, val_best:  71.25%, tr:  83.55%, tr_best:  83.55%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  0.865348/  1.258469, val:  70.83%, val_best:  71.25%, tr:  83.76%, tr_best:  83.76%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  0.853171/  1.251597, val:  69.58%, val_best:  71.25%, tr:  85.29%, tr_best:  85.29%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  0.851955/  1.259470, val:  69.17%, val_best:  71.25%, tr:  83.86%, tr_best:  85.29%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  0.841909/  1.255540, val:  68.33%, val_best:  71.25%, tr:  84.37%, tr_best:  85.29%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  0.839158/  1.255408, val:  68.75%, val_best:  71.25%, tr:  84.78%, tr_best:  85.29%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  0.832510/  1.254215, val:  69.58%, val_best:  71.25%, tr:  82.94%, tr_best:  85.29%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  0.833360/  1.259571, val:  68.75%, val_best:  71.25%, tr:  85.80%, tr_best:  85.80%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  0.826951/  1.255235, val:  70.00%, val_best:  71.25%, tr:  85.60%, tr_best:  85.80%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  0.818573/  1.248546, val:  70.42%, val_best:  71.25%, tr:  86.41%, tr_best:  86.41%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  0.810193/  1.257740, val:  69.17%, val_best:  71.25%, tr:  87.23%, tr_best:  87.23%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  0.811643/  1.256131, val:  70.00%, val_best:  71.25%, tr:  86.52%, tr_best:  87.23%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  0.806954/  1.246399, val:  72.50%, val_best:  72.50%, tr:  87.44%, tr_best:  87.44%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  0.809792/  1.260046, val:  70.42%, val_best:  72.50%, tr:  87.64%, tr_best:  87.64%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  0.795566/  1.254603, val:  72.08%, val_best:  72.50%, tr:  86.11%, tr_best:  87.64%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  0.788062/  1.258137, val:  70.83%, val_best:  72.50%, tr:  88.36%, tr_best:  88.36%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  0.784680/  1.262427, val:  70.83%, val_best:  72.50%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  0.778202/  1.254902, val:  69.58%, val_best:  72.50%, tr:  89.48%, tr_best:  89.48%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  0.770891/  1.256913, val:  70.83%, val_best:  72.50%, tr:  87.64%, tr_best:  89.48%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  0.769780/  1.253805, val:  70.42%, val_best:  72.50%, tr:  89.79%, tr_best:  89.79%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  0.762907/  1.256425, val:  69.58%, val_best:  72.50%, tr:  88.36%, tr_best:  89.79%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  0.762055/  1.254840, val:  70.00%, val_best:  72.50%, tr:  89.27%, tr_best:  89.79%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  0.745042/  1.268152, val:  69.58%, val_best:  72.50%, tr:  90.19%, tr_best:  90.19%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  0.754839/  1.263077, val:  70.83%, val_best:  72.50%, tr:  91.01%, tr_best:  91.01%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  0.741604/  1.259577, val:  71.67%, val_best:  72.50%, tr:  90.40%, tr_best:  91.01%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  0.739925/  1.266422, val:  70.42%, val_best:  72.50%, tr:  89.48%, tr_best:  91.01%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  0.734950/  1.262501, val:  71.67%, val_best:  72.50%, tr:  89.79%, tr_best:  91.01%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  0.729619/  1.267241, val:  71.67%, val_best:  72.50%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  0.727231/  1.262265, val:  71.67%, val_best:  72.50%, tr:  91.11%, tr_best:  91.42%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  0.724686/  1.267102, val:  73.33%, val_best:  73.33%, tr:  92.03%, tr_best:  92.03%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  0.722775/  1.276169, val:  71.67%, val_best:  73.33%, tr:  91.73%, tr_best:  92.03%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  0.718485/  1.276992, val:  70.83%, val_best:  73.33%, tr:  92.03%, tr_best:  92.03%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  0.709335/  1.275546, val:  72.08%, val_best:  73.33%, tr:  92.13%, tr_best:  92.13%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  0.709095/  1.272285, val:  72.08%, val_best:  73.33%, tr:  91.93%, tr_best:  92.13%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  0.701422/  1.280483, val:  70.00%, val_best:  73.33%, tr:  92.44%, tr_best:  92.44%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  0.694418/  1.285691, val:  70.42%, val_best:  73.33%, tr:  92.65%, tr_best:  92.65%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  0.695371/  1.278757, val:  72.08%, val_best:  73.33%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  0.682746/  1.277684, val:  71.67%, val_best:  73.33%, tr:  93.05%, tr_best:  93.05%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  0.689662/  1.289693, val:  70.83%, val_best:  73.33%, tr:  92.75%, tr_best:  93.05%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  0.683175/  1.289434, val:  73.33%, val_best:  73.33%, tr:  92.95%, tr_best:  93.05%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  0.681503/  1.288427, val:  71.25%, val_best:  73.33%, tr:  92.95%, tr_best:  93.05%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  0.671056/  1.288039, val:  71.25%, val_best:  73.33%, tr:  93.16%, tr_best:  93.16%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  0.668530/  1.291209, val:  71.67%, val_best:  73.33%, tr:  93.97%, tr_best:  93.97%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  0.665018/  1.293126, val:  72.92%, val_best:  73.33%, tr:  93.05%, tr_best:  93.97%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  0.663100/  1.292838, val:  72.50%, val_best:  73.33%, tr:  93.97%, tr_best:  93.97%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  0.656817/  1.297092, val:  73.75%, val_best:  73.75%, tr:  93.56%, tr_best:  93.97%\n",
      "epoch-100 lr=['0.0001000'], tr/val_loss:  0.648595/  1.290021, val:  72.92%, val_best:  73.75%, tr:  93.36%, tr_best:  93.97%\n",
      "epoch-101 lr=['0.0001000'], tr/val_loss:  0.642035/  1.299584, val:  73.75%, val_best:  73.75%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-102 lr=['0.0001000'], tr/val_loss:  0.643523/  1.306109, val:  71.25%, val_best:  73.75%, tr:  93.97%, tr_best:  94.48%\n",
      "epoch-103 lr=['0.0001000'], tr/val_loss:  0.641368/  1.304147, val:  72.50%, val_best:  73.75%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-104 lr=['0.0001000'], tr/val_loss:  0.633921/  1.301558, val:  72.92%, val_best:  73.75%, tr:  93.67%, tr_best:  94.48%\n",
      "epoch-105 lr=['0.0001000'], tr/val_loss:  0.643237/  1.310537, val:  75.42%, val_best:  75.42%, tr:  93.67%, tr_best:  94.48%\n",
      "epoch-106 lr=['0.0001000'], tr/val_loss:  0.630526/  1.304259, val:  72.08%, val_best:  75.42%, tr:  93.97%, tr_best:  94.48%\n",
      "epoch-107 lr=['0.0001000'], tr/val_loss:  0.631525/  1.312638, val:  73.33%, val_best:  75.42%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-108 lr=['0.0001000'], tr/val_loss:  0.620059/  1.318122, val:  72.92%, val_best:  75.42%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-109 lr=['0.0001000'], tr/val_loss:  0.618432/  1.318163, val:  72.50%, val_best:  75.42%, tr:  94.89%, tr_best:  94.99%\n",
      "epoch-110 lr=['0.0001000'], tr/val_loss:  0.613905/  1.318917, val:  72.50%, val_best:  75.42%, tr:  94.89%, tr_best:  94.99%\n",
      "epoch-111 lr=['0.0001000'], tr/val_loss:  0.610733/  1.319490, val:  73.33%, val_best:  75.42%, tr:  95.30%, tr_best:  95.30%\n",
      "epoch-112 lr=['0.0001000'], tr/val_loss:  0.606151/  1.320989, val:  73.33%, val_best:  75.42%, tr:  95.10%, tr_best:  95.30%\n",
      "epoch-113 lr=['0.0001000'], tr/val_loss:  0.609845/  1.331760, val:  72.92%, val_best:  75.42%, tr:  95.10%, tr_best:  95.30%\n",
      "epoch-114 lr=['0.0001000'], tr/val_loss:  0.611718/  1.324899, val:  73.33%, val_best:  75.42%, tr:  95.20%, tr_best:  95.30%\n",
      "epoch-115 lr=['0.0001000'], tr/val_loss:  0.600240/  1.318232, val:  72.08%, val_best:  75.42%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-116 lr=['0.0001000'], tr/val_loss:  0.589461/  1.325846, val:  73.33%, val_best:  75.42%, tr:  95.51%, tr_best:  95.71%\n",
      "epoch-117 lr=['0.0001000'], tr/val_loss:  0.597131/  1.327589, val:  73.33%, val_best:  75.42%, tr:  95.81%, tr_best:  95.81%\n",
      "epoch-118 lr=['0.0001000'], tr/val_loss:  0.590407/  1.338576, val:  72.08%, val_best:  75.42%, tr:  95.51%, tr_best:  95.81%\n",
      "epoch-119 lr=['0.0001000'], tr/val_loss:  0.585564/  1.334375, val:  73.33%, val_best:  75.42%, tr:  95.51%, tr_best:  95.81%\n",
      "epoch-120 lr=['0.0001000'], tr/val_loss:  0.576992/  1.327709, val:  72.92%, val_best:  75.42%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-121 lr=['0.0001000'], tr/val_loss:  0.577731/  1.332263, val:  74.17%, val_best:  75.42%, tr:  95.61%, tr_best:  96.22%\n",
      "epoch-122 lr=['0.0001000'], tr/val_loss:  0.582217/  1.337233, val:  72.08%, val_best:  75.42%, tr:  95.91%, tr_best:  96.22%\n",
      "epoch-123 lr=['0.0001000'], tr/val_loss:  0.580718/  1.334409, val:  72.50%, val_best:  75.42%, tr:  96.02%, tr_best:  96.22%\n",
      "epoch-124 lr=['0.0001000'], tr/val_loss:  0.563529/  1.331893, val:  73.75%, val_best:  75.42%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-125 lr=['0.0001000'], tr/val_loss:  0.567145/  1.335019, val:  73.75%, val_best:  75.42%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-126 lr=['0.0001000'], tr/val_loss:  0.578203/  1.333769, val:  72.08%, val_best:  75.42%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-127 lr=['0.0001000'], tr/val_loss:  0.557853/  1.344126, val:  74.17%, val_best:  75.42%, tr:  96.83%, tr_best:  97.04%\n",
      "epoch-128 lr=['0.0001000'], tr/val_loss:  0.559689/  1.342180, val:  73.33%, val_best:  75.42%, tr:  96.63%, tr_best:  97.04%\n",
      "epoch-129 lr=['0.0001000'], tr/val_loss:  0.556062/  1.340739, val:  72.08%, val_best:  75.42%, tr:  96.73%, tr_best:  97.04%\n",
      "epoch-130 lr=['0.0001000'], tr/val_loss:  0.557234/  1.337950, val:  72.50%, val_best:  75.42%, tr:  96.63%, tr_best:  97.04%\n",
      "epoch-131 lr=['0.0001000'], tr/val_loss:  0.553149/  1.350477, val:  71.25%, val_best:  75.42%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-132 lr=['0.0001000'], tr/val_loss:  0.549595/  1.355775, val:  72.92%, val_best:  75.42%, tr:  96.63%, tr_best:  97.24%\n",
      "epoch-133 lr=['0.0001000'], tr/val_loss:  0.538751/  1.353629, val:  72.50%, val_best:  75.42%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-134 lr=['0.0001000'], tr/val_loss:  0.540298/  1.351014, val:  74.17%, val_best:  75.42%, tr:  96.83%, tr_best:  97.55%\n",
      "epoch-135 lr=['0.0001000'], tr/val_loss:  0.535819/  1.349669, val:  72.08%, val_best:  75.42%, tr:  96.94%, tr_best:  97.55%\n",
      "epoch-136 lr=['0.0001000'], tr/val_loss:  0.540805/  1.360220, val:  72.92%, val_best:  75.42%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-137 lr=['0.0001000'], tr/val_loss:  0.542645/  1.353822, val:  72.50%, val_best:  75.42%, tr:  97.24%, tr_best:  97.55%\n",
      "epoch-138 lr=['0.0001000'], tr/val_loss:  0.523189/  1.354467, val:  73.75%, val_best:  75.42%, tr:  97.45%, tr_best:  97.55%\n",
      "epoch-139 lr=['0.0001000'], tr/val_loss:  0.528763/  1.361575, val:  71.25%, val_best:  75.42%, tr:  97.04%, tr_best:  97.55%\n",
      "epoch-140 lr=['0.0001000'], tr/val_loss:  0.527117/  1.367419, val:  72.08%, val_best:  75.42%, tr:  97.34%, tr_best:  97.55%\n",
      "epoch-141 lr=['0.0001000'], tr/val_loss:  0.516704/  1.362710, val:  72.92%, val_best:  75.42%, tr:  97.45%, tr_best:  97.55%\n",
      "epoch-142 lr=['0.0001000'], tr/val_loss:  0.513637/  1.370084, val:  73.33%, val_best:  75.42%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-143 lr=['0.0001000'], tr/val_loss:  0.513933/  1.372571, val:  74.17%, val_best:  75.42%, tr:  97.34%, tr_best:  97.65%\n",
      "epoch-144 lr=['0.0001000'], tr/val_loss:  0.511947/  1.379892, val:  72.92%, val_best:  75.42%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-145 lr=['0.0001000'], tr/val_loss:  0.511120/  1.373324, val:  70.83%, val_best:  75.42%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-146 lr=['0.0001000'], tr/val_loss:  0.509257/  1.373101, val:  71.25%, val_best:  75.42%, tr:  97.65%, tr_best:  97.85%\n",
      "epoch-147 lr=['0.0001000'], tr/val_loss:  0.510506/  1.378280, val:  73.75%, val_best:  75.42%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-148 lr=['0.0001000'], tr/val_loss:  0.497636/  1.388416, val:  73.33%, val_best:  75.42%, tr:  97.55%, tr_best:  97.85%\n",
      "epoch-149 lr=['0.0001000'], tr/val_loss:  0.496994/  1.382222, val:  72.92%, val_best:  75.42%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-150 lr=['0.0001000'], tr/val_loss:  0.495866/  1.383232, val:  71.25%, val_best:  75.42%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-151 lr=['0.0001000'], tr/val_loss:  0.495582/  1.398406, val:  72.50%, val_best:  75.42%, tr:  98.16%, tr_best:  98.37%\n",
      "epoch-152 lr=['0.0001000'], tr/val_loss:  0.487595/  1.390181, val:  71.67%, val_best:  75.42%, tr:  98.16%, tr_best:  98.37%\n",
      "epoch-153 lr=['0.0001000'], tr/val_loss:  0.490205/  1.396171, val:  73.33%, val_best:  75.42%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-154 lr=['0.0001000'], tr/val_loss:  0.482184/  1.389140, val:  71.67%, val_best:  75.42%, tr:  98.26%, tr_best:  98.57%\n",
      "epoch-155 lr=['0.0001000'], tr/val_loss:  0.484605/  1.395373, val:  72.92%, val_best:  75.42%, tr:  98.37%, tr_best:  98.57%\n",
      "epoch-156 lr=['0.0001000'], tr/val_loss:  0.484004/  1.398028, val:  74.17%, val_best:  75.42%, tr:  97.96%, tr_best:  98.57%\n",
      "epoch-157 lr=['0.0001000'], tr/val_loss:  0.475953/  1.406378, val:  70.83%, val_best:  75.42%, tr:  98.47%, tr_best:  98.57%\n",
      "epoch-158 lr=['0.0001000'], tr/val_loss:  0.483031/  1.398549, val:  71.67%, val_best:  75.42%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-159 lr=['0.0001000'], tr/val_loss:  0.475692/  1.408091, val:  73.75%, val_best:  75.42%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-160 lr=['0.0001000'], tr/val_loss:  0.472714/  1.409978, val:  74.17%, val_best:  75.42%, tr:  98.37%, tr_best:  98.67%\n",
      "epoch-161 lr=['0.0001000'], tr/val_loss:  0.469608/  1.407047, val:  74.17%, val_best:  75.42%, tr:  98.47%, tr_best:  98.67%\n",
      "epoch-162 lr=['0.0001000'], tr/val_loss:  0.470813/  1.404920, val:  72.50%, val_best:  75.42%, tr:  98.47%, tr_best:  98.67%\n",
      "epoch-163 lr=['0.0001000'], tr/val_loss:  0.468340/  1.410011, val:  75.00%, val_best:  75.42%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-164 lr=['0.0001000'], tr/val_loss:  0.465962/  1.407883, val:  74.17%, val_best:  75.42%, tr:  98.57%, tr_best:  98.88%\n",
      "epoch-165 lr=['0.0001000'], tr/val_loss:  0.463542/  1.414654, val:  73.33%, val_best:  75.42%, tr:  98.57%, tr_best:  98.88%\n",
      "epoch-166 lr=['0.0001000'], tr/val_loss:  0.456396/  1.416800, val:  73.33%, val_best:  75.42%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-167 lr=['0.0001000'], tr/val_loss:  0.460126/  1.413381, val:  73.33%, val_best:  75.42%, tr:  98.67%, tr_best:  98.88%\n",
      "epoch-168 lr=['0.0001000'], tr/val_loss:  0.453273/  1.413128, val:  74.17%, val_best:  75.42%, tr:  98.67%, tr_best:  98.88%\n",
      "epoch-169 lr=['0.0001000'], tr/val_loss:  0.448121/  1.424608, val:  71.67%, val_best:  75.42%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-170 lr=['0.0001000'], tr/val_loss:  0.447825/  1.427269, val:  73.75%, val_best:  75.42%, tr:  98.77%, tr_best:  98.88%\n",
      "epoch-171 lr=['0.0001000'], tr/val_loss:  0.449066/  1.425338, val:  73.33%, val_best:  75.42%, tr:  98.67%, tr_best:  98.88%\n",
      "epoch-172 lr=['0.0001000'], tr/val_loss:  0.444759/  1.436524, val:  74.58%, val_best:  75.42%, tr:  98.77%, tr_best:  98.88%\n",
      "epoch-173 lr=['0.0001000'], tr/val_loss:  0.441891/  1.437153, val:  74.17%, val_best:  75.42%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-174 lr=['0.0001000'], tr/val_loss:  0.439559/  1.433566, val:  72.50%, val_best:  75.42%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-175 lr=['0.0001000'], tr/val_loss:  0.436780/  1.434805, val:  71.67%, val_best:  75.42%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-176 lr=['0.0001000'], tr/val_loss:  0.435593/  1.443151, val:  74.17%, val_best:  75.42%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-177 lr=['0.0001000'], tr/val_loss:  0.441283/  1.437442, val:  72.08%, val_best:  75.42%, tr:  98.88%, tr_best:  99.08%\n",
      "epoch-178 lr=['0.0001000'], tr/val_loss:  0.432450/  1.443089, val:  74.17%, val_best:  75.42%, tr:  98.88%, tr_best:  99.08%\n",
      "epoch-179 lr=['0.0001000'], tr/val_loss:  0.433461/  1.440186, val:  73.75%, val_best:  75.42%, tr:  98.88%, tr_best:  99.08%\n",
      "epoch-180 lr=['0.0001000'], tr/val_loss:  0.422902/  1.439637, val:  73.75%, val_best:  75.42%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-181 lr=['0.0001000'], tr/val_loss:  0.426575/  1.452862, val:  73.75%, val_best:  75.42%, tr:  98.98%, tr_best:  99.28%\n",
      "epoch-182 lr=['0.0001000'], tr/val_loss:  0.424775/  1.447036, val:  72.92%, val_best:  75.42%, tr:  99.18%, tr_best:  99.28%\n",
      "epoch-183 lr=['0.0001000'], tr/val_loss:  0.422539/  1.457272, val:  73.75%, val_best:  75.42%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-184 lr=['0.0001000'], tr/val_loss:  0.419205/  1.457834, val:  73.33%, val_best:  75.42%, tr:  99.08%, tr_best:  99.28%\n",
      "epoch-185 lr=['0.0001000'], tr/val_loss:  0.419104/  1.463594, val:  72.50%, val_best:  75.42%, tr:  99.08%, tr_best:  99.28%\n",
      "epoch-186 lr=['0.0001000'], tr/val_loss:  0.418239/  1.462353, val:  72.50%, val_best:  75.42%, tr:  99.18%, tr_best:  99.28%\n",
      "epoch-187 lr=['0.0001000'], tr/val_loss:  0.415297/  1.474396, val:  70.83%, val_best:  75.42%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-188 lr=['0.0001000'], tr/val_loss:  0.412898/  1.469928, val:  72.08%, val_best:  75.42%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-189 lr=['0.0001000'], tr/val_loss:  0.406907/  1.479574, val:  71.67%, val_best:  75.42%, tr:  98.88%, tr_best:  99.39%\n",
      "epoch-190 lr=['0.0001000'], tr/val_loss:  0.411236/  1.476883, val:  73.75%, val_best:  75.42%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-191 lr=['0.0001000'], tr/val_loss:  0.409282/  1.487580, val:  72.50%, val_best:  75.42%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-192 lr=['0.0001000'], tr/val_loss:  0.405717/  1.480524, val:  72.08%, val_best:  75.42%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-193 lr=['0.0001000'], tr/val_loss:  0.405157/  1.494397, val:  73.33%, val_best:  75.42%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-194 lr=['0.0001000'], tr/val_loss:  0.405855/  1.495701, val:  73.33%, val_best:  75.42%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-195 lr=['0.0001000'], tr/val_loss:  0.405392/  1.489124, val:  73.33%, val_best:  75.42%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-196 lr=['0.0001000'], tr/val_loss:  0.398018/  1.492804, val:  72.50%, val_best:  75.42%, tr:  99.28%, tr_best:  99.39%\n",
      "epoch-197 lr=['0.0001000'], tr/val_loss:  0.396726/  1.492414, val:  74.58%, val_best:  75.42%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-198 lr=['0.0001000'], tr/val_loss:  0.399123/  1.499479, val:  73.75%, val_best:  75.42%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-199 lr=['0.0001000'], tr/val_loss:  0.388212/  1.495629, val:  73.33%, val_best:  75.42%, tr:  99.39%, tr_best:  99.49%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b14bb173904080bae62b088950a63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▅▄▅▆▆▇█▅▆▆▇▇█▇█▇████▇████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇█▇█████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█▇███████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇█▇█████████████████████████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99387</td></tr><tr><td>tr_epoch_loss</td><td>0.38821</td></tr><tr><td>val_acc_best</td><td>0.75417</td></tr><tr><td>val_acc_now</td><td>0.73333</td></tr><tr><td>val_loss</td><td>1.49563</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-sweep-17</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/47c0zkt2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/47c0zkt2</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_022744-47c0zkt2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8eiacl2y with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3316db3ff8114800834e9347f1899378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113105036525264, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_024057-8eiacl2y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8eiacl2y' target=\"_blank\">youthful-sweep-18</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8eiacl2y' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8eiacl2y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.200257/  1.963433, val:  33.75%, val_best:  33.75%, tr:  17.47%, tr_best:  17.47%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.612809/  1.502315, val:  52.50%, val_best:  52.50%, tr:  48.83%, tr_best:  48.83%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  1.319229/  1.399373, val:  54.17%, val_best:  54.17%, tr:  57.00%, tr_best:  57.00%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  1.204576/  1.362992, val:  57.08%, val_best:  57.08%, tr:  62.21%, tr_best:  62.21%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  1.144690/  1.348632, val:  57.92%, val_best:  57.92%, tr:  62.82%, tr_best:  62.82%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  1.085414/  1.291084, val:  62.08%, val_best:  62.08%, tr:  64.25%, tr_best:  64.25%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  1.028766/  1.270729, val:  63.33%, val_best:  63.33%, tr:  66.91%, tr_best:  66.91%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  1.002337/  1.260750, val:  64.17%, val_best:  64.17%, tr:  68.13%, tr_best:  68.13%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  0.961690/  1.261445, val:  63.33%, val_best:  64.17%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.939497/  1.258922, val:  60.83%, val_best:  64.17%, tr:  70.68%, tr_best:  70.68%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.922273/  1.269150, val:  60.00%, val_best:  64.17%, tr:  69.77%, tr_best:  70.68%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.891821/  1.224548, val:  62.08%, val_best:  64.17%, tr:  72.01%, tr_best:  72.01%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.880291/  1.230423, val:  59.58%, val_best:  64.17%, tr:  74.16%, tr_best:  74.16%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.863125/  1.228143, val:  64.17%, val_best:  64.17%, tr:  76.51%, tr_best:  76.51%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.821415/  1.307757, val:  60.42%, val_best:  64.17%, tr:  75.59%, tr_best:  76.51%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.805111/  1.212557, val:  67.50%, val_best:  67.50%, tr:  78.55%, tr_best:  78.55%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.807929/  1.224711, val:  62.92%, val_best:  67.50%, tr:  76.51%, tr_best:  78.55%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.769923/  1.214597, val:  65.83%, val_best:  67.50%, tr:  81.72%, tr_best:  81.72%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.747036/  1.271080, val:  63.33%, val_best:  67.50%, tr:  81.31%, tr_best:  81.72%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.743961/  1.244238, val:  65.42%, val_best:  67.50%, tr:  79.26%, tr_best:  81.72%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.715465/  1.235161, val:  64.17%, val_best:  67.50%, tr:  84.88%, tr_best:  84.88%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.707739/  1.200252, val:  67.08%, val_best:  67.50%, tr:  84.58%, tr_best:  84.88%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.709503/  1.228547, val:  70.42%, val_best:  70.42%, tr:  82.43%, tr_best:  84.88%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.668734/  1.248012, val:  65.83%, val_best:  70.42%, tr:  86.11%, tr_best:  86.11%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.654331/  1.247338, val:  65.00%, val_best:  70.42%, tr:  87.95%, tr_best:  87.95%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.644361/  1.229483, val:  71.25%, val_best:  71.25%, tr:  88.36%, tr_best:  88.36%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.632797/  1.228203, val:  70.42%, val_best:  71.25%, tr:  88.15%, tr_best:  88.36%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.618532/  1.276007, val:  70.00%, val_best:  71.25%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.609934/  1.261064, val:  70.42%, val_best:  71.25%, tr:  91.01%, tr_best:  91.01%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.577692/  1.303436, val:  65.83%, val_best:  71.25%, tr:  92.24%, tr_best:  92.24%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.571665/  1.291641, val:  65.42%, val_best:  71.25%, tr:  93.16%, tr_best:  93.16%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.577910/  1.321637, val:  66.25%, val_best:  71.25%, tr:  90.70%, tr_best:  93.16%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.558480/  1.314774, val:  67.50%, val_best:  71.25%, tr:  92.85%, tr_best:  93.16%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.560280/  1.317617, val:  69.17%, val_best:  71.25%, tr:  92.34%, tr_best:  93.16%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.532511/  1.363640, val:  64.58%, val_best:  71.25%, tr:  93.67%, tr_best:  93.67%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.538369/  1.354566, val:  67.50%, val_best:  71.25%, tr:  93.26%, tr_best:  93.67%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.508970/  1.347897, val:  65.83%, val_best:  71.25%, tr:  95.30%, tr_best:  95.30%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.509781/  1.359850, val:  67.92%, val_best:  71.25%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.499739/  1.371768, val:  69.58%, val_best:  71.25%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.490092/  1.371360, val:  70.00%, val_best:  71.25%, tr:  95.51%, tr_best:  96.02%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.479144/  1.397188, val:  69.58%, val_best:  71.25%, tr:  95.40%, tr_best:  96.02%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.472979/  1.397076, val:  67.50%, val_best:  71.25%, tr:  96.94%, tr_best:  96.94%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.457328/  1.426906, val:  66.67%, val_best:  71.25%, tr:  96.73%, tr_best:  96.94%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.451069/  1.433495, val:  69.58%, val_best:  71.25%, tr:  96.94%, tr_best:  96.94%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.444882/  1.419277, val:  67.50%, val_best:  71.25%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.437747/  1.444493, val:  68.75%, val_best:  71.25%, tr:  96.94%, tr_best:  97.04%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.422129/  1.450096, val:  65.83%, val_best:  71.25%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.423701/  1.481308, val:  66.67%, val_best:  71.25%, tr:  97.45%, tr_best:  97.85%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.413600/  1.457761, val:  67.50%, val_best:  71.25%, tr:  97.75%, tr_best:  97.85%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.414267/  1.492015, val:  67.08%, val_best:  71.25%, tr:  97.55%, tr_best:  97.85%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.406376/  1.521162, val:  67.50%, val_best:  71.25%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.397541/  1.535680, val:  69.17%, val_best:  71.25%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.388995/  1.545179, val:  65.00%, val_best:  71.25%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.390623/  1.544450, val:  64.58%, val_best:  71.25%, tr:  98.37%, tr_best:  98.47%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.375709/  1.551908, val:  66.67%, val_best:  71.25%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.379179/  1.546880, val:  68.33%, val_best:  71.25%, tr:  98.47%, tr_best:  98.88%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.374657/  1.570104, val:  65.83%, val_best:  71.25%, tr:  98.47%, tr_best:  98.88%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.366088/  1.565649, val:  69.58%, val_best:  71.25%, tr:  98.67%, tr_best:  98.88%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.356249/  1.597082, val:  64.17%, val_best:  71.25%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.349658/  1.581693, val:  65.42%, val_best:  71.25%, tr:  98.77%, tr_best:  98.98%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.351490/  1.581155, val:  67.08%, val_best:  71.25%, tr:  98.77%, tr_best:  98.98%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.341287/  1.606171, val:  65.42%, val_best:  71.25%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.339606/  1.620158, val:  65.42%, val_best:  71.25%, tr:  99.08%, tr_best:  99.18%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.335784/  1.613652, val:  67.08%, val_best:  71.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.331649/  1.657438, val:  68.33%, val_best:  71.25%, tr:  98.77%, tr_best:  99.49%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.325306/  1.652475, val:  67.08%, val_best:  71.25%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.327141/  1.655639, val:  65.83%, val_best:  71.25%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.323797/  1.647362, val:  67.92%, val_best:  71.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.319172/  1.683954, val:  65.00%, val_best:  71.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.300421/  1.679635, val:  67.08%, val_best:  71.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.304698/  1.700534, val:  66.67%, val_best:  71.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.303174/  1.741844, val:  63.75%, val_best:  71.25%, tr:  99.28%, tr_best:  99.59%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.300199/  1.744657, val:  65.00%, val_best:  71.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.299816/  1.701602, val:  68.33%, val_best:  71.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.290658/  1.733547, val:  67.08%, val_best:  71.25%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.288130/  1.741887, val:  66.25%, val_best:  71.25%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.280969/  1.775606, val:  67.08%, val_best:  71.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.285543/  1.774962, val:  67.92%, val_best:  71.25%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.282475/  1.766745, val:  63.33%, val_best:  71.25%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.270164/  1.779068, val:  67.08%, val_best:  71.25%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.269703/  1.795308, val:  67.08%, val_best:  71.25%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.270354/  1.780708, val:  66.25%, val_best:  71.25%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.266313/  1.814557, val:  65.83%, val_best:  71.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.263151/  1.832200, val:  66.67%, val_best:  71.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.254204/  1.823576, val:  68.75%, val_best:  71.25%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.252525/  1.844180, val:  65.42%, val_best:  71.25%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.254418/  1.857021, val:  67.92%, val_best:  71.25%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.248404/  1.874591, val:  65.83%, val_best:  71.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.250232/  1.882378, val:  65.00%, val_best:  71.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.241832/  1.904018, val:  65.83%, val_best:  71.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.246260/  1.900752, val:  65.83%, val_best:  71.25%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.238753/  1.916912, val:  65.00%, val_best:  71.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.234833/  1.923412, val:  67.92%, val_best:  71.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.235869/  1.904665, val:  67.92%, val_best:  71.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.236204/  1.945157, val:  64.17%, val_best:  71.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.232264/  1.944897, val:  65.42%, val_best:  71.25%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.226952/  1.957016, val:  64.58%, val_best:  71.25%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.228863/  1.949686, val:  68.33%, val_best:  71.25%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.220239/  1.953671, val:  67.08%, val_best:  71.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.228307/  1.972498, val:  64.58%, val_best:  71.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.209944/  1.983586, val:  66.25%, val_best:  71.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.208754/  1.979574, val:  65.00%, val_best:  71.25%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.209720/  2.030793, val:  64.58%, val_best:  71.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.208478/  2.030988, val:  64.17%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.209021/  2.085444, val:  63.75%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.208012/  2.044327, val:  64.17%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.203085/  2.058833, val:  65.42%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.207111/  2.080434, val:  62.08%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.201809/  2.063203, val:  63.75%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.197619/  2.078486, val:  65.42%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.200231/  2.078653, val:  65.00%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.190234/  2.117500, val:  60.83%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.189951/  2.127429, val:  63.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.186696/  2.153158, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.192163/  2.150083, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.181349/  2.156111, val:  63.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.181015/  2.160549, val:  62.08%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.182584/  2.148297, val:  63.33%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.180980/  2.176086, val:  61.67%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.179013/  2.177313, val:  63.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.178848/  2.185972, val:  64.58%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.175568/  2.201944, val:  64.17%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.180589/  2.184232, val:  63.75%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.181055/  2.246877, val:  63.33%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.172237/  2.207293, val:  63.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.167186/  2.235198, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.170917/  2.250452, val:  61.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.165582/  2.243530, val:  64.17%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.165191/  2.245540, val:  63.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.161113/  2.267305, val:  62.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.163106/  2.256039, val:  64.17%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.155374/  2.265943, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.152701/  2.287799, val:  63.33%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.152840/  2.288984, val:  62.92%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.152628/  2.315208, val:  62.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.160899/  2.285662, val:  64.17%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.152539/  2.322408, val:  64.17%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.153079/  2.349304, val:  63.33%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.148403/  2.358645, val:  63.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.150493/  2.332308, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.142122/  2.363982, val:  61.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.139797/  2.372165, val:  62.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.138880/  2.373894, val:  63.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.144836/  2.369452, val:  64.17%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.141612/  2.392492, val:  62.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.140588/  2.391940, val:  61.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.142724/  2.399284, val:  63.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.135283/  2.416360, val:  61.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.133606/  2.433312, val:  61.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.131947/  2.416258, val:  61.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.136958/  2.443402, val:  64.58%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.134568/  2.451476, val:  64.17%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.131508/  2.444781, val:  63.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.124655/  2.453745, val:  63.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.124645/  2.462841, val:  64.58%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.122760/  2.439738, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.125248/  2.456166, val:  65.83%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.123607/  2.472109, val:  62.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.124646/  2.494568, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.123674/  2.465013, val:  63.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.120945/  2.486286, val:  62.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.116146/  2.512274, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.121426/  2.535481, val:  61.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.120379/  2.543311, val:  63.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.116769/  2.565202, val:  61.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.118354/  2.550491, val:  62.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.115963/  2.556027, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.115087/  2.533439, val:  60.42%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.114995/  2.565290, val:  60.83%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.110570/  2.551044, val:  62.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.113029/  2.569822, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.115293/  2.576392, val:  62.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.110041/  2.592339, val:  60.83%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.112060/  2.583003, val:  62.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.112755/  2.590066, val:  63.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.109714/  2.605623, val:  62.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.106437/  2.611040, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.107873/  2.607463, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.115373/  2.642320, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.109781/  2.658440, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.102273/  2.629901, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.103393/  2.690553, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.102118/  2.685138, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.100568/  2.661395, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.103481/  2.670939, val:  63.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.101627/  2.700961, val:  61.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.101405/  2.696190, val:  63.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.100067/  2.727470, val:  61.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.095933/  2.706539, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.093655/  2.721907, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.099413/  2.748005, val:  64.58%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.094968/  2.773345, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.095587/  2.782375, val:  62.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.092902/  2.741035, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.093132/  2.796562, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.093874/  2.790134, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.096858/  2.817907, val:  63.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.094759/  2.808986, val:  60.00%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.099528/  2.798470, val:  63.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.094996/  2.833377, val:  61.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0191406334cb47bfb7f7611a63bf3f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▁▆▆▆▇███████▇███▇█████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▄▇▆█▆▆▇▇▇▆▆▆▅▆▆▆▆▆▆▆▆▅▆▅▅▅▅▄▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▅▆▇▇▇███████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▅▇▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▄▇▆█▆▆▇▇▇▆▆▆▅▆▆▆▆▆▆▆▆▅▆▅▅▅▅▄▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_loss</td><td>▂▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.095</td></tr><tr><td>val_acc_best</td><td>0.7125</td></tr><tr><td>val_acc_now</td><td>0.6125</td></tr><tr><td>val_loss</td><td>2.83338</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">youthful-sweep-18</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8eiacl2y' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8eiacl2y</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_024057-8eiacl2y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4efax901 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fc9c42069a4f1f81ba48fbac0ce309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113001235450308, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_025358-4efax901</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4efax901' target=\"_blank\">light-sweep-19</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4efax901' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4efax901</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.302142/  2.292770, val:  17.50%, val_best:  17.50%, tr:  11.85%, tr_best:  11.85%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.274616/  2.249175, val:  14.17%, val_best:  17.50%, tr:  14.91%, tr_best:  14.91%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.188520/  2.148136, val:  26.25%, val_best:  26.25%, tr:  21.45%, tr_best:  21.45%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.038958/  2.001670, val:  42.92%, val_best:  42.92%, tr:  36.67%, tr_best:  36.67%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  1.860954/  1.858871, val:  45.00%, val_best:  45.00%, tr:  44.84%, tr_best:  44.84%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  1.700459/  1.740926, val:  46.25%, val_best:  46.25%, tr:  48.93%, tr_best:  48.93%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  1.591312/  1.644656, val:  54.58%, val_best:  54.58%, tr:  53.32%, tr_best:  53.32%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  1.484670/  1.585618, val:  52.08%, val_best:  54.58%, tr:  57.92%, tr_best:  57.92%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  1.410789/  1.531228, val:  57.92%, val_best:  57.92%, tr:  59.24%, tr_best:  59.24%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  1.345437/  1.496131, val:  56.25%, val_best:  57.92%, tr:  61.18%, tr_best:  61.18%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  1.290164/  1.461807, val:  59.58%, val_best:  59.58%, tr:  62.51%, tr_best:  62.51%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  1.254397/  1.433046, val:  59.58%, val_best:  59.58%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  1.223191/  1.412655, val:  60.83%, val_best:  60.83%, tr:  64.35%, tr_best:  64.35%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  1.187379/  1.389114, val:  60.83%, val_best:  60.83%, tr:  65.58%, tr_best:  65.58%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  1.152310/  1.370674, val:  63.75%, val_best:  63.75%, tr:  64.96%, tr_best:  65.58%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  1.127652/  1.359826, val:  61.67%, val_best:  63.75%, tr:  65.68%, tr_best:  65.68%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  1.094970/  1.338242, val:  63.33%, val_best:  63.75%, tr:  67.01%, tr_best:  67.01%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  1.085213/  1.328311, val:  65.00%, val_best:  65.00%, tr:  69.46%, tr_best:  69.46%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  1.069139/  1.317235, val:  62.08%, val_best:  65.00%, tr:  67.31%, tr_best:  69.46%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  1.045428/  1.307558, val:  63.33%, val_best:  65.00%, tr:  66.80%, tr_best:  69.46%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  1.030763/  1.296510, val:  62.92%, val_best:  65.00%, tr:  68.44%, tr_best:  69.46%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  1.017659/  1.290068, val:  63.75%, val_best:  65.00%, tr:  69.05%, tr_best:  69.46%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  1.003316/  1.288027, val:  62.08%, val_best:  65.00%, tr:  68.03%, tr_best:  69.46%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  0.988371/  1.284154, val:  63.33%, val_best:  65.00%, tr:  70.28%, tr_best:  70.28%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  0.977394/  1.273975, val:  64.17%, val_best:  65.00%, tr:  70.79%, tr_best:  70.79%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  0.966994/  1.271154, val:  62.08%, val_best:  65.00%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  0.955193/  1.258129, val:  62.08%, val_best:  65.00%, tr:  69.87%, tr_best:  71.40%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  0.944829/  1.253295, val:  62.92%, val_best:  65.00%, tr:  73.24%, tr_best:  73.24%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  0.952827/  1.247361, val:  65.42%, val_best:  65.42%, tr:  71.91%, tr_best:  73.24%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  0.935845/  1.239279, val:  63.33%, val_best:  65.42%, tr:  72.73%, tr_best:  73.24%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  0.924811/  1.237969, val:  63.33%, val_best:  65.42%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  0.916721/  1.236141, val:  62.92%, val_best:  65.42%, tr:  73.14%, tr_best:  74.26%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  0.911296/  1.241770, val:  63.75%, val_best:  65.42%, tr:  73.65%, tr_best:  74.26%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  0.907045/  1.231622, val:  64.17%, val_best:  65.42%, tr:  74.77%, tr_best:  74.77%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  0.892012/  1.218141, val:  65.42%, val_best:  65.42%, tr:  75.69%, tr_best:  75.69%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  0.881941/  1.220003, val:  65.83%, val_best:  65.83%, tr:  74.87%, tr_best:  75.69%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  0.876039/  1.220739, val:  62.92%, val_best:  65.83%, tr:  74.16%, tr_best:  75.69%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  0.869898/  1.209145, val:  66.25%, val_best:  66.25%, tr:  75.69%, tr_best:  75.69%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  0.866369/  1.207828, val:  63.33%, val_best:  66.25%, tr:  75.08%, tr_best:  75.69%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  0.857978/  1.198377, val:  67.08%, val_best:  67.08%, tr:  78.86%, tr_best:  78.86%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  0.845289/  1.200556, val:  68.33%, val_best:  68.33%, tr:  76.81%, tr_best:  78.86%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  0.847943/  1.195830, val:  65.42%, val_best:  68.33%, tr:  79.16%, tr_best:  79.16%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  0.831998/  1.187046, val:  67.50%, val_best:  68.33%, tr:  79.06%, tr_best:  79.16%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  0.826920/  1.194041, val:  67.92%, val_best:  68.33%, tr:  79.78%, tr_best:  79.78%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  0.824664/  1.185120, val:  62.92%, val_best:  68.33%, tr:  78.96%, tr_best:  79.78%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  0.815313/  1.176362, val:  67.92%, val_best:  68.33%, tr:  79.78%, tr_best:  79.78%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  0.813368/  1.174244, val:  67.50%, val_best:  68.33%, tr:  80.80%, tr_best:  80.80%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  0.811784/  1.178197, val:  66.67%, val_best:  68.33%, tr:  79.16%, tr_best:  80.80%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  0.801276/  1.167541, val:  68.33%, val_best:  68.33%, tr:  82.74%, tr_best:  82.74%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  0.792019/  1.170009, val:  68.75%, val_best:  68.75%, tr:  81.92%, tr_best:  82.74%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  0.786673/  1.172692, val:  67.50%, val_best:  68.75%, tr:  83.96%, tr_best:  83.96%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  0.779489/  1.165373, val:  68.75%, val_best:  68.75%, tr:  81.41%, tr_best:  83.96%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  0.770834/  1.168623, val:  66.67%, val_best:  68.75%, tr:  82.84%, tr_best:  83.96%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  0.762587/  1.159545, val:  69.17%, val_best:  69.17%, tr:  83.96%, tr_best:  83.96%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  0.761769/  1.163839, val:  67.50%, val_best:  69.17%, tr:  84.58%, tr_best:  84.58%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  0.755802/  1.156433, val:  69.17%, val_best:  69.17%, tr:  84.37%, tr_best:  84.58%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  0.743023/  1.150416, val:  69.17%, val_best:  69.17%, tr:  85.09%, tr_best:  85.09%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  0.739218/  1.150178, val:  70.42%, val_best:  70.42%, tr:  84.68%, tr_best:  85.09%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  0.728171/  1.149348, val:  70.00%, val_best:  70.42%, tr:  85.90%, tr_best:  85.90%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  0.723866/  1.146077, val:  71.25%, val_best:  71.25%, tr:  86.41%, tr_best:  86.41%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  0.722720/  1.142679, val:  70.00%, val_best:  71.25%, tr:  84.98%, tr_best:  86.41%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  0.718856/  1.150035, val:  70.83%, val_best:  71.25%, tr:  86.11%, tr_best:  86.41%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  0.711630/  1.146558, val:  70.42%, val_best:  71.25%, tr:  87.74%, tr_best:  87.74%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  0.703725/  1.141979, val:  70.00%, val_best:  71.25%, tr:  87.64%, tr_best:  87.74%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  0.696172/  1.148429, val:  70.83%, val_best:  71.25%, tr:  88.25%, tr_best:  88.25%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  0.693712/  1.148266, val:  70.42%, val_best:  71.25%, tr:  88.46%, tr_best:  88.46%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  0.690763/  1.136842, val:  71.67%, val_best:  71.67%, tr:  89.07%, tr_best:  89.07%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  0.690040/  1.141196, val:  71.25%, val_best:  71.67%, tr:  89.38%, tr_best:  89.38%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  0.677286/  1.134375, val:  70.83%, val_best:  71.67%, tr:  87.64%, tr_best:  89.38%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  0.673136/  1.135949, val:  71.25%, val_best:  71.67%, tr:  89.48%, tr_best:  89.48%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  0.667142/  1.141858, val:  70.00%, val_best:  71.67%, tr:  89.38%, tr_best:  89.48%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  0.661767/  1.132458, val:  70.42%, val_best:  71.67%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  0.653818/  1.135332, val:  70.00%, val_best:  71.67%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  0.654392/  1.133207, val:  70.42%, val_best:  71.67%, tr:  90.60%, tr_best:  90.60%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  0.648183/  1.134862, val:  70.83%, val_best:  71.67%, tr:  89.89%, tr_best:  90.60%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  0.643766/  1.125529, val:  69.58%, val_best:  71.67%, tr:  90.09%, tr_best:  90.60%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  0.632964/  1.141836, val:  70.83%, val_best:  71.67%, tr:  90.70%, tr_best:  90.70%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  0.638789/  1.134629, val:  69.17%, val_best:  71.67%, tr:  90.81%, tr_best:  90.81%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  0.627401/  1.133600, val:  70.00%, val_best:  71.67%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  0.625302/  1.147846, val:  68.33%, val_best:  71.67%, tr:  90.91%, tr_best:  91.42%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  0.614644/  1.132837, val:  68.33%, val_best:  71.67%, tr:  91.73%, tr_best:  91.73%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  0.614268/  1.145153, val:  69.17%, val_best:  71.67%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  0.611093/  1.141745, val:  70.42%, val_best:  71.67%, tr:  91.73%, tr_best:  91.83%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  0.608242/  1.137766, val:  70.00%, val_best:  71.67%, tr:  92.13%, tr_best:  92.13%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  0.603311/  1.140328, val:  70.42%, val_best:  71.67%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  0.603594/  1.146532, val:  70.83%, val_best:  71.67%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  0.596042/  1.144303, val:  70.83%, val_best:  71.67%, tr:  92.65%, tr_best:  93.36%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  0.587898/  1.141785, val:  70.00%, val_best:  71.67%, tr:  93.26%, tr_best:  93.36%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  0.584471/  1.154649, val:  70.42%, val_best:  71.67%, tr:  92.95%, tr_best:  93.36%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  0.577401/  1.156894, val:  70.83%, val_best:  71.67%, tr:  93.56%, tr_best:  93.56%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  0.578755/  1.145204, val:  70.83%, val_best:  71.67%, tr:  93.67%, tr_best:  93.67%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  0.566695/  1.144869, val:  71.25%, val_best:  71.67%, tr:  92.95%, tr_best:  93.67%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  0.567781/  1.153391, val:  71.25%, val_best:  71.67%, tr:  93.97%, tr_best:  93.97%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  0.565308/  1.152628, val:  70.83%, val_best:  71.67%, tr:  93.67%, tr_best:  93.97%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  0.564767/  1.157778, val:  70.42%, val_best:  71.67%, tr:  94.08%, tr_best:  94.08%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  0.557496/  1.155664, val:  72.08%, val_best:  72.08%, tr:  94.79%, tr_best:  94.79%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  0.548648/  1.156408, val:  71.25%, val_best:  72.08%, tr:  94.59%, tr_best:  94.79%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  0.545656/  1.154599, val:  72.08%, val_best:  72.08%, tr:  94.48%, tr_best:  94.79%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  0.545612/  1.161429, val:  69.58%, val_best:  72.08%, tr:  94.69%, tr_best:  94.79%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  0.540036/  1.163806, val:  70.83%, val_best:  72.08%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-100 lr=['0.0001000'], tr/val_loss:  0.530264/  1.153566, val:  71.25%, val_best:  72.08%, tr:  95.20%, tr_best:  95.20%\n",
      "epoch-101 lr=['0.0001000'], tr/val_loss:  0.525944/  1.155375, val:  72.08%, val_best:  72.08%, tr:  95.81%, tr_best:  95.81%\n",
      "epoch-102 lr=['0.0001000'], tr/val_loss:  0.527719/  1.167524, val:  72.50%, val_best:  72.50%, tr:  95.61%, tr_best:  95.81%\n",
      "epoch-103 lr=['0.0001000'], tr/val_loss:  0.522705/  1.164091, val:  72.92%, val_best:  72.92%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-104 lr=['0.0001000'], tr/val_loss:  0.517862/  1.164737, val:  72.08%, val_best:  72.92%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-105 lr=['0.0001000'], tr/val_loss:  0.521840/  1.167971, val:  73.33%, val_best:  73.33%, tr:  95.10%, tr_best:  96.02%\n",
      "epoch-106 lr=['0.0001000'], tr/val_loss:  0.513323/  1.161431, val:  72.92%, val_best:  73.33%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-107 lr=['0.0001000'], tr/val_loss:  0.511205/  1.174175, val:  72.50%, val_best:  73.33%, tr:  95.71%, tr_best:  96.02%\n",
      "epoch-108 lr=['0.0001000'], tr/val_loss:  0.500724/  1.178569, val:  72.50%, val_best:  73.33%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-109 lr=['0.0001000'], tr/val_loss:  0.500937/  1.183792, val:  71.25%, val_best:  73.33%, tr:  96.12%, tr_best:  96.22%\n",
      "epoch-110 lr=['0.0001000'], tr/val_loss:  0.497550/  1.182750, val:  70.83%, val_best:  73.33%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-111 lr=['0.0001000'], tr/val_loss:  0.498400/  1.182071, val:  70.83%, val_best:  73.33%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-112 lr=['0.0001000'], tr/val_loss:  0.487142/  1.179567, val:  70.83%, val_best:  73.33%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-113 lr=['0.0001000'], tr/val_loss:  0.490082/  1.185704, val:  71.25%, val_best:  73.33%, tr:  96.02%, tr_best:  96.53%\n",
      "epoch-114 lr=['0.0001000'], tr/val_loss:  0.488945/  1.176863, val:  73.75%, val_best:  73.75%, tr:  96.63%, tr_best:  96.63%\n",
      "epoch-115 lr=['0.0001000'], tr/val_loss:  0.477047/  1.182121, val:  71.25%, val_best:  73.75%, tr:  96.53%, tr_best:  96.63%\n",
      "epoch-116 lr=['0.0001000'], tr/val_loss:  0.476811/  1.182766, val:  70.42%, val_best:  73.75%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-117 lr=['0.0001000'], tr/val_loss:  0.474301/  1.182119, val:  70.42%, val_best:  73.75%, tr:  96.63%, tr_best:  96.83%\n",
      "epoch-118 lr=['0.0001000'], tr/val_loss:  0.472592/  1.184272, val:  70.42%, val_best:  73.75%, tr:  96.53%, tr_best:  96.83%\n",
      "epoch-119 lr=['0.0001000'], tr/val_loss:  0.469817/  1.183022, val:  71.25%, val_best:  73.75%, tr:  96.73%, tr_best:  96.83%\n",
      "epoch-120 lr=['0.0001000'], tr/val_loss:  0.460974/  1.176583, val:  70.83%, val_best:  73.75%, tr:  96.73%, tr_best:  96.83%\n",
      "epoch-121 lr=['0.0001000'], tr/val_loss:  0.461555/  1.186523, val:  69.58%, val_best:  73.75%, tr:  96.73%, tr_best:  96.83%\n",
      "epoch-122 lr=['0.0001000'], tr/val_loss:  0.465535/  1.183163, val:  72.92%, val_best:  73.75%, tr:  96.73%, tr_best:  96.83%\n",
      "epoch-123 lr=['0.0001000'], tr/val_loss:  0.461282/  1.184748, val:  72.92%, val_best:  73.75%, tr:  96.73%, tr_best:  96.83%\n",
      "epoch-124 lr=['0.0001000'], tr/val_loss:  0.451651/  1.179401, val:  71.67%, val_best:  73.75%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-125 lr=['0.0001000'], tr/val_loss:  0.450420/  1.189517, val:  69.58%, val_best:  73.75%, tr:  97.14%, tr_best:  97.24%\n",
      "epoch-126 lr=['0.0001000'], tr/val_loss:  0.456933/  1.188836, val:  72.08%, val_best:  73.75%, tr:  97.04%, tr_best:  97.24%\n",
      "epoch-127 lr=['0.0001000'], tr/val_loss:  0.444299/  1.192324, val:  71.25%, val_best:  73.75%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-128 lr=['0.0001000'], tr/val_loss:  0.439958/  1.185189, val:  71.67%, val_best:  73.75%, tr:  97.04%, tr_best:  97.24%\n",
      "epoch-129 lr=['0.0001000'], tr/val_loss:  0.440528/  1.188643, val:  71.25%, val_best:  73.75%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-130 lr=['0.0001000'], tr/val_loss:  0.438769/  1.187092, val:  71.25%, val_best:  73.75%, tr:  96.83%, tr_best:  97.34%\n",
      "epoch-131 lr=['0.0001000'], tr/val_loss:  0.429390/  1.201319, val:  71.25%, val_best:  73.75%, tr:  97.24%, tr_best:  97.34%\n",
      "epoch-132 lr=['0.0001000'], tr/val_loss:  0.427971/  1.207284, val:  72.92%, val_best:  73.75%, tr:  97.14%, tr_best:  97.34%\n",
      "epoch-133 lr=['0.0001000'], tr/val_loss:  0.424919/  1.201181, val:  71.67%, val_best:  73.75%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-134 lr=['0.0001000'], tr/val_loss:  0.421656/  1.194757, val:  73.75%, val_best:  73.75%, tr:  97.04%, tr_best:  97.34%\n",
      "epoch-135 lr=['0.0001000'], tr/val_loss:  0.420530/  1.196090, val:  70.42%, val_best:  73.75%, tr:  97.14%, tr_best:  97.34%\n",
      "epoch-136 lr=['0.0001000'], tr/val_loss:  0.429383/  1.208787, val:  70.42%, val_best:  73.75%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-137 lr=['0.0001000'], tr/val_loss:  0.428991/  1.198125, val:  71.67%, val_best:  73.75%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-138 lr=['0.0001000'], tr/val_loss:  0.416446/  1.202978, val:  71.25%, val_best:  73.75%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-139 lr=['0.0001000'], tr/val_loss:  0.418230/  1.201660, val:  71.25%, val_best:  73.75%, tr:  97.24%, tr_best:  97.45%\n",
      "epoch-140 lr=['0.0001000'], tr/val_loss:  0.419295/  1.203324, val:  72.92%, val_best:  73.75%, tr:  97.34%, tr_best:  97.45%\n",
      "epoch-141 lr=['0.0001000'], tr/val_loss:  0.414078/  1.200835, val:  71.25%, val_best:  73.75%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-142 lr=['0.0001000'], tr/val_loss:  0.407084/  1.211296, val:  71.67%, val_best:  73.75%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-143 lr=['0.0001000'], tr/val_loss:  0.407880/  1.209798, val:  72.08%, val_best:  73.75%, tr:  97.34%, tr_best:  97.85%\n",
      "epoch-144 lr=['0.0001000'], tr/val_loss:  0.403690/  1.209434, val:  72.08%, val_best:  73.75%, tr:  97.75%, tr_best:  97.85%\n",
      "epoch-145 lr=['0.0001000'], tr/val_loss:  0.403020/  1.212850, val:  71.25%, val_best:  73.75%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-146 lr=['0.0001000'], tr/val_loss:  0.397660/  1.207272, val:  70.83%, val_best:  73.75%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-147 lr=['0.0001000'], tr/val_loss:  0.404088/  1.224833, val:  72.08%, val_best:  73.75%, tr:  97.45%, tr_best:  97.85%\n",
      "epoch-148 lr=['0.0001000'], tr/val_loss:  0.389759/  1.228361, val:  72.50%, val_best:  73.75%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-149 lr=['0.0001000'], tr/val_loss:  0.390212/  1.213908, val:  70.83%, val_best:  73.75%, tr:  97.75%, tr_best:  97.85%\n",
      "epoch-150 lr=['0.0001000'], tr/val_loss:  0.390517/  1.215052, val:  72.92%, val_best:  73.75%, tr:  97.55%, tr_best:  97.85%\n",
      "epoch-151 lr=['0.0001000'], tr/val_loss:  0.385176/  1.225361, val:  72.92%, val_best:  73.75%, tr:  97.65%, tr_best:  97.85%\n",
      "epoch-152 lr=['0.0001000'], tr/val_loss:  0.383686/  1.223996, val:  74.17%, val_best:  74.17%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-153 lr=['0.0001000'], tr/val_loss:  0.387657/  1.233170, val:  72.92%, val_best:  74.17%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-154 lr=['0.0001000'], tr/val_loss:  0.374036/  1.222908, val:  73.33%, val_best:  74.17%, tr:  97.96%, tr_best:  98.06%\n",
      "epoch-155 lr=['0.0001000'], tr/val_loss:  0.374614/  1.220679, val:  72.92%, val_best:  74.17%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-156 lr=['0.0001000'], tr/val_loss:  0.373662/  1.229718, val:  73.75%, val_best:  74.17%, tr:  97.75%, tr_best:  98.06%\n",
      "epoch-157 lr=['0.0001000'], tr/val_loss:  0.369635/  1.236282, val:  71.67%, val_best:  74.17%, tr:  97.96%, tr_best:  98.06%\n",
      "epoch-158 lr=['0.0001000'], tr/val_loss:  0.377688/  1.232670, val:  71.67%, val_best:  74.17%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-159 lr=['0.0001000'], tr/val_loss:  0.368709/  1.225708, val:  72.50%, val_best:  74.17%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-160 lr=['0.0001000'], tr/val_loss:  0.363777/  1.233312, val:  73.33%, val_best:  74.17%, tr:  98.16%, tr_best:  98.67%\n",
      "epoch-161 lr=['0.0001000'], tr/val_loss:  0.360829/  1.227024, val:  73.75%, val_best:  74.17%, tr:  98.26%, tr_best:  98.67%\n",
      "epoch-162 lr=['0.0001000'], tr/val_loss:  0.364085/  1.228820, val:  72.50%, val_best:  74.17%, tr:  98.06%, tr_best:  98.67%\n",
      "epoch-163 lr=['0.0001000'], tr/val_loss:  0.360912/  1.234633, val:  72.50%, val_best:  74.17%, tr:  98.16%, tr_best:  98.67%\n",
      "epoch-164 lr=['0.0001000'], tr/val_loss:  0.357616/  1.244621, val:  73.33%, val_best:  74.17%, tr:  98.26%, tr_best:  98.67%\n",
      "epoch-165 lr=['0.0001000'], tr/val_loss:  0.361598/  1.240143, val:  72.50%, val_best:  74.17%, tr:  98.26%, tr_best:  98.67%\n",
      "epoch-166 lr=['0.0001000'], tr/val_loss:  0.354337/  1.247498, val:  72.92%, val_best:  74.17%, tr:  98.37%, tr_best:  98.67%\n",
      "epoch-167 lr=['0.0001000'], tr/val_loss:  0.353854/  1.242270, val:  72.92%, val_best:  74.17%, tr:  98.26%, tr_best:  98.67%\n",
      "epoch-168 lr=['0.0001000'], tr/val_loss:  0.356041/  1.237049, val:  72.92%, val_best:  74.17%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-169 lr=['0.0001000'], tr/val_loss:  0.343610/  1.243642, val:  73.33%, val_best:  74.17%, tr:  98.47%, tr_best:  98.67%\n",
      "epoch-170 lr=['0.0001000'], tr/val_loss:  0.345786/  1.254685, val:  72.92%, val_best:  74.17%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-171 lr=['0.0001000'], tr/val_loss:  0.345386/  1.237776, val:  72.92%, val_best:  74.17%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-172 lr=['0.0001000'], tr/val_loss:  0.343389/  1.242172, val:  72.92%, val_best:  74.17%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-173 lr=['0.0001000'], tr/val_loss:  0.341070/  1.252067, val:  73.75%, val_best:  74.17%, tr:  98.47%, tr_best:  98.67%\n",
      "epoch-174 lr=['0.0001000'], tr/val_loss:  0.334202/  1.242989, val:  73.33%, val_best:  74.17%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-175 lr=['0.0001000'], tr/val_loss:  0.333973/  1.244369, val:  73.75%, val_best:  74.17%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-176 lr=['0.0001000'], tr/val_loss:  0.332637/  1.244860, val:  73.33%, val_best:  74.17%, tr:  98.57%, tr_best:  98.98%\n",
      "epoch-177 lr=['0.0001000'], tr/val_loss:  0.334604/  1.248423, val:  73.75%, val_best:  74.17%, tr:  98.88%, tr_best:  98.98%\n",
      "epoch-178 lr=['0.0001000'], tr/val_loss:  0.331831/  1.245052, val:  72.50%, val_best:  74.17%, tr:  98.47%, tr_best:  98.98%\n",
      "epoch-179 lr=['0.0001000'], tr/val_loss:  0.333521/  1.249092, val:  72.50%, val_best:  74.17%, tr:  98.77%, tr_best:  98.98%\n",
      "epoch-180 lr=['0.0001000'], tr/val_loss:  0.325570/  1.249114, val:  73.75%, val_best:  74.17%, tr:  98.67%, tr_best:  98.98%\n",
      "epoch-181 lr=['0.0001000'], tr/val_loss:  0.327582/  1.255226, val:  70.83%, val_best:  74.17%, tr:  98.67%, tr_best:  98.98%\n",
      "epoch-182 lr=['0.0001000'], tr/val_loss:  0.324125/  1.253312, val:  72.50%, val_best:  74.17%, tr:  98.77%, tr_best:  98.98%\n",
      "epoch-183 lr=['0.0001000'], tr/val_loss:  0.325783/  1.249915, val:  72.50%, val_best:  74.17%, tr:  98.67%, tr_best:  98.98%\n",
      "epoch-184 lr=['0.0001000'], tr/val_loss:  0.322757/  1.261129, val:  72.50%, val_best:  74.17%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-185 lr=['0.0001000'], tr/val_loss:  0.317614/  1.259861, val:  73.33%, val_best:  74.17%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-186 lr=['0.0001000'], tr/val_loss:  0.319438/  1.264876, val:  72.92%, val_best:  74.17%, tr:  98.77%, tr_best:  98.98%\n",
      "epoch-187 lr=['0.0001000'], tr/val_loss:  0.321099/  1.261585, val:  72.92%, val_best:  74.17%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-188 lr=['0.0001000'], tr/val_loss:  0.317100/  1.266074, val:  72.50%, val_best:  74.17%, tr:  98.88%, tr_best:  98.98%\n",
      "epoch-189 lr=['0.0001000'], tr/val_loss:  0.313948/  1.266017, val:  73.33%, val_best:  74.17%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-190 lr=['0.0001000'], tr/val_loss:  0.312408/  1.260939, val:  73.33%, val_best:  74.17%, tr:  98.77%, tr_best:  99.18%\n",
      "epoch-191 lr=['0.0001000'], tr/val_loss:  0.313663/  1.266687, val:  72.92%, val_best:  74.17%, tr:  99.08%, tr_best:  99.18%\n",
      "epoch-192 lr=['0.0001000'], tr/val_loss:  0.308304/  1.265688, val:  73.33%, val_best:  74.17%, tr:  99.08%, tr_best:  99.18%\n",
      "epoch-193 lr=['0.0001000'], tr/val_loss:  0.306328/  1.267938, val:  72.92%, val_best:  74.17%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-194 lr=['0.0001000'], tr/val_loss:  0.307996/  1.271372, val:  72.50%, val_best:  74.17%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-195 lr=['0.0001000'], tr/val_loss:  0.308987/  1.258112, val:  72.50%, val_best:  74.17%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-196 lr=['0.0001000'], tr/val_loss:  0.304918/  1.259429, val:  72.50%, val_best:  74.17%, tr:  99.18%, tr_best:  99.28%\n",
      "epoch-197 lr=['0.0001000'], tr/val_loss:  0.300432/  1.251027, val:  72.92%, val_best:  74.17%, tr:  99.18%, tr_best:  99.28%\n",
      "epoch-198 lr=['0.0001000'], tr/val_loss:  0.304275/  1.248596, val:  73.33%, val_best:  74.17%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-199 lr=['0.0001000'], tr/val_loss:  0.296144/  1.259690, val:  73.33%, val_best:  74.17%, tr:  99.39%, tr_best:  99.39%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21420095d5a6451c80d0d718a94ebf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▅▅▃▅▅▆▇▇▅▅▅▇▇█▆▇█████▇▇▇▇▇██▇████▇████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▇▇▇▇▇▇▇▇▇████▇███████████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▇▇▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▇▇▇▇▇▇▇▇▇████▇███████████████████████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99387</td></tr><tr><td>tr_epoch_loss</td><td>0.29614</td></tr><tr><td>val_acc_best</td><td>0.74167</td></tr><tr><td>val_acc_now</td><td>0.73333</td></tr><tr><td>val_loss</td><td>1.25969</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">light-sweep-19</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4efax901' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4efax901</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_025358-4efax901/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: shk9kpdh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_030656-shk9kpdh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/shk9kpdh' target=\"_blank\">wise-sweep-20</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/shk9kpdh' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/shk9kpdh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  2.448219/  4.092346, val:  42.50%, val_best:  42.50%, tr:  35.75%, tr_best:  35.75%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  2.952480/  5.153693, val:  18.75%, val_best:  42.50%, tr:  36.36%, tr_best:  36.36%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  3.461688/  2.607175, val:  20.00%, val_best:  42.50%, tr:  27.48%, tr_best:  36.36%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  3.610590/  3.153970, val:  27.50%, val_best:  42.50%, tr:  28.80%, tr_best:  36.36%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  3.327405/  2.512822, val:  25.00%, val_best:  42.50%, tr:  25.64%, tr_best:  36.36%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  3.300317/  3.440423, val:  19.17%, val_best:  42.50%, tr:  27.17%, tr_best:  36.36%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  2.877478/  2.755845, val:  28.75%, val_best:  42.50%, tr:  27.89%, tr_best:  36.36%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  3.828017/  3.737038, val:  20.00%, val_best:  42.50%, tr:  24.51%, tr_best:  36.36%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  3.551816/  4.066401, val:  25.00%, val_best:  42.50%, tr:  25.54%, tr_best:  36.36%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  3.170838/  4.107774, val:  27.08%, val_best:  42.50%, tr:  27.68%, tr_best:  36.36%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  4.343871/  4.113514, val:  23.75%, val_best:  42.50%, tr:  23.49%, tr_best:  36.36%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  4.316092/  2.725493, val:  27.08%, val_best:  42.50%, tr:  23.29%, tr_best:  36.36%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  3.400550/  3.105755, val:  29.17%, val_best:  42.50%, tr:  24.31%, tr_best:  36.36%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  4.057975/  4.617524, val:  20.42%, val_best:  42.50%, tr:  25.64%, tr_best:  36.36%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  3.211210/  5.255288, val:  26.25%, val_best:  42.50%, tr:  27.17%, tr_best:  36.36%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  3.882902/  3.808830, val:  25.42%, val_best:  42.50%, tr:  26.15%, tr_best:  36.36%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  4.040216/  4.172449, val:  27.50%, val_best:  42.50%, tr:  25.13%, tr_best:  36.36%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  4.148850/  3.401636, val:  28.75%, val_best:  42.50%, tr:  25.74%, tr_best:  36.36%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  3.140374/  4.664324, val:  32.08%, val_best:  42.50%, tr:  26.15%, tr_best:  36.36%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  4.594037/  6.148160, val:  18.75%, val_best:  42.50%, tr:  27.17%, tr_best:  36.36%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  4.768002/  4.412919, val:  30.00%, val_best:  42.50%, tr:  24.41%, tr_best:  36.36%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  3.900442/  4.360536, val:  22.92%, val_best:  42.50%, tr:  25.74%, tr_best:  36.36%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  4.401895/  3.388826, val:  24.17%, val_best:  42.50%, tr:  26.46%, tr_best:  36.36%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  3.931901/  3.370629, val:  27.08%, val_best:  42.50%, tr:  29.62%, tr_best:  36.36%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  3.403163/  3.575389, val:  24.58%, val_best:  42.50%, tr:  28.80%, tr_best:  36.36%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  4.216846/  4.338555, val:  25.83%, val_best:  42.50%, tr:  24.31%, tr_best:  36.36%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  3.431473/  3.546727, val:  29.58%, val_best:  42.50%, tr:  25.43%, tr_best:  36.36%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  3.226142/  3.463448, val:  30.42%, val_best:  42.50%, tr:  25.23%, tr_best:  36.36%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  4.176338/  4.432234, val:  29.17%, val_best:  42.50%, tr:  28.09%, tr_best:  36.36%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  4.381275/  2.954366, val:  10.00%, val_best:  42.50%, tr:  18.49%, tr_best:  36.36%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  3.805373/  4.051086, val:  16.25%, val_best:  42.50%, tr:  16.75%, tr_best:  36.36%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  4.736128/  4.992465, val:  16.25%, val_best:  42.50%, tr:  17.26%, tr_best:  36.36%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  5.089223/  4.404470, val:  25.00%, val_best:  42.50%, tr:  17.88%, tr_best:  36.36%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  4.133123/  4.381704, val:  20.83%, val_best:  42.50%, tr:  19.00%, tr_best:  36.36%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  4.533505/  8.795813, val:  15.00%, val_best:  42.50%, tr:  19.51%, tr_best:  36.36%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  4.699578/  3.974878, val:  14.58%, val_best:  42.50%, tr:  18.59%, tr_best:  36.36%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  3.576538/  3.786849, val:  20.00%, val_best:  42.50%, tr:  19.61%, tr_best:  36.36%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  4.164759/  5.974380, val:  10.00%, val_best:  42.50%, tr:  18.69%, tr_best:  36.36%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  4.149957/  3.478309, val:  19.17%, val_best:  42.50%, tr:  18.18%, tr_best:  36.36%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  3.826983/  3.073600, val:  27.92%, val_best:  42.50%, tr:  17.88%, tr_best:  36.36%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  3.242475/  3.353849, val:  20.00%, val_best:  42.50%, tr:  21.96%, tr_best:  36.36%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  4.206542/  4.019656, val:  20.00%, val_best:  42.50%, tr:  19.51%, tr_best:  36.36%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  4.552932/  5.070785, val:  10.00%, val_best:  42.50%, tr:  18.39%, tr_best:  36.36%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  6.321408/  9.102879, val:  19.17%, val_best:  42.50%, tr:  17.67%, tr_best:  36.36%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  4.589480/  3.107387, val:  18.75%, val_best:  42.50%, tr:  18.08%, tr_best:  36.36%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  3.780438/  5.024476, val:  10.00%, val_best:  42.50%, tr:  18.08%, tr_best:  36.36%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  4.238036/  6.550352, val:  10.00%, val_best:  42.50%, tr:  18.90%, tr_best:  36.36%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  4.480468/  4.224965, val:  19.17%, val_best:  42.50%, tr:  20.33%, tr_best:  36.36%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  3.469593/  5.706103, val:  11.67%, val_best:  42.50%, tr:  20.22%, tr_best:  36.36%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  5.259829/  5.092421, val:  20.00%, val_best:  42.50%, tr:  16.65%, tr_best:  36.36%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  4.654817/  9.121492, val:  15.83%, val_best:  42.50%, tr:  18.28%, tr_best:  36.36%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  4.498508/  6.524764, val:  10.00%, val_best:  42.50%, tr:  16.45%, tr_best:  36.36%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  5.055273/  4.074111, val:  21.67%, val_best:  42.50%, tr:  19.00%, tr_best:  36.36%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  4.166047/  6.030861, val:  12.08%, val_best:  42.50%, tr:  19.51%, tr_best:  36.36%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  4.485622/  2.970524, val:  18.33%, val_best:  42.50%, tr:  19.20%, tr_best:  36.36%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  4.036369/  2.869370, val:  18.75%, val_best:  42.50%, tr:  18.59%, tr_best:  36.36%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  4.043991/  7.542554, val:  20.00%, val_best:  42.50%, tr:  20.12%, tr_best:  36.36%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  4.342535/  4.476575, val:  18.75%, val_best:  42.50%, tr:  19.41%, tr_best:  36.36%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  3.916110/  4.524834, val:  20.83%, val_best:  42.50%, tr:  18.18%, tr_best:  36.36%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  3.920027/  6.179045, val:  18.33%, val_best:  42.50%, tr:  22.68%, tr_best:  36.36%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  4.243494/  3.489806, val:  19.58%, val_best:  42.50%, tr:  18.90%, tr_best:  36.36%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  4.352355/  5.513671, val:  20.00%, val_best:  42.50%, tr:  19.00%, tr_best:  36.36%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  5.177906/  7.660593, val:  11.25%, val_best:  42.50%, tr:  17.26%, tr_best:  36.36%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  5.491536/  4.146752, val:  20.00%, val_best:  42.50%, tr:  16.65%, tr_best:  36.36%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  3.791927/  4.326230, val:  24.58%, val_best:  42.50%, tr:  22.37%, tr_best:  36.36%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  3.521171/  4.103807, val:  18.75%, val_best:  42.50%, tr:  20.63%, tr_best:  36.36%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  3.301724/  4.665936, val:  17.08%, val_best:  42.50%, tr:  20.22%, tr_best:  36.36%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  4.021813/  3.796932, val:  16.25%, val_best:  42.50%, tr:  18.49%, tr_best:  36.36%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  3.763664/  3.643310, val:  19.17%, val_best:  42.50%, tr:  18.08%, tr_best:  36.36%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  4.615980/  7.187016, val:  20.00%, val_best:  42.50%, tr:  16.65%, tr_best:  36.36%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  4.728230/  5.808611, val:  19.58%, val_best:  42.50%, tr:  21.96%, tr_best:  36.36%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  3.642747/  3.902295, val:  28.33%, val_best:  42.50%, tr:  27.99%, tr_best:  36.36%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  3.663824/  4.398776, val:  25.00%, val_best:  42.50%, tr:  25.74%, tr_best:  36.36%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  4.032463/  3.021630, val:  32.50%, val_best:  42.50%, tr:  25.13%, tr_best:  36.36%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  3.185339/  4.297225, val:  20.42%, val_best:  42.50%, tr:  27.17%, tr_best:  36.36%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  3.618440/  2.953925, val:  25.42%, val_best:  42.50%, tr:  28.60%, tr_best:  36.36%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  3.375843/  3.349464, val:  24.58%, val_best:  42.50%, tr:  25.64%, tr_best:  36.36%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  3.180921/  5.686557, val:  20.00%, val_best:  42.50%, tr:  28.29%, tr_best:  36.36%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  3.796919/  5.124572, val:  22.08%, val_best:  42.50%, tr:  31.36%, tr_best:  36.36%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  3.714303/  5.147515, val:  30.00%, val_best:  42.50%, tr:  30.03%, tr_best:  36.36%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  3.938396/  4.258386, val:  29.17%, val_best:  42.50%, tr:  25.84%, tr_best:  36.36%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  3.596904/  5.007172, val:  20.00%, val_best:  42.50%, tr:  26.76%, tr_best:  36.36%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  3.856385/  3.929132, val:  29.17%, val_best:  42.50%, tr:  27.07%, tr_best:  36.36%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  3.594746/  4.426995, val:  27.92%, val_best:  42.50%, tr:  25.54%, tr_best:  36.36%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  4.437721/  3.770720, val:  26.25%, val_best:  42.50%, tr:  27.68%, tr_best:  36.36%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  3.498287/  5.123460, val:  22.08%, val_best:  42.50%, tr:  26.97%, tr_best:  36.36%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  3.969364/  3.765904, val:  28.75%, val_best:  42.50%, tr:  25.84%, tr_best:  36.36%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  3.205201/  3.222533, val:  29.58%, val_best:  42.50%, tr:  25.43%, tr_best:  36.36%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  3.493572/  2.922624, val:  30.00%, val_best:  42.50%, tr:  28.50%, tr_best:  36.36%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  2.955672/  3.494520, val:  32.92%, val_best:  42.50%, tr:  29.62%, tr_best:  36.36%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  3.376311/  4.020737, val:  29.17%, val_best:  42.50%, tr:  29.83%, tr_best:  36.36%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  3.489762/  3.695497, val:  15.42%, val_best:  42.50%, tr:  24.31%, tr_best:  36.36%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  3.505622/  3.943834, val:  17.50%, val_best:  42.50%, tr:  17.36%, tr_best:  36.36%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  3.220919/  4.160336, val:  18.75%, val_best:  42.50%, tr:  20.12%, tr_best:  36.36%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  3.759054/  4.764435, val:  18.75%, val_best:  42.50%, tr:  21.55%, tr_best:  36.36%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  3.770270/  4.336833, val:  19.58%, val_best:  42.50%, tr:  16.75%, tr_best:  36.36%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  3.731904/  2.867257, val:  18.33%, val_best:  42.50%, tr:  19.00%, tr_best:  36.36%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  4.308729/  2.735834, val:  19.58%, val_best:  42.50%, tr:  19.00%, tr_best:  36.36%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  3.805139/  3.939532, val:  18.33%, val_best:  42.50%, tr:  16.14%, tr_best:  36.36%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  4.301467/  3.533271, val:  23.33%, val_best:  42.50%, tr:  18.39%, tr_best:  36.36%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss:  3.642200/  3.812322, val:  10.00%, val_best:  42.50%, tr:  18.18%, tr_best:  36.36%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss:  4.204004/  3.027095, val:  19.17%, val_best:  42.50%, tr:  18.39%, tr_best:  36.36%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss:  3.391821/  5.656765, val:  18.33%, val_best:  42.50%, tr:  18.28%, tr_best:  36.36%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss:  4.142138/  4.539307, val:  18.75%, val_best:  42.50%, tr:  19.20%, tr_best:  36.36%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss:  3.669509/  4.863477, val:  20.00%, val_best:  42.50%, tr:  17.98%, tr_best:  36.36%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss:  3.869252/  4.284205, val:  16.25%, val_best:  42.50%, tr:  19.10%, tr_best:  36.36%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss:  4.152263/  5.539056, val:  14.58%, val_best:  42.50%, tr:  16.96%, tr_best:  36.36%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss:  4.500976/  3.858552, val:  15.00%, val_best:  42.50%, tr:  21.65%, tr_best:  36.36%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss:  3.844345/  4.578251, val:  20.00%, val_best:  42.50%, tr:  19.61%, tr_best:  36.36%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss:  3.734558/  3.659852, val:  24.17%, val_best:  42.50%, tr:  19.20%, tr_best:  36.36%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss:  3.332617/  2.860888, val:  23.75%, val_best:  42.50%, tr:  16.85%, tr_best:  36.36%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss:  3.386670/  3.032499, val:  18.75%, val_best:  42.50%, tr:  19.61%, tr_best:  36.36%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss:  3.834901/  4.125380, val:  20.42%, val_best:  42.50%, tr:  17.77%, tr_best:  36.36%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss:  3.759076/  4.344411, val:  18.75%, val_best:  42.50%, tr:  20.33%, tr_best:  36.36%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss:  3.966530/  3.891530, val:  15.42%, val_best:  42.50%, tr:  16.65%, tr_best:  36.36%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss:  3.597954/  4.556252, val:  14.17%, val_best:  42.50%, tr:  20.12%, tr_best:  36.36%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss:  4.116366/  3.997388, val:  20.00%, val_best:  42.50%, tr:  17.16%, tr_best:  36.36%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss:  3.917294/  4.259533, val:  19.17%, val_best:  42.50%, tr:  18.39%, tr_best:  36.36%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss:  3.548780/  3.646471, val:  18.33%, val_best:  42.50%, tr:  20.53%, tr_best:  36.36%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss:  3.555454/  3.657199, val:  25.42%, val_best:  42.50%, tr:  21.65%, tr_best:  36.36%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss:  3.742631/  5.879096, val:  17.08%, val_best:  42.50%, tr:  26.15%, tr_best:  36.36%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss:  3.460870/  6.313023, val:  25.42%, val_best:  42.50%, tr:  26.56%, tr_best:  36.36%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss:  3.667599/  3.629804, val:  25.42%, val_best:  42.50%, tr:  25.13%, tr_best:  36.36%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss:  4.372660/  4.303949, val:  34.17%, val_best:  42.50%, tr:  26.86%, tr_best:  36.36%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss:  3.294068/  4.167262, val:  20.00%, val_best:  42.50%, tr:  27.78%, tr_best:  36.36%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss:  3.140451/  3.780231, val:  29.58%, val_best:  42.50%, tr:  29.72%, tr_best:  36.36%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss:  2.832121/  4.837486, val:  30.00%, val_best:  42.50%, tr:  25.84%, tr_best:  36.36%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss:  3.910437/  3.279099, val:  31.67%, val_best:  42.50%, tr:  24.21%, tr_best:  36.36%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss:  4.339915/  4.475148, val:  19.17%, val_best:  42.50%, tr:  27.17%, tr_best:  36.36%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss:  3.425368/  2.667244, val:  29.17%, val_best:  42.50%, tr:  27.99%, tr_best:  36.36%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss:  3.696156/  3.470870, val:  29.17%, val_best:  42.50%, tr:  25.13%, tr_best:  36.36%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss:  3.592852/  4.374283, val:  21.25%, val_best:  42.50%, tr:  26.05%, tr_best:  36.36%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss:  3.357623/  5.188552, val:  20.42%, val_best:  42.50%, tr:  26.97%, tr_best:  36.36%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss:  3.164974/  3.320058, val:  29.17%, val_best:  42.50%, tr:  27.99%, tr_best:  36.36%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss:  3.856383/  4.587616, val:  20.00%, val_best:  42.50%, tr:  27.27%, tr_best:  36.36%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss:  3.600780/  4.368287, val:  25.83%, val_best:  42.50%, tr:  27.89%, tr_best:  36.36%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss:  4.064199/  4.054301, val:  30.00%, val_best:  42.50%, tr:  26.25%, tr_best:  36.36%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss:  3.447671/  5.394406, val:  28.75%, val_best:  42.50%, tr:  25.33%, tr_best:  36.36%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss:  4.225642/  3.498075, val:  35.42%, val_best:  42.50%, tr:  25.74%, tr_best:  36.36%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss:  3.212847/  3.830050, val:  27.50%, val_best:  42.50%, tr:  27.68%, tr_best:  36.36%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss:  3.552531/  3.928660, val:  18.75%, val_best:  42.50%, tr:  27.89%, tr_best:  36.36%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss:  3.610167/  4.398098, val:  32.50%, val_best:  42.50%, tr:  26.05%, tr_best:  36.36%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss:  3.715254/  5.078954, val:  19.17%, val_best:  42.50%, tr:  26.35%, tr_best:  36.36%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss:  3.948337/  6.781703, val:  18.33%, val_best:  42.50%, tr:  24.21%, tr_best:  36.36%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss:  4.795432/  4.618850, val:  20.83%, val_best:  42.50%, tr:  25.74%, tr_best:  36.36%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss:  3.597454/  5.959017, val:  19.17%, val_best:  42.50%, tr:  27.99%, tr_best:  36.36%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss:  3.538336/  3.710361, val:  27.92%, val_best:  42.50%, tr:  26.66%, tr_best:  36.36%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss:  3.480946/  4.123700, val:  29.17%, val_best:  42.50%, tr:  29.32%, tr_best:  36.36%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss:  4.152401/  5.466401, val:  24.17%, val_best:  42.50%, tr:  27.27%, tr_best:  36.36%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss:  4.069608/  5.798045, val:  29.17%, val_best:  42.50%, tr:  25.13%, tr_best:  36.36%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss:  4.202197/  6.684723, val:  10.00%, val_best:  42.50%, tr:  25.03%, tr_best:  36.36%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss:  4.929594/  4.671206, val:  22.92%, val_best:  42.50%, tr:  25.23%, tr_best:  36.36%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss:  3.807183/  4.912701, val:  27.50%, val_best:  42.50%, tr:  23.70%, tr_best:  36.36%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss:  3.667783/  5.348293, val:  10.00%, val_best:  42.50%, tr:  28.40%, tr_best:  36.36%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss:  3.576412/  3.782683, val:  28.75%, val_best:  42.50%, tr:  26.97%, tr_best:  36.36%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss:  3.924393/  3.800952, val:  30.42%, val_best:  42.50%, tr:  26.56%, tr_best:  36.36%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss:  3.522265/  3.653390, val:  36.25%, val_best:  42.50%, tr:  26.76%, tr_best:  36.36%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss:  4.337602/  3.750204, val:  29.17%, val_best:  42.50%, tr:  27.68%, tr_best:  36.36%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss:  3.404971/  6.187933, val:  21.67%, val_best:  42.50%, tr:  29.42%, tr_best:  36.36%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss:  3.336682/  3.443847, val:  22.92%, val_best:  42.50%, tr:  26.97%, tr_best:  36.36%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss:  3.081844/  3.427920, val:  29.17%, val_best:  42.50%, tr:  28.09%, tr_best:  36.36%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss:  3.414018/  2.945932, val:  22.08%, val_best:  42.50%, tr:  26.46%, tr_best:  36.36%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss:  2.914556/  3.726068, val:  26.67%, val_best:  42.50%, tr:  28.09%, tr_best:  36.36%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss:  3.337454/  3.023097, val:  30.00%, val_best:  42.50%, tr:  28.50%, tr_best:  36.36%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss:  3.364919/  5.627576, val:  15.83%, val_best:  42.50%, tr:  25.03%, tr_best:  36.36%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss:  3.577626/  4.077771, val:  26.67%, val_best:  42.50%, tr:  27.27%, tr_best:  36.36%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss:  4.026039/  3.965615, val:  19.17%, val_best:  42.50%, tr:  27.68%, tr_best:  36.36%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss:  4.081036/  5.481442, val:  28.75%, val_best:  42.50%, tr:  24.92%, tr_best:  36.36%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss:  4.632263/  6.040407, val:  21.67%, val_best:  42.50%, tr:  25.94%, tr_best:  36.36%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss:  4.127551/  4.488836, val:  22.92%, val_best:  42.50%, tr:  28.29%, tr_best:  36.36%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss:  3.365601/  3.853552, val:  31.25%, val_best:  42.50%, tr:  25.74%, tr_best:  36.36%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss:  3.087733/  2.221102, val:  19.17%, val_best:  42.50%, tr:  27.89%, tr_best:  36.36%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss:  2.922681/  2.580759, val:  29.17%, val_best:  42.50%, tr:  28.09%, tr_best:  36.36%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss:  3.147510/  6.227427, val:  21.67%, val_best:  42.50%, tr:  26.56%, tr_best:  36.36%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss:  4.363399/  2.819830, val:  32.50%, val_best:  42.50%, tr:  26.76%, tr_best:  36.36%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss:  3.369464/  6.068998, val:  29.58%, val_best:  42.50%, tr:  26.35%, tr_best:  36.36%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss:  3.677335/  3.456371, val:  29.17%, val_best:  42.50%, tr:  28.70%, tr_best:  36.36%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss:  3.627215/  3.539315, val:  26.67%, val_best:  42.50%, tr:  26.86%, tr_best:  36.36%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss:  2.710206/  2.933482, val:  30.00%, val_best:  42.50%, tr:  29.01%, tr_best:  36.36%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss:  3.017267/  2.560407, val:  30.42%, val_best:  42.50%, tr:  26.25%, tr_best:  36.36%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss:  2.983324/  3.781522, val:  31.67%, val_best:  42.50%, tr:  29.42%, tr_best:  36.36%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss:  3.107763/  3.925239, val:  28.33%, val_best:  42.50%, tr:  26.97%, tr_best:  36.36%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss:  3.180980/  5.328767, val:  20.83%, val_best:  42.50%, tr:  25.33%, tr_best:  36.36%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss:  3.638821/  4.151760, val:  30.83%, val_best:  42.50%, tr:  25.84%, tr_best:  36.36%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss:  3.664747/  3.625880, val:  22.50%, val_best:  42.50%, tr:  28.29%, tr_best:  36.36%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss:  3.927507/  3.543137, val:  28.75%, val_best:  42.50%, tr:  23.90%, tr_best:  36.36%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss:  3.399024/  3.937625, val:  30.83%, val_best:  42.50%, tr:  27.89%, tr_best:  36.36%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss:  3.630013/  3.537416, val:  23.33%, val_best:  42.50%, tr:  26.05%, tr_best:  36.36%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss:  3.595032/  3.468686, val:  27.50%, val_best:  42.50%, tr:  26.46%, tr_best:  36.36%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss:  4.221743/  4.852005, val:  19.17%, val_best:  42.50%, tr:  27.17%, tr_best:  36.36%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss:  3.988936/  3.444945, val:  20.00%, val_best:  42.50%, tr:  27.68%, tr_best:  36.36%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss:  4.236383/  8.113465, val:  19.17%, val_best:  42.50%, tr:  27.89%, tr_best:  36.36%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss:  4.540973/  2.827081, val:  30.00%, val_best:  42.50%, tr:  26.76%, tr_best:  36.36%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss:  3.517788/  5.273194, val:  20.83%, val_best:  42.50%, tr:  27.37%, tr_best:  36.36%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss:  3.274349/  3.235833, val:  26.67%, val_best:  42.50%, tr:  28.29%, tr_best:  36.36%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss:  3.367996/  6.181247, val:  10.00%, val_best:  42.50%, tr:  23.70%, tr_best:  36.36%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss:  4.136074/  5.801286, val:  28.75%, val_best:  42.50%, tr:  26.05%, tr_best:  36.36%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss:  4.158184/  2.549938, val:  30.00%, val_best:  42.50%, tr:  26.66%, tr_best:  36.36%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss:  3.550944/  4.027351, val:  25.42%, val_best:  42.50%, tr:  28.29%, tr_best:  36.36%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss:  3.317745/  5.404030, val:  28.33%, val_best:  42.50%, tr:  24.92%, tr_best:  36.36%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c017f6d8cf3437484a332e7e6b40d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▅▇▄▅█▄▃▄▅▁▃▅▄▅▆▆▄▄▅▃▄▄▅▂█▂▆▆▇▅▆▅▆▅▆▆▅▆▆</td></tr><tr><td>summary_val_acc</td><td>▄▄▆▆▄▆▁▄▄▄▃▄▄▄▇▆▇▅█▄▁▂▅▄▃▄▄▆█▄▅▇▇▄▇▇█▇▄▁</td></tr><tr><td>tr_acc</td><td>█▅▅▄▅▄▂▂▃▁▂▂▂▂▅▅▆▅▆▂▁▁▁▁▄▅▄▅▄▅▄▄▅▅▄▅▆▅▅▃</td></tr><tr><td>tr_epoch_loss</td><td>▁▂▂▄▇▅▆▃▂▇▇▆▆▃▃▃▄▃▁▄▃▅▂▅▄▂▃▃▃▃█▄▁▅▂▄▁▃▆▂</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_now</td><td>▄▄▆▆▄▆▁▄▄▄▃▄▄▄▇▆▇▅█▄▁▂▅▄▃▄▄▆█▄▅▇▇▄▇▇█▇▄▁</td></tr><tr><td>val_loss</td><td>▄▂▂▂▅▃▁▂▂▁█▁▂▂▂▁▄▄▂▁▂▄▁▂▄▂▃▃▃▄▃▂▂▂▂▂▂▂▇▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.33333</td></tr><tr><td>tr_acc</td><td>0.24923</td></tr><tr><td>tr_epoch_loss</td><td>3.31774</td></tr><tr><td>val_acc_best</td><td>0.425</td></tr><tr><td>val_acc_now</td><td>0.28333</td></tr><tr><td>val_loss</td><td>5.40403</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wise-sweep-20</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/shk9kpdh' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/shk9kpdh</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_030656-shk9kpdh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rakrvs5e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36d29d3110e4dedb136d4d8e53f778e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113030121972163, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_031828-rakrvs5e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rakrvs5e' target=\"_blank\">atomic-sweep-21</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rakrvs5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rakrvs5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.650523/  1.809633, val:  50.42%, val_best:  50.42%, tr:  46.37%, tr_best:  46.37%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.224860/  1.546530, val:  47.92%, val_best:  50.42%, tr:  57.61%, tr_best:  57.61%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.031689/  1.316883, val:  60.42%, val_best:  60.42%, tr:  62.31%, tr_best:  62.31%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.857369/  1.568385, val:  54.17%, val_best:  60.42%, tr:  69.56%, tr_best:  69.56%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.809764/  1.429737, val:  61.67%, val_best:  61.67%, tr:  70.99%, tr_best:  70.99%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.705702/  1.412422, val:  55.00%, val_best:  61.67%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.682620/  1.295533, val:  64.58%, val_best:  64.58%, tr:  75.49%, tr_best:  75.49%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.655775/  1.454656, val:  57.08%, val_best:  64.58%, tr:  77.32%, tr_best:  77.32%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.593926/  1.396330, val:  65.00%, val_best:  65.00%, tr:  80.18%, tr_best:  80.18%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.528461/  1.668303, val:  62.92%, val_best:  65.00%, tr:  83.96%, tr_best:  83.96%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.536107/  1.440842, val:  63.75%, val_best:  65.00%, tr:  83.96%, tr_best:  83.96%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.452969/  1.614856, val:  67.50%, val_best:  67.50%, tr:  87.23%, tr_best:  87.23%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.400621/  1.482475, val:  75.00%, val_best:  75.00%, tr:  89.58%, tr_best:  89.58%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.401059/  1.571544, val:  67.50%, val_best:  75.00%, tr:  88.46%, tr_best:  89.58%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.325718/  1.856637, val:  72.08%, val_best:  75.00%, tr:  93.16%, tr_best:  93.16%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.338218/  1.765587, val:  75.00%, val_best:  75.00%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.345111/  1.987395, val:  69.58%, val_best:  75.00%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.390009/  1.878539, val:  73.33%, val_best:  75.00%, tr:  93.05%, tr_best:  94.99%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.312552/  1.926844, val:  69.17%, val_best:  75.00%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.343754/  2.175007, val:  65.00%, val_best:  75.00%, tr:  95.10%, tr_best:  95.71%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.312918/  1.844460, val:  70.42%, val_best:  75.00%, tr:  94.89%, tr_best:  95.71%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.276978/  2.247390, val:  66.67%, val_best:  75.00%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.269056/  2.056133, val:  74.17%, val_best:  75.00%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.280359/  2.259836, val:  69.58%, val_best:  75.00%, tr:  96.02%, tr_best:  97.85%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.244569/  2.006345, val:  75.42%, val_best:  75.42%, tr:  97.75%, tr_best:  97.85%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.245305/  2.222386, val:  72.50%, val_best:  75.42%, tr:  97.75%, tr_best:  97.85%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.222881/  2.530860, val:  68.75%, val_best:  75.42%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.205459/  2.064229, val:  77.50%, val_best:  77.50%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.201109/  2.238928, val:  78.33%, val_best:  78.33%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.184172/  2.464322, val:  71.67%, val_best:  78.33%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.190229/  2.438641, val:  68.75%, val_best:  78.33%, tr:  98.77%, tr_best:  98.98%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.183502/  2.306172, val:  76.25%, val_best:  78.33%, tr:  98.47%, tr_best:  98.98%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.178915/  2.580554, val:  69.58%, val_best:  78.33%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.217588/  2.834964, val:  73.33%, val_best:  78.33%, tr:  98.57%, tr_best:  99.18%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.196427/  2.751322, val:  70.42%, val_best:  78.33%, tr:  98.98%, tr_best:  99.18%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.193885/  2.612392, val:  73.33%, val_best:  78.33%, tr:  98.98%, tr_best:  99.18%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.157244/  2.625715, val:  75.42%, val_best:  78.33%, tr:  98.98%, tr_best:  99.18%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.136502/  2.739001, val:  75.00%, val_best:  78.33%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.158134/  2.722824, val:  73.75%, val_best:  78.33%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.140588/  2.684775, val:  76.67%, val_best:  78.33%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.132264/  2.741524, val:  75.42%, val_best:  78.33%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.167737/  2.677091, val:  71.67%, val_best:  78.33%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.175500/  2.802460, val:  77.08%, val_best:  78.33%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.175880/  2.497113, val:  76.25%, val_best:  78.33%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.211332/  2.852363, val:  75.42%, val_best:  78.33%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.282915/  2.521007, val:  75.00%, val_best:  78.33%, tr:  98.57%, tr_best:  99.80%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.271601/  2.775603, val:  70.83%, val_best:  78.33%, tr:  98.26%, tr_best:  99.80%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.239663/  2.644240, val:  76.25%, val_best:  78.33%, tr:  98.98%, tr_best:  99.80%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.226195/  2.668791, val:  70.83%, val_best:  78.33%, tr:  99.08%, tr_best:  99.80%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.211646/  2.941883, val:  69.58%, val_best:  78.33%, tr:  99.08%, tr_best:  99.80%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.201892/  2.651845, val:  70.00%, val_best:  78.33%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.231589/  3.175814, val:  67.50%, val_best:  78.33%, tr:  99.08%, tr_best:  99.80%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.239971/  2.988309, val:  67.92%, val_best:  78.33%, tr:  98.57%, tr_best:  99.80%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.224009/  3.031294, val:  70.42%, val_best:  78.33%, tr:  99.08%, tr_best:  99.80%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.179201/  2.956212, val:  72.08%, val_best:  78.33%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.219766/  3.419595, val:  69.17%, val_best:  78.33%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.216647/  2.925517, val:  72.50%, val_best:  78.33%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.231214/  3.169985, val:  71.67%, val_best:  78.33%, tr:  98.77%, tr_best:  99.80%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.207588/  3.270820, val:  72.92%, val_best:  78.33%, tr:  98.88%, tr_best:  99.80%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.170264/  3.241656, val:  74.58%, val_best:  78.33%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.204904/  3.064901, val:  78.33%, val_best:  78.33%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.205763/  3.264446, val:  71.67%, val_best:  78.33%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.222028/  3.360885, val:  73.33%, val_best:  78.33%, tr:  99.28%, tr_best:  99.80%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.192238/  3.283493, val:  70.83%, val_best:  78.33%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.259489/  3.140619, val:  72.50%, val_best:  78.33%, tr:  98.77%, tr_best:  99.80%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.238051/  3.182302, val:  71.25%, val_best:  78.33%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.248748/  3.573536, val:  73.75%, val_best:  78.33%, tr:  99.08%, tr_best:  99.80%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.255995/  3.319255, val:  73.33%, val_best:  78.33%, tr:  99.08%, tr_best:  99.80%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.261374/  3.211711, val:  75.00%, val_best:  78.33%, tr:  98.67%, tr_best:  99.80%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.186683/  3.385442, val:  74.17%, val_best:  78.33%, tr:  98.98%, tr_best:  99.80%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.221137/  3.159971, val:  77.50%, val_best:  78.33%, tr:  99.08%, tr_best:  99.80%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.235997/  3.600123, val:  67.92%, val_best:  78.33%, tr:  99.28%, tr_best:  99.80%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.329617/  3.387515, val:  75.83%, val_best:  78.33%, tr:  97.96%, tr_best:  99.80%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.271391/  3.365674, val:  74.58%, val_best:  78.33%, tr:  98.57%, tr_best:  99.80%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.256412/  3.390972, val:  76.25%, val_best:  78.33%, tr:  98.77%, tr_best:  99.80%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.215278/  3.593851, val:  76.67%, val_best:  78.33%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.217674/  3.333182, val:  75.83%, val_best:  78.33%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.213608/  3.717244, val:  76.25%, val_best:  78.33%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.252683/  3.733248, val:  72.08%, val_best:  78.33%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.185882/  3.525650, val:  75.42%, val_best:  78.33%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.208558/  3.583767, val:  72.50%, val_best:  78.33%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.160438/  3.611238, val:  72.92%, val_best:  78.33%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.142140/  3.785291, val:  70.00%, val_best:  78.33%, tr:  99.39%, tr_best:  99.90%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.254377/  3.838686, val:  71.67%, val_best:  78.33%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.191256/  3.649952, val:  72.92%, val_best:  78.33%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.243239/  3.781807, val:  69.17%, val_best:  78.33%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.193223/  3.834695, val:  69.58%, val_best:  78.33%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.196656/  3.929714, val:  73.75%, val_best:  78.33%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.249354/  3.864558, val:  72.08%, val_best:  78.33%, tr:  99.39%, tr_best:  99.90%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.210341/  3.641712, val:  75.42%, val_best:  78.33%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.189951/  3.598311, val:  73.33%, val_best:  78.33%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.242447/  3.968081, val:  71.67%, val_best:  78.33%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.223676/  4.141198, val:  70.83%, val_best:  78.33%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.292546/  3.706851, val:  72.92%, val_best:  78.33%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.260784/  3.596349, val:  70.42%, val_best:  78.33%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.264433/  3.692587, val:  72.08%, val_best:  78.33%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.254892/  3.592908, val:  79.58%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.195092/  3.738981, val:  70.00%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.150635/  3.961933, val:  72.08%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.222017/  3.943621, val:  72.92%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.251846/  4.223532, val:  72.08%, val_best:  79.58%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.233999/  4.340574, val:  72.92%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.233037/  3.625801, val:  73.75%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.370160/  3.898686, val:  71.25%, val_best:  79.58%, tr:  98.47%, tr_best:  99.90%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.330601/  3.792429, val:  72.08%, val_best:  79.58%, tr:  98.88%, tr_best:  99.90%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.389525/  3.757289, val:  76.67%, val_best:  79.58%, tr:  98.06%, tr_best:  99.90%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.354089/  3.533221, val:  78.33%, val_best:  79.58%, tr:  98.88%, tr_best:  99.90%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.417854/  3.888727, val:  68.33%, val_best:  79.58%, tr:  97.55%, tr_best:  99.90%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.336302/  3.543686, val:  71.25%, val_best:  79.58%, tr:  98.98%, tr_best:  99.90%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.325836/  3.654904, val:  71.67%, val_best:  79.58%, tr:  98.77%, tr_best:  99.90%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.233148/  3.566735, val:  71.67%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.288576/  3.820624, val:  72.50%, val_best:  79.58%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.303916/  3.978005, val:  70.83%, val_best:  79.58%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.222560/  4.123238, val:  74.58%, val_best:  79.58%, tr:  99.39%, tr_best:  99.90%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.213046/  3.977554, val:  74.17%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.237994/  3.721657, val:  75.42%, val_best:  79.58%, tr:  99.18%, tr_best:  99.90%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.273617/  4.064671, val:  74.58%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.285825/  3.819340, val:  73.33%, val_best:  79.58%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.276274/  3.843203, val:  72.08%, val_best:  79.58%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.286301/  4.331538, val:  68.33%, val_best:  79.58%, tr:  98.88%, tr_best:  99.90%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.409576/  3.847910, val:  74.58%, val_best:  79.58%, tr:  98.47%, tr_best:  99.90%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.331938/  3.578585, val:  77.50%, val_best:  79.58%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.344120/  4.510115, val:  71.67%, val_best:  79.58%, tr:  98.88%, tr_best:  99.90%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.316383/  3.803147, val:  77.08%, val_best:  79.58%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.251039/  4.221034, val:  70.83%, val_best:  79.58%, tr:  99.18%, tr_best:  99.90%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.271784/  3.984027, val:  72.50%, val_best:  79.58%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.302634/  4.416686, val:  69.58%, val_best:  79.58%, tr:  98.98%, tr_best:  99.90%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.307003/  4.180243, val:  71.67%, val_best:  79.58%, tr:  98.98%, tr_best:  99.90%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.270644/  4.465869, val:  71.25%, val_best:  79.58%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.388595/  4.630271, val:  70.42%, val_best:  79.58%, tr:  98.77%, tr_best:  99.90%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.398881/  4.558104, val:  68.75%, val_best:  79.58%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.435189/  4.016200, val:  69.58%, val_best:  79.58%, tr:  98.16%, tr_best:  99.90%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.420076/  3.723094, val:  76.67%, val_best:  79.58%, tr:  97.96%, tr_best:  99.90%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.408941/  4.260265, val:  72.08%, val_best:  79.58%, tr:  98.88%, tr_best:  99.90%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.364088/  4.099721, val:  73.75%, val_best:  79.58%, tr:  98.67%, tr_best:  99.90%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.332236/  4.241843, val:  72.08%, val_best:  79.58%, tr:  98.77%, tr_best:  99.90%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.315358/  4.354415, val:  76.25%, val_best:  79.58%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.341244/  4.616377, val:  71.67%, val_best:  79.58%, tr:  98.77%, tr_best:  99.90%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.357002/  4.104233, val:  73.33%, val_best:  79.58%, tr:  98.47%, tr_best:  99.90%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.306924/  4.768012, val:  72.92%, val_best:  79.58%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.499729/  4.904350, val:  69.17%, val_best:  79.58%, tr:  97.75%, tr_best:  99.90%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.519185/  4.854603, val:  70.83%, val_best:  79.58%, tr:  98.47%, tr_best:  99.90%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.436808/  5.198194, val:  69.17%, val_best:  79.58%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.431213/  4.827930, val:  72.08%, val_best:  79.58%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.418724/  4.658226, val:  70.83%, val_best:  79.58%, tr:  98.67%, tr_best:  99.90%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.428674/  4.736563, val:  69.58%, val_best:  79.58%, tr:  98.98%, tr_best:  99.90%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.464615/  4.648893, val:  74.17%, val_best:  79.58%, tr:  98.77%, tr_best:  99.90%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.436161/  5.111501, val:  66.25%, val_best:  79.58%, tr:  98.67%, tr_best:  99.90%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.566624/  4.591357, val:  70.00%, val_best:  79.58%, tr:  96.63%, tr_best:  99.90%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.368999/  4.352724, val:  73.75%, val_best:  79.58%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.367577/  3.946305, val:  74.17%, val_best:  79.58%, tr:  99.18%, tr_best:  99.90%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.380350/  4.002984, val:  74.58%, val_best:  79.58%, tr:  99.39%, tr_best:  99.90%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.373368/  4.186925, val:  75.00%, val_best:  79.58%, tr:  98.67%, tr_best:  99.90%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.380524/  3.902895, val:  75.83%, val_best:  79.58%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.470353/  4.037612, val:  73.75%, val_best:  79.58%, tr:  98.67%, tr_best:  99.90%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.587124/  4.839304, val:  72.92%, val_best:  79.58%, tr:  97.75%, tr_best:  99.90%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.483144/  4.699540, val:  70.00%, val_best:  79.58%, tr:  98.47%, tr_best:  99.90%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.577416/  5.007420, val:  67.50%, val_best:  79.58%, tr:  97.85%, tr_best:  99.90%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.581403/  4.747959, val:  69.58%, val_best:  79.58%, tr:  98.26%, tr_best:  99.90%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.485773/  4.510604, val:  73.75%, val_best:  79.58%, tr:  98.88%, tr_best:  99.90%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.453653/  4.352986, val:  71.25%, val_best:  79.58%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.409350/  5.045284, val:  67.92%, val_best:  79.58%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.415249/  5.030747, val:  69.58%, val_best:  79.58%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.586172/  5.243403, val:  73.33%, val_best:  79.58%, tr:  97.65%, tr_best:  99.90%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.540342/  4.542424, val:  71.25%, val_best:  79.58%, tr:  98.57%, tr_best:  99.90%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.519247/  4.610806, val:  69.17%, val_best:  79.58%, tr:  98.88%, tr_best:  99.90%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.573518/  4.230892, val:  75.83%, val_best:  79.58%, tr:  98.26%, tr_best:  99.90%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.522216/  4.242898, val:  72.92%, val_best:  79.58%, tr:  98.57%, tr_best:  99.90%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.558733/  4.117521, val:  75.42%, val_best:  79.58%, tr:  98.06%, tr_best:  99.90%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.603602/  4.397953, val:  74.17%, val_best:  79.58%, tr:  97.55%, tr_best:  99.90%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.565857/  4.325177, val:  77.50%, val_best:  79.58%, tr:  98.06%, tr_best:  99.90%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.589936/  4.534132, val:  75.00%, val_best:  79.58%, tr:  97.85%, tr_best:  99.90%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.442746/  4.287640, val:  75.83%, val_best:  79.58%, tr:  98.57%, tr_best:  99.90%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.520649/  4.909757, val:  69.17%, val_best:  79.58%, tr:  97.65%, tr_best:  99.90%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.368887/  4.642381, val:  72.92%, val_best:  79.58%, tr:  99.18%, tr_best:  99.90%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.414827/  4.949771, val:  72.08%, val_best:  79.58%, tr:  99.39%, tr_best:  99.90%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.411530/  4.942063, val:  74.58%, val_best:  79.58%, tr:  99.39%, tr_best:  99.90%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.540397/  4.501589, val:  75.83%, val_best:  79.58%, tr:  97.65%, tr_best:  99.90%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.487939/  4.653814, val:  74.58%, val_best:  79.58%, tr:  98.77%, tr_best:  99.90%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.444512/  4.640429, val:  71.67%, val_best:  79.58%, tr:  98.57%, tr_best:  99.90%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.481126/  4.904673, val:  72.08%, val_best:  79.58%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.426122/  5.036511, val:  72.08%, val_best:  79.58%, tr:  99.39%, tr_best:  99.90%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.430408/  4.989545, val:  76.25%, val_best:  79.58%, tr:  99.18%, tr_best:  99.90%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.464476/  5.481808, val:  71.25%, val_best:  79.58%, tr:  98.77%, tr_best:  99.90%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.384310/  5.191491, val:  70.83%, val_best:  79.58%, tr:  99.18%, tr_best:  99.90%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.430539/  4.745837, val:  73.75%, val_best:  79.58%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.503873/  5.117366, val:  73.33%, val_best:  79.58%, tr:  98.37%, tr_best:  99.90%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.537892/  4.734717, val:  74.17%, val_best:  79.58%, tr:  99.18%, tr_best:  99.90%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.600422/  4.745525, val:  73.33%, val_best:  79.58%, tr:  97.96%, tr_best:  99.90%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.407137/  4.694707, val:  73.33%, val_best:  79.58%, tr:  99.39%, tr_best:  99.90%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.375994/  4.654133, val:  74.17%, val_best:  79.58%, tr:  98.98%, tr_best:  99.90%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.449742/  4.941847, val:  71.67%, val_best:  79.58%, tr:  99.18%, tr_best:  99.90%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.445427/  4.666839, val:  72.92%, val_best:  79.58%, tr:  98.77%, tr_best:  99.90%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.469515/  4.748416, val:  72.50%, val_best:  79.58%, tr:  99.18%, tr_best:  99.90%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.403857/  4.801169, val:  75.00%, val_best:  79.58%, tr:  98.88%, tr_best:  99.90%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.406770/  4.999417, val:  71.67%, val_best:  79.58%, tr:  98.98%, tr_best:  99.90%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.445769/  5.897760, val:  69.17%, val_best:  79.58%, tr:  99.18%, tr_best:  99.90%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.642578/  5.667691, val:  67.92%, val_best:  79.58%, tr:  97.34%, tr_best:  99.90%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.619270/  4.971097, val:  70.83%, val_best:  79.58%, tr:  97.85%, tr_best:  99.90%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.576688/  4.722472, val:  74.17%, val_best:  79.58%, tr:  98.37%, tr_best:  99.90%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b046ec660ee44c99bafa6c134d6542d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▂█▇█████▇██████████▇██████▇██████▇███▇</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▇▅▆▆▇▇▇▆▆█▆▅▇▇▆▇█▆█▆▇▇▆▆▆▆▆▇▇▆▇█▇▆▇▆▆</td></tr><tr><td>tr_acc</td><td>▁▄▅▇▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▂▂▂▁▁▁▂▁▁▁▂▂▂▁▂▂▂▂▂▂▂▃▂▃▂▃▃▃▄▃▄▄▃▃▃▃▃</td></tr><tr><td>val_acc_best</td><td>▁▄▄▇▇▇██████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▇▅▆▆▇▇▇▆▆█▆▅▇▇▆▇█▆█▆▇▇▆▆▆▆▆▇▇▆▇█▇▆▇▆▆</td></tr><tr><td>val_loss</td><td>▁▁▁▂▂▃▃▃▄▄▃▄▄▄▅▅▅▅▅▅▆▅▅▆▆▆▆▆█▇▆▇▇▆▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98366</td></tr><tr><td>tr_epoch_loss</td><td>0.57669</td></tr><tr><td>val_acc_best</td><td>0.79583</td></tr><tr><td>val_acc_now</td><td>0.74167</td></tr><tr><td>val_loss</td><td>4.72247</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-sweep-21</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rakrvs5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rakrvs5e</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_031828-rakrvs5e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2xqmdivq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_033005-2xqmdivq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2xqmdivq' target=\"_blank\">valiant-sweep-22</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2xqmdivq' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2xqmdivq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.302108/  2.296801, val:  15.83%, val_best:  15.83%, tr:  10.73%, tr_best:  10.73%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.290461/  2.277997, val:  20.00%, val_best:  20.00%, tr:  15.93%, tr_best:  15.93%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.258657/  2.238568, val:  20.83%, val_best:  20.83%, tr:  19.20%, tr_best:  19.20%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.188931/  2.172017, val:  27.08%, val_best:  27.08%, tr:  25.74%, tr_best:  25.74%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  2.091489/  2.077526, val:  38.75%, val_best:  38.75%, tr:  33.81%, tr_best:  33.81%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  1.966382/  1.974866, val:  46.67%, val_best:  46.67%, tr:  39.43%, tr_best:  39.43%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  1.849129/  1.867188, val:  52.08%, val_best:  52.08%, tr:  45.76%, tr_best:  45.76%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  1.731624/  1.787370, val:  49.17%, val_best:  52.08%, tr:  48.62%, tr_best:  48.62%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  1.649840/  1.729600, val:  50.42%, val_best:  52.08%, tr:  53.32%, tr_best:  53.32%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  1.583505/  1.685118, val:  55.00%, val_best:  55.00%, tr:  56.79%, tr_best:  56.79%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  1.536749/  1.647779, val:  52.92%, val_best:  55.00%, tr:  58.02%, tr_best:  58.02%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  1.497640/  1.618320, val:  59.17%, val_best:  59.17%, tr:  59.65%, tr_best:  59.65%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  1.465524/  1.597470, val:  56.67%, val_best:  59.17%, tr:  61.70%, tr_best:  61.70%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  1.434262/  1.579457, val:  59.17%, val_best:  59.17%, tr:  62.21%, tr_best:  62.21%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  1.408197/  1.568654, val:  57.08%, val_best:  59.17%, tr:  61.90%, tr_best:  62.21%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  1.381607/  1.558247, val:  57.08%, val_best:  59.17%, tr:  62.10%, tr_best:  62.21%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  1.361415/  1.548674, val:  58.33%, val_best:  59.17%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  1.348448/  1.541571, val:  61.67%, val_best:  61.67%, tr:  65.47%, tr_best:  65.47%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  1.333915/  1.537515, val:  59.17%, val_best:  61.67%, tr:  65.88%, tr_best:  65.88%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  1.317196/  1.537032, val:  61.67%, val_best:  61.67%, tr:  64.35%, tr_best:  65.88%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  1.305383/  1.525738, val:  62.08%, val_best:  62.08%, tr:  66.19%, tr_best:  66.19%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  1.288316/  1.523096, val:  64.17%, val_best:  64.17%, tr:  66.50%, tr_best:  66.50%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  1.280148/  1.520571, val:  60.42%, val_best:  64.17%, tr:  64.96%, tr_best:  66.50%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  1.268424/  1.515072, val:  62.50%, val_best:  64.17%, tr:  67.31%, tr_best:  67.31%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  1.252751/  1.509908, val:  65.42%, val_best:  65.42%, tr:  68.85%, tr_best:  68.85%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  1.243837/  1.514724, val:  62.08%, val_best:  65.42%, tr:  71.09%, tr_best:  71.09%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  1.232582/  1.508824, val:  61.25%, val_best:  65.42%, tr:  67.93%, tr_best:  71.09%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  1.223399/  1.500931, val:  65.00%, val_best:  65.42%, tr:  70.28%, tr_best:  71.09%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  1.227019/  1.493091, val:  67.50%, val_best:  67.50%, tr:  68.85%, tr_best:  71.09%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  1.210625/  1.489205, val:  66.25%, val_best:  67.50%, tr:  70.58%, tr_best:  71.09%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  1.206786/  1.487159, val:  63.75%, val_best:  67.50%, tr:  69.97%, tr_best:  71.09%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  1.198865/  1.481236, val:  60.00%, val_best:  67.50%, tr:  68.95%, tr_best:  71.09%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  1.185334/  1.482130, val:  61.67%, val_best:  67.50%, tr:  71.81%, tr_best:  71.81%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  1.187212/  1.473570, val:  62.50%, val_best:  67.50%, tr:  70.68%, tr_best:  71.81%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  1.171775/  1.470811, val:  63.33%, val_best:  67.50%, tr:  71.71%, tr_best:  71.81%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  1.161937/  1.473229, val:  63.75%, val_best:  67.50%, tr:  70.38%, tr_best:  71.81%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  1.157462/  1.472749, val:  60.83%, val_best:  67.50%, tr:  71.20%, tr_best:  71.81%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  1.152048/  1.471012, val:  62.50%, val_best:  67.50%, tr:  71.91%, tr_best:  71.91%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  1.148186/  1.466919, val:  60.00%, val_best:  67.50%, tr:  71.20%, tr_best:  71.91%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  1.142566/  1.470193, val:  62.08%, val_best:  67.50%, tr:  74.46%, tr_best:  74.46%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  1.131085/  1.465604, val:  61.25%, val_best:  67.50%, tr:  72.42%, tr_best:  74.46%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  1.139406/  1.469143, val:  60.42%, val_best:  67.50%, tr:  75.38%, tr_best:  75.38%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  1.121430/  1.454002, val:  63.33%, val_best:  67.50%, tr:  75.08%, tr_best:  75.38%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  1.119122/  1.463548, val:  61.25%, val_best:  67.50%, tr:  75.28%, tr_best:  75.38%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  1.120125/  1.460874, val:  62.08%, val_best:  67.50%, tr:  73.34%, tr_best:  75.38%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  1.107221/  1.457772, val:  63.75%, val_best:  67.50%, tr:  73.54%, tr_best:  75.38%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  1.108171/  1.454068, val:  62.50%, val_best:  67.50%, tr:  74.87%, tr_best:  75.38%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  1.110672/  1.462906, val:  62.92%, val_best:  67.50%, tr:  73.03%, tr_best:  75.38%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  1.107010/  1.456510, val:  62.50%, val_best:  67.50%, tr:  76.10%, tr_best:  76.10%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  1.101820/  1.456224, val:  61.67%, val_best:  67.50%, tr:  75.89%, tr_best:  76.10%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  1.092016/  1.453967, val:  61.67%, val_best:  67.50%, tr:  75.38%, tr_best:  76.10%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  1.087390/  1.458576, val:  61.67%, val_best:  67.50%, tr:  73.65%, tr_best:  76.10%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  1.082024/  1.455261, val:  62.08%, val_best:  67.50%, tr:  73.95%, tr_best:  76.10%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  1.078836/  1.456778, val:  64.58%, val_best:  67.50%, tr:  75.69%, tr_best:  76.10%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  1.086228/  1.464377, val:  57.92%, val_best:  67.50%, tr:  75.79%, tr_best:  76.10%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  1.080992/  1.462571, val:  62.92%, val_best:  67.50%, tr:  75.69%, tr_best:  76.10%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  1.065242/  1.465730, val:  63.75%, val_best:  67.50%, tr:  78.04%, tr_best:  78.04%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  1.072046/  1.462276, val:  62.50%, val_best:  67.50%, tr:  75.49%, tr_best:  78.04%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  1.062741/  1.469997, val:  62.50%, val_best:  67.50%, tr:  75.08%, tr_best:  78.04%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  1.066142/  1.469122, val:  61.67%, val_best:  67.50%, tr:  79.16%, tr_best:  79.16%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  1.065046/  1.469768, val:  62.92%, val_best:  67.50%, tr:  74.26%, tr_best:  79.16%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  1.071472/  1.472033, val:  57.92%, val_best:  67.50%, tr:  75.49%, tr_best:  79.16%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  1.063913/  1.464157, val:  62.92%, val_best:  67.50%, tr:  76.81%, tr_best:  79.16%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  1.057207/  1.460649, val:  61.25%, val_best:  67.50%, tr:  78.14%, tr_best:  79.16%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  1.050503/  1.472593, val:  59.58%, val_best:  67.50%, tr:  78.65%, tr_best:  79.16%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  1.051667/  1.468168, val:  62.92%, val_best:  67.50%, tr:  77.12%, tr_best:  79.16%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  1.053112/  1.466509, val:  64.17%, val_best:  67.50%, tr:  78.24%, tr_best:  79.16%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  1.058258/  1.468151, val:  62.50%, val_best:  67.50%, tr:  77.63%, tr_best:  79.16%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  1.045113/  1.469236, val:  62.08%, val_best:  67.50%, tr:  75.49%, tr_best:  79.16%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  1.040189/  1.468385, val:  61.25%, val_best:  67.50%, tr:  78.35%, tr_best:  79.16%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  1.036684/  1.472246, val:  59.58%, val_best:  67.50%, tr:  79.37%, tr_best:  79.37%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  1.036452/  1.468217, val:  63.33%, val_best:  67.50%, tr:  79.37%, tr_best:  79.37%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  1.033185/  1.480582, val:  63.33%, val_best:  67.50%, tr:  79.47%, tr_best:  79.47%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  1.032511/  1.473224, val:  59.58%, val_best:  67.50%, tr:  79.67%, tr_best:  79.67%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  1.023138/  1.475520, val:  62.92%, val_best:  67.50%, tr:  77.22%, tr_best:  79.67%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  1.031381/  1.480268, val:  62.08%, val_best:  67.50%, tr:  78.14%, tr_best:  79.67%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  1.019337/  1.496661, val:  60.83%, val_best:  67.50%, tr:  79.78%, tr_best:  79.78%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  1.029935/  1.493565, val:  61.67%, val_best:  67.50%, tr:  79.06%, tr_best:  79.78%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  1.022986/  1.489734, val:  62.08%, val_best:  67.50%, tr:  79.37%, tr_best:  79.78%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  1.026880/  1.494798, val:  60.83%, val_best:  67.50%, tr:  78.35%, tr_best:  79.78%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  1.017594/  1.487448, val:  62.08%, val_best:  67.50%, tr:  78.45%, tr_best:  79.78%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  1.015971/  1.500731, val:  62.08%, val_best:  67.50%, tr:  79.67%, tr_best:  79.78%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  1.019108/  1.500185, val:  61.67%, val_best:  67.50%, tr:  80.18%, tr_best:  80.18%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  1.016511/  1.506409, val:  62.08%, val_best:  67.50%, tr:  81.31%, tr_best:  81.31%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  1.023832/  1.520005, val:  60.00%, val_best:  67.50%, tr:  81.51%, tr_best:  81.51%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  1.018262/  1.516894, val:  60.83%, val_best:  67.50%, tr:  80.69%, tr_best:  81.51%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  1.007824/  1.513519, val:  65.83%, val_best:  67.50%, tr:  81.10%, tr_best:  81.51%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  1.009465/  1.507769, val:  61.67%, val_best:  67.50%, tr:  83.04%, tr_best:  83.04%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  1.007173/  1.516224, val:  62.08%, val_best:  67.50%, tr:  82.53%, tr_best:  83.04%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  1.000365/  1.520948, val:  64.58%, val_best:  67.50%, tr:  82.33%, tr_best:  83.04%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  1.004080/  1.526051, val:  63.75%, val_best:  67.50%, tr:  81.61%, tr_best:  83.04%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  0.991551/  1.511736, val:  62.08%, val_best:  67.50%, tr:  81.51%, tr_best:  83.04%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  1.001748/  1.531097, val:  63.33%, val_best:  67.50%, tr:  81.61%, tr_best:  83.04%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  1.005919/  1.535572, val:  60.42%, val_best:  67.50%, tr:  82.84%, tr_best:  83.04%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  1.011814/  1.531858, val:  61.25%, val_best:  67.50%, tr:  82.84%, tr_best:  83.04%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  1.001305/  1.536249, val:  62.92%, val_best:  67.50%, tr:  83.45%, tr_best:  83.45%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  0.998796/  1.539825, val:  63.75%, val_best:  67.50%, tr:  83.66%, tr_best:  83.66%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  1.004948/  1.534261, val:  62.50%, val_best:  67.50%, tr:  81.21%, tr_best:  83.66%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  1.008949/  1.562169, val:  60.42%, val_best:  67.50%, tr:  83.55%, tr_best:  83.66%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  1.000748/  1.565258, val:  63.33%, val_best:  67.50%, tr:  82.02%, tr_best:  83.66%\n",
      "epoch-100 lr=['0.0001000'], tr/val_loss:  0.990135/  1.556757, val:  64.17%, val_best:  67.50%, tr:  82.53%, tr_best:  83.66%\n",
      "epoch-101 lr=['0.0001000'], tr/val_loss:  0.985248/  1.559691, val:  61.25%, val_best:  67.50%, tr:  84.17%, tr_best:  84.17%\n",
      "epoch-102 lr=['0.0001000'], tr/val_loss:  0.994978/  1.574984, val:  61.25%, val_best:  67.50%, tr:  84.37%, tr_best:  84.37%\n",
      "epoch-103 lr=['0.0001000'], tr/val_loss:  0.995268/  1.571268, val:  62.50%, val_best:  67.50%, tr:  83.76%, tr_best:  84.37%\n",
      "epoch-104 lr=['0.0001000'], tr/val_loss:  0.993390/  1.586108, val:  63.75%, val_best:  67.50%, tr:  85.50%, tr_best:  85.50%\n",
      "epoch-105 lr=['0.0001000'], tr/val_loss:  1.007613/  1.588069, val:  63.75%, val_best:  67.50%, tr:  83.04%, tr_best:  85.50%\n",
      "epoch-106 lr=['0.0001000'], tr/val_loss:  0.998600/  1.578697, val:  64.58%, val_best:  67.50%, tr:  85.29%, tr_best:  85.50%\n",
      "epoch-107 lr=['0.0001000'], tr/val_loss:  0.994981/  1.587608, val:  65.00%, val_best:  67.50%, tr:  83.55%, tr_best:  85.50%\n",
      "epoch-108 lr=['0.0001000'], tr/val_loss:  0.988272/  1.592573, val:  63.33%, val_best:  67.50%, tr:  85.19%, tr_best:  85.50%\n",
      "epoch-109 lr=['0.0001000'], tr/val_loss:  0.986143/  1.596307, val:  64.58%, val_best:  67.50%, tr:  85.09%, tr_best:  85.50%\n",
      "epoch-110 lr=['0.0001000'], tr/val_loss:  0.986211/  1.592785, val:  62.50%, val_best:  67.50%, tr:  85.50%, tr_best:  85.50%\n",
      "epoch-111 lr=['0.0001000'], tr/val_loss:  0.987929/  1.593767, val:  62.92%, val_best:  67.50%, tr:  86.11%, tr_best:  86.11%\n",
      "epoch-112 lr=['0.0001000'], tr/val_loss:  0.982165/  1.615755, val:  60.42%, val_best:  67.50%, tr:  86.72%, tr_best:  86.72%\n",
      "epoch-113 lr=['0.0001000'], tr/val_loss:  0.999241/  1.587790, val:  62.92%, val_best:  67.50%, tr:  85.50%, tr_best:  86.72%\n",
      "epoch-114 lr=['0.0001000'], tr/val_loss:  0.995714/  1.591826, val:  64.17%, val_best:  67.50%, tr:  86.01%, tr_best:  86.72%\n",
      "epoch-115 lr=['0.0001000'], tr/val_loss:  0.981550/  1.601519, val:  62.92%, val_best:  67.50%, tr:  87.13%, tr_best:  87.13%\n",
      "epoch-116 lr=['0.0001000'], tr/val_loss:  0.982879/  1.585769, val:  63.75%, val_best:  67.50%, tr:  86.82%, tr_best:  87.13%\n",
      "epoch-117 lr=['0.0001000'], tr/val_loss:  0.989340/  1.592538, val:  62.92%, val_best:  67.50%, tr:  86.52%, tr_best:  87.13%\n",
      "epoch-118 lr=['0.0001000'], tr/val_loss:  0.984712/  1.607946, val:  63.33%, val_best:  67.50%, tr:  85.09%, tr_best:  87.13%\n",
      "epoch-119 lr=['0.0001000'], tr/val_loss:  0.983986/  1.601508, val:  63.75%, val_best:  67.50%, tr:  86.93%, tr_best:  87.13%\n",
      "epoch-120 lr=['0.0001000'], tr/val_loss:  0.975043/  1.609699, val:  64.17%, val_best:  67.50%, tr:  87.54%, tr_best:  87.54%\n",
      "epoch-121 lr=['0.0001000'], tr/val_loss:  0.985360/  1.611998, val:  65.83%, val_best:  67.50%, tr:  86.62%, tr_best:  87.54%\n",
      "epoch-122 lr=['0.0001000'], tr/val_loss:  0.994198/  1.608202, val:  63.33%, val_best:  67.50%, tr:  87.13%, tr_best:  87.54%\n",
      "epoch-123 lr=['0.0001000'], tr/val_loss:  0.990277/  1.620639, val:  65.42%, val_best:  67.50%, tr:  86.82%, tr_best:  87.54%\n",
      "epoch-124 lr=['0.0001000'], tr/val_loss:  0.982268/  1.631011, val:  64.58%, val_best:  67.50%, tr:  86.82%, tr_best:  87.54%\n",
      "epoch-125 lr=['0.0001000'], tr/val_loss:  0.978843/  1.622099, val:  64.58%, val_best:  67.50%, tr:  87.54%, tr_best:  87.54%\n",
      "epoch-126 lr=['0.0001000'], tr/val_loss:  0.998327/  1.623545, val:  63.75%, val_best:  67.50%, tr:  88.36%, tr_best:  88.36%\n",
      "epoch-127 lr=['0.0001000'], tr/val_loss:  0.982907/  1.633619, val:  65.83%, val_best:  67.50%, tr:  87.23%, tr_best:  88.36%\n",
      "epoch-128 lr=['0.0001000'], tr/val_loss:  0.981735/  1.624678, val:  62.50%, val_best:  67.50%, tr:  87.84%, tr_best:  88.36%\n",
      "epoch-129 lr=['0.0001000'], tr/val_loss:  0.984958/  1.640084, val:  64.17%, val_best:  67.50%, tr:  87.33%, tr_best:  88.36%\n",
      "epoch-130 lr=['0.0001000'], tr/val_loss:  0.996863/  1.633191, val:  64.58%, val_best:  67.50%, tr:  86.41%, tr_best:  88.36%\n",
      "epoch-131 lr=['0.0001000'], tr/val_loss:  0.984954/  1.656753, val:  62.50%, val_best:  67.50%, tr:  87.74%, tr_best:  88.36%\n",
      "epoch-132 lr=['0.0001000'], tr/val_loss:  0.979693/  1.646172, val:  63.33%, val_best:  67.50%, tr:  86.21%, tr_best:  88.36%\n",
      "epoch-133 lr=['0.0001000'], tr/val_loss:  0.979359/  1.659853, val:  63.75%, val_best:  67.50%, tr:  88.87%, tr_best:  88.87%\n",
      "epoch-134 lr=['0.0001000'], tr/val_loss:  0.978408/  1.664185, val:  64.17%, val_best:  67.50%, tr:  88.97%, tr_best:  88.97%\n",
      "epoch-135 lr=['0.0001000'], tr/val_loss:  0.977908/  1.652297, val:  66.25%, val_best:  67.50%, tr:  87.74%, tr_best:  88.97%\n",
      "epoch-136 lr=['0.0001000'], tr/val_loss:  0.987673/  1.658732, val:  64.58%, val_best:  67.50%, tr:  88.36%, tr_best:  88.97%\n",
      "epoch-137 lr=['0.0001000'], tr/val_loss:  0.993337/  1.665241, val:  63.75%, val_best:  67.50%, tr:  88.36%, tr_best:  88.97%\n",
      "epoch-138 lr=['0.0001000'], tr/val_loss:  0.978854/  1.664251, val:  65.42%, val_best:  67.50%, tr:  88.25%, tr_best:  88.97%\n",
      "epoch-139 lr=['0.0001000'], tr/val_loss:  0.985530/  1.669817, val:  64.58%, val_best:  67.50%, tr:  88.05%, tr_best:  88.97%\n",
      "epoch-140 lr=['0.0001000'], tr/val_loss:  0.982228/  1.683021, val:  64.58%, val_best:  67.50%, tr:  88.25%, tr_best:  88.97%\n",
      "epoch-141 lr=['0.0001000'], tr/val_loss:  0.973423/  1.671087, val:  64.17%, val_best:  67.50%, tr:  89.17%, tr_best:  89.17%\n",
      "epoch-142 lr=['0.0001000'], tr/val_loss:  0.964558/  1.686602, val:  63.75%, val_best:  67.50%, tr:  88.56%, tr_best:  89.17%\n",
      "epoch-143 lr=['0.0001000'], tr/val_loss:  0.971586/  1.678930, val:  64.58%, val_best:  67.50%, tr:  91.01%, tr_best:  91.01%\n",
      "epoch-144 lr=['0.0001000'], tr/val_loss:  0.967004/  1.683940, val:  64.17%, val_best:  67.50%, tr:  90.40%, tr_best:  91.01%\n",
      "epoch-145 lr=['0.0001000'], tr/val_loss:  0.966020/  1.698724, val:  65.00%, val_best:  67.50%, tr:  90.19%, tr_best:  91.01%\n",
      "epoch-146 lr=['0.0001000'], tr/val_loss:  0.966742/  1.697453, val:  65.83%, val_best:  67.50%, tr:  90.30%, tr_best:  91.01%\n",
      "epoch-147 lr=['0.0001000'], tr/val_loss:  0.977978/  1.705654, val:  67.50%, val_best:  67.50%, tr:  90.40%, tr_best:  91.01%\n",
      "epoch-148 lr=['0.0001000'], tr/val_loss:  0.963454/  1.706277, val:  65.00%, val_best:  67.50%, tr:  89.17%, tr_best:  91.01%\n",
      "epoch-149 lr=['0.0001000'], tr/val_loss:  0.961110/  1.711800, val:  66.67%, val_best:  67.50%, tr:  90.40%, tr_best:  91.01%\n",
      "epoch-150 lr=['0.0001000'], tr/val_loss:  0.967939/  1.722921, val:  65.00%, val_best:  67.50%, tr:  89.89%, tr_best:  91.01%\n",
      "epoch-151 lr=['0.0001000'], tr/val_loss:  0.958915/  1.731695, val:  65.83%, val_best:  67.50%, tr:  90.60%, tr_best:  91.01%\n",
      "epoch-152 lr=['0.0001000'], tr/val_loss:  0.962479/  1.730390, val:  64.17%, val_best:  67.50%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-153 lr=['0.0001000'], tr/val_loss:  0.978841/  1.738320, val:  64.58%, val_best:  67.50%, tr:  90.30%, tr_best:  91.42%\n",
      "epoch-154 lr=['0.0001000'], tr/val_loss:  0.959249/  1.727504, val:  65.00%, val_best:  67.50%, tr:  90.91%, tr_best:  91.42%\n",
      "epoch-155 lr=['0.0001000'], tr/val_loss:  0.971325/  1.728160, val:  66.25%, val_best:  67.50%, tr:  91.01%, tr_best:  91.42%\n",
      "epoch-156 lr=['0.0001000'], tr/val_loss:  0.967177/  1.740423, val:  64.58%, val_best:  67.50%, tr:  89.68%, tr_best:  91.42%\n",
      "epoch-157 lr=['0.0001000'], tr/val_loss:  0.960854/  1.749077, val:  64.58%, val_best:  67.50%, tr:  90.91%, tr_best:  91.42%\n",
      "epoch-158 lr=['0.0001000'], tr/val_loss:  0.969922/  1.746017, val:  65.00%, val_best:  67.50%, tr:  91.22%, tr_best:  91.42%\n",
      "epoch-159 lr=['0.0001000'], tr/val_loss:  0.966157/  1.748389, val:  65.00%, val_best:  67.50%, tr:  91.01%, tr_best:  91.42%\n",
      "epoch-160 lr=['0.0001000'], tr/val_loss:  0.973345/  1.752835, val:  65.83%, val_best:  67.50%, tr:  90.09%, tr_best:  91.42%\n",
      "epoch-161 lr=['0.0001000'], tr/val_loss:  0.961225/  1.752287, val:  66.67%, val_best:  67.50%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-162 lr=['0.0001000'], tr/val_loss:  0.968121/  1.760005, val:  65.42%, val_best:  67.50%, tr:  91.01%, tr_best:  91.42%\n",
      "epoch-163 lr=['0.0001000'], tr/val_loss:  0.969760/  1.754089, val:  65.83%, val_best:  67.50%, tr:  91.01%, tr_best:  91.42%\n",
      "epoch-164 lr=['0.0001000'], tr/val_loss:  0.969916/  1.769383, val:  65.83%, val_best:  67.50%, tr:  90.81%, tr_best:  91.42%\n",
      "epoch-165 lr=['0.0001000'], tr/val_loss:  0.975686/  1.775254, val:  66.67%, val_best:  67.50%, tr:  90.91%, tr_best:  91.42%\n",
      "epoch-166 lr=['0.0001000'], tr/val_loss:  0.965644/  1.772518, val:  65.42%, val_best:  67.50%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-167 lr=['0.0001000'], tr/val_loss:  0.976449/  1.781745, val:  66.67%, val_best:  67.50%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-168 lr=['0.0001000'], tr/val_loss:  0.972633/  1.777670, val:  67.08%, val_best:  67.50%, tr:  91.93%, tr_best:  91.93%\n",
      "epoch-169 lr=['0.0001000'], tr/val_loss:  0.962371/  1.792559, val:  65.42%, val_best:  67.50%, tr:  92.54%, tr_best:  92.54%\n",
      "epoch-170 lr=['0.0001000'], tr/val_loss:  0.967905/  1.793386, val:  64.17%, val_best:  67.50%, tr:  90.70%, tr_best:  92.54%\n",
      "epoch-171 lr=['0.0001000'], tr/val_loss:  0.962271/  1.793825, val:  66.25%, val_best:  67.50%, tr:  92.44%, tr_best:  92.54%\n",
      "epoch-172 lr=['0.0001000'], tr/val_loss:  0.962489/  1.817481, val:  65.83%, val_best:  67.50%, tr:  91.62%, tr_best:  92.54%\n",
      "epoch-173 lr=['0.0001000'], tr/val_loss:  0.961676/  1.830293, val:  63.33%, val_best:  67.50%, tr:  92.54%, tr_best:  92.54%\n",
      "epoch-174 lr=['0.0001000'], tr/val_loss:  0.966021/  1.817568, val:  64.17%, val_best:  67.50%, tr:  92.54%, tr_best:  92.54%\n",
      "epoch-175 lr=['0.0001000'], tr/val_loss:  0.961038/  1.822057, val:  62.92%, val_best:  67.50%, tr:  92.13%, tr_best:  92.54%\n",
      "epoch-176 lr=['0.0001000'], tr/val_loss:  0.962443/  1.832299, val:  64.58%, val_best:  67.50%, tr:  92.03%, tr_best:  92.54%\n",
      "epoch-177 lr=['0.0001000'], tr/val_loss:  0.972391/  1.827134, val:  66.25%, val_best:  67.50%, tr:  92.54%, tr_best:  92.54%\n",
      "epoch-178 lr=['0.0001000'], tr/val_loss:  0.966627/  1.823961, val:  65.42%, val_best:  67.50%, tr:  91.52%, tr_best:  92.54%\n",
      "epoch-179 lr=['0.0001000'], tr/val_loss:  0.964595/  1.838549, val:  65.83%, val_best:  67.50%, tr:  92.75%, tr_best:  92.75%\n",
      "epoch-180 lr=['0.0001000'], tr/val_loss:  0.957185/  1.842103, val:  65.42%, val_best:  67.50%, tr:  93.16%, tr_best:  93.16%\n",
      "epoch-181 lr=['0.0001000'], tr/val_loss:  0.962996/  1.842340, val:  65.00%, val_best:  67.50%, tr:  93.05%, tr_best:  93.16%\n",
      "epoch-182 lr=['0.0001000'], tr/val_loss:  0.957330/  1.850402, val:  65.83%, val_best:  67.50%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-183 lr=['0.0001000'], tr/val_loss:  0.963753/  1.858260, val:  65.83%, val_best:  67.50%, tr:  92.95%, tr_best:  93.36%\n",
      "epoch-184 lr=['0.0001000'], tr/val_loss:  0.968138/  1.862714, val:  66.25%, val_best:  67.50%, tr:  93.05%, tr_best:  93.36%\n",
      "epoch-185 lr=['0.0001000'], tr/val_loss:  0.957600/  1.889388, val:  66.25%, val_best:  67.50%, tr:  92.75%, tr_best:  93.36%\n",
      "epoch-186 lr=['0.0001000'], tr/val_loss:  0.967556/  1.868682, val:  65.42%, val_best:  67.50%, tr:  93.05%, tr_best:  93.36%\n",
      "epoch-187 lr=['0.0001000'], tr/val_loss:  0.970153/  1.884139, val:  66.67%, val_best:  67.50%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-188 lr=['0.0001000'], tr/val_loss:  0.958501/  1.884270, val:  67.50%, val_best:  67.50%, tr:  92.85%, tr_best:  93.36%\n",
      "epoch-189 lr=['0.0001000'], tr/val_loss:  0.950412/  1.893104, val:  65.83%, val_best:  67.50%, tr:  92.95%, tr_best:  93.36%\n",
      "epoch-190 lr=['0.0001000'], tr/val_loss:  0.952404/  1.892055, val:  67.92%, val_best:  67.92%, tr:  93.26%, tr_best:  93.36%\n",
      "epoch-191 lr=['0.0001000'], tr/val_loss:  0.959581/  1.900202, val:  65.83%, val_best:  67.92%, tr:  92.34%, tr_best:  93.36%\n",
      "epoch-192 lr=['0.0001000'], tr/val_loss:  0.950126/  1.900082, val:  66.25%, val_best:  67.92%, tr:  92.95%, tr_best:  93.36%\n",
      "epoch-193 lr=['0.0001000'], tr/val_loss:  0.944694/  1.902526, val:  68.75%, val_best:  68.75%, tr:  93.26%, tr_best:  93.36%\n",
      "epoch-194 lr=['0.0001000'], tr/val_loss:  0.949009/  1.921198, val:  66.67%, val_best:  68.75%, tr:  92.65%, tr_best:  93.36%\n",
      "epoch-195 lr=['0.0001000'], tr/val_loss:  0.950347/  1.908825, val:  65.83%, val_best:  68.75%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-196 lr=['0.0001000'], tr/val_loss:  0.951404/  1.916285, val:  67.50%, val_best:  68.75%, tr:  93.56%, tr_best:  93.56%\n",
      "epoch-197 lr=['0.0001000'], tr/val_loss:  0.942118/  1.921767, val:  65.00%, val_best:  68.75%, tr:  94.38%, tr_best:  94.38%\n",
      "epoch-198 lr=['0.0001000'], tr/val_loss:  0.952373/  1.910608, val:  65.83%, val_best:  68.75%, tr:  92.95%, tr_best:  94.38%\n",
      "epoch-199 lr=['0.0001000'], tr/val_loss:  0.936374/  1.915998, val:  67.08%, val_best:  68.75%, tr:  93.97%, tr_best:  94.38%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525d7e51c38948659d7d446f444b7c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄▅▄▆▆▅▇█▅▅▆▇▇▇▆▇▆█▇▆▇▅▇▇▆█▇██▇▇██▇█▇██</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▇▇▇█▇▇▇▇▇▇▇█▇▇▇████▇███▇█████████████</td></tr><tr><td>tr_acc</td><td>▁▃▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▇▇▇█▇▇▇▇▇▇▇█▇▇▇████▇███▇█████████████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.93973</td></tr><tr><td>tr_epoch_loss</td><td>0.93637</td></tr><tr><td>val_acc_best</td><td>0.6875</td></tr><tr><td>val_acc_now</td><td>0.67083</td></tr><tr><td>val_loss</td><td>1.916</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">valiant-sweep-22</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2xqmdivq' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2xqmdivq</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_033005-2xqmdivq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 11wzk3ji with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_034257-11wzk3ji</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/11wzk3ji' target=\"_blank\">denim-sweep-23</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/11wzk3ji' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/11wzk3ji</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.133919/  1.826032, val:  44.17%, val_best:  44.17%, tr:  23.08%, tr_best:  23.08%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.522817/  1.522927, val:  57.08%, val_best:  57.08%, tr:  52.60%, tr_best:  52.60%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  1.321335/  1.442540, val:  57.50%, val_best:  57.50%, tr:  60.16%, tr_best:  60.16%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  1.179908/  1.522818, val:  56.67%, val_best:  57.50%, tr:  65.37%, tr_best:  65.37%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  1.086555/  1.436737, val:  58.33%, val_best:  58.33%, tr:  66.60%, tr_best:  66.60%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  1.036361/  1.404326, val:  66.25%, val_best:  66.25%, tr:  67.93%, tr_best:  67.93%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  0.945741/  1.393567, val:  62.08%, val_best:  66.25%, tr:  74.97%, tr_best:  74.97%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  0.898744/  1.422730, val:  63.75%, val_best:  66.25%, tr:  75.08%, tr_best:  75.08%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  0.860830/  1.546266, val:  64.58%, val_best:  66.25%, tr:  76.61%, tr_best:  76.61%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.806133/  1.522006, val:  69.58%, val_best:  69.58%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.783032/  1.616973, val:  62.50%, val_best:  69.58%, tr:  81.72%, tr_best:  81.72%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.704639/  1.630793, val:  69.58%, val_best:  69.58%, tr:  84.27%, tr_best:  84.27%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.707992/  1.606511, val:  68.75%, val_best:  69.58%, tr:  87.54%, tr_best:  87.54%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.665526/  1.797652, val:  67.92%, val_best:  69.58%, tr:  86.82%, tr_best:  87.54%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.599946/  2.183324, val:  62.92%, val_best:  69.58%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.608924/  2.038687, val:  68.33%, val_best:  69.58%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.553178/  2.031146, val:  70.00%, val_best:  70.00%, tr:  90.91%, tr_best:  90.91%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.536532/  2.066687, val:  70.42%, val_best:  70.42%, tr:  91.01%, tr_best:  91.01%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.519118/  2.167079, val:  69.58%, val_best:  70.42%, tr:  91.73%, tr_best:  91.73%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.513653/  2.236549, val:  67.50%, val_best:  70.42%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.455570/  2.490696, val:  62.92%, val_best:  70.42%, tr:  93.46%, tr_best:  93.46%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.443792/  2.519110, val:  63.75%, val_best:  70.42%, tr:  93.36%, tr_best:  93.46%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.422528/  2.514316, val:  67.08%, val_best:  70.42%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.402570/  2.502496, val:  74.58%, val_best:  74.58%, tr:  95.51%, tr_best:  96.73%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.343912/  2.611645, val:  69.58%, val_best:  74.58%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.311206/  2.594914, val:  74.58%, val_best:  74.58%, tr:  98.67%, tr_best:  98.98%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.284159/  2.686938, val:  77.50%, val_best:  77.50%, tr:  98.57%, tr_best:  98.98%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.254454/  2.889026, val:  73.33%, val_best:  77.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.246491/  2.749185, val:  76.25%, val_best:  77.50%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.213048/  2.826905, val:  71.67%, val_best:  77.50%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.232522/  2.969642, val:  72.92%, val_best:  77.50%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.199059/  3.066490, val:  71.67%, val_best:  77.50%, tr:  99.28%, tr_best:  99.80%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.256937/  3.085670, val:  72.92%, val_best:  77.50%, tr:  98.57%, tr_best:  99.80%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.281734/  3.378034, val:  70.00%, val_best:  77.50%, tr:  98.26%, tr_best:  99.80%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.190194/  3.112411, val:  72.92%, val_best:  77.50%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.156765/  3.211388, val:  73.75%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.132588/  3.206459, val:  76.25%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.104913/  3.413366, val:  72.92%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.110965/  3.377041, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.144304/  3.373658, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.090310/  3.437358, val:  73.75%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.096054/  3.469375, val:  74.58%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.080698/  3.571242, val:  72.50%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.091174/  3.590534, val:  73.75%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.049584/  3.558010, val:  76.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.057244/  3.673374, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.058531/  3.628712, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.042354/  3.728561, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.041708/  3.790869, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.024641/  3.758943, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.038173/  3.897368, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.025874/  3.899831, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.034160/  3.927983, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.030341/  3.955723, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.026818/  3.995395, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.025110/  3.954152, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.017469/  4.039579, val:  73.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.014194/  3.954540, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.018646/  4.069886, val:  73.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.018040/  4.016391, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.017551/  3.919804, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.015884/  4.064029, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.012995/  4.092643, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.012482/  4.049683, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.020077/  4.060364, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.016118/  4.162215, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.017928/  4.172625, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.017996/  4.198344, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.017896/  4.175790, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.013696/  4.242747, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.008673/  4.183822, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.014787/  4.258516, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.013183/  4.219481, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.011049/  4.295837, val:  73.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.009736/  4.235743, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.008682/  4.259464, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.008257/  4.268900, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.007265/  4.244084, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.005158/  4.299711, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.003818/  4.316986, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.003993/  4.334470, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.013977/  4.376269, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.011412/  4.436135, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.006860/  4.469292, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.005923/  4.478921, val:  74.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.015778/  4.444875, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.012066/  4.399466, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.008258/  4.396526, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.005577/  4.358472, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.004609/  4.368003, val:  73.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.003179/  4.423648, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.003736/  4.431959, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.002718/  4.440759, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.003571/  4.407391, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.005151/  4.499129, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.009211/  4.419019, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.005384/  4.416721, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.003482/  4.417145, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.004092/  4.501905, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.002491/  4.474473, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.002752/  4.467135, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.002329/  4.493072, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.002705/  4.502539, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.004404/  4.493348, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.002786/  4.549574, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.004336/  4.579082, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.002514/  4.570199, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.001798/  4.574138, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.001243/  4.568419, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.001771/  4.569366, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.002881/  4.535177, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.002562/  4.571669, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.003508/  4.567947, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.003384/  4.559421, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.002848/  4.566570, val:  73.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.006933/  4.548745, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.004403/  4.620530, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.002700/  4.594342, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.002416/  4.626934, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.001355/  4.624177, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.001220/  4.598546, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.000880/  4.564038, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.001582/  4.610964, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.001876/  4.577811, val:  73.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.001114/  4.606339, val:  74.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.000900/  4.621504, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.000903/  4.613173, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.000739/  4.621625, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.000709/  4.631998, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.000654/  4.604633, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.000650/  4.611117, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.000688/  4.639235, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.000590/  4.613906, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.000881/  4.612198, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.000665/  4.625223, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.000581/  4.630059, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.000584/  4.651197, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.000536/  4.641198, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.000804/  4.642617, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.001352/  4.665639, val:  73.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.001292/  4.609461, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.003188/  4.660063, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.001840/  4.710630, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.001994/  4.686853, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.000887/  4.694886, val:  73.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.000869/  4.686532, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.000946/  4.712016, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.000623/  4.691298, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.000738/  4.696649, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.000762/  4.720848, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.000583/  4.698869, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.000478/  4.684853, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.000409/  4.688794, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.000488/  4.689363, val:  74.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.000428/  4.680291, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.000373/  4.679943, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.000380/  4.674675, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.000839/  4.704665, val:  74.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.000547/  4.695706, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.000402/  4.714833, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.000708/  4.700526, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.000725/  4.674927, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.000527/  4.682393, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.000512/  4.726449, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.000488/  4.703509, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.001750/  4.752549, val:  74.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.003435/  4.777679, val:  74.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.002613/  4.694148, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.002926/  4.703310, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.004541/  4.774714, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.002987/  4.788787, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.003520/  4.777284, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.002467/  4.787256, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.002265/  4.812568, val:  72.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.003042/  4.822174, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.001484/  4.801978, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.001251/  4.787773, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.001444/  4.784527, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.000778/  4.764607, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.000484/  4.794524, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.000528/  4.824072, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.000461/  4.805476, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.000402/  4.805714, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.000391/  4.802330, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.000361/  4.791816, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.000324/  4.793601, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.000334/  4.792658, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.000311/  4.796567, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.000335/  4.791876, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.000442/  4.798708, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.000440/  4.791014, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.000309/  4.791021, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.000326/  4.813035, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.000299/  4.796403, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.000256/  4.786163, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.000287/  4.792151, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.000281/  4.784091, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.000269/  4.791296, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.000260/  4.796659, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.000267/  4.800542, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0285156e9a425cbb7448717d811334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄▇▆▇██████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▅▅▄▇▆▇▆▇▇▇█▇▇▇█▇▆█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>tr_acc</td><td>▁▃▅▇▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▅▅▅▇██████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▅▅▄▇▆▇▆▇▇▇█▇▇▇█▇▆█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss</td><td>▁▁▁▂▃▃▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00027</td></tr><tr><td>val_acc_best</td><td>0.7875</td></tr><tr><td>val_acc_now</td><td>0.76667</td></tr><tr><td>val_loss</td><td>4.80054</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">denim-sweep-23</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/11wzk3ji' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/11wzk3ji</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_034257-11wzk3ji/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cevjjeks with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_035427-cevjjeks</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cevjjeks' target=\"_blank\">pleasant-sweep-24</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cevjjeks' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cevjjeks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.455364/  3.373314, val:  47.08%, val_best:  47.08%, tr:  29.01%, tr_best:  29.01%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.899303/  4.065806, val:  42.92%, val_best:  47.08%, tr:  50.56%, tr_best:  50.56%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.841858/  2.977270, val:  50.42%, val_best:  50.42%, tr:  60.16%, tr_best:  60.16%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.253934/  3.583849, val:  52.50%, val_best:  52.50%, tr:  60.47%, tr_best:  60.47%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.284685/  2.790559, val:  60.83%, val_best:  60.83%, tr:  63.13%, tr_best:  63.13%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.206790/  3.586533, val:  43.75%, val_best:  60.83%, tr:  66.19%, tr_best:  66.19%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  2.032120/  2.877899, val:  59.17%, val_best:  60.83%, tr:  71.20%, tr_best:  71.20%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.859921/  2.757711, val:  65.00%, val_best:  65.00%, tr:  73.44%, tr_best:  73.44%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.269061/  2.538214, val:  61.25%, val_best:  65.00%, tr:  80.39%, tr_best:  80.39%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.770663/  4.782606, val:  47.50%, val_best:  65.00%, tr:  76.71%, tr_best:  80.39%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.529406/  2.785819, val:  69.17%, val_best:  69.17%, tr:  82.02%, tr_best:  82.02%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.363541/  3.904569, val:  63.33%, val_best:  69.17%, tr:  85.80%, tr_best:  85.80%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.260231/  3.125469, val:  72.92%, val_best:  72.92%, tr:  87.13%, tr_best:  87.13%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.796107/  4.112435, val:  63.75%, val_best:  72.92%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.209577/  3.940642, val:  69.58%, val_best:  72.92%, tr:  88.56%, tr_best:  94.18%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.647990/  4.118855, val:  67.08%, val_best:  72.92%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.599359/  4.069872, val:  72.08%, val_best:  72.92%, tr:  97.14%, tr_best:  97.45%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.475531/  4.281201, val:  71.67%, val_best:  72.92%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.580022/  4.580234, val:  71.67%, val_best:  72.92%, tr:  97.96%, tr_best:  98.57%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.448384/  4.395160, val:  74.17%, val_best:  74.17%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.502847/  4.769383, val:  74.17%, val_best:  74.17%, tr:  98.37%, tr_best:  99.08%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.467506/  5.777108, val:  62.50%, val_best:  74.17%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.439506/  5.072851, val:  75.00%, val_best:  75.00%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.391265/  4.975450, val:  77.50%, val_best:  77.50%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.316636/  5.297064, val:  72.92%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.316510/  5.813638, val:  72.50%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.326009/  5.664336, val:  75.83%, val_best:  77.50%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.285759/  6.206834, val:  72.50%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.270163/  5.966508, val:  77.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.244985/  6.298667, val:  72.50%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.246563/  6.228109, val:  74.58%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.221981/  6.327847, val:  74.17%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.230896/  6.721930, val:  73.33%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.236195/  6.821122, val:  76.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.260149/  6.947402, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.234202/  7.143996, val:  74.58%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.227742/  7.199620, val:  73.33%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.204173/  7.538287, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.208373/  7.380743, val:  74.17%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.180105/  7.486423, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.172812/  7.634453, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.159975/  7.830173, val:  72.92%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.164854/  7.798486, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.157036/  8.050690, val:  73.33%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.150259/  7.958678, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.138799/  8.034661, val:  76.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.127395/  8.194040, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.112811/  8.276849, val:  74.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.119447/  8.182576, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.095452/  8.408009, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.108105/  8.468323, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.095019/  8.482285, val:  73.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.129161/  8.493018, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.141743/  8.629195, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.120833/  8.731537, val:  74.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.112675/  9.014659, val:  73.33%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.094863/  9.139616, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.088568/  8.881405, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.092855/  8.862494, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.072382/  8.998873, val:  74.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.076029/  8.958289, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.076767/  9.134044, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.084932/  9.052380, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.090477/  9.452507, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.068453/  9.373864, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.055379/  9.239755, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.077865/  9.305613, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.085018/  9.284686, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.068068/  9.379801, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.059529/  9.937382, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.067661/  9.792639, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.112919/  9.732192, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.096688/  9.458345, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.092632/  9.677931, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.058023/  9.619805, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.042556/  9.681328, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.045145/  9.705740, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.054567/  9.878834, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.048619/  9.878566, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.045700/ 10.021071, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.051370/ 10.074734, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.050688/ 10.091349, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.046144/ 10.250488, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.051321/ 10.455877, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.076008/ 10.161240, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.074219/ 10.356946, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.061660/ 10.750109, val:  72.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.065983/ 10.609365, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.052743/ 10.595515, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.051047/ 10.522212, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.042896/ 10.429729, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.049704/ 10.883442, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.043988/ 10.867670, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.053324/ 10.953720, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.046914/ 11.034394, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.063884/ 10.997526, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.056148/ 11.119863, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.032605/ 11.369781, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.034621/ 11.243969, val:  70.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.047027/ 11.076337, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.043138/ 11.147341, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.035790/ 11.402915, val:  72.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.046923/ 11.524949, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.061858/ 11.470734, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.045064/ 11.567190, val:  72.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.053622/ 11.045960, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.053999/ 11.255358, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.063582/ 11.098380, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.053845/ 11.573768, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.040993/ 11.182286, val:  71.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.034809/ 11.126560, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.038356/ 11.534221, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.051015/ 11.275218, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.044745/ 11.501328, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.055348/ 11.288826, val:  72.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.045178/ 11.330770, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.027010/ 11.593373, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.032124/ 11.412524, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.024338/ 11.457870, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.022957/ 11.610807, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.021009/ 11.641711, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.030886/ 11.753980, val:  72.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.035931/ 11.799728, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.041879/ 11.626575, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.051224/ 11.743142, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.051511/ 11.701236, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.040506/ 11.786373, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.032917/ 12.081877, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.037155/ 11.840179, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.044507/ 12.021312, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.041192/ 11.571972, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.040804/ 11.734069, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.040200/ 11.923837, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.055504/ 11.991071, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.041231/ 11.969332, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.049483/ 11.969682, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.052719/ 12.133442, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.048881/ 12.285796, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.043769/ 12.205515, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.043626/ 12.152830, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.040309/ 12.001695, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.063958/ 12.032341, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.050620/ 11.953169, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.054848/ 12.172704, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.062071/ 12.277184, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.043486/ 12.403327, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.051368/ 12.308645, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.055324/ 12.326088, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.047564/ 12.532583, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.040698/ 12.314151, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.036783/ 12.357987, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.037749/ 12.477654, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.047275/ 12.533868, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.037323/ 12.887048, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.037260/ 12.536792, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.046000/ 12.586699, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.033685/ 12.588768, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.022820/ 12.531190, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.024235/ 12.496548, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.035306/ 12.505904, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.032243/ 12.608229, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.026841/ 12.528405, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.031090/ 12.580680, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.026403/ 12.612243, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.028503/ 12.838509, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.025058/ 12.948985, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.023306/ 12.979784, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.028699/ 13.092006, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.031330/ 13.227590, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.032601/ 13.120907, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.024078/ 12.884663, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.021035/ 13.058984, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.030342/ 13.301166, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.021803/ 13.073792, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.019944/ 13.226153, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.023320/ 13.254568, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.031605/ 13.674371, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.026874/ 13.350432, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.019806/ 13.381299, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.010835/ 13.403846, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.016270/ 13.132707, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.019914/ 13.267259, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.014904/ 13.403132, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.019521/ 13.272013, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.018455/ 13.261113, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.022093/ 13.394824, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.024963/ 13.345602, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.023281/ 13.357985, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.022322/ 13.512047, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.016415/ 13.294991, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.020197/ 13.348522, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.023382/ 13.457898, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.014728/ 13.435902, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.012764/ 13.402011, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.021103/ 13.519627, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.024281/ 13.531902, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.022408/ 13.226461, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.017604/ 13.515037, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.019275/ 13.617903, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.020969/ 13.529392, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8019fdf505ce46ddb37328f52afc744c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▅▇█▇██████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▂▆▇▇▇▇▇██▇████▇▇█▇▇▇▇▇▇█▇████████▇████</td></tr><tr><td>tr_acc</td><td>▁▃▅█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▅▇▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▂▆▇▇▇▇▇██▇████▇▇█▇▇▇▇▇▇█▇████████▇████</td></tr><tr><td>val_loss</td><td>▁▁▂▁▂▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02097</td></tr><tr><td>val_acc_best</td><td>0.78333</td></tr><tr><td>val_acc_now</td><td>0.77917</td></tr><tr><td>val_loss</td><td>13.52939</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pleasant-sweep-24</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cevjjeks' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cevjjeks</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_035427-cevjjeks/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bdtvriy1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63317eef9d6e4aee9950035050ba7c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112648497025172, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_040714-bdtvriy1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bdtvriy1' target=\"_blank\">dandy-sweep-25</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bdtvriy1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bdtvriy1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=0, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=0, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.206309/  1.839610, val:  37.50%, val_best:  37.50%, tr:  16.14%, tr_best:  16.14%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.447634/  1.450399, val:  55.00%, val_best:  55.00%, tr:  54.65%, tr_best:  54.65%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.210675/  1.391495, val:  58.33%, val_best:  58.33%, tr:  59.04%, tr_best:  59.04%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.031376/  1.417855, val:  53.33%, val_best:  58.33%, tr:  67.42%, tr_best:  67.42%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.971092/  1.319667, val:  58.75%, val_best:  58.75%, tr:  66.91%, tr_best:  67.42%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.902215/  1.279631, val:  60.83%, val_best:  60.83%, tr:  67.72%, tr_best:  67.72%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.813989/  1.206874, val:  63.33%, val_best:  63.33%, tr:  73.24%, tr_best:  73.24%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.767998/  1.271002, val:  61.25%, val_best:  63.33%, tr:  73.44%, tr_best:  73.44%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.727984/  1.278680, val:  62.08%, val_best:  63.33%, tr:  75.59%, tr_best:  75.59%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.652672/  1.361745, val:  64.17%, val_best:  64.17%, tr:  80.18%, tr_best:  80.18%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.622348/  1.297516, val:  63.75%, val_best:  64.17%, tr:  80.59%, tr_best:  80.59%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.589241/  1.294984, val:  66.67%, val_best:  66.67%, tr:  84.68%, tr_best:  84.68%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.567903/  1.241659, val:  72.92%, val_best:  72.92%, tr:  86.52%, tr_best:  86.52%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.533634/  1.358114, val:  66.67%, val_best:  72.92%, tr:  86.41%, tr_best:  86.52%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.459453/  1.430737, val:  70.00%, val_best:  72.92%, tr:  88.36%, tr_best:  88.36%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.418875/  1.437424, val:  68.33%, val_best:  72.92%, tr:  90.70%, tr_best:  90.70%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.377669/  1.508198, val:  70.00%, val_best:  72.92%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.399457/  1.506872, val:  71.67%, val_best:  72.92%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.349822/  1.549620, val:  70.00%, val_best:  72.92%, tr:  93.46%, tr_best:  94.48%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.334671/  1.544585, val:  67.08%, val_best:  72.92%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.292253/  1.577467, val:  73.33%, val_best:  73.33%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.278912/  1.752577, val:  67.92%, val_best:  73.33%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.237377/  1.682381, val:  73.33%, val_best:  73.33%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.252649/  1.595761, val:  73.33%, val_best:  73.33%, tr:  97.45%, tr_best:  98.57%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.186041/  1.788218, val:  75.00%, val_best:  75.00%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.154228/  1.739510, val:  73.33%, val_best:  75.00%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.161138/  1.808353, val:  75.00%, val_best:  75.00%, tr:  98.37%, tr_best:  99.80%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.154090/  1.824987, val:  75.83%, val_best:  75.83%, tr:  99.08%, tr_best:  99.80%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.119032/  1.900591, val:  80.00%, val_best:  80.00%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.101307/  2.035150, val:  76.25%, val_best:  80.00%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.111554/  1.998455, val:  79.17%, val_best:  80.00%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.104596/  2.107725, val:  75.42%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.107245/  2.014449, val:  79.17%, val_best:  80.00%, tr:  99.39%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.120852/  1.959913, val:  77.08%, val_best:  80.00%, tr:  99.39%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.106709/  2.174119, val:  72.92%, val_best:  80.00%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.097233/  2.109442, val:  75.42%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.063356/  2.090015, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.058730/  2.323702, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.060405/  2.250103, val:  77.08%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.057473/  2.298175, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.039034/  2.298766, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.040286/  2.323667, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.033221/  2.391841, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.025522/  2.371153, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.025056/  2.375999, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.024736/  2.364393, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.022383/  2.401385, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.017966/  2.496916, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.016993/  2.494390, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.013976/  2.573827, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.012528/  2.597630, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.013137/  2.624229, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.013912/  2.637437, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.016415/  2.654492, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.024584/  2.658624, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.032580/  2.591763, val:  77.92%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.022568/  2.675340, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.019092/  2.706290, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.018463/  2.763576, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.025992/  2.667649, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.013128/  2.719157, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.013594/  2.680163, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.010535/  2.743062, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.012464/  2.752401, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.012439/  2.826082, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.008443/  2.874258, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.008966/  2.818060, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.011051/  2.819476, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.011412/  2.828658, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.006240/  2.819767, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.007993/  2.834508, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.008475/  2.849049, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.007794/  2.857189, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.007277/  2.885160, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.006906/  2.859917, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.005797/  2.878273, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.004503/  2.932020, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.005572/  2.910145, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.004115/  2.905699, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.005563/  2.922689, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.003886/  2.878061, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.006926/  2.929669, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.004821/  2.940753, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.003175/  2.945684, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.003157/  2.955010, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.003090/  2.954209, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.003289/  2.942860, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.003009/  2.915268, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.003202/  2.945193, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.002701/  2.947950, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.003961/  2.943550, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.008147/  3.009158, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.006316/  2.988840, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.005286/  3.048542, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.003872/  3.003692, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.004077/  3.009473, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.003360/  2.955266, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.002732/  2.961970, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.002716/  2.980536, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.003666/  2.985763, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.003524/  3.020920, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.004123/  3.084101, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.003177/  3.012201, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.002321/  3.053295, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.002445/  3.047549, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.003803/  3.037272, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.002613/  3.044888, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.010278/  3.150820, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.007641/  3.093969, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.046793/  3.168989, val:  77.92%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.060397/  2.959403, val:  76.67%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.063504/  3.343072, val:  74.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.047603/  3.076497, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.020690/  2.988597, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.015134/  2.941309, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.009942/  3.085530, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.011261/  3.094648, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.008250/  2.993011, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.007561/  2.999631, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.007627/  2.964701, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.004968/  3.022542, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.003187/  3.010592, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.003439/  3.016236, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.002325/  3.072217, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.003130/  3.050987, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.001581/  3.044828, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.001857/  3.000542, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.002411/  3.052533, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.001916/  3.059165, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.001806/  3.041830, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.002463/  3.013267, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.002023/  3.077500, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.001669/  3.072171, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.002274/  3.028098, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.003000/  3.027041, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.001947/  3.047664, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.002854/  3.169020, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.001443/  3.071034, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.001321/  3.064950, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.000930/  3.094842, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.001016/  3.098996, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.000880/  3.134990, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.000994/  3.103562, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.001487/  3.130092, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.001155/  3.109646, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.001559/  3.135847, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.000929/  3.138188, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.001234/  3.140346, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.002432/  3.152355, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.001310/  3.126656, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.001517/  3.121356, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.001212/  3.161338, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.001209/  3.187250, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.000993/  3.162266, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.000878/  3.137544, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.000849/  3.132954, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.000739/  3.140924, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.000770/  3.150578, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.001111/  3.153942, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.000860/  3.172507, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.000840/  3.173173, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.000767/  3.164078, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.000645/  3.148427, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.000621/  3.147560, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.000816/  3.144960, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.000729/  3.140982, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.000528/  3.142384, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.000531/  3.138597, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.000544/  3.145115, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.000486/  3.150679, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.000464/  3.148479, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.000449/  3.153770, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.000421/  3.143974, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.000501/  3.149831, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.000551/  3.160415, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.000383/  3.157870, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.000450/  3.165188, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.000346/  3.168107, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.000488/  3.165091, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.001631/  3.192520, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.000779/  3.208113, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.000843/  3.209053, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.000620/  3.201775, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.000513/  3.184113, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.000602/  3.221927, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.000415/  3.208902, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.000392/  3.214973, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.000365/  3.209356, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.000820/  3.189199, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.000520/  3.184774, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.000349/  3.201527, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.000338/  3.213350, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.000317/  3.199720, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.000355/  3.205625, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.000407/  3.221318, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.000309/  3.245463, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.000353/  3.242853, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.000301/  3.251111, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.000320/  3.242484, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.000317/  3.247361, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c14f3c3cfb417fa60bcbcbceba89da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄█▇▇██████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▃▅▄▆▇█▇██▇▇▇██▇▇████▇▇▇▇▇██▇██████████</td></tr><tr><td>tr_acc</td><td>▁▃▅▇▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▃▆▆▆██████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▃▅▄▆▇█▇██▇▇▇██▇▇████▇▇▇▇▇██▇██████████</td></tr><tr><td>val_loss</td><td>▂▁▁▂▂▃▄▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00032</td></tr><tr><td>val_acc_best</td><td>0.81667</td></tr><tr><td>val_acc_now</td><td>0.7875</td></tr><tr><td>val_loss</td><td>3.24736</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dandy-sweep-25</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bdtvriy1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bdtvriy1</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_040714-bdtvriy1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3a9wwoti with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b338bd6830d4bf4b6b7a11cff88f138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113842747484644, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_041843-3a9wwoti</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3a9wwoti' target=\"_blank\">playful-sweep-26</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3a9wwoti' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3a9wwoti</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.305365/  2.302841, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:   8.17%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.304994/  2.302639, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:   8.17%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.305079/  2.302667, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:   8.99%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.304731/  2.302708, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:   8.99%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  2.304833/  2.302682, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:   8.99%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  2.304569/  2.302663, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:   8.99%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  2.305086/  2.302687, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:   9.70%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  2.304632/  2.302616, val:  10.00%, val_best:  10.00%, tr:   7.46%, tr_best:   9.70%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  2.304248/  2.301879, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:   9.70%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  2.300481/  2.295480, val:  12.08%, val_best:  12.08%, tr:   9.40%, tr_best:   9.70%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  2.268817/  2.253939, val:  12.50%, val_best:  12.50%, tr:  10.42%, tr_best:  10.42%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  2.207837/  2.208783, val:  22.08%, val_best:  22.08%, tr:  19.71%, tr_best:  19.71%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  2.144655/  2.135748, val:  35.42%, val_best:  35.42%, tr:  27.37%, tr_best:  27.37%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  2.041451/  2.005190, val:  37.50%, val_best:  37.50%, tr:  37.49%, tr_best:  37.49%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  1.866221/  1.844668, val:  43.75%, val_best:  43.75%, tr:  42.90%, tr_best:  42.90%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  1.692452/  1.726154, val:  49.58%, val_best:  49.58%, tr:  52.30%, tr_best:  52.30%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  1.563980/  1.650515, val:  50.83%, val_best:  50.83%, tr:  54.24%, tr_best:  54.24%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  1.473720/  1.575017, val:  58.33%, val_best:  58.33%, tr:  58.32%, tr_best:  58.32%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  1.405635/  1.570488, val:  57.50%, val_best:  58.33%, tr:  61.18%, tr_best:  61.18%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  1.369985/  1.539935, val:  56.25%, val_best:  58.33%, tr:  59.45%, tr_best:  61.18%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  1.317927/  1.534495, val:  55.42%, val_best:  58.33%, tr:  62.51%, tr_best:  62.51%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  1.286670/  1.503629, val:  59.58%, val_best:  59.58%, tr:  60.98%, tr_best:  62.51%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  1.268695/  1.514358, val:  61.25%, val_best:  61.25%, tr:  63.64%, tr_best:  63.64%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  1.232520/  1.488444, val:  62.08%, val_best:  62.08%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  1.191927/  1.473212, val:  60.42%, val_best:  62.08%, tr:  66.09%, tr_best:  66.09%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  1.175041/  1.448173, val:  63.33%, val_best:  63.33%, tr:  66.19%, tr_best:  66.19%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  1.159790/  1.429764, val:  60.42%, val_best:  63.33%, tr:  65.78%, tr_best:  66.19%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  1.127274/  1.459897, val:  57.92%, val_best:  63.33%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  1.123573/  1.415784, val:  59.58%, val_best:  63.33%, tr:  68.03%, tr_best:  69.36%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  1.095694/  1.435054, val:  61.67%, val_best:  63.33%, tr:  69.56%, tr_best:  69.56%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  1.081149/  1.421830, val:  59.17%, val_best:  63.33%, tr:  69.77%, tr_best:  69.77%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  1.074311/  1.441893, val:  56.25%, val_best:  63.33%, tr:  67.01%, tr_best:  69.77%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  1.062450/  1.428945, val:  60.00%, val_best:  63.33%, tr:  70.17%, tr_best:  70.17%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  1.041052/  1.401577, val:  62.08%, val_best:  63.33%, tr:  72.01%, tr_best:  72.01%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  1.018039/  1.404781, val:  58.75%, val_best:  63.33%, tr:  70.58%, tr_best:  72.01%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  1.014070/  1.414909, val:  57.08%, val_best:  63.33%, tr:  72.22%, tr_best:  72.22%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.986536/  1.390658, val:  61.67%, val_best:  63.33%, tr:  72.73%, tr_best:  72.73%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.984306/  1.402335, val:  59.58%, val_best:  63.33%, tr:  71.60%, tr_best:  72.73%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.969381/  1.395233, val:  58.75%, val_best:  63.33%, tr:  74.46%, tr_best:  74.46%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.959659/  1.402839, val:  60.83%, val_best:  63.33%, tr:  73.34%, tr_best:  74.46%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.947773/  1.373998, val:  60.83%, val_best:  63.33%, tr:  72.63%, tr_best:  74.46%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.930798/  1.398378, val:  58.75%, val_best:  63.33%, tr:  75.59%, tr_best:  75.59%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.915794/  1.386652, val:  61.67%, val_best:  63.33%, tr:  73.34%, tr_best:  75.59%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.904446/  1.395302, val:  58.33%, val_best:  63.33%, tr:  76.51%, tr_best:  76.51%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.892240/  1.380727, val:  62.92%, val_best:  63.33%, tr:  75.38%, tr_best:  76.51%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.874108/  1.372581, val:  62.92%, val_best:  63.33%, tr:  76.81%, tr_best:  76.81%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.875960/  1.368329, val:  62.92%, val_best:  63.33%, tr:  76.92%, tr_best:  76.92%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.853609/  1.368273, val:  65.83%, val_best:  65.83%, tr:  77.73%, tr_best:  77.73%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.857133/  1.363494, val:  65.42%, val_best:  65.83%, tr:  78.14%, tr_best:  78.14%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.846432/  1.340609, val:  67.50%, val_best:  67.50%, tr:  77.53%, tr_best:  78.14%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.829859/  1.358656, val:  65.42%, val_best:  67.50%, tr:  79.88%, tr_best:  79.88%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.817287/  1.367014, val:  64.58%, val_best:  67.50%, tr:  79.26%, tr_best:  79.88%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.803116/  1.407795, val:  59.17%, val_best:  67.50%, tr:  79.67%, tr_best:  79.88%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.804488/  1.363513, val:  65.42%, val_best:  67.50%, tr:  79.57%, tr_best:  79.88%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.788194/  1.365171, val:  65.42%, val_best:  67.50%, tr:  82.12%, tr_best:  82.12%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.782207/  1.381408, val:  65.83%, val_best:  67.50%, tr:  79.78%, tr_best:  82.12%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.762015/  1.431027, val:  61.67%, val_best:  67.50%, tr:  83.04%, tr_best:  83.04%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.765712/  1.381568, val:  65.00%, val_best:  67.50%, tr:  81.72%, tr_best:  83.04%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.737623/  1.400590, val:  68.33%, val_best:  68.33%, tr:  81.82%, tr_best:  83.04%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.735730/  1.389349, val:  70.83%, val_best:  70.83%, tr:  83.45%, tr_best:  83.45%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.725846/  1.370837, val:  70.83%, val_best:  70.83%, tr:  84.68%, tr_best:  84.68%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.719573/  1.405846, val:  64.17%, val_best:  70.83%, tr:  85.39%, tr_best:  85.39%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.707915/  1.390126, val:  73.33%, val_best:  73.33%, tr:  86.72%, tr_best:  86.72%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.702764/  1.378298, val:  69.17%, val_best:  73.33%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.699158/  1.424443, val:  68.75%, val_best:  73.33%, tr:  84.27%, tr_best:  88.76%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.681961/  1.412144, val:  70.42%, val_best:  73.33%, tr:  86.52%, tr_best:  88.76%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.675301/  1.410702, val:  70.42%, val_best:  73.33%, tr:  89.17%, tr_best:  89.17%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.665592/  1.408295, val:  70.42%, val_best:  73.33%, tr:  87.44%, tr_best:  89.17%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.647451/  1.426969, val:  72.08%, val_best:  73.33%, tr:  87.54%, tr_best:  89.17%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.637111/  1.434765, val:  70.42%, val_best:  73.33%, tr:  90.81%, tr_best:  90.81%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.627455/  1.434895, val:  72.92%, val_best:  73.33%, tr:  89.58%, tr_best:  90.81%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.622537/  1.474993, val:  66.25%, val_best:  73.33%, tr:  90.70%, tr_best:  90.81%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.615578/  1.471676, val:  70.42%, val_best:  73.33%, tr:  91.11%, tr_best:  91.11%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.619660/  1.484382, val:  70.00%, val_best:  73.33%, tr:  92.24%, tr_best:  92.24%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.595050/  1.440020, val:  76.25%, val_best:  76.25%, tr:  92.54%, tr_best:  92.54%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.594585/  1.457885, val:  70.83%, val_best:  76.25%, tr:  90.40%, tr_best:  92.54%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.571130/  1.462730, val:  68.33%, val_best:  76.25%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.583456/  1.456992, val:  74.17%, val_best:  76.25%, tr:  92.24%, tr_best:  92.95%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.566365/  1.446300, val:  74.17%, val_best:  76.25%, tr:  92.03%, tr_best:  92.95%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.556312/  1.450551, val:  72.92%, val_best:  76.25%, tr:  92.13%, tr_best:  92.95%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.539458/  1.479142, val:  73.33%, val_best:  76.25%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.544730/  1.489129, val:  72.92%, val_best:  76.25%, tr:  93.46%, tr_best:  94.99%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.530484/  1.489626, val:  72.08%, val_best:  76.25%, tr:  93.16%, tr_best:  94.99%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.520215/  1.520845, val:  74.17%, val_best:  76.25%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.518587/  1.488183, val:  77.08%, val_best:  77.08%, tr:  95.81%, tr_best:  96.12%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.516759/  1.539339, val:  70.42%, val_best:  77.08%, tr:  95.40%, tr_best:  96.12%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.503375/  1.551210, val:  72.92%, val_best:  77.08%, tr:  93.36%, tr_best:  96.12%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.490405/  1.537894, val:  75.42%, val_best:  77.08%, tr:  95.91%, tr_best:  96.12%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.476189/  1.539094, val:  74.17%, val_best:  77.08%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.458091/  1.545563, val:  76.25%, val_best:  77.08%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.469845/  1.528781, val:  75.83%, val_best:  77.08%, tr:  95.91%, tr_best:  98.16%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.462437/  1.556192, val:  75.42%, val_best:  77.08%, tr:  95.81%, tr_best:  98.16%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.444847/  1.594491, val:  73.33%, val_best:  77.08%, tr:  96.94%, tr_best:  98.16%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.450716/  1.569063, val:  75.42%, val_best:  77.08%, tr:  97.65%, tr_best:  98.16%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.435006/  1.574937, val:  72.50%, val_best:  77.08%, tr:  97.75%, tr_best:  98.16%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.429879/  1.647828, val:  70.83%, val_best:  77.08%, tr:  97.34%, tr_best:  98.16%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.430242/  1.652655, val:  72.92%, val_best:  77.08%, tr:  97.55%, tr_best:  98.16%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.420570/  1.632021, val:  72.92%, val_best:  77.08%, tr:  96.63%, tr_best:  98.16%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.400982/  1.652877, val:  72.50%, val_best:  77.08%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.411732/  1.650480, val:  68.75%, val_best:  77.08%, tr:  98.26%, tr_best:  98.47%\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.387099/  1.644950, val:  75.42%, val_best:  77.08%, tr:  98.37%, tr_best:  98.47%\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.379260/  1.644175, val:  75.00%, val_best:  77.08%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.379209/  1.674743, val:  75.42%, val_best:  77.08%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.378884/  1.684163, val:  77.50%, val_best:  77.50%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.359290/  1.734268, val:  67.92%, val_best:  77.50%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.356506/  1.697208, val:  77.08%, val_best:  77.50%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.361964/  1.752180, val:  72.50%, val_best:  77.50%, tr:  98.88%, tr_best:  99.39%\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.337721/  1.745867, val:  72.92%, val_best:  77.50%, tr:  98.77%, tr_best:  99.39%\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.338038/  1.727547, val:  77.08%, val_best:  77.50%, tr:  98.77%, tr_best:  99.39%\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.325772/  1.782407, val:  76.67%, val_best:  77.50%, tr:  98.77%, tr_best:  99.39%\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.327873/  1.775111, val:  79.17%, val_best:  79.17%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.313977/  1.774781, val:  78.75%, val_best:  79.17%, tr:  99.28%, tr_best:  99.39%\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.310951/  1.789696, val:  77.50%, val_best:  79.17%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.311144/  1.839340, val:  74.58%, val_best:  79.17%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.312435/  1.844881, val:  75.00%, val_best:  79.17%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.304488/  1.876688, val:  75.00%, val_best:  79.17%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.287069/  1.854050, val:  79.17%, val_best:  79.17%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.293300/  1.867108, val:  74.58%, val_best:  79.17%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.277842/  1.883388, val:  78.75%, val_best:  79.17%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.277948/  1.887998, val:  76.67%, val_best:  79.17%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.274425/  1.909334, val:  77.92%, val_best:  79.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.263149/  1.878111, val:  80.42%, val_best:  80.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.264661/  1.914983, val:  77.08%, val_best:  80.42%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.262589/  1.948405, val:  77.50%, val_best:  80.42%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.251847/  1.926184, val:  75.83%, val_best:  80.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.243996/  1.947142, val:  77.92%, val_best:  80.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.239446/  2.002187, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.243439/  2.008360, val:  75.42%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.229616/  1.985208, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.227648/  2.028227, val:  78.33%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.216452/  2.011555, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.219689/  2.015426, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.201604/  2.043691, val:  79.58%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.205842/  2.046292, val:  80.83%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.195998/  2.082553, val:  77.92%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.199329/  2.077071, val:  82.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.197724/  2.068258, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.198163/  2.117312, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.187806/  2.144309, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.187170/  2.119164, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.186397/  2.201897, val:  73.75%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.178487/  2.165218, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.168472/  2.192458, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.175716/  2.177025, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.167661/  2.190080, val:  79.58%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.162592/  2.180312, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.156795/  2.225870, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.153036/  2.251381, val:  78.75%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.155224/  2.244984, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.148340/  2.247402, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.142424/  2.304069, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.146014/  2.304688, val:  77.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.141402/  2.281482, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.141383/  2.318595, val:  77.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.137842/  2.335600, val:  77.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.135983/  2.330894, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.132403/  2.343621, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.128377/  2.360812, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.123422/  2.375025, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.128003/  2.378421, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.119352/  2.389107, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.116222/  2.411565, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.116775/  2.416245, val:  77.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.110821/  2.412977, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.109984/  2.418495, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.112370/  2.435098, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.112988/  2.479150, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.110583/  2.455532, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.107318/  2.477101, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.106827/  2.485532, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.100804/  2.480160, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.097893/  2.518975, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.095591/  2.502668, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.089229/  2.529114, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.091034/  2.501387, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.088826/  2.516310, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.088312/  2.538991, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.088902/  2.532566, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.090610/  2.564671, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.085228/  2.583915, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.082727/  2.583214, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.083800/  2.611783, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.087371/  2.618216, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.085342/  2.634552, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.079612/  2.629613, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.072572/  2.617373, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.074345/  2.682221, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.075024/  2.676584, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.080192/  2.660061, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.078185/  2.665684, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.072140/  2.695946, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.073279/  2.712234, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.070102/  2.724150, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.073860/  2.717012, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.067209/  2.717652, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.064947/  2.763545, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.065585/  2.745015, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.062212/  2.728256, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.063699/  2.734243, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.064896/  2.763636, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1df6d6886f24784b0d4a0957f2beb96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▁▅▄▅▅▇▇▇▄▆▆▇██▆███████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▅▅▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇███▇██▇███▇▇██████</td></tr><tr><td>tr_acc</td><td>▁▁▁▄▅▅▆▆▆▆▆▇▇▇▇▇▇███████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>███▆▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▁▅▅▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇███▇██▇███▇▇██████</td></tr><tr><td>val_loss</td><td>▆▆▆▃▂▁▁▁▁▁▁▁▁▁▂▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0649</td></tr><tr><td>val_acc_best</td><td>0.825</td></tr><tr><td>val_acc_now</td><td>0.8</td></tr><tr><td>val_loss</td><td>2.76364</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-sweep-26</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3a9wwoti' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3a9wwoti</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_041843-3a9wwoti/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rj6x1z90 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b300535b4a66471abb859edd9af2998e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113178191913499, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_043014-rj6x1z90</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rj6x1z90' target=\"_blank\">zesty-sweep-27</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rj6x1z90' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rj6x1z90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  6.104451/ 12.408695, val:  22.92%, val_best:  22.92%, tr:  21.25%, tr_best:  21.25%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  9.405617/ 11.245322, val:  35.83%, val_best:  35.83%, tr:  37.18%, tr_best:  37.18%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  8.646005/  8.800406, val:  43.33%, val_best:  43.33%, tr:  44.02%, tr_best:  44.02%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  7.288555/  7.073647, val:  40.42%, val_best:  43.33%, tr:  53.32%, tr_best:  53.32%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  7.737248/  5.949828, val:  50.42%, val_best:  50.42%, tr:  46.17%, tr_best:  53.32%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  6.746396/ 10.727661, val:  37.50%, val_best:  50.42%, tr:  52.91%, tr_best:  53.32%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  5.854382/  6.920156, val:  52.92%, val_best:  52.92%, tr:  55.06%, tr_best:  55.06%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  4.896102/  7.970634, val:  39.17%, val_best:  52.92%, tr:  56.08%, tr_best:  56.08%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  4.005354/  9.079133, val:  48.33%, val_best:  52.92%, tr:  63.13%, tr_best:  63.13%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  5.494167/  8.894923, val:  46.67%, val_best:  52.92%, tr:  63.84%, tr_best:  63.84%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  4.959172/  5.377693, val:  50.83%, val_best:  52.92%, tr:  61.29%, tr_best:  63.84%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  4.321651/  7.140476, val:  49.58%, val_best:  52.92%, tr:  67.21%, tr_best:  67.21%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  4.482275/  7.333644, val:  49.58%, val_best:  52.92%, tr:  64.15%, tr_best:  67.21%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  3.530829/  7.500224, val:  56.25%, val_best:  56.25%, tr:  70.79%, tr_best:  70.79%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  4.236125/ 13.522959, val:  46.67%, val_best:  56.25%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  3.796385/  6.482576, val:  60.83%, val_best:  60.83%, tr:  75.18%, tr_best:  75.18%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  2.846920/  7.696304, val:  52.92%, val_best:  60.83%, tr:  78.24%, tr_best:  78.24%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  2.697276/  7.516246, val:  55.42%, val_best:  60.83%, tr:  81.72%, tr_best:  81.72%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  1.921232/  7.665833, val:  60.42%, val_best:  60.83%, tr:  85.80%, tr_best:  85.80%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  1.595709/  6.318179, val:  64.17%, val_best:  64.17%, tr:  90.50%, tr_best:  90.50%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  2.802898/  6.815664, val:  61.25%, val_best:  64.17%, tr:  84.98%, tr_best:  90.50%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  1.938255/ 10.478773, val:  43.75%, val_best:  64.17%, tr:  88.46%, tr_best:  90.50%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  1.531603/  6.966139, val:  64.17%, val_best:  64.17%, tr:  92.34%, tr_best:  92.34%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  1.242835/  7.156806, val:  66.25%, val_best:  66.25%, tr:  93.87%, tr_best:  93.87%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  1.299094/  7.706964, val:  56.67%, val_best:  66.25%, tr:  92.44%, tr_best:  93.87%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.873439/  6.417044, val:  67.08%, val_best:  67.08%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.753883/  7.807230, val:  64.17%, val_best:  67.08%, tr:  96.53%, tr_best:  97.45%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.761798/  6.466825, val:  71.25%, val_best:  71.25%, tr:  96.53%, tr_best:  97.45%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.467949/  6.140565, val:  70.00%, val_best:  71.25%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.529963/  6.656901, val:  64.58%, val_best:  71.25%, tr:  98.37%, tr_best:  99.08%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.442642/  6.431249, val:  66.67%, val_best:  71.25%, tr:  98.77%, tr_best:  99.08%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.399504/  5.749011, val:  72.92%, val_best:  72.92%, tr:  98.77%, tr_best:  99.08%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.410140/  6.699229, val:  64.58%, val_best:  72.92%, tr:  98.88%, tr_best:  99.08%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.521834/  5.834877, val:  70.00%, val_best:  72.92%, tr:  97.34%, tr_best:  99.08%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.421495/  6.859036, val:  65.00%, val_best:  72.92%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.304874/  6.375874, val:  65.42%, val_best:  72.92%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.178296/  6.153040, val:  71.67%, val_best:  72.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.183264/  6.838583, val:  65.83%, val_best:  72.92%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.203953/  6.321167, val:  68.75%, val_best:  72.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.151880/  6.025387, val:  73.33%, val_best:  73.33%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.172989/  5.737017, val:  75.83%, val_best:  75.83%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.173056/  5.999352, val:  69.17%, val_best:  75.83%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.128567/  6.314424, val:  70.83%, val_best:  75.83%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.119287/  6.048436, val:  70.42%, val_best:  75.83%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.087602/  6.209592, val:  68.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.095120/  5.788483, val:  74.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.069676/  5.963834, val:  72.50%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.097641/  5.663496, val:  73.75%, val_best:  75.83%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.060009/  5.658386, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.085287/  6.159375, val:  72.92%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.059300/  5.609413, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.062604/  5.866974, val:  72.92%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.042266/  5.691049, val:  75.42%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.032472/  5.767197, val:  74.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.028175/  5.955770, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.032294/  5.981978, val:  72.92%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.023010/  5.807823, val:  71.67%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.015695/  5.750301, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.014032/  5.966507, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.027082/  5.714025, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.026603/  5.785972, val:  72.08%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.037954/  5.960350, val:  69.58%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.032083/  5.879421, val:  70.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.068285/  5.466600, val:  78.33%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.059703/  5.756718, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.052276/  5.793463, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.031238/  5.908401, val:  70.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.032516/  5.737232, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.016339/  5.623982, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.013785/  5.543766, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.017960/  5.810667, val:  75.83%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.020451/  5.589261, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.011055/  5.698411, val:  72.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.009199/  5.529145, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.010007/  5.685806, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.013841/  5.638896, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.007852/  5.763380, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.006806/  5.685585, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.014641/  5.575741, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.009029/  5.622623, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.007838/  5.536713, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.005316/  5.577407, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.005710/  5.683157, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.007779/  5.590489, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.003961/  5.656262, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.003904/  5.614245, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.002821/  5.584024, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.004164/  5.448453, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.002612/  5.468295, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.002735/  5.521860, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.007309/  5.680557, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.003367/  5.529819, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.003557/  5.494003, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.007802/  5.592093, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.004706/  5.585871, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.006012/  5.604115, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.006019/  5.596121, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.007887/  5.493236, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.005344/  5.746715, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.007211/  5.600481, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss:  0.010887/  5.498770, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss:  0.005117/  5.430585, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss:  0.005610/  5.692046, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss:  0.004775/  5.635615, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss:  0.004935/  5.736686, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss:  0.003982/  5.634544, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss:  0.001081/  5.747770, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss:  0.000526/  5.690586, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss:  0.000283/  5.709857, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss:  0.000306/  5.714178, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss:  0.000233/  5.732199, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss:  0.000475/  5.678368, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss:  0.000228/  5.669680, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss:  0.000173/  5.711006, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss:  0.000183/  5.699229, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss:  0.000160/  5.705275, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss:  0.000180/  5.666767, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss:  0.000330/  5.682128, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss:  0.000360/  5.708430, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss:  0.000195/  5.686882, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss:  0.000164/  5.695911, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss:  0.000149/  5.703588, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss:  0.000153/  5.704798, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss:  0.000146/  5.703024, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss:  0.000160/  5.687282, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss:  0.000134/  5.692333, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss:  0.000142/  5.689324, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss:  0.000123/  5.688247, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss:  0.000187/  5.723728, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss:  0.000147/  5.755147, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss:  0.000125/  5.726407, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss:  0.000126/  5.708992, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss:  0.000115/  5.682009, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss:  0.000502/  5.814360, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss:  0.007635/  5.798118, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss:  0.005385/  5.731215, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss:  0.001775/  5.810250, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss:  0.001956/  5.762446, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss:  0.001137/  5.751992, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss:  0.000636/  5.804710, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss:  0.001497/  5.776482, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss:  0.008637/  5.808769, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss:  0.002555/  5.807416, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss:  0.001013/  5.701323, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss:  0.001005/  5.775955, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss:  0.000497/  5.786131, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss:  0.002791/  5.664281, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss:  0.003347/  5.705086, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss:  0.000579/  5.699217, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss:  0.000227/  5.664361, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss:  0.000134/  5.668339, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss:  0.000160/  5.689256, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss:  0.000134/  5.662357, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss:  0.000821/  5.680472, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss:  0.001257/  5.803351, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss:  0.001339/  5.658925, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss:  0.003446/  5.772974, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss:  0.003442/  5.676703, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss:  0.002976/  5.712402, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss:  0.005157/  5.618739, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss:  0.002491/  5.667279, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss:  0.007487/  5.667727, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss:  0.002945/  5.719901, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss:  0.000891/  5.670191, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss:  0.000262/  5.660770, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss:  0.000120/  5.652505, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss:  0.000105/  5.661847, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss:  0.000293/  5.671389, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss:  0.000735/  5.677655, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss:  0.000131/  5.712216, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss:  0.000117/  5.695220, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss:  0.000107/  5.713678, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss:  0.000093/  5.724354, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss:  0.000120/  5.692312, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss:  0.000113/  5.687668, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss:  0.000498/  5.733787, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss:  0.000112/  5.687705, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss:  0.000097/  5.679769, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss:  0.000084/  5.681123, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss:  0.000077/  5.680458, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss:  0.000070/  5.657591, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss:  0.000132/  5.658465, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss:  0.000119/  5.667171, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss:  0.000086/  5.680641, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss:  0.000076/  5.693544, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss:  0.000346/  5.678256, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss:  0.000656/  5.779871, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss:  0.000189/  5.749375, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss:  0.000109/  5.739730, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss:  0.000076/  5.733337, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss:  0.000083/  5.737081, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss:  0.000067/  5.727843, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss:  0.000069/  5.707055, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss:  0.000665/  5.741195, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss:  0.000112/  5.699194, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss:  0.000072/  5.700742, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss:  0.000069/  5.691124, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss:  0.000071/  5.680468, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss:  0.000064/  5.670603, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss:  0.000065/  5.661845, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca77d5a33e354700ae0b080abff15e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▅▁▆▅███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▃▅▆▆▆▇█▆▇▇▇▇██████▇██████▇████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▅▆▆▇▇████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▃▅▆▆▆▇█▆▇▇▇▇██████▇██████▇████████████</td></tr><tr><td>val_loss</td><td>█▇▅▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>7e-05</td></tr><tr><td>val_acc_best</td><td>0.78333</td></tr><tr><td>val_acc_now</td><td>0.76667</td></tr><tr><td>val_loss</td><td>5.66184</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-sweep-27</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rj6x1z90' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rj6x1z90</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_043014-rj6x1z90/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b2vuk2cm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_044255-b2vuk2cm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b2vuk2cm' target=\"_blank\">valiant-sweep-28</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b2vuk2cm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b2vuk2cm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 26.251949/ 30.670582, val:  37.50%, val_best:  37.50%, tr:  25.94%, tr_best:  25.94%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 25.884537/ 29.975765, val:  31.67%, val_best:  37.50%, tr:  43.82%, tr_best:  43.82%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 18.756996/ 20.243078, val:  45.00%, val_best:  45.00%, tr:  51.28%, tr_best:  51.28%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 19.011457/ 13.357888, val:  54.58%, val_best:  54.58%, tr:  55.36%, tr_best:  55.36%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 19.964314/ 28.762327, val:  48.33%, val_best:  54.58%, tr:  58.63%, tr_best:  58.63%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 12.154491/ 13.913876, val:  54.17%, val_best:  54.58%, tr:  67.21%, tr_best:  67.21%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 10.393832/ 18.975069, val:  59.17%, val_best:  59.17%, tr:  68.64%, tr_best:  68.64%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 11.780157/ 27.523699, val:  47.92%, val_best:  59.17%, tr:  69.66%, tr_best:  69.66%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  7.949461/ 12.939748, val:  64.17%, val_best:  64.17%, tr:  75.59%, tr_best:  75.59%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  8.045184/ 23.222069, val:  52.08%, val_best:  64.17%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  3.937126/ 17.383034, val:  58.33%, val_best:  64.17%, tr:  87.84%, tr_best:  87.84%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  6.695576/ 13.871948, val:  72.92%, val_best:  72.92%, tr:  84.58%, tr_best:  87.84%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  2.283961/ 14.183827, val:  67.08%, val_best:  72.92%, tr:  94.59%, tr_best:  94.59%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  1.722878/ 15.843213, val:  68.75%, val_best:  72.92%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  1.020521/ 13.990766, val:  72.92%, val_best:  72.92%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.787527/ 15.985489, val:  62.08%, val_best:  72.92%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.987467/ 14.105516, val:  68.75%, val_best:  72.92%, tr:  97.55%, tr_best:  98.37%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.596682/ 15.411995, val:  69.17%, val_best:  72.92%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.591424/ 14.991618, val:  72.08%, val_best:  72.92%, tr:  98.98%, tr_best:  99.18%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.215321/ 14.515713, val:  68.33%, val_best:  72.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.192238/ 15.230154, val:  69.58%, val_best:  72.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.175100/ 16.137661, val:  65.00%, val_best:  72.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.156258/ 14.864548, val:  72.08%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.065365/ 15.772489, val:  68.75%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.060119/ 15.633853, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.067896/ 15.295259, val:  69.17%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.047544/ 15.602401, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.043821/ 14.891628, val:  74.17%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.030495/ 14.868330, val:  74.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.009866/ 14.977221, val:  72.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.007869/ 14.964316, val:  73.33%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.011607/ 15.271211, val:  70.42%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.011736/ 14.895971, val:  72.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.007468/ 15.116207, val:  71.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.009348/ 14.720979, val:  74.17%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.011639/ 15.013283, val:  73.33%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.014885/ 14.788749, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.005221/ 14.623324, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.007504/ 14.642341, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.010591/ 14.729590, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.006790/ 14.745022, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.008187/ 14.771711, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.009659/ 14.796012, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.006795/ 14.862912, val:  71.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.007007/ 14.741839, val:  75.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.002358/ 14.894827, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000713/ 14.965619, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.001262/ 14.830208, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.002964/ 14.673545, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.001186/ 14.693970, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.001840/ 14.671230, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000366/ 14.664744, val:  75.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000028/ 14.648660, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000011/ 14.627991, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000011/ 14.630059, val:  74.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000008/ 14.633136, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000008/ 14.647804, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000007/ 14.649074, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000006/ 14.639254, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000006/ 14.631464, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000005/ 14.632982, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000005/ 14.635208, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000006/ 14.634324, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000005/ 14.648169, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000005/ 14.664107, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.684225, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.680351, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.691271, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.678105, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.675159, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.679320, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000005/ 14.672697, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000005/ 14.673911, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000005/ 14.692774, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.692068, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.692954, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.686124, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.695716, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.701522, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.695776, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.698174, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.702322, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.690298, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.699636, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.691858, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000004/ 14.694559, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000003/ 14.686625, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000003/ 14.682833, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000003/ 14.678411, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000002/ 14.680149, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000002/ 14.685628, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000002/ 14.691158, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000002/ 14.687958, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000002/ 14.688122, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000002/ 14.686864, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000002/ 14.683796, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000003/ 14.683667, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000002/ 14.682261, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000002/ 14.682655, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000002/ 14.680716, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.686396, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.680488, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.684413, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.692204, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.698128, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss:  0.000003/ 14.704396, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.710526, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.710843, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.711040, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.714870, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.715187, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.715672, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.714513, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.710268, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.710613, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.715854, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.720146, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.718434, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss:  0.000004/ 14.726215, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.726405, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.725592, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.720932, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.716708, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.709592, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.707385, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.700458, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.704866, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.698426, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.694971, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.695703, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.694898, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.692845, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.693144, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss:  0.000004/ 14.697488, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.702562, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.707527, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.707695, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.712194, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.712489, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.713799, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.701275, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.695823, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.690969, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.689795, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.690065, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.690450, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.691961, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.692230, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.697701, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.696975, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.691958, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.693316, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss:  0.000004/ 14.686919, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.685444, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.688735, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.693811, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.691217, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.690438, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.684718, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.689021, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.689431, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.690307, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.690471, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.690098, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.690303, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.690667, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.690807, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.691625, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.689509, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.689731, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss:  0.000004/ 14.689679, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.687002, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.702691, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.703112, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.702144, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.713428, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.713633, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.713949, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.714170, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.713542, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.722895, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss:  0.000004/ 14.728334, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.731146, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.729640, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.725384, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.725666, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.730309, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.725083, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.725361, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.728565, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.730473, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss:  0.000002/ 14.725955, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.724496, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.719721, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.713870, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.709027, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.709248, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.711520, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.721624, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss:  0.000001/ 14.721949, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588d763349504220b6fb696918d77251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▇█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▄▆▇▇██▇███████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▆█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▄▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▆█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▄▆▇▇██▇███████████████████████████████</td></tr><tr><td>val_loss</td><td>█▁▅▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0</td></tr><tr><td>val_acc_best</td><td>0.75417</td></tr><tr><td>val_acc_now</td><td>0.7375</td></tr><tr><td>val_loss</td><td>14.72195</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">valiant-sweep-28</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b2vuk2cm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b2vuk2cm</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_044255-b2vuk2cm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2ujy40ac with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_045556-2ujy40ac</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2ujy40ac' target=\"_blank\">genial-sweep-29</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2ujy40ac' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2ujy40ac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  3.400673/  4.643974, val:  20.83%, val_best:  20.83%, tr:  26.97%, tr_best:  26.97%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  6.447851/  7.660490, val:  12.08%, val_best:  20.83%, tr:  15.22%, tr_best:  26.97%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  8.297127/  9.156064, val:  10.00%, val_best:  20.83%, tr:  18.49%, tr_best:  26.97%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  9.171396/  7.066060, val:  10.83%, val_best:  20.83%, tr:  15.32%, tr_best:  26.97%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 10.122885/  8.760010, val:  18.33%, val_best:  20.83%, tr:  13.07%, tr_best:  26.97%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  8.298299/ 12.629545, val:  12.92%, val_best:  20.83%, tr:  16.34%, tr_best:  26.97%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  9.916300/ 10.799106, val:  13.75%, val_best:  20.83%, tr:  16.45%, tr_best:  26.97%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  9.773000/ 10.882369, val:  13.75%, val_best:  20.83%, tr:  15.42%, tr_best:  26.97%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 10.425594/  7.741261, val:  13.75%, val_best:  20.83%, tr:  13.89%, tr_best:  26.97%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  8.159190/  8.306570, val:  10.00%, val_best:  20.83%, tr:  16.14%, tr_best:  26.97%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  9.879216/ 10.778070, val:  10.00%, val_best:  20.83%, tr:  15.12%, tr_best:  26.97%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 11.230578/  9.665487, val:  10.00%, val_best:  20.83%, tr:  11.34%, tr_best:  26.97%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 11.574933/ 12.987069, val:  12.50%, val_best:  20.83%, tr:  12.97%, tr_best:  26.97%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 19.030094/ 12.165541, val:  10.83%, val_best:  20.83%, tr:  11.03%, tr_best:  26.97%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 11.147627/  7.146117, val:  18.33%, val_best:  20.83%, tr:  15.22%, tr_best:  26.97%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  8.569322/ 10.777178, val:  12.92%, val_best:  20.83%, tr:  17.98%, tr_best:  26.97%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  9.568440/ 15.350843, val:  10.00%, val_best:  20.83%, tr:  12.97%, tr_best:  26.97%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 11.375221/ 15.980848, val:  10.00%, val_best:  20.83%, tr:  12.16%, tr_best:  26.97%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss: 12.963171/  9.609429, val:  10.00%, val_best:  20.83%, tr:  10.11%, tr_best:  26.97%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss: 16.112070/ 16.623440, val:  10.00%, val_best:  20.83%, tr:  10.83%, tr_best:  26.97%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 14.746546/ 19.299799, val:  10.00%, val_best:  20.83%, tr:  11.13%, tr_best:  26.97%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 15.916730/ 16.961678, val:  10.00%, val_best:  20.83%, tr:   9.60%, tr_best:  26.97%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss: 19.060162/ 15.293541, val:  10.00%, val_best:  20.83%, tr:   9.50%, tr_best:  26.97%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss: 16.446768/ 15.977633, val:  10.00%, val_best:  20.83%, tr:   9.70%, tr_best:  26.97%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss: 16.630016/ 15.698059, val:  10.00%, val_best:  20.83%, tr:   8.68%, tr_best:  26.97%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss: 15.173941/ 11.129235, val:  10.00%, val_best:  20.83%, tr:  11.34%, tr_best:  26.97%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss: 16.229216/ 17.917717, val:  10.00%, val_best:  20.83%, tr:   9.60%, tr_best:  26.97%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss: 16.513338/ 20.323227, val:  10.00%, val_best:  20.83%, tr:  11.34%, tr_best:  26.97%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss: 17.220675/ 12.733370, val:  10.00%, val_best:  20.83%, tr:   9.81%, tr_best:  26.97%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss: 14.756652/ 22.475550, val:  10.00%, val_best:  20.83%, tr:  11.24%, tr_best:  26.97%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss: 19.958080/ 10.621619, val:  10.00%, val_best:  20.83%, tr:  10.73%, tr_best:  26.97%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss: 12.407032/  8.727142, val:  10.00%, val_best:  20.83%, tr:   9.19%, tr_best:  26.97%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss: 15.605545/ 17.287922, val:  10.00%, val_best:  20.83%, tr:  11.75%, tr_best:  26.97%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss: 16.066179/ 13.835128, val:  10.00%, val_best:  20.83%, tr:   9.40%, tr_best:  26.97%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss: 18.698433/ 21.414612, val:  10.00%, val_best:  20.83%, tr:   8.89%, tr_best:  26.97%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss: 13.797203/ 13.042461, val:  10.00%, val_best:  20.83%, tr:   8.99%, tr_best:  26.97%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss: 14.529630/ 20.856266, val:  10.00%, val_best:  20.83%, tr:   9.19%, tr_best:  26.97%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss: 15.483742/ 18.735863, val:  10.00%, val_best:  20.83%, tr:  10.42%, tr_best:  26.97%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss: 16.758230/ 20.878626, val:  10.00%, val_best:  20.83%, tr:   7.97%, tr_best:  26.97%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss: 16.435036/ 14.498974, val:  10.00%, val_best:  20.83%, tr:   9.70%, tr_best:  26.97%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss: 14.716289/ 13.468637, val:  10.00%, val_best:  20.83%, tr:  10.01%, tr_best:  26.97%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss: 14.871906/ 16.644785, val:  10.00%, val_best:  20.83%, tr:  10.01%, tr_best:  26.97%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss: 16.544207/ 16.970734, val:  10.00%, val_best:  20.83%, tr:   8.89%, tr_best:  26.97%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss: 15.182940/ 18.357813, val:  10.00%, val_best:  20.83%, tr:  10.32%, tr_best:  26.97%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss: 15.214786/ 14.013872, val:  10.00%, val_best:  20.83%, tr:   8.78%, tr_best:  26.97%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss: 15.454721/ 14.067523, val:  10.00%, val_best:  20.83%, tr:  10.83%, tr_best:  26.97%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss: 18.119822/ 18.602551, val:  10.00%, val_best:  20.83%, tr:  10.21%, tr_best:  26.97%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss: 17.108332/ 19.393734, val:  10.00%, val_best:  20.83%, tr:   9.91%, tr_best:  26.97%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss: 14.466417/ 18.876120, val:  10.00%, val_best:  20.83%, tr:   9.40%, tr_best:  26.97%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss: 14.150521/ 18.471592, val:  10.00%, val_best:  20.83%, tr:  10.93%, tr_best:  26.97%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss: 10.996269/ 15.055857, val:  10.00%, val_best:  20.83%, tr:   9.91%, tr_best:  26.97%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss: 11.178751/  7.093573, val:  10.00%, val_best:  20.83%, tr:  11.13%, tr_best:  26.97%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss: 13.009730/ 16.089396, val:  10.00%, val_best:  20.83%, tr:   9.40%, tr_best:  26.97%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss: 13.434436/ 10.413006, val:  10.00%, val_best:  20.83%, tr:   9.30%, tr_best:  26.97%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss: 13.562251/ 19.415199, val:  10.00%, val_best:  20.83%, tr:  10.11%, tr_best:  26.97%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss: 18.327341/ 15.046647, val:  10.00%, val_best:  20.83%, tr:   8.89%, tr_best:  26.97%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss: 16.676533/  9.996819, val:  10.00%, val_best:  20.83%, tr:  10.32%, tr_best:  26.97%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss: 16.899155/  8.841719, val:  10.00%, val_best:  20.83%, tr:   9.50%, tr_best:  26.97%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss: 15.004455/ 14.125495, val:  10.00%, val_best:  20.83%, tr:   8.68%, tr_best:  26.97%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss: 16.311464/  9.630463, val:  10.00%, val_best:  20.83%, tr:   9.50%, tr_best:  26.97%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss: 14.411048/ 12.702707, val:  10.00%, val_best:  20.83%, tr:   9.19%, tr_best:  26.97%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss: 18.679935/ 16.395836, val:  10.00%, val_best:  20.83%, tr:   9.40%, tr_best:  26.97%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss: 14.910014/ 19.505074, val:  10.00%, val_best:  20.83%, tr:  10.21%, tr_best:  26.97%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss: 15.729452/  9.848350, val:  10.00%, val_best:  20.83%, tr:  10.11%, tr_best:  26.97%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss: 15.159283/ 16.698029, val:  10.00%, val_best:  20.83%, tr:   8.48%, tr_best:  26.97%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss: 15.569047/ 15.179411, val:  10.00%, val_best:  20.83%, tr:   8.78%, tr_best:  26.97%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss: 13.159494/ 12.367296, val:  10.00%, val_best:  20.83%, tr:  11.34%, tr_best:  26.97%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss: 14.479085/ 20.105194, val:  10.00%, val_best:  20.83%, tr:  10.01%, tr_best:  26.97%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss: 15.717676/ 12.471171, val:  10.00%, val_best:  20.83%, tr:  10.11%, tr_best:  26.97%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss: 14.705818/ 14.328948, val:  10.00%, val_best:  20.83%, tr:   7.76%, tr_best:  26.97%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss: 17.665489/ 13.257401, val:  10.00%, val_best:  20.83%, tr:   9.50%, tr_best:  26.97%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss: 13.257479/ 16.209259, val:  10.00%, val_best:  20.83%, tr:  11.54%, tr_best:  26.97%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss: 15.349658/ 13.226386, val:  10.00%, val_best:  20.83%, tr:   9.91%, tr_best:  26.97%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss: 16.536243/ 18.764835, val:  10.00%, val_best:  20.83%, tr:  11.75%, tr_best:  26.97%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss: 18.882917/ 18.056107, val:  10.00%, val_best:  20.83%, tr:  10.32%, tr_best:  26.97%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss: 16.291208/  7.298122, val:  10.00%, val_best:  20.83%, tr:   9.70%, tr_best:  26.97%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss: 13.392303/  9.682051, val:  10.00%, val_best:  20.83%, tr:  11.24%, tr_best:  26.97%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss: 13.334938/ 13.775981, val:  10.00%, val_best:  20.83%, tr:   9.91%, tr_best:  26.97%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss: 17.188799/ 10.165958, val:  10.00%, val_best:  20.83%, tr:  10.62%, tr_best:  26.97%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss: 14.035094/  8.748745, val:  10.00%, val_best:  20.83%, tr:  11.03%, tr_best:  26.97%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss: 14.940212/ 20.254108, val:  10.00%, val_best:  20.83%, tr:  10.73%, tr_best:  26.97%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss: 20.515450/ 25.002018, val:  10.00%, val_best:  20.83%, tr:   7.76%, tr_best:  26.97%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss: 18.453974/ 15.737460, val:  10.00%, val_best:  20.83%, tr:  10.62%, tr_best:  26.97%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss: 14.234928/ 25.083282, val:  10.00%, val_best:  20.83%, tr:  11.03%, tr_best:  26.97%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss: 16.107597/ 15.651436, val:  10.00%, val_best:  20.83%, tr:   9.91%, tr_best:  26.97%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss: 15.854720/ 10.188041, val:  10.00%, val_best:  20.83%, tr:   8.78%, tr_best:  26.97%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss: 16.850679/ 20.023355, val:  10.00%, val_best:  20.83%, tr:   9.40%, tr_best:  26.97%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss: 14.722817/ 16.603529, val:  10.00%, val_best:  20.83%, tr:  10.21%, tr_best:  26.97%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss: 15.027577/  8.854498, val:  10.00%, val_best:  20.83%, tr:   9.50%, tr_best:  26.97%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss: 18.161879/ 21.947897, val:  10.00%, val_best:  20.83%, tr:   8.68%, tr_best:  26.97%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss: 15.815712/ 10.032659, val:  10.00%, val_best:  20.83%, tr:   9.81%, tr_best:  26.97%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss: 13.611102/ 11.942283, val:  10.00%, val_best:  20.83%, tr:  10.21%, tr_best:  26.97%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss: 14.934464/ 27.231094, val:  10.00%, val_best:  20.83%, tr:   9.09%, tr_best:  26.97%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss: 17.090351/ 12.255357, val:  10.00%, val_best:  20.83%, tr:   8.58%, tr_best:  26.97%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss: 14.304381/ 12.772491, val:  10.00%, val_best:  20.83%, tr:   9.30%, tr_best:  26.97%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss: 15.358915/ 19.270145, val:  10.00%, val_best:  20.83%, tr:   9.70%, tr_best:  26.97%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss: 17.347437/ 11.141294, val:  10.00%, val_best:  20.83%, tr:  10.11%, tr_best:  26.97%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss: 13.871923/ 14.660114, val:  10.00%, val_best:  20.83%, tr:   9.50%, tr_best:  26.97%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss: 11.699871/ 17.394724, val:  10.00%, val_best:  20.83%, tr:   9.70%, tr_best:  26.97%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss: 13.996898/ 12.774557, val:  10.00%, val_best:  20.83%, tr:  10.62%, tr_best:  26.97%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss: 13.848610/  6.707286, val:  10.00%, val_best:  20.83%, tr:  10.01%, tr_best:  26.97%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss: 16.357254/ 15.149867, val:  10.00%, val_best:  20.83%, tr:  10.52%, tr_best:  26.97%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss: 16.438240/ 20.058762, val:  10.00%, val_best:  20.83%, tr:   9.70%, tr_best:  26.97%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss: 14.556984/  9.467278, val:  10.00%, val_best:  20.83%, tr:  10.32%, tr_best:  26.97%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss: 13.743703/ 17.841459, val:  10.00%, val_best:  20.83%, tr:   9.09%, tr_best:  26.97%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss: 15.266725/ 14.941952, val:  10.00%, val_best:  20.83%, tr:   9.60%, tr_best:  26.97%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss: 13.426910/  7.467767, val:  10.00%, val_best:  20.83%, tr:  10.42%, tr_best:  26.97%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss: 11.346660/ 12.643097, val:  10.00%, val_best:  20.83%, tr:   9.50%, tr_best:  26.97%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss: 12.976845/ 27.899679, val:  10.00%, val_best:  20.83%, tr:  10.32%, tr_best:  26.97%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss: 16.227306/ 20.676386, val:  10.00%, val_best:  20.83%, tr:  10.52%, tr_best:  26.97%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss: 18.552746/ 14.113251, val:  10.00%, val_best:  20.83%, tr:   9.09%, tr_best:  26.97%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss: 14.823068/ 16.336531, val:  10.00%, val_best:  20.83%, tr:   9.40%, tr_best:  26.97%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss: 18.137810/ 13.693861, val:  10.00%, val_best:  20.83%, tr:   9.60%, tr_best:  26.97%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss: 13.135022/ 21.460522, val:  10.00%, val_best:  20.83%, tr:  10.93%, tr_best:  26.97%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss: 16.439661/ 10.810780, val:  10.00%, val_best:  20.83%, tr:  10.62%, tr_best:  26.97%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss: 10.322718/ 17.469763, val:  10.00%, val_best:  20.83%, tr:  10.42%, tr_best:  26.97%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss: 15.787974/ 10.943082, val:  10.00%, val_best:  20.83%, tr:  10.32%, tr_best:  26.97%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss: 13.946533/ 11.178205, val:  10.00%, val_best:  20.83%, tr:   8.78%, tr_best:  26.97%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss: 11.070371/ 13.588581, val:  10.00%, val_best:  20.83%, tr:  10.73%, tr_best:  26.97%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss: 14.017087/ 14.511827, val:  10.00%, val_best:  20.83%, tr:   9.60%, tr_best:  26.97%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss: 13.752177/  9.039577, val:  10.00%, val_best:  20.83%, tr:  10.32%, tr_best:  26.97%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss: 13.669538/ 14.502956, val:  10.00%, val_best:  20.83%, tr:  11.24%, tr_best:  26.97%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss: 12.596415/ 15.090830, val:  10.00%, val_best:  20.83%, tr:  11.13%, tr_best:  26.97%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss: 14.452083/  9.027978, val:  10.00%, val_best:  20.83%, tr:   8.38%, tr_best:  26.97%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss: 12.928680/ 19.143665, val:  10.00%, val_best:  20.83%, tr:  10.83%, tr_best:  26.97%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss: 13.526454/ 14.445611, val:  10.00%, val_best:  20.83%, tr:  10.42%, tr_best:  26.97%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss: 15.117200/ 24.781582, val:  10.00%, val_best:  20.83%, tr:  10.21%, tr_best:  26.97%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss: 15.011981/ 20.989212, val:  10.00%, val_best:  20.83%, tr:  10.93%, tr_best:  26.97%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss: 14.707728/ 19.267616, val:  10.00%, val_best:  20.83%, tr:   8.17%, tr_best:  26.97%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss: 16.471146/ 10.761217, val:  10.00%, val_best:  20.83%, tr:   9.30%, tr_best:  26.97%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss: 11.429485/ 14.287648, val:  10.00%, val_best:  20.83%, tr:  10.52%, tr_best:  26.97%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss: 13.715210/ 14.876809, val:  10.00%, val_best:  20.83%, tr:  10.83%, tr_best:  26.97%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss: 14.483955/ 16.645372, val:  10.00%, val_best:  20.83%, tr:   9.70%, tr_best:  26.97%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss: 13.581104/ 11.376741, val:  10.00%, val_best:  20.83%, tr:   8.27%, tr_best:  26.97%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss: 16.311483/ 19.203995, val:  10.00%, val_best:  20.83%, tr:   8.58%, tr_best:  26.97%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss: 19.778332/ 17.385262, val:  10.00%, val_best:  20.83%, tr:  10.01%, tr_best:  26.97%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss: 13.584117/ 24.434134, val:  10.00%, val_best:  20.83%, tr:   9.91%, tr_best:  26.97%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss: 15.967773/  7.702685, val:  10.00%, val_best:  20.83%, tr:  10.73%, tr_best:  26.97%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss: 15.447517/ 24.105093, val:  10.00%, val_best:  20.83%, tr:   8.68%, tr_best:  26.97%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss: 16.835758/ 14.670810, val:  10.00%, val_best:  20.83%, tr:   9.30%, tr_best:  26.97%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss: 17.604136/  7.878733, val:  10.00%, val_best:  20.83%, tr:  10.11%, tr_best:  26.97%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss: 17.335241/  8.681518, val:  10.00%, val_best:  20.83%, tr:  12.16%, tr_best:  26.97%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss: 12.994471/ 25.261927, val:  10.00%, val_best:  20.83%, tr:   9.60%, tr_best:  26.97%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss: 18.449903/ 11.555226, val:  10.00%, val_best:  20.83%, tr:   8.48%, tr_best:  26.97%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss: 13.763140/ 17.147017, val:  10.00%, val_best:  20.83%, tr:  10.42%, tr_best:  26.97%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss: 18.291395/ 30.596230, val:  10.00%, val_best:  20.83%, tr:   9.70%, tr_best:  26.97%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss: 16.523750/ 13.306340, val:  10.00%, val_best:  20.83%, tr:  10.01%, tr_best:  26.97%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss: 13.043630/ 14.550856, val:  10.00%, val_best:  20.83%, tr:  11.34%, tr_best:  26.97%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss: 12.613971/ 10.783751, val:  10.00%, val_best:  20.83%, tr:   9.50%, tr_best:  26.97%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss: 13.122978/ 18.888994, val:  10.00%, val_best:  20.83%, tr:  10.11%, tr_best:  26.97%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss: 17.753908/ 17.880226, val:  10.00%, val_best:  20.83%, tr:  11.13%, tr_best:  26.97%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss: 14.353028/ 14.604996, val:  10.00%, val_best:  20.83%, tr:  10.42%, tr_best:  26.97%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss: 15.405934/ 24.367815, val:  10.00%, val_best:  20.83%, tr:   8.48%, tr_best:  26.97%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss: 16.387165/ 14.029024, val:  10.00%, val_best:  20.83%, tr:  11.34%, tr_best:  26.97%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss: 13.833475/ 11.794763, val:  10.00%, val_best:  20.83%, tr:  11.75%, tr_best:  26.97%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss: 13.568027/ 22.974815, val:  10.00%, val_best:  20.83%, tr:   8.89%, tr_best:  26.97%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss: 18.365206/ 10.069161, val:  10.00%, val_best:  20.83%, tr:   8.89%, tr_best:  26.97%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss: 18.460581/ 14.516077, val:  10.00%, val_best:  20.83%, tr:   8.78%, tr_best:  26.97%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss: 15.341595/ 12.621304, val:  10.00%, val_best:  20.83%, tr:   9.40%, tr_best:  26.97%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss: 12.681101/ 13.111256, val:  10.00%, val_best:  20.83%, tr:   8.89%, tr_best:  26.97%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss: 14.999321/ 14.363284, val:  10.00%, val_best:  20.83%, tr:   9.81%, tr_best:  26.97%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss: 13.848096/ 12.628078, val:  10.00%, val_best:  20.83%, tr:   8.78%, tr_best:  26.97%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss: 14.753186/ 13.955245, val:  10.00%, val_best:  20.83%, tr:  10.21%, tr_best:  26.97%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss: 11.512619/ 12.368614, val:  10.00%, val_best:  20.83%, tr:  10.83%, tr_best:  26.97%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss: 10.654255/ 20.984762, val:  10.00%, val_best:  20.83%, tr:   9.70%, tr_best:  26.97%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss: 11.309318/ 15.794895, val:  10.00%, val_best:  20.83%, tr:  10.62%, tr_best:  26.97%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss: 12.157002/  8.230474, val:  10.00%, val_best:  20.83%, tr:   9.81%, tr_best:  26.97%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss: 13.600959/ 12.994365, val:  10.00%, val_best:  20.83%, tr:   8.38%, tr_best:  26.97%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss: 15.495883/ 18.341299, val:  10.00%, val_best:  20.83%, tr:   9.09%, tr_best:  26.97%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss: 14.384319/ 12.292644, val:  10.00%, val_best:  20.83%, tr:  11.03%, tr_best:  26.97%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss: 12.528030/ 10.447775, val:  10.00%, val_best:  20.83%, tr:  10.21%, tr_best:  26.97%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss: 15.463010/ 11.673002, val:  10.00%, val_best:  20.83%, tr:  10.11%, tr_best:  26.97%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss: 12.077724/ 11.363810, val:  10.00%, val_best:  20.83%, tr:  11.03%, tr_best:  26.97%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss: 14.780547/ 13.131556, val:  10.00%, val_best:  20.83%, tr:   8.58%, tr_best:  26.97%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss: 16.280848/ 14.582249, val:  10.00%, val_best:  20.83%, tr:  10.42%, tr_best:  26.97%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss: 13.258450/ 15.876507, val:  10.00%, val_best:  20.83%, tr:   9.70%, tr_best:  26.97%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss: 13.832576/ 18.696024, val:  10.00%, val_best:  20.83%, tr:   8.99%, tr_best:  26.97%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss: 14.341407/ 12.019177, val:  10.00%, val_best:  20.83%, tr:  11.03%, tr_best:  26.97%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss: 15.155520/ 13.334093, val:  10.00%, val_best:  20.83%, tr:   9.19%, tr_best:  26.97%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss: 14.454194/ 20.610933, val:  10.00%, val_best:  20.83%, tr:  10.01%, tr_best:  26.97%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss: 15.184972/ 11.815538, val:  10.00%, val_best:  20.83%, tr:  11.34%, tr_best:  26.97%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss: 17.684031/ 15.037535, val:  10.00%, val_best:  20.83%, tr:   9.50%, tr_best:  26.97%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss: 16.692200/ 13.075065, val:  10.00%, val_best:  20.83%, tr:   9.81%, tr_best:  26.97%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss: 12.719571/ 10.997602, val:  10.00%, val_best:  20.83%, tr:   8.68%, tr_best:  26.97%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss: 14.240954/ 12.032283, val:  10.00%, val_best:  20.83%, tr:   9.70%, tr_best:  26.97%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss: 15.568285/ 11.519794, val:  10.00%, val_best:  20.83%, tr:  10.73%, tr_best:  26.97%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss: 17.176451/ 18.560970, val:  10.00%, val_best:  20.83%, tr:  12.05%, tr_best:  26.97%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss: 13.110614/ 13.218650, val:  10.00%, val_best:  20.83%, tr:  10.93%, tr_best:  26.97%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss: 15.868793/ 15.085040, val:  10.00%, val_best:  20.83%, tr:  10.83%, tr_best:  26.97%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss: 13.010381/ 22.587248, val:  10.00%, val_best:  20.83%, tr:   8.78%, tr_best:  26.97%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss: 14.639760/  8.250159, val:  10.00%, val_best:  20.83%, tr:   9.40%, tr_best:  26.97%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss: 14.803694/ 16.408459, val:  10.00%, val_best:  20.83%, tr:   8.38%, tr_best:  26.97%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss: 12.081041/  8.052934, val:  10.00%, val_best:  20.83%, tr:  10.01%, tr_best:  26.97%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss: 15.123796/ 19.975885, val:  10.00%, val_best:  20.83%, tr:  10.01%, tr_best:  26.97%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss: 16.631308/ 11.134294, val:  10.00%, val_best:  20.83%, tr:   9.40%, tr_best:  26.97%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss: 14.634463/ 15.976694, val:  10.00%, val_best:  20.83%, tr:  10.21%, tr_best:  26.97%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss: 14.913708/ 11.730597, val:  10.00%, val_best:  20.83%, tr:   9.50%, tr_best:  26.97%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss: 15.105864/ 17.252628, val:  10.00%, val_best:  20.83%, tr:  10.73%, tr_best:  26.97%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss: 16.859659/ 23.756317, val:  10.00%, val_best:  20.83%, tr:  11.34%, tr_best:  26.97%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss: 15.337310/ 17.917686, val:  10.00%, val_best:  20.83%, tr:  10.52%, tr_best:  26.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5970f90e2b80481683b657f53750f7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▃▃▃▁▃▁▁▆▅▅▃▅▅▃▅▆█▁▁▃▅▃▅▁▁▃▃▆▆▃▃▆▅▅▃▁▁▁▆</td></tr><tr><td>summary_val_acc</td><td>▆█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tr_acc</td><td>▆▇▇█▃▃▃▂▂▁▂▂▂▁▃▂▃▁▁▂▂▂▂▂▂▃▃▂▄▂▂▁▂▂▂▁▃▄▁▂</td></tr><tr><td>tr_epoch_loss</td><td>▁▂▂▂▆▆▅▅▅▆▃▅▅▆▅▆▅▆▇▇▅▅▇▆▅▄▅█▇▇▅▅▅▄▄▅▆▇▅▅</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_now</td><td>▆█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▃▁▂▄▂▆▅▃▃▃▅▃▃▄▁▂▂▅▂▁▁▃▂▂▅▃▄▂█▃▆▃▁▂▅▂▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.33333</td></tr><tr><td>tr_acc</td><td>0.10521</td></tr><tr><td>tr_epoch_loss</td><td>15.33731</td></tr><tr><td>val_acc_best</td><td>0.20833</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>17.91769</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">genial-sweep-29</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2ujy40ac' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2ujy40ac</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_045556-2ujy40ac/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 35scdxcq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_050722-35scdxcq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/35scdxcq' target=\"_blank\">fiery-sweep-30</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/35scdxcq' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/35scdxcq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.644651/  1.731101, val:  48.33%, val_best:  48.33%, tr:  44.23%, tr_best:  44.23%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.208877/  1.344571, val:  55.83%, val_best:  55.83%, tr:  57.30%, tr_best:  57.30%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.040024/  1.507414, val:  52.08%, val_best:  55.83%, tr:  62.00%, tr_best:  62.00%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.850905/  1.390925, val:  58.75%, val_best:  58.75%, tr:  71.09%, tr_best:  71.09%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.765954/  1.276999, val:  62.92%, val_best:  62.92%, tr:  74.16%, tr_best:  74.16%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.693903/  1.393636, val:  57.50%, val_best:  62.92%, tr:  75.28%, tr_best:  75.28%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.675394/  1.580388, val:  56.67%, val_best:  62.92%, tr:  77.83%, tr_best:  77.83%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.574139/  1.372500, val:  64.58%, val_best:  64.58%, tr:  78.35%, tr_best:  78.35%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.543697/  1.472250, val:  69.17%, val_best:  69.17%, tr:  81.92%, tr_best:  81.92%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.514437/  1.537081, val:  64.17%, val_best:  69.17%, tr:  84.07%, tr_best:  84.07%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.484536/  1.618829, val:  68.33%, val_best:  69.17%, tr:  85.90%, tr_best:  85.90%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.432783/  1.839389, val:  68.33%, val_best:  69.17%, tr:  89.07%, tr_best:  89.07%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.460165/  1.812087, val:  61.67%, val_best:  69.17%, tr:  89.27%, tr_best:  89.27%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.504065/  1.672133, val:  63.33%, val_best:  69.17%, tr:  87.95%, tr_best:  89.27%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.370687/  1.949677, val:  70.00%, val_best:  70.00%, tr:  92.65%, tr_best:  92.65%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.354307/  2.018980, val:  67.92%, val_best:  70.00%, tr:  93.05%, tr_best:  93.05%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.312354/  1.797018, val:  74.58%, val_best:  74.58%, tr:  94.69%, tr_best:  94.69%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.256539/  2.266956, val:  66.67%, val_best:  74.58%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.229622/  2.147930, val:  73.33%, val_best:  74.58%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.189002/  2.217773, val:  67.50%, val_best:  74.58%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.222737/  2.480609, val:  66.25%, val_best:  74.58%, tr:  97.34%, tr_best:  98.67%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.195603/  2.758762, val:  63.75%, val_best:  74.58%, tr:  98.16%, tr_best:  98.67%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.203511/  2.790453, val:  69.58%, val_best:  74.58%, tr:  98.26%, tr_best:  98.67%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.211554/  2.619023, val:  69.58%, val_best:  74.58%, tr:  97.96%, tr_best:  98.67%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.206763/  2.427031, val:  74.58%, val_best:  74.58%, tr:  97.65%, tr_best:  98.67%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.129359/  2.286584, val:  78.75%, val_best:  78.75%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.235548/  2.542770, val:  73.75%, val_best:  78.75%, tr:  97.75%, tr_best:  99.28%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.206388/  2.741729, val:  68.33%, val_best:  78.75%, tr:  98.77%, tr_best:  99.28%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.237341/  2.597523, val:  72.92%, val_best:  78.75%, tr:  98.47%, tr_best:  99.28%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.240590/  2.833857, val:  66.67%, val_best:  78.75%, tr:  98.47%, tr_best:  99.28%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.281545/  2.232596, val:  76.25%, val_best:  78.75%, tr:  97.04%, tr_best:  99.28%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.290933/  2.809065, val:  68.33%, val_best:  78.75%, tr:  96.73%, tr_best:  99.28%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.318323/  2.914472, val:  68.75%, val_best:  78.75%, tr:  97.34%, tr_best:  99.28%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.274084/  2.482688, val:  72.50%, val_best:  78.75%, tr:  97.85%, tr_best:  99.28%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.168273/  2.381028, val:  73.75%, val_best:  78.75%, tr:  99.18%, tr_best:  99.28%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.202907/  2.734004, val:  72.50%, val_best:  78.75%, tr:  98.88%, tr_best:  99.28%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.148863/  2.413526, val:  75.00%, val_best:  78.75%, tr:  98.98%, tr_best:  99.28%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.122023/  2.797217, val:  75.00%, val_best:  78.75%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.157032/  2.918069, val:  76.25%, val_best:  78.75%, tr:  99.08%, tr_best:  99.69%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.118648/  2.441899, val:  77.50%, val_best:  78.75%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.184421/  2.770601, val:  73.33%, val_best:  78.75%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.282217/  3.220662, val:  64.58%, val_best:  78.75%, tr:  97.45%, tr_best:  99.80%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.318623/  2.715542, val:  73.33%, val_best:  78.75%, tr:  96.73%, tr_best:  99.80%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.229039/  2.807116, val:  72.08%, val_best:  78.75%, tr:  98.47%, tr_best:  99.80%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.257416/  2.564440, val:  71.67%, val_best:  78.75%, tr:  98.06%, tr_best:  99.80%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.245156/  2.587174, val:  71.67%, val_best:  78.75%, tr:  98.26%, tr_best:  99.80%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.224698/  3.118161, val:  69.58%, val_best:  78.75%, tr:  98.47%, tr_best:  99.80%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.277761/  2.506688, val:  72.08%, val_best:  78.75%, tr:  97.85%, tr_best:  99.80%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.228890/  2.780784, val:  70.42%, val_best:  78.75%, tr:  98.57%, tr_best:  99.80%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.173283/  2.578962, val:  75.00%, val_best:  78.75%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.174713/  3.233587, val:  67.92%, val_best:  78.75%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.261037/  3.001736, val:  67.08%, val_best:  78.75%, tr:  97.14%, tr_best:  99.80%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.209512/  2.871423, val:  74.17%, val_best:  78.75%, tr:  98.88%, tr_best:  99.80%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.144440/  2.827030, val:  71.25%, val_best:  78.75%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.116858/  2.956306, val:  73.75%, val_best:  78.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.114168/  3.037163, val:  75.83%, val_best:  78.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.115467/  3.057495, val:  75.42%, val_best:  78.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.134404/  3.106428, val:  75.83%, val_best:  78.75%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.161275/  3.136311, val:  75.00%, val_best:  78.75%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.197008/  3.078448, val:  74.17%, val_best:  78.75%, tr:  98.98%, tr_best:  99.90%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.215346/  2.835816, val:  74.17%, val_best:  78.75%, tr:  98.26%, tr_best:  99.90%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.196933/  3.119985, val:  69.58%, val_best:  78.75%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.211770/  3.195943, val:  70.42%, val_best:  78.75%, tr:  99.18%, tr_best:  99.90%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.228073/  3.236933, val:  72.50%, val_best:  78.75%, tr:  98.88%, tr_best:  99.90%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.212407/  3.109900, val:  75.83%, val_best:  78.75%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.212514/  3.291849, val:  72.08%, val_best:  78.75%, tr:  98.37%, tr_best:  99.90%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.223369/  3.858721, val:  67.92%, val_best:  78.75%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.220751/  3.658262, val:  68.75%, val_best:  78.75%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.248544/  3.645768, val:  67.50%, val_best:  78.75%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.263193/  3.234514, val:  70.42%, val_best:  78.75%, tr:  98.37%, tr_best:  99.90%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.197136/  3.307581, val:  73.33%, val_best:  78.75%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.155006/  3.396611, val:  73.33%, val_best:  78.75%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.136008/  3.660688, val:  69.17%, val_best:  78.75%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.137362/  3.541519, val:  69.58%, val_best:  78.75%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.159912/  3.405766, val:  72.50%, val_best:  78.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.243565/  3.969773, val:  64.17%, val_best:  78.75%, tr:  98.77%, tr_best:  99.90%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.249803/  3.179925, val:  72.50%, val_best:  78.75%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.207354/  3.552130, val:  73.75%, val_best:  78.75%, tr:  99.39%, tr_best:  99.90%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.225432/  3.572951, val:  75.83%, val_best:  78.75%, tr:  99.18%, tr_best:  99.90%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.128039/  3.342916, val:  71.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.147552/  3.939454, val:  74.17%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.151258/  3.921322, val:  73.33%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.153064/  4.130844, val:  71.25%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.197138/  4.097692, val:  67.08%, val_best:  78.75%, tr:  99.28%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.163812/  3.727324, val:  76.67%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.175217/  4.140792, val:  69.58%, val_best:  78.75%, tr:  99.28%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.215379/  3.240702, val:  75.83%, val_best:  78.75%, tr:  99.39%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.238937/  4.026453, val:  73.33%, val_best:  78.75%, tr:  99.18%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.167407/  3.962930, val:  72.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.175906/  4.205744, val:  72.92%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.187643/  4.159274, val:  67.50%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.179018/  4.663013, val:  63.75%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.151818/  4.467987, val:  72.08%, val_best:  78.75%, tr:  99.49%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.219210/  4.454582, val:  73.33%, val_best:  78.75%, tr:  99.39%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.198288/  4.162801, val:  70.42%, val_best:  78.75%, tr:  99.49%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.233413/  4.586595, val:  67.08%, val_best:  78.75%, tr:  98.98%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.188828/  4.726988, val:  70.42%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.181498/  4.026166, val:  75.83%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.170234/  3.885897, val:  74.58%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.205561/  3.460114, val:  77.50%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.188114/  3.497218, val:  76.25%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.225473/  3.995853, val:  72.50%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.201155/  4.425441, val:  74.17%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.256430/  4.386180, val:  72.08%, val_best:  78.75%, tr:  99.39%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.259076/  3.948415, val:  72.92%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.267618/  3.884815, val:  75.42%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.466165/  4.376616, val:  70.00%, val_best:  78.75%, tr:  97.85%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.382917/  3.947086, val:  69.17%, val_best:  78.75%, tr:  98.06%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.368880/  4.115976, val:  72.92%, val_best:  78.75%, tr:  98.57%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.547926/  4.081206, val:  76.25%, val_best:  78.75%, tr:  96.73%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.439912/  3.861862, val:  70.00%, val_best:  78.75%, tr:  98.37%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.387310/  3.934700, val:  70.42%, val_best:  78.75%, tr:  98.57%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.337739/  3.887999, val:  71.25%, val_best:  78.75%, tr:  98.37%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.339241/  4.356231, val:  65.83%, val_best:  78.75%, tr:  97.96%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.382229/  4.449813, val:  64.58%, val_best:  78.75%, tr:  98.06%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.293784/  3.924660, val:  71.25%, val_best:  78.75%, tr:  99.08%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.306259/  3.909357, val:  72.08%, val_best:  78.75%, tr:  98.37%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.336510/  3.991595, val:  71.25%, val_best:  78.75%, tr:  98.47%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.288749/  4.111876, val:  67.92%, val_best:  78.75%, tr:  98.06%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.257497/  3.824673, val:  71.67%, val_best:  78.75%, tr:  98.47%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.228317/  3.523014, val:  74.58%, val_best:  78.75%, tr:  98.98%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.193320/  3.742519, val:  76.67%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.199142/  3.949578, val:  72.08%, val_best:  78.75%, tr:  99.49%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.241402/  4.442022, val:  68.75%, val_best:  78.75%, tr:  99.08%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.353296/  4.038594, val:  68.75%, val_best:  78.75%, tr:  97.65%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.312859/  3.679406, val:  73.33%, val_best:  78.75%, tr:  99.18%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.265337/  4.241055, val:  68.33%, val_best:  78.75%, tr:  99.49%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.287384/  3.930080, val:  71.25%, val_best:  78.75%, tr:  98.06%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.229362/  4.024133, val:  72.92%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.261724/  4.069397, val:  71.25%, val_best:  78.75%, tr:  98.98%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.317619/  4.728967, val:  69.17%, val_best:  78.75%, tr:  99.39%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.353030/  4.481632, val:  70.00%, val_best:  78.75%, tr:  99.08%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.364019/  4.656842, val:  70.00%, val_best:  78.75%, tr:  98.57%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.347038/  4.773951, val:  69.17%, val_best:  78.75%, tr:  99.08%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.422237/  4.363055, val:  69.17%, val_best:  78.75%, tr:  98.47%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.301341/  4.257559, val:  73.75%, val_best:  78.75%, tr:  99.49%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.339942/  4.664255, val:  68.75%, val_best:  78.75%, tr:  98.98%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.326463/  4.269104, val:  70.42%, val_best:  78.75%, tr:  98.67%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.388824/  4.537281, val:  72.08%, val_best:  78.75%, tr:  98.57%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.303332/  4.334388, val:  73.33%, val_best:  78.75%, tr:  98.77%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.275133/  4.153476, val:  73.75%, val_best:  78.75%, tr:  99.49%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.275179/  4.461082, val:  70.42%, val_best:  78.75%, tr:  99.08%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.267418/  4.339355, val:  68.33%, val_best:  78.75%, tr:  99.08%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.287424/  4.320220, val:  69.58%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.311279/  3.886379, val:  75.00%, val_best:  78.75%, tr:  99.08%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.262188/  4.823255, val:  64.58%, val_best:  78.75%, tr:  99.18%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.329853/  4.314743, val:  72.92%, val_best:  78.75%, tr:  98.57%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.323341/  4.405070, val:  73.75%, val_best:  78.75%, tr:  98.98%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.370144/  4.221964, val:  70.42%, val_best:  78.75%, tr:  98.88%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.308039/  4.182561, val:  71.25%, val_best:  78.75%, tr:  99.39%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.271192/  3.921243, val:  77.08%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.265360/  3.846920, val:  75.42%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.220810/  4.290243, val:  72.08%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.239216/  4.585446, val:  72.08%, val_best:  78.75%, tr:  99.39%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.317903/  4.694593, val:  73.75%, val_best:  78.75%, tr:  98.77%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.428321/  4.539134, val:  73.75%, val_best:  78.75%, tr:  98.47%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.372427/  4.760114, val:  71.25%, val_best:  78.75%, tr:  99.39%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.314526/  4.214078, val:  76.67%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.299113/  4.818833, val:  68.75%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.243853/  4.403983, val:  72.92%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.335080/  4.940509, val:  68.75%, val_best:  78.75%, tr:  99.28%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.404894/  4.989918, val:  66.67%, val_best:  78.75%, tr:  98.77%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.435358/  5.024306, val:  67.08%, val_best:  78.75%, tr:  98.37%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.359647/  4.572565, val:  70.42%, val_best:  78.75%, tr:  98.67%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.415017/  4.163352, val:  70.83%, val_best:  78.75%, tr:  97.45%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.425025/  4.520006, val:  69.17%, val_best:  78.75%, tr:  98.26%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.399484/  4.837964, val:  70.00%, val_best:  78.75%, tr:  98.26%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.453201/  4.671013, val:  70.83%, val_best:  78.75%, tr:  98.16%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.445361/  4.689527, val:  63.33%, val_best:  78.75%, tr:  98.77%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.410285/  4.582815, val:  70.00%, val_best:  78.75%, tr:  99.28%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.293130/  4.755806, val:  67.92%, val_best:  78.75%, tr:  98.67%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.303714/  4.570408, val:  70.42%, val_best:  78.75%, tr:  99.49%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.377530/  4.862115, val:  69.17%, val_best:  78.75%, tr:  98.37%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.441103/  4.632483, val:  70.00%, val_best:  78.75%, tr:  98.26%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.354187/  4.381585, val:  72.92%, val_best:  78.75%, tr:  98.88%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.434208/  4.513740, val:  69.58%, val_best:  78.75%, tr:  98.67%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.494498/  4.790456, val:  70.83%, val_best:  78.75%, tr:  97.65%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.505777/  4.480674, val:  69.17%, val_best:  78.75%, tr:  98.67%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.519091/  4.607264, val:  68.75%, val_best:  78.75%, tr:  96.94%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.618020/  4.442614, val:  72.08%, val_best:  78.75%, tr:  96.83%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.599740/  4.480688, val:  72.08%, val_best:  78.75%, tr:  96.42%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.546201/  4.613162, val:  65.42%, val_best:  78.75%, tr:  97.24%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.536745/  4.475823, val:  74.17%, val_best:  78.75%, tr:  96.02%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.535319/  5.732686, val:  72.92%, val_best:  78.75%, tr:  97.04%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.555857/  5.361866, val:  69.58%, val_best:  78.75%, tr:  96.73%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.739963/  5.212709, val:  69.58%, val_best:  78.75%, tr:  96.22%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.524012/  5.006956, val:  69.17%, val_best:  78.75%, tr:  96.94%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.562216/  4.848728, val:  66.67%, val_best:  78.75%, tr:  97.65%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.681767/  4.789108, val:  65.83%, val_best:  78.75%, tr:  95.71%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.638515/  5.075886, val:  66.67%, val_best:  78.75%, tr:  96.42%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.558078/  4.739522, val:  71.67%, val_best:  78.75%, tr:  96.83%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.521023/  5.042491, val:  67.50%, val_best:  78.75%, tr:  97.96%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.686074/  4.461243, val:  71.25%, val_best:  78.75%, tr:  96.42%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.638876/  4.335609, val:  70.00%, val_best:  78.75%, tr:  96.42%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.617206/  4.739296, val:  69.58%, val_best:  78.75%, tr:  97.14%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.500608/  4.942860, val:  67.92%, val_best:  78.75%, tr:  98.26%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.512105/  5.096638, val:  70.42%, val_best:  78.75%, tr:  98.37%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.615000/  4.602976, val:  65.83%, val_best:  78.75%, tr:  97.75%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.617501/  4.472176, val:  67.92%, val_best:  78.75%, tr:  98.16%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.670990/  4.673874, val:  70.00%, val_best:  78.75%, tr:  96.53%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5143cfd95c746508b4cae4c0cef94ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▃█▆█████▃▆██████████████▆██████▆███▆██</td></tr><tr><td>summary_val_acc</td><td>▁▂▄▅▅█▄▇▆▆▅▆▇▆▆▄▆▅▆▅▇▅▅▆▇▅▅▆▅▄▇▆▅▅▅▆▆▅▅▅</td></tr><tr><td>tr_acc</td><td>▁▄▅▇████████████████████████████████▇▇██</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▃▁▁▂▁▁▂▁▁▂▂▁▂▁▁▁▁▁▃▃▂▂▃▃▂▂▂▂▃▂▃▂▃▄▄▄▃</td></tr><tr><td>val_acc_best</td><td>▁▃▅▅▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▄▅▅█▄▇▆▆▅▆▇▆▆▄▆▅▆▅▇▅▅▆▇▅▅▆▅▄▇▆▅▅▅▆▆▅▅▅</td></tr><tr><td>val_loss</td><td>▁▁▁▂▃▃▄▃▄▃▅▄▄▅▅▆▅▆▆▇▅▇▆▆▅▆▇▇▇█▆▇██▇█▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.96527</td></tr><tr><td>tr_epoch_loss</td><td>0.67099</td></tr><tr><td>val_acc_best</td><td>0.7875</td></tr><tr><td>val_acc_now</td><td>0.7</td></tr><tr><td>val_loss</td><td>4.67387</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fiery-sweep-30</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/35scdxcq' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/35scdxcq</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_050722-35scdxcq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k9juewsr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_051852-k9juewsr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k9juewsr' target=\"_blank\">eager-sweep-31</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k9juewsr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k9juewsr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  2.521025/  2.623674, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.11%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  2.493251/  2.401668, val:  10.00%, val_best:  10.00%, tr:   7.76%, tr_best:  10.11%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  2.471030/  2.441476, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  10.11%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  2.438038/  2.464993, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.11%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  2.435606/  2.453039, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.11%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  2.476113/  2.438021, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.11%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  2.473328/  2.466004, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.11%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  2.467133/  2.461899, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.11%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  2.441164/  2.368249, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.11%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  2.425792/  2.485260, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  10.11%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  2.458742/  2.376372, val:  10.00%, val_best:  10.00%, tr:  10.62%, tr_best:  10.62%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  2.451229/  2.414626, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  10.62%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  2.431249/  2.397279, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.62%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  2.449168/  2.426764, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.62%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  2.429092/  2.594516, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  10.62%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  2.468597/  2.423184, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.62%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  2.495309/  2.347171, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.62%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  2.464427/  2.479951, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.62%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  2.455464/  2.348071, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  10.62%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  2.458870/  2.634810, val:  10.00%, val_best:  10.00%, tr:  12.16%, tr_best:  12.16%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  2.532136/  2.630572, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  12.16%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  2.469809/  2.483881, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  2.461555/  2.482759, val:  10.00%, val_best:  10.00%, tr:  10.93%, tr_best:  12.16%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  2.477140/  2.466634, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  12.16%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  2.456871/  2.388859, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  2.456265/  2.433586, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  12.16%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  2.501490/  2.344921, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  12.16%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  2.454522/  2.404290, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  12.16%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  2.434424/  2.400815, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  12.16%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  2.412901/  2.418111, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  12.16%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  2.445441/  2.436164, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  12.16%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  2.433619/  2.442688, val:  10.00%, val_best:  10.00%, tr:  11.03%, tr_best:  12.16%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  2.522616/  2.481343, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  12.16%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  2.509247/  2.416504, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  2.487263/  2.553780, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  12.16%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  2.492729/  2.449025, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  12.16%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  2.446717/  2.395209, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  2.473824/  2.632330, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  12.16%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  2.493902/  2.432965, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  2.467361/  2.406721, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  12.16%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  2.513397/  2.540425, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  12.16%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  2.518451/  2.505286, val:  10.00%, val_best:  10.00%, tr:  11.03%, tr_best:  12.16%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  2.514067/  2.395865, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  12.16%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  2.499207/  2.696059, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  12.16%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  2.522714/  2.515920, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  12.16%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  2.478578/  2.442224, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  12.16%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  2.504034/  2.360473, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  12.16%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  2.447176/  2.486052, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  12.16%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  2.491940/  2.418895, val:  10.00%, val_best:  10.00%, tr:   7.56%, tr_best:  12.16%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  2.443237/  2.339746, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  2.434119/  2.539275, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  12.16%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  2.478645/  2.349289, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  12.16%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  2.445746/  2.427385, val:  10.00%, val_best:  10.00%, tr:  10.73%, tr_best:  12.16%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  2.430613/  2.369613, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  12.16%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  2.454437/  2.403330, val:  10.00%, val_best:  10.00%, tr:  10.93%, tr_best:  12.16%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  2.488044/  2.367314, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  12.16%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  2.432323/  2.550211, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  12.16%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  2.514803/  2.424870, val:  10.00%, val_best:  10.00%, tr:   7.46%, tr_best:  12.16%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  2.499758/  2.403315, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  12.16%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  2.465906/  2.430620, val:  10.00%, val_best:  10.00%, tr:  10.62%, tr_best:  12.16%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  2.475043/  2.453509, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  12.16%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  2.549381/  2.397437, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  12.16%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  2.499790/  2.493794, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  2.446735/  2.381926, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  12.16%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  2.424499/  2.391928, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  12.16%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  2.439331/  2.378643, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  12.16%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  2.469206/  2.384904, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  12.16%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  2.469288/  2.343184, val:  10.00%, val_best:  10.00%, tr:  10.83%, tr_best:  12.16%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  2.409304/  2.465325, val:  10.00%, val_best:  10.00%, tr:  11.13%, tr_best:  12.16%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  2.487381/  2.429583, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  12.16%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  2.452406/  2.391922, val:  10.00%, val_best:  10.00%, tr:  10.62%, tr_best:  12.16%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  2.482408/  2.467104, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  12.16%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  2.527320/  2.480932, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  2.479205/  2.480726, val:  10.00%, val_best:  10.00%, tr:  10.83%, tr_best:  12.16%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  2.477507/  2.345716, val:  10.00%, val_best:  10.00%, tr:  11.03%, tr_best:  12.16%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  2.493409/  2.556630, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  2.454170/  2.432463, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  12.16%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  2.490857/  2.434645, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  12.16%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  2.463014/  2.595895, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  12.16%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  2.508440/  2.478057, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  12.16%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  2.421750/  2.417755, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  12.16%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  2.482896/  2.511716, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  12.16%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  2.528684/  2.642655, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  2.477893/  2.486252, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  2.489694/  2.640080, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  12.16%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  2.458414/  2.434928, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  2.435616/  2.556397, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  12.16%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  2.441567/  2.417959, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  12.16%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  2.466650/  2.553967, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  12.16%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  2.461328/  2.407372, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  12.16%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  2.459862/  2.420143, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  12.16%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  2.478359/  2.570884, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  12.16%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  2.444057/  2.349605, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  12.16%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  2.427895/  2.393444, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  12.16%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  2.470145/  2.520800, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  12.16%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  2.432628/  2.381572, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  2.468352/  2.435434, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  12.16%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  2.457888/  2.466409, val:  10.00%, val_best:  10.00%, tr:  11.44%, tr_best:  12.16%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  2.474241/  2.495883, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  12.16%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  2.501154/  2.424966, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  12.16%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss:  2.470341/  2.369984, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  12.16%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss:  2.465582/  2.457305, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss:  2.440573/  2.417991, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  12.16%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss:  2.472541/  2.452158, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  12.16%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss:  2.437140/  2.406457, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  12.16%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss:  2.474601/  2.475328, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  12.16%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss:  2.488275/  2.510417, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  12.16%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss:  2.469220/  2.552616, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  12.16%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss:  2.457648/  2.585857, val:  10.00%, val_best:  10.00%, tr:  10.73%, tr_best:  12.16%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss:  2.432788/  2.507341, val:  10.00%, val_best:  10.00%, tr:  10.93%, tr_best:  12.16%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss:  2.440783/  2.412386, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss:  2.436619/  2.394386, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  12.16%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss:  2.449306/  2.474765, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  12.16%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss:  2.397465/  2.464104, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  12.16%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss:  2.483588/  2.415327, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  12.16%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss:  2.480891/  2.453266, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  12.16%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss:  2.466471/  2.412106, val:  10.00%, val_best:  10.00%, tr:  10.62%, tr_best:  12.16%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss:  2.454636/  2.418582, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss:  2.453915/  2.434074, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  12.16%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss:  2.473955/  2.394259, val:  10.00%, val_best:  10.00%, tr:   7.46%, tr_best:  12.16%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss:  2.429055/  2.512172, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss:  2.481782/  2.491548, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  12.16%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss:  2.442995/  2.383230, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  12.16%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss:  2.461144/  2.457068, val:  10.00%, val_best:  10.00%, tr:  10.73%, tr_best:  12.16%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss:  2.471466/  2.449440, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  12.16%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss:  2.449295/  2.368953, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  12.16%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss:  2.462941/  2.422987, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  12.16%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss:  2.454797/  2.478289, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss:  2.474415/  2.499948, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  12.16%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss:  2.467486/  2.492317, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  12.16%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss:  2.427468/  2.446410, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss:  2.490823/  2.443931, val:  10.00%, val_best:  10.00%, tr:   7.25%, tr_best:  12.16%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss:  2.427893/  2.601966, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  12.16%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss:  2.503289/  2.352132, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  12.16%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss:  2.464512/  2.402880, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss:  2.415959/  2.597785, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  12.16%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss:  2.505052/  2.431525, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  12.16%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss:  2.423840/  2.349890, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  12.16%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss:  2.441096/  2.434938, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  12.16%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss:  2.511076/  2.456762, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  12.16%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss:  2.472740/  2.419081, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  12.16%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss:  2.456767/  2.479287, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  12.16%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss:  2.459419/  2.564484, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  12.16%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss:  2.462037/  2.424728, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  12.16%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss:  2.472733/  2.514994, val:  10.00%, val_best:  10.00%, tr:  11.44%, tr_best:  12.16%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss:  2.471397/  2.415987, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  12.16%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss:  2.501726/  2.408120, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  12.16%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss:  2.443827/  2.397658, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  12.16%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss:  2.471834/  2.355157, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  12.16%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss:  2.432959/  2.512670, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  12.16%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss:  2.475534/  2.518557, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  12.16%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss:  2.508356/  2.425012, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  12.16%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss:  2.433449/  2.363049, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  12.16%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss:  2.462079/  2.367487, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  12.16%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss:  2.520627/  2.377795, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss:  2.519250/  2.497020, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  12.16%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss:  2.530669/  2.549479, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  12.16%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss:  2.462529/  2.403583, val:  10.00%, val_best:  10.00%, tr:   7.56%, tr_best:  12.16%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss:  2.488178/  2.422633, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  12.16%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss:  2.445605/  2.417470, val:  10.00%, val_best:  10.00%, tr:  10.73%, tr_best:  12.16%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss:  2.479612/  2.491461, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  12.16%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss:  2.462124/  2.590248, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  12.16%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss:  2.486257/  2.439016, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  12.16%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss:  2.481750/  2.501852, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  12.16%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss:  2.455119/  2.450525, val:  10.00%, val_best:  10.00%, tr:  10.62%, tr_best:  12.16%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss:  2.471915/  2.546503, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  12.16%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss:  2.467320/  2.448147, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  12.16%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss:  2.491375/  2.357271, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  12.16%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss:  2.506375/  2.810368, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  12.16%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss:  2.493750/  2.357830, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  12.16%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss:  2.435308/  2.447722, val:  10.00%, val_best:  10.00%, tr:  11.13%, tr_best:  12.16%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss:  2.432728/  2.394905, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  12.16%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss:  2.417565/  2.378902, val:  10.00%, val_best:  10.00%, tr:  10.73%, tr_best:  12.16%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss:  2.426235/  2.387818, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  12.16%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss:  2.446126/  2.478004, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  12.16%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss:  2.465328/  2.472278, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  12.16%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss:  2.473129/  2.499148, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  12.16%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss:  2.434765/  2.387627, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  12.16%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss:  2.415322/  2.525317, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss:  2.488586/  2.430354, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  12.16%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss:  2.425070/  2.498958, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  12.16%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss:  2.525460/  2.525607, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  12.16%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss:  2.559721/  2.479031, val:  10.00%, val_best:  10.00%, tr:  10.62%, tr_best:  12.16%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss:  2.543828/  2.400837, val:  10.00%, val_best:  10.00%, tr:  11.03%, tr_best:  12.16%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss:  2.467758/  2.447422, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  12.16%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss:  2.468333/  2.417039, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  12.16%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss:  2.458506/  2.379263, val:  10.00%, val_best:  10.00%, tr:  10.62%, tr_best:  12.16%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss:  2.468211/  2.543723, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  12.16%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss:  2.442948/  2.428665, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  12.16%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss:  2.422938/  2.441498, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  12.16%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss:  2.501872/  2.415618, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss:  2.445419/  2.382577, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss:  2.435375/  2.379586, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss:  2.487515/  2.391618, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  12.16%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss:  2.434353/  2.494987, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  12.16%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss:  2.433291/  2.529586, val:  10.00%, val_best:  10.00%, tr:  10.83%, tr_best:  12.16%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss:  2.466886/  2.415954, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  12.16%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss:  2.407041/  2.622403, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  12.16%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss:  2.517615/  2.549862, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  12.16%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss:  2.456017/  2.480169, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  12.16%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8e3fa8eeef4fdb860327112cad0e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅█▄▂▁▅▁▁▂▂▄▄▁▂▄▂▇▁▅▅▂▂▁▄▂▁▂▄▁▅▂▂▂▂▄▄▂▁▂</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tr_acc</td><td>▂▅▅▃█▄▄▅▆▂▄▆▆▃▅▄▅▅▆▄▄▄▅▆▅▅▁▃▂▅▄▃▄▅▇▄▆▆▅▆</td></tr><tr><td>tr_epoch_loss</td><td>▆▅▂▅▄▄▁▃▇█▂▄▅▃▅▆▇▄▄▅▅▆▃▄▂▅▆▁▄▅▇█▅▄▂▅▂▄▃▂</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂▃▄▂█▃▂▂▆▅▅▂▃▁▄▆▄▃▂▃▁▅▂▂▅▃▃▇▄▂▂▄▄▃▃▄▄▁▁▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.08887</td></tr><tr><td>tr_epoch_loss</td><td>2.45602</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.48017</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-sweep-31</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k9juewsr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k9juewsr</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_051852-k9juewsr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xjhvbs5o with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_053023-xjhvbs5o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xjhvbs5o' target=\"_blank\">amber-sweep-32</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xjhvbs5o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xjhvbs5o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.303353/  2.303047, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.303309/  2.302968, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.303253/  2.302913, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.303180/  2.302890, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  2.303389/  2.302858, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  2.303003/  2.302804, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  2.302944/  2.302766, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  2.303002/  2.302772, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  2.302786/  2.302740, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  2.302638/  2.302746, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  2.303135/  2.302742, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.01%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  2.303039/  2.302714, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.01%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  2.302702/  2.302692, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  2.302847/  2.302688, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  2.303035/  2.302699, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  2.302881/  2.302661, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  2.302944/  2.302657, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.01%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  2.302719/  2.302647, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  2.302800/  2.302642, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  2.302750/  2.302643, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  2.302774/  2.302649, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.01%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  2.302997/  2.302649, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.11%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  2.302886/  2.302623, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  2.302808/  2.302628, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  2.302805/  2.302622, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  2.302895/  2.302615, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.11%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  2.302811/  2.302609, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  2.302836/  2.302608, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  2.302742/  2.302602, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  2.302640/  2.302599, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  2.302813/  2.302604, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  2.302789/  2.302609, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  2.302845/  2.302606, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  2.302791/  2.302604, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  2.302884/  2.302604, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  2.302788/  2.302602, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  2.302829/  2.302599, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.11%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  2.302872/  2.302601, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  2.302744/  2.302599, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.11%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  2.302754/  2.302598, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.11%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  2.302806/  2.302597, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.11%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  2.302747/  2.302596, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  10.11%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  2.302743/  2.302602, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  2.302774/  2.302598, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.11%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  2.302738/  2.302598, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  10.11%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  2.302781/  2.302605, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.11%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  2.302685/  2.302597, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.11%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  2.302820/  2.302601, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.11%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  2.302758/  2.302609, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.11%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  2.302779/  2.302603, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.11%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  2.302781/  2.302607, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.11%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  2.302691/  2.302607, val:  10.00%, val_best:  10.00%, tr:   7.76%, tr_best:  10.11%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  2.302883/  2.302611, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.11%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  2.302826/  2.302607, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.11%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  2.302753/  2.302602, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.11%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  2.302773/  2.302601, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  2.302884/  2.302602, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.11%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  2.302813/  2.302602, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.11%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  2.302797/  2.302603, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.11%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  2.302886/  2.302601, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  10.11%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  2.302739/  2.302602, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.11%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  2.302776/  2.302601, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.11%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  2.302853/  2.302598, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.11%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  2.302755/  2.302597, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  2.302690/  2.302604, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.11%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  2.302775/  2.302602, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  2.302693/  2.302601, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  2.302906/  2.302607, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.11%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  2.302793/  2.302599, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.11%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  2.302770/  2.302602, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  2.302866/  2.302597, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.11%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  2.302885/  2.302596, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  10.11%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  2.302852/  2.302598, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  2.302803/  2.302592, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.11%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  2.302764/  2.302594, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.11%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  2.302853/  2.302596, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  2.302876/  2.302599, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  10.11%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  2.302864/  2.302595, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.11%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  2.302786/  2.302591, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  2.302822/  2.302595, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.11%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  2.302797/  2.302598, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  10.11%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  2.302728/  2.302593, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.11%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  2.302796/  2.302601, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  10.42%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  2.302846/  2.302593, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.42%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  2.302759/  2.302595, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.42%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  2.302730/  2.302597, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.42%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  2.302884/  2.302599, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  10.42%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  2.302802/  2.302597, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.42%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  2.302900/  2.302597, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.42%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  2.302804/  2.302595, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.42%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  2.302787/  2.302590, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.42%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  2.302755/  2.302593, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  10.42%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  2.302809/  2.302600, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.42%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  2.302807/  2.302593, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.42%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  2.302743/  2.302597, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.42%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  2.302758/  2.302598, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  10.42%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  2.302812/  2.302597, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.42%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  2.302837/  2.302595, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.42%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  2.302900/  2.302598, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.42%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  2.302818/  2.302597, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.42%\n",
      "epoch-100 lr=['0.0001000'], tr/val_loss:  2.302847/  2.302597, val:  10.00%, val_best:  10.00%, tr:   7.76%, tr_best:  10.42%\n",
      "epoch-101 lr=['0.0001000'], tr/val_loss:  2.302790/  2.302595, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  10.42%\n",
      "epoch-102 lr=['0.0001000'], tr/val_loss:  2.302742/  2.302575, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  10.42%\n",
      "epoch-103 lr=['0.0001000'], tr/val_loss:  2.302818/  2.302575, val:  10.42%, val_best:  10.42%, tr:   9.81%, tr_best:  10.42%\n",
      "epoch-104 lr=['0.0001000'], tr/val_loss:  2.302790/  2.302575, val:  10.42%, val_best:  10.42%, tr:  10.01%, tr_best:  10.42%\n",
      "epoch-105 lr=['0.0001000'], tr/val_loss:  2.302836/  2.302574, val:  10.00%, val_best:  10.42%, tr:   9.81%, tr_best:  10.42%\n",
      "epoch-106 lr=['0.0001000'], tr/val_loss:  2.302774/  2.302572, val:  10.00%, val_best:  10.42%, tr:   8.38%, tr_best:  10.42%\n",
      "epoch-107 lr=['0.0001000'], tr/val_loss:  2.302835/  2.302574, val:  10.42%, val_best:  10.42%, tr:   9.09%, tr_best:  10.42%\n",
      "epoch-108 lr=['0.0001000'], tr/val_loss:  2.302773/  2.302574, val:  10.00%, val_best:  10.42%, tr:   8.78%, tr_best:  10.42%\n",
      "epoch-109 lr=['0.0001000'], tr/val_loss:  2.302821/  2.302573, val:  10.00%, val_best:  10.42%, tr:   8.68%, tr_best:  10.42%\n",
      "epoch-110 lr=['0.0001000'], tr/val_loss:  2.302753/  2.302572, val:  10.00%, val_best:  10.42%, tr:   8.68%, tr_best:  10.42%\n",
      "epoch-111 lr=['0.0001000'], tr/val_loss:  2.302760/  2.302573, val:  10.00%, val_best:  10.42%, tr:   9.50%, tr_best:  10.42%\n",
      "epoch-112 lr=['0.0001000'], tr/val_loss:  2.302790/  2.302574, val:  10.00%, val_best:  10.42%, tr:   8.78%, tr_best:  10.42%\n",
      "epoch-113 lr=['0.0001000'], tr/val_loss:  2.302757/  2.302553, val:  10.42%, val_best:  10.42%, tr:   8.27%, tr_best:  10.42%\n",
      "epoch-114 lr=['0.0001000'], tr/val_loss:  2.302766/  2.302554, val:  10.00%, val_best:  10.42%, tr:   9.19%, tr_best:  10.42%\n",
      "epoch-115 lr=['0.0001000'], tr/val_loss:  2.302676/  2.302531, val:  10.42%, val_best:  10.42%, tr:   9.50%, tr_best:  10.42%\n",
      "epoch-116 lr=['0.0001000'], tr/val_loss:  2.302708/  2.302507, val:  10.42%, val_best:  10.42%, tr:   8.89%, tr_best:  10.42%\n",
      "epoch-117 lr=['0.0001000'], tr/val_loss:  2.302774/  2.302554, val:  10.00%, val_best:  10.42%, tr:   9.91%, tr_best:  10.42%\n",
      "epoch-118 lr=['0.0001000'], tr/val_loss:  2.302819/  2.302538, val:  10.00%, val_best:  10.42%, tr:   9.09%, tr_best:  10.42%\n",
      "epoch-119 lr=['0.0001000'], tr/val_loss:  2.302679/  2.302552, val:  10.00%, val_best:  10.42%, tr:   8.99%, tr_best:  10.42%\n",
      "epoch-120 lr=['0.0001000'], tr/val_loss:  2.302681/  2.302527, val:  10.00%, val_best:  10.42%, tr:  10.01%, tr_best:  10.42%\n",
      "epoch-121 lr=['0.0001000'], tr/val_loss:  2.302631/  2.302510, val:   9.58%, val_best:  10.42%, tr:   9.70%, tr_best:  10.42%\n",
      "epoch-122 lr=['0.0001000'], tr/val_loss:  2.302688/  2.302496, val:  10.42%, val_best:  10.42%, tr:  10.52%, tr_best:  10.52%\n",
      "epoch-123 lr=['0.0001000'], tr/val_loss:  2.302667/  2.302485, val:  10.42%, val_best:  10.42%, tr:  10.32%, tr_best:  10.52%\n",
      "epoch-124 lr=['0.0001000'], tr/val_loss:  2.302611/  2.302486, val:  10.42%, val_best:  10.42%, tr:   9.70%, tr_best:  10.52%\n",
      "epoch-125 lr=['0.0001000'], tr/val_loss:  2.302501/  2.302392, val:  10.42%, val_best:  10.42%, tr:  10.73%, tr_best:  10.73%\n",
      "epoch-126 lr=['0.0001000'], tr/val_loss:  2.302487/  2.302384, val:   9.58%, val_best:  10.42%, tr:  11.24%, tr_best:  11.24%\n",
      "epoch-127 lr=['0.0001000'], tr/val_loss:  2.302655/  2.302348, val:  10.00%, val_best:  10.42%, tr:  10.11%, tr_best:  11.24%\n",
      "epoch-128 lr=['0.0001000'], tr/val_loss:  2.302358/  2.302345, val:  10.00%, val_best:  10.42%, tr:  10.21%, tr_best:  11.24%\n",
      "epoch-129 lr=['0.0001000'], tr/val_loss:  2.302431/  2.302317, val:  10.00%, val_best:  10.42%, tr:   9.91%, tr_best:  11.24%\n",
      "epoch-130 lr=['0.0001000'], tr/val_loss:  2.302351/  2.302258, val:  10.83%, val_best:  10.83%, tr:  10.21%, tr_best:  11.24%\n",
      "epoch-131 lr=['0.0001000'], tr/val_loss:  2.302368/  2.302185, val:  10.83%, val_best:  10.83%, tr:  10.32%, tr_best:  11.24%\n",
      "epoch-132 lr=['0.0001000'], tr/val_loss:  2.302214/  2.302112, val:  10.83%, val_best:  10.83%, tr:  10.62%, tr_best:  11.24%\n",
      "epoch-133 lr=['0.0001000'], tr/val_loss:  2.302165/  2.302037, val:  10.83%, val_best:  10.83%, tr:  11.44%, tr_best:  11.44%\n",
      "epoch-134 lr=['0.0001000'], tr/val_loss:  2.302065/  2.301899, val:  10.00%, val_best:  10.83%, tr:  12.05%, tr_best:  12.05%\n",
      "epoch-135 lr=['0.0001000'], tr/val_loss:  2.302068/  2.301815, val:  10.42%, val_best:  10.83%, tr:  11.75%, tr_best:  12.05%\n",
      "epoch-136 lr=['0.0001000'], tr/val_loss:  2.301682/  2.301757, val:  10.83%, val_best:  10.83%, tr:  11.75%, tr_best:  12.05%\n",
      "epoch-137 lr=['0.0001000'], tr/val_loss:  2.301393/  2.301672, val:  10.00%, val_best:  10.83%, tr:  12.05%, tr_best:  12.05%\n",
      "epoch-138 lr=['0.0001000'], tr/val_loss:  2.301425/  2.301540, val:   9.58%, val_best:  10.83%, tr:  12.36%, tr_best:  12.36%\n",
      "epoch-139 lr=['0.0001000'], tr/val_loss:  2.301126/  2.301483, val:  10.00%, val_best:  10.83%, tr:  12.36%, tr_best:  12.36%\n",
      "epoch-140 lr=['0.0001000'], tr/val_loss:  2.300937/  2.301399, val:  10.42%, val_best:  10.83%, tr:  11.95%, tr_best:  12.36%\n",
      "epoch-141 lr=['0.0001000'], tr/val_loss:  2.300697/  2.301235, val:  10.00%, val_best:  10.83%, tr:  12.56%, tr_best:  12.56%\n",
      "epoch-142 lr=['0.0001000'], tr/val_loss:  2.300469/  2.301202, val:  10.83%, val_best:  10.83%, tr:  11.64%, tr_best:  12.56%\n",
      "epoch-143 lr=['0.0001000'], tr/val_loss:  2.300224/  2.300976, val:  10.00%, val_best:  10.83%, tr:  13.18%, tr_best:  13.18%\n",
      "epoch-144 lr=['0.0001000'], tr/val_loss:  2.299834/  2.300972, val:  10.00%, val_best:  10.83%, tr:  13.18%, tr_best:  13.18%\n",
      "epoch-145 lr=['0.0001000'], tr/val_loss:  2.298884/  2.300661, val:  10.00%, val_best:  10.83%, tr:  13.99%, tr_best:  13.99%\n",
      "epoch-146 lr=['0.0001000'], tr/val_loss:  2.298852/  2.300384, val:  10.83%, val_best:  10.83%, tr:  14.40%, tr_best:  14.40%\n",
      "epoch-147 lr=['0.0001000'], tr/val_loss:  2.298376/  2.300103, val:  10.83%, val_best:  10.83%, tr:  11.85%, tr_best:  14.40%\n",
      "epoch-148 lr=['0.0001000'], tr/val_loss:  2.297750/  2.299736, val:   9.17%, val_best:  10.83%, tr:  12.97%, tr_best:  14.40%\n",
      "epoch-149 lr=['0.0001000'], tr/val_loss:  2.296854/  2.299543, val:  11.67%, val_best:  11.67%, tr:  13.69%, tr_best:  14.40%\n",
      "epoch-150 lr=['0.0001000'], tr/val_loss:  2.296506/  2.298944, val:  14.17%, val_best:  14.17%, tr:  13.48%, tr_best:  14.40%\n",
      "epoch-151 lr=['0.0001000'], tr/val_loss:  2.295365/  2.298355, val:  12.50%, val_best:  14.17%, tr:  12.87%, tr_best:  14.40%\n",
      "epoch-152 lr=['0.0001000'], tr/val_loss:  2.294148/  2.298012, val:  12.50%, val_best:  14.17%, tr:  13.59%, tr_best:  14.40%\n",
      "epoch-153 lr=['0.0001000'], tr/val_loss:  2.292820/  2.297328, val:  13.33%, val_best:  14.17%, tr:  15.32%, tr_best:  15.32%\n",
      "epoch-154 lr=['0.0001000'], tr/val_loss:  2.291317/  2.296538, val:  14.58%, val_best:  14.58%, tr:  15.63%, tr_best:  15.63%\n",
      "epoch-155 lr=['0.0001000'], tr/val_loss:  2.290022/  2.295772, val:  12.92%, val_best:  14.58%, tr:  15.02%, tr_best:  15.63%\n",
      "epoch-156 lr=['0.0001000'], tr/val_loss:  2.288527/  2.295080, val:  13.33%, val_best:  14.58%, tr:  15.63%, tr_best:  15.63%\n",
      "epoch-157 lr=['0.0001000'], tr/val_loss:  2.286472/  2.294146, val:  12.92%, val_best:  14.58%, tr:  15.12%, tr_best:  15.63%\n",
      "epoch-158 lr=['0.0001000'], tr/val_loss:  2.284598/  2.293106, val:  13.33%, val_best:  14.58%, tr:  15.83%, tr_best:  15.83%\n",
      "epoch-159 lr=['0.0001000'], tr/val_loss:  2.282275/  2.292069, val:  15.00%, val_best:  15.00%, tr:  17.67%, tr_best:  17.67%\n",
      "epoch-160 lr=['0.0001000'], tr/val_loss:  2.280246/  2.290299, val:  15.83%, val_best:  15.83%, tr:  15.63%, tr_best:  17.67%\n",
      "epoch-161 lr=['0.0001000'], tr/val_loss:  2.277402/  2.289500, val:  16.25%, val_best:  16.25%, tr:  15.32%, tr_best:  17.67%\n",
      "epoch-162 lr=['0.0001000'], tr/val_loss:  2.274830/  2.287695, val:  17.08%, val_best:  17.08%, tr:  17.67%, tr_best:  17.67%\n",
      "epoch-163 lr=['0.0001000'], tr/val_loss:  2.270001/  2.286080, val:  18.75%, val_best:  18.75%, tr:  15.12%, tr_best:  17.67%\n",
      "epoch-164 lr=['0.0001000'], tr/val_loss:  2.268582/  2.284229, val:  15.42%, val_best:  18.75%, tr:  15.32%, tr_best:  17.67%\n",
      "epoch-165 lr=['0.0001000'], tr/val_loss:  2.265014/  2.282431, val:  17.50%, val_best:  18.75%, tr:  17.06%, tr_best:  17.67%\n",
      "epoch-166 lr=['0.0001000'], tr/val_loss:  2.261679/  2.279866, val:  16.67%, val_best:  18.75%, tr:  16.75%, tr_best:  17.67%\n",
      "epoch-167 lr=['0.0001000'], tr/val_loss:  2.257488/  2.278578, val:  18.33%, val_best:  18.75%, tr:  18.39%, tr_best:  18.39%\n",
      "epoch-168 lr=['0.0001000'], tr/val_loss:  2.254119/  2.275865, val:  15.42%, val_best:  18.75%, tr:  15.32%, tr_best:  18.39%\n",
      "epoch-169 lr=['0.0001000'], tr/val_loss:  2.246366/  2.273089, val:  14.58%, val_best:  18.75%, tr:  15.42%, tr_best:  18.39%\n",
      "epoch-170 lr=['0.0001000'], tr/val_loss:  2.245838/  2.270500, val:  14.58%, val_best:  18.75%, tr:  15.32%, tr_best:  18.39%\n",
      "epoch-171 lr=['0.0001000'], tr/val_loss:  2.241931/  2.268217, val:  13.75%, val_best:  18.75%, tr:  15.93%, tr_best:  18.39%\n",
      "epoch-172 lr=['0.0001000'], tr/val_loss:  2.238238/  2.265715, val:  14.58%, val_best:  18.75%, tr:  15.22%, tr_best:  18.39%\n",
      "epoch-173 lr=['0.0001000'], tr/val_loss:  2.233557/  2.263681, val:  21.25%, val_best:  21.25%, tr:  16.75%, tr_best:  18.39%\n",
      "epoch-174 lr=['0.0001000'], tr/val_loss:  2.227709/  2.261953, val:  15.42%, val_best:  21.25%, tr:  17.26%, tr_best:  18.39%\n",
      "epoch-175 lr=['0.0001000'], tr/val_loss:  2.222243/  2.260130, val:  16.25%, val_best:  21.25%, tr:  17.47%, tr_best:  18.39%\n",
      "epoch-176 lr=['0.0001000'], tr/val_loss:  2.219693/  2.258297, val:  22.92%, val_best:  22.92%, tr:  19.20%, tr_best:  19.20%\n",
      "epoch-177 lr=['0.0001000'], tr/val_loss:  2.221058/  2.255907, val:  24.58%, val_best:  24.58%, tr:  22.37%, tr_best:  22.37%\n",
      "epoch-178 lr=['0.0001000'], tr/val_loss:  2.212106/  2.253818, val:  26.25%, val_best:  26.25%, tr:  22.88%, tr_best:  22.88%\n",
      "epoch-179 lr=['0.0001000'], tr/val_loss:  2.214836/  2.251634, val:  25.00%, val_best:  26.25%, tr:  20.22%, tr_best:  22.88%\n",
      "epoch-180 lr=['0.0001000'], tr/val_loss:  2.205777/  2.249226, val:  18.75%, val_best:  26.25%, tr:  19.92%, tr_best:  22.88%\n",
      "epoch-181 lr=['0.0001000'], tr/val_loss:  2.200503/  2.246682, val:  24.58%, val_best:  26.25%, tr:  24.21%, tr_best:  24.21%\n",
      "epoch-182 lr=['0.0001000'], tr/val_loss:  2.202719/  2.244675, val:  26.25%, val_best:  26.25%, tr:  26.05%, tr_best:  26.05%\n",
      "epoch-183 lr=['0.0001000'], tr/val_loss:  2.200608/  2.241117, val:  25.83%, val_best:  26.25%, tr:  25.33%, tr_best:  26.05%\n",
      "epoch-184 lr=['0.0001000'], tr/val_loss:  2.197354/  2.239602, val:  25.83%, val_best:  26.25%, tr:  25.54%, tr_best:  26.05%\n",
      "epoch-185 lr=['0.0001000'], tr/val_loss:  2.196245/  2.236630, val:  24.58%, val_best:  26.25%, tr:  25.54%, tr_best:  26.05%\n",
      "epoch-186 lr=['0.0001000'], tr/val_loss:  2.192519/  2.234858, val:  25.83%, val_best:  26.25%, tr:  26.25%, tr_best:  26.25%\n",
      "epoch-187 lr=['0.0001000'], tr/val_loss:  2.190061/  2.232373, val:  25.00%, val_best:  26.25%, tr:  27.68%, tr_best:  27.68%\n",
      "epoch-188 lr=['0.0001000'], tr/val_loss:  2.184021/  2.229858, val:  26.25%, val_best:  26.25%, tr:  27.48%, tr_best:  27.68%\n",
      "epoch-189 lr=['0.0001000'], tr/val_loss:  2.182573/  2.227771, val:  25.83%, val_best:  26.25%, tr:  28.70%, tr_best:  28.70%\n",
      "epoch-190 lr=['0.0001000'], tr/val_loss:  2.181241/  2.224236, val:  26.25%, val_best:  26.25%, tr:  28.70%, tr_best:  28.70%\n",
      "epoch-191 lr=['0.0001000'], tr/val_loss:  2.174375/  2.221940, val:  25.83%, val_best:  26.25%, tr:  29.42%, tr_best:  29.42%\n",
      "epoch-192 lr=['0.0001000'], tr/val_loss:  2.171633/  2.218434, val:  26.67%, val_best:  26.67%, tr:  29.11%, tr_best:  29.42%\n",
      "epoch-193 lr=['0.0001000'], tr/val_loss:  2.170662/  2.216202, val:  27.50%, val_best:  27.50%, tr:  29.93%, tr_best:  29.93%\n",
      "epoch-194 lr=['0.0001000'], tr/val_loss:  2.167613/  2.214831, val:  27.92%, val_best:  27.92%, tr:  28.91%, tr_best:  29.93%\n",
      "epoch-195 lr=['0.0001000'], tr/val_loss:  2.164902/  2.212339, val:  29.17%, val_best:  29.17%, tr:  30.64%, tr_best:  30.64%\n",
      "epoch-196 lr=['0.0001000'], tr/val_loss:  2.161531/  2.209682, val:  30.00%, val_best:  30.00%, tr:  31.77%, tr_best:  31.77%\n",
      "epoch-197 lr=['0.0001000'], tr/val_loss:  2.155952/  2.206700, val:  32.08%, val_best:  32.08%, tr:  31.46%, tr_best:  31.77%\n",
      "epoch-198 lr=['0.0001000'], tr/val_loss:  2.151196/  2.203657, val:  32.50%, val_best:  32.50%, tr:  33.20%, tr_best:  33.20%\n",
      "epoch-199 lr=['0.0001000'], tr/val_loss:  2.148106/  2.200392, val:  32.92%, val_best:  32.92%, tr:  32.69%, tr_best:  33.20%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b05bebd0bc4d58962e965b8c942d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▂▂▅▃▇▃▁▅▃▆▃▅▅▃▂▂▃▂▃▂▅▃▁▂▃▂▂▃▂▅▃▅▆█▇▇▁▆</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▃▆▄▇▇█</td></tr><tr><td>tr_acc</td><td>▂▂▂▂▂▁▂▁▁▁▁▁▁▂▁▂▁▁▂▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▃▄▅▇██</td></tr><tr><td>tr_epoch_loss</td><td>███████████████████████████████▇▇▆▅▄▃▂▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▃▃▄▄▆▇▇▇█</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▃▆▄▇▇█</td></tr><tr><td>val_loss</td><td>███████████████████████████████▇▇▆▅▅▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.33333</td></tr><tr><td>tr_acc</td><td>0.32686</td></tr><tr><td>tr_epoch_loss</td><td>2.14811</td></tr><tr><td>val_acc_best</td><td>0.32917</td></tr><tr><td>val_acc_now</td><td>0.32917</td></tr><tr><td>val_loss</td><td>2.20039</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-sweep-32</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xjhvbs5o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xjhvbs5o</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_053023-xjhvbs5o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 39poma7o with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_054148-39poma7o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/39poma7o' target=\"_blank\">resilient-sweep-33</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/39poma7o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/39poma7o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 23.225613/ 25.189178, val:  36.67%, val_best:  36.67%, tr:  30.85%, tr_best:  30.85%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 19.241438/ 15.426819, val:  51.25%, val_best:  51.25%, tr:  49.85%, tr_best:  49.85%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 18.431015/ 27.028257, val:  44.58%, val_best:  51.25%, tr:  53.32%, tr_best:  53.32%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 13.998452/ 26.203451, val:  42.50%, val_best:  51.25%, tr:  58.53%, tr_best:  58.53%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 16.884771/ 17.005167, val:  54.58%, val_best:  54.58%, tr:  57.51%, tr_best:  58.53%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 14.483225/ 25.561556, val:  41.67%, val_best:  54.58%, tr:  65.27%, tr_best:  65.27%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 16.380598/ 21.987843, val:  57.50%, val_best:  57.50%, tr:  66.09%, tr_best:  66.09%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 10.925919/ 28.117252, val:  42.50%, val_best:  57.50%, tr:  72.22%, tr_best:  72.22%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  8.763234/ 17.871927, val:  52.08%, val_best:  57.50%, tr:  73.85%, tr_best:  73.85%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  5.577591/ 18.271938, val:  54.17%, val_best:  57.50%, tr:  82.74%, tr_best:  82.74%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  6.587426/ 14.598850, val:  61.67%, val_best:  61.67%, tr:  81.82%, tr_best:  82.74%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  4.392766/ 18.404076, val:  57.08%, val_best:  61.67%, tr:  88.15%, tr_best:  88.15%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  4.669849/ 17.567146, val:  61.25%, val_best:  61.67%, tr:  88.25%, tr_best:  88.25%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  5.094130/ 18.348789, val:  61.25%, val_best:  61.67%, tr:  86.62%, tr_best:  88.25%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  5.379752/ 25.765522, val:  55.42%, val_best:  61.67%, tr:  87.74%, tr_best:  88.25%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  3.818907/ 17.816853, val:  67.08%, val_best:  67.08%, tr:  92.65%, tr_best:  92.65%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  2.238161/ 16.079102, val:  70.83%, val_best:  70.83%, tr:  95.20%, tr_best:  95.20%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  1.324817/ 16.546907, val:  68.33%, val_best:  70.83%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  1.241626/ 14.957658, val:  70.00%, val_best:  70.83%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.848299/ 15.494287, val:  71.25%, val_best:  71.25%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.613549/ 14.976354, val:  75.00%, val_best:  75.00%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.482000/ 15.677079, val:  72.08%, val_best:  75.00%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.258128/ 14.266846, val:  71.25%, val_best:  75.00%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.213754/ 16.252136, val:  70.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.216112/ 15.051206, val:  74.17%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.263794/ 14.916959, val:  73.33%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.198291/ 16.204620, val:  70.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.343032/ 15.756665, val:  71.25%, val_best:  75.00%, tr:  99.39%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.137649/ 16.313528, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.077214/ 15.874478, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.053620/ 16.058552, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.053417/ 15.784018, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.078854/ 16.575785, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.092443/ 17.494759, val:  71.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.081970/ 16.773142, val:  70.83%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.045574/ 16.772537, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.032461/ 16.240225, val:  71.67%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.028049/ 17.037632, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.098673/ 16.560453, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.033711/ 16.506109, val:  71.67%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.027632/ 16.593651, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.016616/ 16.150251, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.013387/ 16.177694, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.032108/ 16.388706, val:  71.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.026188/ 16.385349, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.023751/ 16.932915, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.026823/ 16.009560, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.029240/ 16.235868, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.019875/ 17.161047, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.009278/ 17.116131, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.011482/ 16.409838, val:  71.67%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.022961/ 15.991633, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.012337/ 16.807205, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.024340/ 16.371851, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.007115/ 15.902465, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.005729/ 16.411833, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.007040/ 16.418861, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.013443/ 16.556978, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.012195/ 16.191225, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.001825/ 16.535460, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.002682/ 15.778115, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.011696/ 16.738829, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.010318/ 16.337276, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.009331/ 16.588875, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.003781/ 16.288843, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.006370/ 16.206520, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.002561/ 16.581844, val:  71.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.002971/ 16.605730, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.005822/ 16.208992, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.003191/ 16.673183, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000078/ 16.615572, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000014/ 16.572430, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000012/ 16.571228, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000009/ 16.586277, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000006/ 16.563425, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000005/ 16.563469, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000003/ 16.548725, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000003/ 16.548630, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000002/ 16.545408, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000002/ 16.553171, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000003/ 16.560951, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000001/ 16.583584, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000001/ 16.585619, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000001/ 16.580666, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000001/ 16.584118, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000002/ 16.577412, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000183/ 16.558859, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000111/ 16.435703, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.001676/ 16.397955, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000698/ 16.451918, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000443/ 16.330490, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000019/ 16.320869, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000005/ 16.350409, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000630/ 16.248186, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.001537/ 16.417900, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000092/ 16.435196, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000804/ 16.154575, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.002686/ 16.086241, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.004073/ 16.492596, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.001641/ 16.221828, val:  71.67%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss:  0.002387/ 16.178732, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss:  0.001232/ 16.692612, val:  70.83%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss:  0.002451/ 16.380892, val:  71.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss:  0.001216/ 16.298384, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss:  0.001092/ 16.343430, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss:  0.000085/ 16.383074, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss:  0.000070/ 16.485241, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss:  0.000021/ 16.466187, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss:  0.000009/ 16.430138, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss:  0.000008/ 16.421183, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss:  0.000004/ 16.420938, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss:  0.000008/ 16.389559, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss:  0.000003/ 16.382645, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss:  0.000003/ 16.368887, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss:  0.000418/ 16.458292, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss:  0.000394/ 16.573444, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss:  0.001565/ 16.639147, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss:  0.000549/ 16.187952, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss:  0.002944/ 16.380548, val:  71.67%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss:  0.000708/ 16.437624, val:  71.67%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss:  0.000662/ 16.297823, val:  71.67%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss:  0.000054/ 16.282099, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss:  0.000009/ 16.281395, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss:  0.000008/ 16.315195, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss:  0.000003/ 16.311150, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss:  0.000002/ 16.325079, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss:  0.000002/ 16.319794, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss:  0.000002/ 16.307247, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss:  0.000005/ 16.312943, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.308784, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.308073, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss:  0.000002/ 16.306314, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss:  0.000003/ 16.311398, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.306273, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.306273, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.306765, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss:  0.000002/ 16.303720, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.309286, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.307943, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.308643, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.304296, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss:  0.000002/ 16.294249, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.298307, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.304070, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss:  0.000002/ 16.299946, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.302706, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss:  0.000002/ 16.299139, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.298002, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.298544, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.305677, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.306614, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss:  0.000002/ 16.304394, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.300499, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.300167, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss:  0.000002/ 16.307278, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.307287, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.320004, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.320253, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.319672, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.322124, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.325920, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.324017, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.305397, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.309807, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.312717, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.310678, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.314981, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.317198, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.320919, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.316721, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.323961, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.332420, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.327866, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.327663, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.327473, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.328167, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.324814, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.323334, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.323153, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.323013, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.312691, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.311913, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.308096, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.301786, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.301659, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.301659, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.301491, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.301989, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.301220, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.308521, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.308243, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.308167, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.308104, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.307867, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.307747, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.307570, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.304211, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.304026, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss:  0.000000/ 16.302967, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss:  0.000001/ 16.304928, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae4eff736ed4bbf86fbc5049f2dd6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▇▅▇████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▃▁▄▇▇███████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▆▇████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▆▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▃▁▄▇▇███████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▃▃▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0</td></tr><tr><td>val_acc_best</td><td>0.75</td></tr><tr><td>val_acc_now</td><td>0.72917</td></tr><tr><td>val_loss</td><td>16.30493</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resilient-sweep-33</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/39poma7o' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/39poma7o</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_054148-39poma7o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: phvqhjsp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3a0fd7e63c46ea9f4cc8f4afc801bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112864589732555, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_055433-phvqhjsp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/phvqhjsp' target=\"_blank\">snowy-sweep-34</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/phvqhjsp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/phvqhjsp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  2.576370/  3.515324, val:  40.83%, val_best:  40.83%, tr:  33.50%, tr_best:  33.50%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  2.906720/  3.889262, val:  39.17%, val_best:  40.83%, tr:  38.20%, tr_best:  38.20%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  4.045086/  2.999120, val:  42.50%, val_best:  42.50%, tr:  36.16%, tr_best:  38.20%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  3.866660/  4.916968, val:  40.42%, val_best:  42.50%, tr:  38.82%, tr_best:  38.82%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  3.635437/  5.106606, val:  39.17%, val_best:  42.50%, tr:  38.30%, tr_best:  38.82%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  3.654588/  4.636941, val:  31.25%, val_best:  42.50%, tr:  40.25%, tr_best:  40.25%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  3.250056/  3.900925, val:  41.67%, val_best:  42.50%, tr:  41.68%, tr_best:  41.68%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  3.646788/  5.594611, val:  37.50%, val_best:  42.50%, tr:  38.20%, tr_best:  41.68%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  3.778826/  3.918368, val:  36.25%, val_best:  42.50%, tr:  38.51%, tr_best:  41.68%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  3.558185/  3.283677, val:  39.17%, val_best:  42.50%, tr:  36.87%, tr_best:  41.68%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  3.946430/  3.333538, val:  27.08%, val_best:  42.50%, tr:  35.65%, tr_best:  41.68%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  3.875316/  3.420602, val:  27.08%, val_best:  42.50%, tr:  32.79%, tr_best:  41.68%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  3.576570/  3.817711, val:  31.25%, val_best:  42.50%, tr:  35.34%, tr_best:  41.68%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  3.529283/  3.648376, val:  37.92%, val_best:  42.50%, tr:  36.06%, tr_best:  41.68%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  3.389760/  6.826800, val:  20.42%, val_best:  42.50%, tr:  36.26%, tr_best:  41.68%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  4.207729/  5.098745, val:  29.58%, val_best:  42.50%, tr:  36.06%, tr_best:  41.68%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  3.734057/  3.864444, val:  34.58%, val_best:  42.50%, tr:  35.44%, tr_best:  41.68%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  3.864155/  5.018754, val:  22.50%, val_best:  42.50%, tr:  39.02%, tr_best:  41.68%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  4.387457/  5.375487, val:  35.00%, val_best:  42.50%, tr:  38.00%, tr_best:  41.68%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  3.866905/  5.111381, val:  28.33%, val_best:  42.50%, tr:  41.47%, tr_best:  41.68%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  3.331709/  4.570072, val:  27.92%, val_best:  42.50%, tr:  37.18%, tr_best:  41.68%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  4.331484/  4.869716, val:  29.17%, val_best:  42.50%, tr:  29.72%, tr_best:  41.68%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  3.992839/  3.991209, val:  35.83%, val_best:  42.50%, tr:  34.53%, tr_best:  41.68%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  4.050239/  5.174985, val:  33.75%, val_best:  42.50%, tr:  33.50%, tr_best:  41.68%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  4.427379/  4.634904, val:  40.83%, val_best:  42.50%, tr:  36.47%, tr_best:  41.68%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  3.552423/  4.634621, val:  40.83%, val_best:  42.50%, tr:  37.08%, tr_best:  41.68%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  3.827683/  4.774487, val:  28.75%, val_best:  42.50%, tr:  38.71%, tr_best:  41.68%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  3.495421/  4.443351, val:  30.42%, val_best:  42.50%, tr:  33.61%, tr_best:  41.68%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  3.629365/  5.671144, val:  32.92%, val_best:  42.50%, tr:  40.25%, tr_best:  41.68%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  4.251927/  3.417416, val:  30.00%, val_best:  42.50%, tr:  37.28%, tr_best:  41.68%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  3.528525/  3.804637, val:  30.83%, val_best:  42.50%, tr:  34.22%, tr_best:  41.68%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  3.845845/  7.548125, val:  31.25%, val_best:  42.50%, tr:  38.20%, tr_best:  41.68%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  3.848985/  6.132215, val:  28.75%, val_best:  42.50%, tr:  37.59%, tr_best:  41.68%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  4.272022/  4.289876, val:  41.25%, val_best:  42.50%, tr:  36.16%, tr_best:  41.68%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  3.861828/  4.698497, val:  37.92%, val_best:  42.50%, tr:  39.02%, tr_best:  41.68%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  3.599724/  2.848399, val:  36.67%, val_best:  42.50%, tr:  39.02%, tr_best:  41.68%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  3.271747/  7.039339, val:  32.50%, val_best:  42.50%, tr:  36.87%, tr_best:  41.68%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  4.191923/  7.311462, val:  27.50%, val_best:  42.50%, tr:  38.92%, tr_best:  41.68%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  3.905496/  4.770477, val:  33.75%, val_best:  42.50%, tr:  40.04%, tr_best:  41.68%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  4.336159/  4.463021, val:  39.17%, val_best:  42.50%, tr:  37.69%, tr_best:  41.68%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  3.563240/  3.556688, val:  25.00%, val_best:  42.50%, tr:  36.47%, tr_best:  41.68%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  3.905300/  4.124846, val:  28.75%, val_best:  42.50%, tr:  38.30%, tr_best:  41.68%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  3.458838/  4.464051, val:  37.08%, val_best:  42.50%, tr:  39.53%, tr_best:  41.68%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  3.559249/  5.796428, val:  44.17%, val_best:  44.17%, tr:  36.47%, tr_best:  41.68%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  3.989167/  4.225895, val:  37.08%, val_best:  44.17%, tr:  41.16%, tr_best:  41.68%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  3.953031/  6.826293, val:  45.83%, val_best:  45.83%, tr:  37.79%, tr_best:  41.68%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  4.808585/  5.847899, val:  35.00%, val_best:  45.83%, tr:  36.16%, tr_best:  41.68%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  4.762950/  8.923110, val:  26.67%, val_best:  45.83%, tr:  38.71%, tr_best:  41.68%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  5.884272/  6.543066, val:  25.42%, val_best:  45.83%, tr:  31.66%, tr_best:  41.68%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  4.918027/  3.648975, val:  45.00%, val_best:  45.83%, tr:  35.44%, tr_best:  41.68%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  3.275865/  6.890352, val:  38.75%, val_best:  45.83%, tr:  37.79%, tr_best:  41.68%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  3.967422/  4.631372, val:  35.00%, val_best:  45.83%, tr:  38.30%, tr_best:  41.68%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  3.863648/  3.437770, val:  25.83%, val_best:  45.83%, tr:  40.96%, tr_best:  41.68%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  4.079792/  4.370030, val:  37.08%, val_best:  45.83%, tr:  38.82%, tr_best:  41.68%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  3.769391/  4.546638, val:  18.75%, val_best:  45.83%, tr:  40.14%, tr_best:  41.68%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  3.657433/  4.443268, val:  44.17%, val_best:  45.83%, tr:  39.02%, tr_best:  41.68%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  3.981233/  8.199100, val:  30.42%, val_best:  45.83%, tr:  41.88%, tr_best:  41.88%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  5.128685/  6.186894, val:  35.42%, val_best:  45.83%, tr:  37.18%, tr_best:  41.88%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  4.270280/  9.711362, val:  27.08%, val_best:  45.83%, tr:  38.82%, tr_best:  41.88%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  4.442695/  6.508743, val:  28.33%, val_best:  45.83%, tr:  41.06%, tr_best:  41.88%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  3.890115/  5.130253, val:  33.33%, val_best:  45.83%, tr:  41.16%, tr_best:  41.88%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  4.018010/  7.139060, val:  23.33%, val_best:  45.83%, tr:  36.26%, tr_best:  41.88%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  5.233200/  5.241721, val:  26.67%, val_best:  45.83%, tr:  32.18%, tr_best:  41.88%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  3.783209/  3.736875, val:  25.42%, val_best:  45.83%, tr:  27.58%, tr_best:  41.88%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  3.217025/  4.032836, val:  31.67%, val_best:  45.83%, tr:  31.97%, tr_best:  41.88%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  3.246006/  3.028285, val:  24.58%, val_best:  45.83%, tr:  31.97%, tr_best:  41.88%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  3.261272/  3.573726, val:  17.92%, val_best:  45.83%, tr:  34.32%, tr_best:  41.88%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  3.783699/  4.674482, val:  31.25%, val_best:  45.83%, tr:  30.34%, tr_best:  41.88%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  3.652191/  3.941726, val:  22.50%, val_best:  45.83%, tr:  31.15%, tr_best:  41.88%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  3.871018/  5.606217, val:  39.58%, val_best:  45.83%, tr:  30.13%, tr_best:  41.88%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  3.631300/  4.919510, val:  29.17%, val_best:  45.83%, tr:  30.44%, tr_best:  41.88%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  3.908681/  4.020741, val:  25.83%, val_best:  45.83%, tr:  32.89%, tr_best:  41.88%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  4.588473/  5.356997, val:  28.33%, val_best:  45.83%, tr:  28.80%, tr_best:  41.88%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  3.756644/  5.381501, val:  37.92%, val_best:  45.83%, tr:  29.72%, tr_best:  41.88%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  3.795265/  5.354538, val:  32.92%, val_best:  45.83%, tr:  31.56%, tr_best:  41.88%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  4.047373/  3.868098, val:  36.25%, val_best:  45.83%, tr:  29.21%, tr_best:  41.88%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  3.991557/  5.916082, val:  13.75%, val_best:  45.83%, tr:  32.69%, tr_best:  41.88%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  3.662719/  6.462180, val:  25.00%, val_best:  45.83%, tr:  32.18%, tr_best:  41.88%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  5.170012/  6.396303, val:  18.75%, val_best:  45.83%, tr:  30.23%, tr_best:  41.88%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  4.430404/  4.333401, val:  43.33%, val_best:  45.83%, tr:  32.79%, tr_best:  41.88%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  3.437185/  4.489481, val:  33.75%, val_best:  45.83%, tr:  34.42%, tr_best:  41.88%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  3.582162/  5.207993, val:  19.58%, val_best:  45.83%, tr:  32.38%, tr_best:  41.88%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  3.539285/  4.161197, val:  26.25%, val_best:  45.83%, tr:  33.30%, tr_best:  41.88%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  3.377255/  5.701694, val:  28.33%, val_best:  45.83%, tr:  32.79%, tr_best:  41.88%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  4.040481/  5.068340, val:  24.17%, val_best:  45.83%, tr:  32.69%, tr_best:  41.88%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  3.578247/  3.910359, val:  18.33%, val_best:  45.83%, tr:  31.77%, tr_best:  41.88%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  3.697058/  6.321772, val:  28.75%, val_best:  45.83%, tr:  30.75%, tr_best:  41.88%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  3.941661/  3.676175, val:  34.17%, val_best:  45.83%, tr:  34.53%, tr_best:  41.88%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  5.126143/  8.487045, val:  29.17%, val_best:  45.83%, tr:  32.79%, tr_best:  41.88%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  4.326341/  4.885995, val:  27.50%, val_best:  45.83%, tr:  33.81%, tr_best:  41.88%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  3.230377/  5.040440, val:  34.58%, val_best:  45.83%, tr:  36.26%, tr_best:  41.88%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  4.973699/  7.188538, val:  23.75%, val_best:  45.83%, tr:  33.71%, tr_best:  41.88%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  4.225222/  7.264874, val:  32.08%, val_best:  45.83%, tr:  31.05%, tr_best:  41.88%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  5.094158/  4.874848, val:  32.08%, val_best:  45.83%, tr:  31.46%, tr_best:  41.88%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  5.146536/  5.934512, val:  32.08%, val_best:  45.83%, tr:  31.26%, tr_best:  41.88%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  3.940790/  5.455382, val:  33.75%, val_best:  45.83%, tr:  35.04%, tr_best:  41.88%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  4.398570/  4.476871, val:  35.00%, val_best:  45.83%, tr:  32.89%, tr_best:  41.88%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  5.015929/  5.092679, val:  32.50%, val_best:  45.83%, tr:  27.68%, tr_best:  41.88%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  4.969712/  8.524036, val:  27.50%, val_best:  45.83%, tr:  24.92%, tr_best:  41.88%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  5.052137/  6.838771, val:  37.08%, val_best:  45.83%, tr:  29.62%, tr_best:  41.88%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss:  6.075233/ 11.639683, val:  14.17%, val_best:  45.83%, tr:  32.48%, tr_best:  41.88%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss:  6.424156/  4.216889, val:  26.25%, val_best:  45.83%, tr:  29.32%, tr_best:  41.88%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss:  3.517260/  4.982085, val:  19.17%, val_best:  45.83%, tr:  34.63%, tr_best:  41.88%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss:  4.083782/  4.721765, val:  36.25%, val_best:  45.83%, tr:  31.77%, tr_best:  41.88%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss:  3.595313/  5.160808, val:  26.67%, val_best:  45.83%, tr:  31.26%, tr_best:  41.88%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss:  3.362629/  4.256614, val:  42.08%, val_best:  45.83%, tr:  32.38%, tr_best:  41.88%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss:  3.527441/  4.424755, val:  20.83%, val_best:  45.83%, tr:  32.89%, tr_best:  41.88%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss:  3.324434/  4.109657, val:  28.33%, val_best:  45.83%, tr:  33.50%, tr_best:  41.88%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss:  3.554009/  8.608449, val:  32.92%, val_best:  45.83%, tr:  34.01%, tr_best:  41.88%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss:  6.058043/ 10.746542, val:  33.75%, val_best:  45.83%, tr:  31.66%, tr_best:  41.88%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss:  8.858249/  7.483792, val:  18.33%, val_best:  45.83%, tr:  24.31%, tr_best:  41.88%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss:  4.762047/  5.537531, val:  10.00%, val_best:  45.83%, tr:  19.31%, tr_best:  41.88%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss:  5.527988/  6.099655, val:  25.42%, val_best:  45.83%, tr:  18.90%, tr_best:  41.88%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss:  5.210772/  5.954439, val:  20.83%, val_best:  45.83%, tr:  17.88%, tr_best:  41.88%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss:  5.218216/  7.742346, val:  17.08%, val_best:  45.83%, tr:  19.00%, tr_best:  41.88%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss:  5.187648/  6.685263, val:  20.42%, val_best:  45.83%, tr:  17.57%, tr_best:  41.88%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss:  5.050696/  4.801995, val:  22.92%, val_best:  45.83%, tr:  19.00%, tr_best:  41.88%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss:  5.857241/  4.656018, val:  16.25%, val_best:  45.83%, tr:  17.88%, tr_best:  41.88%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss:  5.882879/  7.278187, val:  19.58%, val_best:  45.83%, tr:  20.84%, tr_best:  41.88%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss:  5.536453/  4.810659, val:  20.42%, val_best:  45.83%, tr:  20.02%, tr_best:  41.88%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss:  5.406766/  9.716866, val:  20.00%, val_best:  45.83%, tr:  16.04%, tr_best:  41.88%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss:  5.787507/  9.017092, val:  24.17%, val_best:  45.83%, tr:  18.28%, tr_best:  41.88%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss:  5.103187/  5.285221, val:  12.50%, val_best:  45.83%, tr:  19.41%, tr_best:  41.88%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss:  5.520483/  5.875714, val:  19.58%, val_best:  45.83%, tr:  19.71%, tr_best:  41.88%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss:  5.245731/  6.469943, val:  10.00%, val_best:  45.83%, tr:  21.25%, tr_best:  41.88%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss:  5.576818/  9.155621, val:  21.67%, val_best:  45.83%, tr:  17.36%, tr_best:  41.88%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss:  5.571006/  8.828294, val:  12.92%, val_best:  45.83%, tr:  20.02%, tr_best:  41.88%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss:  5.710000/  6.241796, val:  10.00%, val_best:  45.83%, tr:  16.55%, tr_best:  41.88%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss:  5.415446/  6.442001, val:  10.00%, val_best:  45.83%, tr:  17.47%, tr_best:  41.88%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss:  5.645862/  8.285806, val:  15.83%, val_best:  45.83%, tr:  17.36%, tr_best:  41.88%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss:  5.797242/  5.199461, val:  18.75%, val_best:  45.83%, tr:  19.82%, tr_best:  41.88%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss:  4.580760/  5.274450, val:  17.50%, val_best:  45.83%, tr:  19.10%, tr_best:  41.88%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss:  4.037124/  7.039680, val:  20.83%, val_best:  45.83%, tr:  19.71%, tr_best:  41.88%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss:  5.863089/  4.598786, val:  19.58%, val_best:  45.83%, tr:  19.00%, tr_best:  41.88%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss:  5.455714/  5.923809, val:  18.33%, val_best:  45.83%, tr:  18.39%, tr_best:  41.88%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss:  5.337911/ 11.746414, val:  12.08%, val_best:  45.83%, tr:  20.02%, tr_best:  41.88%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss:  6.896966/  5.512546, val:  21.25%, val_best:  45.83%, tr:  18.49%, tr_best:  41.88%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss:  5.591915/  8.219581, val:  19.17%, val_best:  45.83%, tr:  15.83%, tr_best:  41.88%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss:  6.928025/  6.938823, val:  18.33%, val_best:  45.83%, tr:  17.88%, tr_best:  41.88%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss:  4.315140/  5.173530, val:  17.50%, val_best:  45.83%, tr:  20.74%, tr_best:  41.88%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss:  6.512416/  5.443151, val:  10.00%, val_best:  45.83%, tr:  20.22%, tr_best:  41.88%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss:  6.445088/  8.565026, val:  15.42%, val_best:  45.83%, tr:  18.79%, tr_best:  41.88%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss:  6.092279/  8.328817, val:  20.42%, val_best:  45.83%, tr:  16.34%, tr_best:  41.88%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss:  6.521932/  5.902595, val:  10.00%, val_best:  45.83%, tr:  22.06%, tr_best:  41.88%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss:  5.671666/  3.746820, val:  19.58%, val_best:  45.83%, tr:  18.49%, tr_best:  41.88%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss:  4.424838/  9.926220, val:  18.75%, val_best:  45.83%, tr:  19.10%, tr_best:  41.88%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss:  6.477771/  6.891905, val:  19.58%, val_best:  45.83%, tr:  19.51%, tr_best:  41.88%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss:  4.874105/  5.443567, val:  20.00%, val_best:  45.83%, tr:  19.92%, tr_best:  41.88%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss:  4.418161/  5.002366, val:  19.17%, val_best:  45.83%, tr:  19.61%, tr_best:  41.88%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss:  4.706792/  4.971039, val:  27.92%, val_best:  45.83%, tr:  19.10%, tr_best:  41.88%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss:  5.306770/  7.813136, val:  18.33%, val_best:  45.83%, tr:  19.41%, tr_best:  41.88%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss:  5.884647/  5.913188, val:  19.17%, val_best:  45.83%, tr:  17.16%, tr_best:  41.88%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss:  4.839455/  5.611854, val:  19.17%, val_best:  45.83%, tr:  20.53%, tr_best:  41.88%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss:  4.438161/  6.463192, val:  25.00%, val_best:  45.83%, tr:  21.86%, tr_best:  41.88%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss:  4.970534/  7.981448, val:  17.92%, val_best:  45.83%, tr:  21.65%, tr_best:  41.88%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss:  8.042410/  7.296888, val:  19.58%, val_best:  45.83%, tr:  18.69%, tr_best:  41.88%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss:  7.369289/  8.864335, val:  18.75%, val_best:  45.83%, tr:  17.67%, tr_best:  41.88%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss:  5.696342/  6.920894, val:  22.08%, val_best:  45.83%, tr:  17.26%, tr_best:  41.88%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss:  4.572739/  4.783306, val:  22.08%, val_best:  45.83%, tr:  19.71%, tr_best:  41.88%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss:  4.931853/  4.210393, val:  25.00%, val_best:  45.83%, tr:  18.08%, tr_best:  41.88%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss:  4.431821/  6.017051, val:  20.42%, val_best:  45.83%, tr:  20.02%, tr_best:  41.88%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss:  4.856369/  6.638400, val:  10.00%, val_best:  45.83%, tr:  18.79%, tr_best:  41.88%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss:  5.023643/  6.943045, val:  19.17%, val_best:  45.83%, tr:  20.22%, tr_best:  41.88%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss:  7.945190/  5.322341, val:  22.50%, val_best:  45.83%, tr:  18.18%, tr_best:  41.88%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss:  5.067245/ 10.229623, val:  10.00%, val_best:  45.83%, tr:  19.71%, tr_best:  41.88%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss:  7.250618/  8.068198, val:  10.00%, val_best:  45.83%, tr:  11.44%, tr_best:  41.88%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss:  5.619693/  6.226255, val:  10.42%, val_best:  45.83%, tr:  10.21%, tr_best:  41.88%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss:  5.439947/  6.753206, val:  10.83%, val_best:  45.83%, tr:   9.60%, tr_best:  41.88%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss:  5.179875/  5.226426, val:  10.42%, val_best:  45.83%, tr:   9.40%, tr_best:  41.88%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss:  6.337522/  3.923754, val:  10.42%, val_best:  45.83%, tr:  10.83%, tr_best:  41.88%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss:  5.206975/  4.043652, val:  10.83%, val_best:  45.83%, tr:   8.17%, tr_best:  41.88%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss:  4.854773/  5.274337, val:  10.42%, val_best:  45.83%, tr:   9.19%, tr_best:  41.88%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss:  4.525621/  5.293985, val:  10.42%, val_best:  45.83%, tr:   8.27%, tr_best:  41.88%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss:  4.791345/  7.422869, val:  10.42%, val_best:  45.83%, tr:  10.32%, tr_best:  41.88%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss:  5.748402/  7.123433, val:  10.83%, val_best:  45.83%, tr:  10.11%, tr_best:  41.88%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss:  4.846857/  5.001620, val:  10.42%, val_best:  45.83%, tr:  10.83%, tr_best:  41.88%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss:  5.381580/  7.571540, val:  10.00%, val_best:  45.83%, tr:   9.50%, tr_best:  41.88%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss:  6.203859/  5.131554, val:  10.00%, val_best:  45.83%, tr:  10.01%, tr_best:  41.88%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss:  4.375540/  3.833115, val:  10.42%, val_best:  45.83%, tr:   8.78%, tr_best:  41.88%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss:  5.107752/  8.929286, val:  10.42%, val_best:  45.83%, tr:   8.89%, tr_best:  41.88%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss:  5.511516/  5.470483, val:  10.00%, val_best:  45.83%, tr:   9.40%, tr_best:  41.88%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss:  5.317878/ 10.798085, val:  10.83%, val_best:  45.83%, tr:  11.13%, tr_best:  41.88%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss:  6.989860/  5.722170, val:  10.42%, val_best:  45.83%, tr:  11.54%, tr_best:  41.88%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss:  5.516747/  4.529793, val:  10.42%, val_best:  45.83%, tr:   9.09%, tr_best:  41.88%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss:  4.850850/  5.262199, val:  10.42%, val_best:  45.83%, tr:   9.30%, tr_best:  41.88%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss:  5.886480/  6.644568, val:  10.42%, val_best:  45.83%, tr:   9.70%, tr_best:  41.88%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss:  5.074499/  5.381958, val:  10.42%, val_best:  45.83%, tr:   9.40%, tr_best:  41.88%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss:  5.897445/  7.722157, val:  10.42%, val_best:  45.83%, tr:  10.73%, tr_best:  41.88%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss:  4.899799/  6.964928, val:  10.42%, val_best:  45.83%, tr:   9.09%, tr_best:  41.88%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss:  5.622255/  5.771911, val:  10.42%, val_best:  45.83%, tr:  10.73%, tr_best:  41.88%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss:  5.684036/  7.398456, val:  10.42%, val_best:  45.83%, tr:   9.91%, tr_best:  41.88%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss:  7.716456/ 10.219070, val:  10.42%, val_best:  45.83%, tr:   8.68%, tr_best:  41.88%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss:  5.323284/  9.454362, val:  10.42%, val_best:  45.83%, tr:   9.40%, tr_best:  41.88%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss:  7.030829/  6.270634, val:  10.42%, val_best:  45.83%, tr:  10.21%, tr_best:  41.88%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss:  3.610404/  5.834683, val:  10.42%, val_best:  45.83%, tr:   8.78%, tr_best:  41.88%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss:  4.761574/  3.626203, val:  10.42%, val_best:  45.83%, tr:   9.70%, tr_best:  41.88%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss:  4.430221/  3.462893, val:  10.42%, val_best:  45.83%, tr:  11.13%, tr_best:  41.88%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss:  5.421039/  5.344256, val:  10.42%, val_best:  45.83%, tr:  10.73%, tr_best:  41.88%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss:  5.218238/  4.877863, val:  10.42%, val_best:  45.83%, tr:   9.81%, tr_best:  41.88%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss:  4.542332/  6.237630, val:  10.42%, val_best:  45.83%, tr:   9.40%, tr_best:  41.88%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffcb4643c12447a9577dbd8aa1cd398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▄▅▆▃▃▆▇▆▇▅█▄▅▄▄▅▄▆▆▅▃▅▃▂▂▃▅▄▂▅▃▃▂▃▂▃▁▂▁▂</td></tr><tr><td>summary_val_acc</td><td>▇▅▇▅▅▇▅▆▄▇▇▃▆▄▄▇█▃▅▆▂▃▃▄▃▁▃▁▂▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>tr_acc</td><td>▇█▇▇█▇▇▇▇█▇██▆▆▅▆▆▆▆▆▆▄▃▃▄▃▃▃▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>tr_epoch_loss</td><td>▁▂▂▃▂▂▃▁▂▂▁▂▂▁▂▂▃▂▃▃▅▂█▄▄▄▃▄▅▃▅▇▃▄▄▄▄▄▇▃</td></tr><tr><td>val_acc_best</td><td>▁▃▃▃▃▃▃▃▃▆██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▇▅▇▅▅▇▅▆▄▇▇▃▆▄▄▇█▃▅▆▂▃▃▄▃▁▃▁▂▃▃▃▃▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂▂▁▃▃▂▁▄▁▂▄▂▃▁▂▂▂▂▂▂█▂▅▂▆▄▃█▅▇▃▄▃▄▂▅▃▃▇▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.33333</td></tr><tr><td>tr_acc</td><td>0.09397</td></tr><tr><td>tr_epoch_loss</td><td>4.54233</td></tr><tr><td>val_acc_best</td><td>0.45833</td></tr><tr><td>val_acc_now</td><td>0.10417</td></tr><tr><td>val_loss</td><td>6.23763</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snowy-sweep-34</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/phvqhjsp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/phvqhjsp</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_055433-phvqhjsp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pb3coulp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906d25a2c1f2439c9de4b2d3efd759fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113010957423184, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_060615-pb3coulp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pb3coulp' target=\"_blank\">fresh-sweep-35</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pb3coulp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pb3coulp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  6.968967/ 12.892779, val:  32.92%, val_best:  32.92%, tr:  24.31%, tr_best:  24.31%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 11.297113/ 11.807050, val:  37.50%, val_best:  37.50%, tr:  39.94%, tr_best:  39.94%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  9.794628/ 11.488698, val:  42.08%, val_best:  42.08%, tr:  45.56%, tr_best:  45.56%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 10.506624/  8.844748, val:  48.75%, val_best:  48.75%, tr:  48.72%, tr_best:  48.72%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 12.197660/ 14.756106, val:  55.00%, val_best:  55.00%, tr:  49.23%, tr_best:  49.23%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 11.216008/ 14.270194, val:  51.25%, val_best:  55.00%, tr:  55.26%, tr_best:  55.26%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  8.709145/ 11.570879, val:  53.75%, val_best:  55.00%, tr:  60.67%, tr_best:  60.67%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  8.607407/ 14.681826, val:  47.50%, val_best:  55.00%, tr:  63.74%, tr_best:  63.74%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  7.048895/  9.775298, val:  63.33%, val_best:  63.33%, tr:  66.09%, tr_best:  66.09%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  9.264606/ 15.814405, val:  47.50%, val_best:  63.33%, tr:  65.58%, tr_best:  66.09%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  9.372565/ 15.683470, val:  47.92%, val_best:  63.33%, tr:  67.11%, tr_best:  67.11%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  8.569919/ 13.388145, val:  54.17%, val_best:  63.33%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  8.136249/ 12.169218, val:  65.42%, val_best:  65.42%, tr:  70.38%, tr_best:  71.40%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  6.596111/ 14.638546, val:  53.75%, val_best:  65.42%, tr:  79.06%, tr_best:  79.06%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  6.501486/ 17.559948, val:  49.17%, val_best:  65.42%, tr:  80.08%, tr_best:  80.08%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  6.332120/ 15.606147, val:  60.83%, val_best:  65.42%, tr:  78.55%, tr_best:  80.08%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  6.505549/ 16.341661, val:  62.50%, val_best:  65.42%, tr:  80.18%, tr_best:  80.18%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  5.457846/ 16.865242, val:  57.50%, val_best:  65.42%, tr:  87.13%, tr_best:  87.13%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  4.912185/ 17.990784, val:  58.33%, val_best:  65.42%, tr:  87.54%, tr_best:  87.54%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  4.400748/ 16.407101, val:  62.08%, val_best:  65.42%, tr:  92.44%, tr_best:  92.44%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  4.185438/ 18.382195, val:  60.00%, val_best:  65.42%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  4.055796/ 22.769011, val:  59.58%, val_best:  65.42%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  3.797467/ 23.445349, val:  54.17%, val_best:  65.42%, tr:  93.46%, tr_best:  94.48%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  3.693496/ 19.844145, val:  64.58%, val_best:  65.42%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  3.514473/ 20.221237, val:  69.17%, val_best:  69.17%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  3.432356/ 20.516262, val:  63.75%, val_best:  69.17%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  2.985775/ 21.539291, val:  70.00%, val_best:  70.00%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  2.666410/ 21.497095, val:  68.75%, val_best:  70.00%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  2.692494/ 22.101463, val:  72.08%, val_best:  72.08%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  2.358470/ 22.238386, val:  68.75%, val_best:  72.08%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  2.206936/ 22.200445, val:  71.67%, val_best:  72.08%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  2.246473/ 22.704481, val:  72.50%, val_best:  72.50%, tr:  98.98%, tr_best:  99.28%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  1.997154/ 23.881559, val:  70.83%, val_best:  72.50%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  2.162479/ 24.799156, val:  68.75%, val_best:  72.50%, tr:  98.88%, tr_best:  99.39%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  1.987929/ 26.768513, val:  65.42%, val_best:  72.50%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  1.900154/ 26.554743, val:  69.58%, val_best:  72.50%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  1.735244/ 26.286802, val:  70.00%, val_best:  72.50%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  1.649551/ 27.074697, val:  72.08%, val_best:  72.50%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  1.486817/ 26.753979, val:  70.83%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  1.533832/ 28.343050, val:  71.67%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  1.255062/ 29.893135, val:  71.67%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  1.330937/ 29.519566, val:  67.92%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  1.304858/ 28.940958, val:  71.67%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  1.180869/ 29.894485, val:  72.08%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  1.412627/ 28.591619, val:  73.33%, val_best:  73.33%, tr:  99.49%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.912622/ 29.424923, val:  66.67%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  1.093914/ 29.369030, val:  72.50%, val_best:  73.33%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.903843/ 30.655949, val:  69.17%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  1.015508/ 30.075924, val:  72.50%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  1.353904/ 31.394085, val:  67.50%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  1.019655/ 31.744005, val:  72.92%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.838014/ 33.064255, val:  70.00%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  1.011135/ 32.225941, val:  71.67%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  1.105888/ 31.166309, val:  68.75%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.859841/ 31.714590, val:  74.17%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.850812/ 32.605949, val:  71.67%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.707440/ 32.811768, val:  72.50%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.781403/ 32.455822, val:  72.08%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.818103/ 33.698563, val:  70.42%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.813212/ 31.720036, val:  75.83%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.825312/ 32.765991, val:  72.08%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.860439/ 33.319206, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.645091/ 34.199871, val:  73.33%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.612424/ 34.578079, val:  70.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.924008/ 32.860878, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  1.143708/ 35.746429, val:  73.75%, val_best:  75.83%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.697906/ 35.064743, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.546441/ 34.685474, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.668824/ 36.153870, val:  72.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.643753/ 35.748840, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.668358/ 36.209171, val:  70.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.598144/ 36.822018, val:  70.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.451431/ 35.869743, val:  71.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.557433/ 36.918167, val:  70.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.498588/ 36.338150, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.378018/ 36.490185, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.417645/ 37.450218, val:  70.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.405962/ 38.207207, val:  71.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.396830/ 38.140068, val:  72.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.393295/ 37.905827, val:  70.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.291502/ 38.802979, val:  68.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.399310/ 38.180065, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.414036/ 39.579975, val:  72.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.352440/ 38.335232, val:  70.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.332717/ 37.858936, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.371048/ 37.467773, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.314696/ 38.640755, val:  72.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.267330/ 37.957909, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.282604/ 39.250957, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.258124/ 39.136787, val:  70.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.307772/ 39.169853, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.249830/ 39.132008, val:  72.08%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.210442/ 39.283321, val:  70.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.212648/ 40.257072, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.237735/ 40.222038, val:  70.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.314796/ 41.190762, val:  70.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.298527/ 40.015064, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.251882/ 40.354053, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.232856/ 39.917442, val:  71.67%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.228215/ 40.522820, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss:  0.282806/ 41.055325, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss:  0.239187/ 41.671268, val:  72.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss:  0.244926/ 40.878216, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss:  0.288098/ 41.127178, val:  70.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss:  0.343707/ 39.972492, val:  75.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss:  0.312311/ 41.383705, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss:  0.253502/ 40.136612, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss:  0.268946/ 40.444901, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss:  0.275042/ 41.082016, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss:  0.240941/ 41.927822, val:  70.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss:  0.226027/ 40.874260, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss:  0.251751/ 41.485779, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss:  0.347528/ 41.664242, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss:  0.204547/ 40.630646, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss:  0.224185/ 41.162952, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss:  0.215189/ 41.086250, val:  72.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss:  0.149066/ 41.360035, val:  71.67%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss:  0.244270/ 40.275120, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss:  0.309238/ 40.722202, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss:  0.265391/ 41.439716, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss:  0.198432/ 41.317207, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss:  0.238043/ 41.858109, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss:  0.313917/ 41.967968, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss:  0.210377/ 41.835926, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss:  0.159061/ 41.026726, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss:  0.154393/ 42.054985, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss:  0.204930/ 41.793079, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss:  0.273475/ 41.934700, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss:  0.224277/ 41.708672, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss:  0.188719/ 42.289127, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss:  0.198115/ 43.033123, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss:  0.258778/ 42.323711, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss:  0.174147/ 42.681248, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss:  0.137407/ 43.115223, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss:  0.159993/ 42.711098, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss:  0.187661/ 42.494686, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss:  0.197342/ 41.905506, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss:  0.172835/ 41.983147, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss:  0.198038/ 41.932312, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss:  0.268754/ 42.579479, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss:  0.154005/ 42.729900, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss:  0.201350/ 42.691826, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss:  0.110716/ 42.427917, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss:  0.128137/ 43.463135, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss:  0.220368/ 41.715328, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss:  0.150999/ 42.100441, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss:  0.161083/ 43.045692, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss:  0.181130/ 43.160606, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss:  0.109562/ 42.860519, val:  72.50%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss:  0.111087/ 43.189644, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss:  0.133324/ 42.221813, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss:  0.125888/ 41.887032, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss:  0.153367/ 42.883297, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss:  0.150506/ 43.316082, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss:  0.087778/ 42.551079, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss:  0.263706/ 42.439430, val:  71.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss:  0.190611/ 42.923828, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss:  0.124533/ 42.333977, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss:  0.094460/ 42.094460, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss:  0.166637/ 42.407284, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss:  0.337691/ 42.776478, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss:  0.174954/ 44.115803, val:  69.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss:  0.185273/ 45.940830, val:  67.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss:  0.193391/ 44.283649, val:  72.50%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss:  0.169301/ 44.719196, val:  70.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss:  0.159153/ 43.322208, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss:  0.131075/ 43.938023, val:  72.50%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss:  0.125617/ 43.709965, val:  71.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss:  0.158460/ 44.680363, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss:  0.139064/ 44.409679, val:  71.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss:  0.116443/ 44.676044, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss:  0.106059/ 44.085876, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss:  0.097803/ 43.339798, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss:  0.084813/ 43.457428, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss:  0.078536/ 43.102577, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss:  0.122786/ 43.208336, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss:  0.121184/ 43.646023, val:  70.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss:  0.120857/ 43.429237, val:  71.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss:  0.118100/ 44.808723, val:  70.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss:  0.195597/ 43.725334, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss:  0.189110/ 44.194031, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss:  0.188437/ 45.069687, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss:  0.196881/ 43.433418, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss:  0.109860/ 44.192276, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss:  0.156777/ 44.654068, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss:  0.272957/ 46.077778, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss:  0.434656/ 44.823837, val:  72.50%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss:  0.312970/ 44.727642, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss:  0.294978/ 45.023125, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss:  0.210940/ 44.422741, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss:  0.223007/ 45.763496, val:  69.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss:  0.232654/ 46.348064, val:  69.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss:  0.202662/ 44.798294, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss:  0.214053/ 44.813358, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss:  0.180923/ 45.946560, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss:  0.172880/ 45.699017, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss:  0.221176/ 45.486538, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss:  0.166749/ 46.159428, val:  72.50%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss:  0.229821/ 44.241917, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss:  0.237483/ 46.938004, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180252f81ff94a20a471146bd0dd4fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▄▄▁▅▆███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▃▅▆▆▇▇▇███▇█▇█▇█▇████▇███▇███▇█▇▇▇█▇▇█</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▅▄▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▆▆▆▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▃▅▆▆▇▇▇███▇█▇█▇█▇████▇███▇███▇█▇▇▇█▇▇█</td></tr><tr><td>val_loss</td><td>▁▁▂▂▂▃▃▄▅▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.23748</td></tr><tr><td>val_acc_best</td><td>0.76667</td></tr><tr><td>val_acc_now</td><td>0.72917</td></tr><tr><td>val_loss</td><td>46.938</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-sweep-35</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pb3coulp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pb3coulp</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_060615-pb3coulp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wxh7gnmg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_061925-wxh7gnmg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wxh7gnmg' target=\"_blank\">giddy-sweep-36</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wxh7gnmg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wxh7gnmg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  2.094249/  2.578005, val:  32.08%, val_best:  32.08%, tr:  32.58%, tr_best:  32.58%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  2.218858/  2.607536, val:  31.67%, val_best:  32.08%, tr:  34.01%, tr_best:  34.01%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  2.165291/  2.655835, val:  27.08%, val_best:  32.08%, tr:  38.51%, tr_best:  38.51%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  1.949733/  2.369757, val:  35.83%, val_best:  35.83%, tr:  41.78%, tr_best:  41.78%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  2.007385/  1.728118, val:  50.42%, val_best:  50.42%, tr:  39.02%, tr_best:  41.78%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  1.593687/  2.228899, val:  40.83%, val_best:  50.42%, tr:  49.44%, tr_best:  49.44%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  1.807368/  2.080873, val:  45.83%, val_best:  50.42%, tr:  50.36%, tr_best:  50.36%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  1.692124/  2.522413, val:  38.33%, val_best:  50.42%, tr:  51.79%, tr_best:  51.79%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  1.683251/  2.047519, val:  46.67%, val_best:  50.42%, tr:  51.69%, tr_best:  51.79%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  1.980875/  2.176951, val:  47.50%, val_best:  50.42%, tr:  51.48%, tr_best:  51.79%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  1.818460/  1.811015, val:  50.42%, val_best:  50.42%, tr:  48.11%, tr_best:  51.79%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  1.701020/  2.251360, val:  42.08%, val_best:  50.42%, tr:  53.73%, tr_best:  53.73%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  1.675732/  1.689688, val:  40.00%, val_best:  50.42%, tr:  51.28%, tr_best:  53.73%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  1.690214/  2.195374, val:  39.58%, val_best:  50.42%, tr:  51.89%, tr_best:  53.73%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  1.756285/  2.239343, val:  50.00%, val_best:  50.42%, tr:  53.22%, tr_best:  53.73%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  1.650121/  2.174021, val:  42.50%, val_best:  50.42%, tr:  53.42%, tr_best:  53.73%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  1.660728/  1.869455, val:  54.17%, val_best:  54.17%, tr:  54.55%, tr_best:  54.55%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  1.740162/  2.173548, val:  54.58%, val_best:  54.58%, tr:  53.42%, tr_best:  54.55%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  1.840609/  2.356311, val:  51.67%, val_best:  54.58%, tr:  54.14%, tr_best:  54.55%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  1.629024/  2.442183, val:  36.25%, val_best:  54.58%, tr:  56.18%, tr_best:  56.18%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  1.867734/  1.947935, val:  50.42%, val_best:  54.58%, tr:  53.63%, tr_best:  56.18%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  1.533122/  1.749650, val:  44.17%, val_best:  54.58%, tr:  57.71%, tr_best:  57.71%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  1.702050/  1.788261, val:  44.58%, val_best:  54.58%, tr:  54.85%, tr_best:  57.71%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  1.524956/  2.152530, val:  40.83%, val_best:  54.58%, tr:  53.52%, tr_best:  57.71%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  1.502593/  1.799637, val:  45.83%, val_best:  54.58%, tr:  54.24%, tr_best:  57.71%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  1.725528/  2.245373, val:  47.92%, val_best:  54.58%, tr:  54.03%, tr_best:  57.71%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  1.383338/  2.200478, val:  51.25%, val_best:  54.58%, tr:  62.31%, tr_best:  62.31%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  1.520934/  2.225347, val:  50.00%, val_best:  54.58%, tr:  56.18%, tr_best:  62.31%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  1.689335/  2.032768, val:  51.25%, val_best:  54.58%, tr:  57.41%, tr_best:  62.31%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  1.363845/  2.105360, val:  40.00%, val_best:  54.58%, tr:  59.24%, tr_best:  62.31%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  1.512432/  1.893717, val:  48.33%, val_best:  54.58%, tr:  57.00%, tr_best:  62.31%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  1.469316/  2.256132, val:  50.83%, val_best:  54.58%, tr:  59.86%, tr_best:  62.31%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  1.925848/  2.579507, val:  34.58%, val_best:  54.58%, tr:  57.51%, tr_best:  62.31%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  1.520730/  1.745674, val:  51.67%, val_best:  54.58%, tr:  57.20%, tr_best:  62.31%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  1.537661/  2.076813, val:  46.67%, val_best:  54.58%, tr:  57.92%, tr_best:  62.31%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  1.308544/  1.452369, val:  60.83%, val_best:  60.83%, tr:  62.10%, tr_best:  62.31%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  1.265486/  2.239973, val:  51.67%, val_best:  60.83%, tr:  61.80%, tr_best:  62.31%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  1.856259/  2.611356, val:  55.83%, val_best:  60.83%, tr:  59.24%, tr_best:  62.31%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  1.513931/  2.190734, val:  47.92%, val_best:  60.83%, tr:  61.39%, tr_best:  62.31%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  1.269199/  1.747068, val:  56.25%, val_best:  60.83%, tr:  63.53%, tr_best:  63.53%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  1.103021/  2.145697, val:  51.25%, val_best:  60.83%, tr:  65.68%, tr_best:  65.68%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  1.304005/  1.977968, val:  48.75%, val_best:  60.83%, tr:  62.41%, tr_best:  65.68%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  1.370906/  2.155460, val:  53.75%, val_best:  60.83%, tr:  62.82%, tr_best:  65.68%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  1.529719/  2.643270, val:  52.50%, val_best:  60.83%, tr:  62.61%, tr_best:  65.68%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  1.744746/  2.158898, val:  47.08%, val_best:  60.83%, tr:  59.14%, tr_best:  65.68%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  1.337391/  1.939481, val:  50.83%, val_best:  60.83%, tr:  57.71%, tr_best:  65.68%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  1.559823/  2.092633, val:  50.42%, val_best:  60.83%, tr:  61.59%, tr_best:  65.68%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  1.412133/  2.086069, val:  51.25%, val_best:  60.83%, tr:  61.70%, tr_best:  65.68%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  1.495251/  1.670096, val:  56.67%, val_best:  60.83%, tr:  61.80%, tr_best:  65.68%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  1.156530/  1.916417, val:  49.17%, val_best:  60.83%, tr:  66.29%, tr_best:  66.29%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  1.199307/  2.992832, val:  44.17%, val_best:  60.83%, tr:  63.43%, tr_best:  66.29%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  1.480914/  1.979718, val:  48.33%, val_best:  60.83%, tr:  63.74%, tr_best:  66.29%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  1.323376/  1.685395, val:  59.58%, val_best:  60.83%, tr:  63.23%, tr_best:  66.29%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  1.350485/  1.960556, val:  55.42%, val_best:  60.83%, tr:  63.64%, tr_best:  66.29%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  1.321170/  2.282041, val:  47.08%, val_best:  60.83%, tr:  65.58%, tr_best:  66.29%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  1.246470/  2.229219, val:  50.00%, val_best:  60.83%, tr:  64.35%, tr_best:  66.29%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  1.472009/  2.814412, val:  50.42%, val_best:  60.83%, tr:  64.56%, tr_best:  66.29%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  1.758966/  2.207422, val:  47.50%, val_best:  60.83%, tr:  61.39%, tr_best:  66.29%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  1.226967/  1.828622, val:  53.33%, val_best:  60.83%, tr:  64.76%, tr_best:  66.29%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  1.342533/  1.832594, val:  62.92%, val_best:  62.92%, tr:  64.15%, tr_best:  66.29%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  1.325873/  2.471828, val:  40.83%, val_best:  62.92%, tr:  66.09%, tr_best:  66.29%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  1.259734/  1.797778, val:  57.50%, val_best:  62.92%, tr:  67.21%, tr_best:  67.21%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  1.262206/  2.147226, val:  52.50%, val_best:  62.92%, tr:  65.07%, tr_best:  67.21%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  1.387611/  1.767802, val:  58.33%, val_best:  62.92%, tr:  62.51%, tr_best:  67.21%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  1.383422/  2.229377, val:  49.17%, val_best:  62.92%, tr:  62.41%, tr_best:  67.21%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  1.327472/  1.931496, val:  56.67%, val_best:  62.92%, tr:  63.23%, tr_best:  67.21%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  1.295693/  1.813805, val:  57.50%, val_best:  62.92%, tr:  65.88%, tr_best:  67.21%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  1.454409/  1.809192, val:  54.17%, val_best:  62.92%, tr:  66.39%, tr_best:  67.21%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  1.243837/  1.721501, val:  57.50%, val_best:  62.92%, tr:  66.70%, tr_best:  67.21%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  1.224129/  2.669795, val:  53.75%, val_best:  62.92%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  1.305551/  2.072551, val:  55.83%, val_best:  62.92%, tr:  66.19%, tr_best:  69.36%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  1.523823/  2.156187, val:  51.25%, val_best:  62.92%, tr:  66.19%, tr_best:  69.36%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  1.396259/  1.931826, val:  55.42%, val_best:  62.92%, tr:  61.49%, tr_best:  69.36%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  1.337193/  2.373752, val:  51.25%, val_best:  62.92%, tr:  65.47%, tr_best:  69.36%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  1.356912/  2.153581, val:  53.75%, val_best:  62.92%, tr:  64.66%, tr_best:  69.36%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  1.260112/  1.780472, val:  56.25%, val_best:  62.92%, tr:  64.45%, tr_best:  69.36%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  1.106945/  2.289684, val:  47.08%, val_best:  62.92%, tr:  66.09%, tr_best:  69.36%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  1.307725/  2.392724, val:  51.67%, val_best:  62.92%, tr:  66.29%, tr_best:  69.36%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  1.618800/  2.380683, val:  52.50%, val_best:  62.92%, tr:  63.43%, tr_best:  69.36%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  1.243644/  2.234593, val:  50.42%, val_best:  62.92%, tr:  68.44%, tr_best:  69.36%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  1.246555/  2.036763, val:  45.42%, val_best:  62.92%, tr:  65.68%, tr_best:  69.36%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  1.184088/  2.076956, val:  54.17%, val_best:  62.92%, tr:  67.21%, tr_best:  69.36%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  1.600093/  3.057023, val:  57.50%, val_best:  62.92%, tr:  63.23%, tr_best:  69.36%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  1.499901/  2.690311, val:  41.25%, val_best:  62.92%, tr:  64.76%, tr_best:  69.36%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  1.451154/  1.961961, val:  61.67%, val_best:  62.92%, tr:  64.04%, tr_best:  69.36%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  1.197428/  1.979916, val:  53.75%, val_best:  62.92%, tr:  67.01%, tr_best:  69.36%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  1.131670/  1.640624, val:  62.08%, val_best:  62.92%, tr:  69.05%, tr_best:  69.36%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.992965/  2.006769, val:  57.50%, val_best:  62.92%, tr:  72.52%, tr_best:  72.52%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  1.280651/  2.433803, val:  60.00%, val_best:  62.92%, tr:  68.34%, tr_best:  72.52%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  1.227991/  2.275561, val:  57.50%, val_best:  62.92%, tr:  69.36%, tr_best:  72.52%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  1.090675/  1.608363, val:  60.83%, val_best:  62.92%, tr:  70.28%, tr_best:  72.52%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  1.042239/  2.082973, val:  58.33%, val_best:  62.92%, tr:  70.68%, tr_best:  72.52%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  1.105343/  1.722581, val:  56.25%, val_best:  62.92%, tr:  65.17%, tr_best:  72.52%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  1.158640/  1.798763, val:  57.92%, val_best:  62.92%, tr:  70.28%, tr_best:  72.52%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  1.172558/  1.996583, val:  49.58%, val_best:  62.92%, tr:  68.23%, tr_best:  72.52%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  1.464366/  2.534515, val:  56.67%, val_best:  62.92%, tr:  67.42%, tr_best:  72.52%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  1.386168/  2.346257, val:  52.50%, val_best:  62.92%, tr:  64.25%, tr_best:  72.52%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  1.310001/  1.961270, val:  55.42%, val_best:  62.92%, tr:  67.11%, tr_best:  72.52%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  1.328051/  2.474627, val:  49.17%, val_best:  62.92%, tr:  63.43%, tr_best:  72.52%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  1.286116/  2.452305, val:  52.08%, val_best:  62.92%, tr:  66.29%, tr_best:  72.52%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss:  1.284262/  2.537538, val:  51.25%, val_best:  62.92%, tr:  67.62%, tr_best:  72.52%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss:  1.476970/  2.554829, val:  52.92%, val_best:  62.92%, tr:  67.62%, tr_best:  72.52%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss:  1.693097/  2.176530, val:  49.58%, val_best:  62.92%, tr:  64.76%, tr_best:  72.52%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss:  1.958924/  2.942611, val:  42.92%, val_best:  62.92%, tr:  62.51%, tr_best:  72.52%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss:  1.474265/  2.235904, val:  52.08%, val_best:  62.92%, tr:  67.93%, tr_best:  72.52%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss:  1.415572/  2.348854, val:  47.50%, val_best:  62.92%, tr:  64.86%, tr_best:  72.52%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss:  1.522284/  2.449075, val:  49.58%, val_best:  62.92%, tr:  62.61%, tr_best:  72.52%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss:  1.552582/  2.494677, val:  45.42%, val_best:  62.92%, tr:  67.82%, tr_best:  72.52%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss:  1.455700/  3.215284, val:  49.58%, val_best:  62.92%, tr:  64.86%, tr_best:  72.52%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss:  1.305229/  2.433808, val:  48.75%, val_best:  62.92%, tr:  66.91%, tr_best:  72.52%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss:  1.396113/  2.465033, val:  52.50%, val_best:  62.92%, tr:  65.88%, tr_best:  72.52%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss:  1.374928/  2.274682, val:  56.25%, val_best:  62.92%, tr:  66.70%, tr_best:  72.52%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss:  1.503001/  2.782994, val:  55.00%, val_best:  62.92%, tr:  63.23%, tr_best:  72.52%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss:  1.325518/  2.110161, val:  45.83%, val_best:  62.92%, tr:  65.78%, tr_best:  72.52%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss:  1.452551/  2.178407, val:  52.08%, val_best:  62.92%, tr:  62.10%, tr_best:  72.52%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss:  1.406185/  2.167790, val:  52.50%, val_best:  62.92%, tr:  63.33%, tr_best:  72.52%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss:  1.202986/  1.918661, val:  56.67%, val_best:  62.92%, tr:  65.99%, tr_best:  72.52%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss:  1.354723/  2.779074, val:  47.08%, val_best:  62.92%, tr:  64.86%, tr_best:  72.52%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss:  1.395006/  2.267132, val:  55.42%, val_best:  62.92%, tr:  63.43%, tr_best:  72.52%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss:  1.605632/  1.998780, val:  53.33%, val_best:  62.92%, tr:  66.50%, tr_best:  72.52%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss:  1.715795/  2.360229, val:  52.50%, val_best:  62.92%, tr:  63.74%, tr_best:  72.52%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss:  1.403281/  3.422261, val:  51.67%, val_best:  62.92%, tr:  68.13%, tr_best:  72.52%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss:  1.463522/  2.119691, val:  52.50%, val_best:  62.92%, tr:  66.91%, tr_best:  72.52%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss:  1.395906/  2.918267, val:  43.75%, val_best:  62.92%, tr:  66.70%, tr_best:  72.52%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss:  1.377535/  2.170663, val:  54.17%, val_best:  62.92%, tr:  68.85%, tr_best:  72.52%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss:  1.322028/  2.011058, val:  53.33%, val_best:  62.92%, tr:  69.87%, tr_best:  72.52%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss:  1.358021/  2.403703, val:  55.83%, val_best:  62.92%, tr:  68.74%, tr_best:  72.52%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss:  1.726642/  2.507746, val:  56.67%, val_best:  62.92%, tr:  66.91%, tr_best:  72.52%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss:  1.336083/  2.612903, val:  48.75%, val_best:  62.92%, tr:  68.44%, tr_best:  72.52%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss:  1.330173/  1.933696, val:  54.58%, val_best:  62.92%, tr:  65.58%, tr_best:  72.52%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss:  1.486585/  1.916416, val:  56.25%, val_best:  62.92%, tr:  65.99%, tr_best:  72.52%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss:  1.270777/  2.143449, val:  59.58%, val_best:  62.92%, tr:  65.27%, tr_best:  72.52%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss:  1.161168/  2.227178, val:  56.25%, val_best:  62.92%, tr:  68.74%, tr_best:  72.52%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss:  1.312171/  2.196098, val:  50.00%, val_best:  62.92%, tr:  68.44%, tr_best:  72.52%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss:  1.225873/  2.181436, val:  52.08%, val_best:  62.92%, tr:  69.77%, tr_best:  72.52%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss:  1.211621/  2.127965, val:  59.17%, val_best:  62.92%, tr:  68.23%, tr_best:  72.52%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss:  1.369902/  1.987594, val:  50.83%, val_best:  62.92%, tr:  68.34%, tr_best:  72.52%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss:  1.321390/  2.747686, val:  41.67%, val_best:  62.92%, tr:  65.78%, tr_best:  72.52%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss:  1.219206/  2.027270, val:  57.08%, val_best:  62.92%, tr:  67.31%, tr_best:  72.52%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss:  1.151809/  1.800485, val:  65.42%, val_best:  65.42%, tr:  67.62%, tr_best:  72.52%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss:  1.127230/  1.972407, val:  50.83%, val_best:  65.42%, tr:  71.50%, tr_best:  72.52%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss:  1.192413/  2.405936, val:  60.42%, val_best:  65.42%, tr:  68.95%, tr_best:  72.52%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss:  1.330143/  2.096145, val:  55.42%, val_best:  65.42%, tr:  67.31%, tr_best:  72.52%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss:  1.305122/  2.791681, val:  51.25%, val_best:  65.42%, tr:  65.27%, tr_best:  72.52%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss:  1.358576/  2.312000, val:  53.75%, val_best:  65.42%, tr:  67.72%, tr_best:  72.52%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss:  1.187728/  3.451456, val:  50.42%, val_best:  65.42%, tr:  69.05%, tr_best:  72.52%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss:  1.399248/  1.880944, val:  53.75%, val_best:  65.42%, tr:  68.34%, tr_best:  72.52%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss:  1.281862/  2.808823, val:  42.92%, val_best:  65.42%, tr:  67.21%, tr_best:  72.52%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss:  1.599998/  2.500600, val:  57.92%, val_best:  65.42%, tr:  62.82%, tr_best:  72.52%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss:  1.435024/  2.356215, val:  55.00%, val_best:  65.42%, tr:  64.56%, tr_best:  72.52%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss:  1.588081/  2.247756, val:  55.42%, val_best:  65.42%, tr:  64.15%, tr_best:  72.52%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss:  1.704473/  2.864186, val:  52.08%, val_best:  65.42%, tr:  66.50%, tr_best:  72.52%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss:  1.564526/  2.645007, val:  57.50%, val_best:  65.42%, tr:  64.76%, tr_best:  72.52%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss:  1.350593/  2.055026, val:  57.92%, val_best:  65.42%, tr:  64.76%, tr_best:  72.52%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss:  1.629796/  2.606220, val:  52.08%, val_best:  65.42%, tr:  65.99%, tr_best:  72.52%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss:  1.707687/  2.287178, val:  53.33%, val_best:  65.42%, tr:  64.04%, tr_best:  72.52%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss:  1.439388/  2.283847, val:  53.33%, val_best:  65.42%, tr:  65.58%, tr_best:  72.52%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss:  1.559200/  2.880502, val:  49.17%, val_best:  65.42%, tr:  65.99%, tr_best:  72.52%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss:  1.627671/  2.497374, val:  49.17%, val_best:  65.42%, tr:  63.94%, tr_best:  72.52%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss:  1.362993/  2.494374, val:  43.33%, val_best:  65.42%, tr:  66.80%, tr_best:  72.52%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss:  1.258723/  2.026854, val:  58.33%, val_best:  65.42%, tr:  69.87%, tr_best:  72.52%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss:  1.179304/  2.245237, val:  52.92%, val_best:  65.42%, tr:  71.20%, tr_best:  72.52%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss:  1.238057/  2.040889, val:  54.58%, val_best:  65.42%, tr:  67.72%, tr_best:  72.52%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss:  1.528560/  2.541978, val:  47.50%, val_best:  65.42%, tr:  62.61%, tr_best:  72.52%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss:  1.322521/  2.531211, val:  52.08%, val_best:  65.42%, tr:  66.91%, tr_best:  72.52%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss:  1.320266/  2.689977, val:  45.83%, val_best:  65.42%, tr:  66.91%, tr_best:  72.52%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss:  1.790097/  2.721930, val:  49.58%, val_best:  65.42%, tr:  61.70%, tr_best:  72.52%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss:  1.499394/  2.369528, val:  53.33%, val_best:  65.42%, tr:  65.58%, tr_best:  72.52%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss:  1.368639/  2.316284, val:  50.83%, val_best:  65.42%, tr:  64.25%, tr_best:  72.52%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss:  1.501240/  2.318141, val:  50.42%, val_best:  65.42%, tr:  66.80%, tr_best:  72.52%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss:  1.367775/  2.373834, val:  57.50%, val_best:  65.42%, tr:  66.91%, tr_best:  72.52%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss:  1.506824/  2.304367, val:  54.58%, val_best:  65.42%, tr:  66.80%, tr_best:  72.52%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss:  1.377218/  2.477949, val:  50.00%, val_best:  65.42%, tr:  62.31%, tr_best:  72.52%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss:  1.432069/  2.655115, val:  55.42%, val_best:  65.42%, tr:  64.35%, tr_best:  72.52%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss:  1.676252/  2.387450, val:  46.67%, val_best:  65.42%, tr:  67.21%, tr_best:  72.52%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss:  1.116265/  2.291001, val:  55.83%, val_best:  65.42%, tr:  68.74%, tr_best:  72.52%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss:  1.241485/  1.960714, val:  61.25%, val_best:  65.42%, tr:  66.91%, tr_best:  72.52%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss:  1.321291/  3.260827, val:  42.50%, val_best:  65.42%, tr:  67.72%, tr_best:  72.52%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss:  1.320677/  2.164203, val:  52.92%, val_best:  65.42%, tr:  66.39%, tr_best:  72.52%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss:  1.156588/  2.120038, val:  56.25%, val_best:  65.42%, tr:  70.38%, tr_best:  72.52%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss:  1.180768/  2.044349, val:  62.08%, val_best:  65.42%, tr:  70.79%, tr_best:  72.52%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss:  1.105062/  2.274351, val:  61.25%, val_best:  65.42%, tr:  72.22%, tr_best:  72.52%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss:  1.197882/  1.839862, val:  59.17%, val_best:  65.42%, tr:  67.93%, tr_best:  72.52%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss:  1.215190/  2.435709, val:  47.50%, val_best:  65.42%, tr:  70.48%, tr_best:  72.52%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss:  1.222761/  1.985767, val:  52.08%, val_best:  65.42%, tr:  69.36%, tr_best:  72.52%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss:  1.210896/  2.172480, val:  59.17%, val_best:  65.42%, tr:  68.85%, tr_best:  72.52%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss:  1.228065/  3.269958, val:  43.33%, val_best:  65.42%, tr:  69.87%, tr_best:  72.52%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss:  1.505442/  2.937378, val:  49.17%, val_best:  65.42%, tr:  65.07%, tr_best:  72.52%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss:  1.341039/  2.310904, val:  55.42%, val_best:  65.42%, tr:  67.42%, tr_best:  72.52%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss:  1.301419/  2.709297, val:  57.92%, val_best:  65.42%, tr:  66.91%, tr_best:  72.52%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss:  1.286938/  2.458086, val:  52.08%, val_best:  65.42%, tr:  67.52%, tr_best:  72.52%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss:  1.263695/  3.255677, val:  39.58%, val_best:  65.42%, tr:  69.77%, tr_best:  72.52%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss:  1.456281/  2.983441, val:  43.75%, val_best:  65.42%, tr:  68.23%, tr_best:  72.52%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss:  1.280408/  2.296601, val:  47.08%, val_best:  65.42%, tr:  66.29%, tr_best:  72.52%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss:  1.040735/  2.026329, val:  55.42%, val_best:  65.42%, tr:  70.28%, tr_best:  72.52%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss:  1.136023/  3.267804, val:  47.08%, val_best:  65.42%, tr:  72.83%, tr_best:  72.83%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss:  1.571870/  2.378724, val:  52.92%, val_best:  65.42%, tr:  68.23%, tr_best:  72.83%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss:  1.504224/  2.582611, val:  49.17%, val_best:  65.42%, tr:  65.78%, tr_best:  72.83%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss:  1.530568/  2.353967, val:  57.08%, val_best:  65.42%, tr:  68.95%, tr_best:  72.83%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss:  1.283227/  2.546932, val:  48.33%, val_best:  65.42%, tr:  67.01%, tr_best:  72.83%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd22b7ad170a475e9ac265762b73b6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▄▁▄▄▄▇▅▇▅▄▃▇▆▃▇▅▄▆▅▅▆▇▆▅▆▇▇█▆▄▆▅▇▅▄▇▄▅▅</td></tr><tr><td>summary_val_acc</td><td>▁▃▅▃▂▅▃▆▆▅▄▅▃▇▆▇▅▆▇▆▆▅▆▇▆▆▇▇█▅▆▆▇▅▇██▄▃▅</td></tr><tr><td>tr_acc</td><td>▁▄▄▅▅▅▆▆▇▆▆▇▇▆▇▆▇▇▇▆▇▆▇▇▆▇▇▇▇▇▇▆▇▆▇▇█▇▇█</td></tr><tr><td>tr_epoch_loss</td><td>█▄▇▄▄▅▃▂▁▅▂▂▂▂▄▂▂▂▂▃▂▄▃▂▅▃▂▂▂▂▅▅▂▅▃▂▁▂▂▁</td></tr><tr><td>val_acc_best</td><td>▁▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▅▃▂▅▃▆▆▅▄▅▃▇▆▇▅▆▇▆▆▅▆▇▆▆▇▇█▅▆▆▇▅▇██▄▃▅</td></tr><tr><td>val_loss</td><td>▄▃▃▃▄▃▂▃▃▃▆▃▄▂▃▁▃▂▃▃▄▄▄▂▃▃▃▂▄█▆▃▂▅▃▂▂▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.66667</td></tr><tr><td>tr_acc</td><td>0.67007</td></tr><tr><td>tr_epoch_loss</td><td>1.28323</td></tr><tr><td>val_acc_best</td><td>0.65417</td></tr><tr><td>val_acc_now</td><td>0.48333</td></tr><tr><td>val_loss</td><td>2.54693</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">giddy-sweep-36</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wxh7gnmg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wxh7gnmg</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_061925-wxh7gnmg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cpf2t43y with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_063056-cpf2t43y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cpf2t43y' target=\"_blank\">amber-sweep-37</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cpf2t43y' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cpf2t43y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.266704/  4.001255, val:  44.58%, val_best:  44.58%, tr:  38.82%, tr_best:  38.82%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  3.017327/  5.052432, val:  48.33%, val_best:  48.33%, tr:  49.64%, tr_best:  49.64%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  4.538775/  6.458039, val:  46.25%, val_best:  48.33%, tr:  53.52%, tr_best:  53.52%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  3.608266/  4.491246, val:  46.25%, val_best:  48.33%, tr:  57.20%, tr_best:  57.20%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  3.485008/  3.423576, val:  57.50%, val_best:  57.50%, tr:  61.80%, tr_best:  61.80%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  3.742605/  6.722138, val:  46.25%, val_best:  57.50%, tr:  58.73%, tr_best:  61.80%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  3.261400/  4.857804, val:  59.58%, val_best:  59.58%, tr:  63.13%, tr_best:  63.13%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  3.876178/ 11.551793, val:  33.75%, val_best:  59.58%, tr:  61.70%, tr_best:  63.13%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  4.679958/  4.542853, val:  57.50%, val_best:  59.58%, tr:  61.18%, tr_best:  63.13%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  4.522553/ 10.057282, val:  45.83%, val_best:  59.58%, tr:  62.61%, tr_best:  63.13%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  5.269765/  5.649603, val:  58.75%, val_best:  59.58%, tr:  62.31%, tr_best:  63.13%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  4.222803/ 10.147297, val:  41.25%, val_best:  59.58%, tr:  66.50%, tr_best:  66.50%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  4.391339/  5.832880, val:  50.00%, val_best:  59.58%, tr:  69.46%, tr_best:  69.46%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  4.748509/  9.557524, val:  48.33%, val_best:  59.58%, tr:  66.50%, tr_best:  69.46%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  3.860688/  5.568928, val:  59.17%, val_best:  59.58%, tr:  72.11%, tr_best:  72.11%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  2.967992/  6.621643, val:  52.50%, val_best:  59.58%, tr:  76.40%, tr_best:  76.40%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  3.831720/  8.112243, val:  55.42%, val_best:  59.58%, tr:  73.85%, tr_best:  76.40%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  3.397060/  8.760357, val:  54.17%, val_best:  59.58%, tr:  76.61%, tr_best:  76.61%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  4.018588/  7.896219, val:  57.92%, val_best:  59.58%, tr:  75.59%, tr_best:  76.61%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  2.698782/  6.277733, val:  55.42%, val_best:  59.58%, tr:  82.64%, tr_best:  82.64%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  2.611003/  6.969414, val:  57.08%, val_best:  59.58%, tr:  82.53%, tr_best:  82.64%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  2.769577/  7.210834, val:  56.25%, val_best:  59.58%, tr:  82.12%, tr_best:  82.64%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  2.932908/  7.342147, val:  55.00%, val_best:  59.58%, tr:  82.23%, tr_best:  82.64%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  3.010837/  6.956712, val:  65.00%, val_best:  65.00%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  2.979462/  8.714815, val:  59.58%, val_best:  65.00%, tr:  84.58%, tr_best:  84.58%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  2.661945/  7.912468, val:  60.83%, val_best:  65.00%, tr:  85.09%, tr_best:  85.09%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  3.503094/ 10.324692, val:  55.83%, val_best:  65.00%, tr:  81.72%, tr_best:  85.09%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  3.162741/  9.510179, val:  59.58%, val_best:  65.00%, tr:  84.37%, tr_best:  85.09%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  3.230182/  7.917077, val:  67.50%, val_best:  67.50%, tr:  85.39%, tr_best:  85.39%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  2.293604/  8.532235, val:  61.67%, val_best:  67.50%, tr:  92.75%, tr_best:  92.75%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  2.680141/  8.302860, val:  68.33%, val_best:  68.33%, tr:  90.81%, tr_best:  92.75%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  2.226172/  9.950930, val:  58.75%, val_best:  68.33%, tr:  94.69%, tr_best:  94.69%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  3.470644/ 10.597169, val:  60.42%, val_best:  68.33%, tr:  85.60%, tr_best:  94.69%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  3.544769/  9.016484, val:  69.58%, val_best:  69.58%, tr:  85.39%, tr_best:  94.69%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  4.017544/  9.398105, val:  69.17%, val_best:  69.58%, tr:  86.01%, tr_best:  94.69%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  2.224288/  9.917053, val:  67.92%, val_best:  69.58%, tr:  94.59%, tr_best:  94.69%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  1.989296/  8.747776, val:  72.50%, val_best:  72.50%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  3.313784/ 10.609116, val:  66.25%, val_best:  72.50%, tr:  90.60%, tr_best:  96.53%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  2.254483/ 10.729080, val:  63.75%, val_best:  72.50%, tr:  94.79%, tr_best:  96.53%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  2.228902/  9.335532, val:  72.50%, val_best:  72.50%, tr:  95.71%, tr_best:  96.53%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  1.981429/ 11.479642, val:  65.00%, val_best:  72.50%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  2.577973/ 10.376537, val:  68.33%, val_best:  72.50%, tr:  93.26%, tr_best:  96.53%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  2.190448/ 10.836289, val:  64.58%, val_best:  72.50%, tr:  95.30%, tr_best:  96.53%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  2.401412/ 11.273045, val:  64.17%, val_best:  72.50%, tr:  95.10%, tr_best:  96.53%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  2.513961/ 11.678286, val:  64.17%, val_best:  72.50%, tr:  92.34%, tr_best:  96.53%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  2.153162/ 10.504354, val:  73.75%, val_best:  73.75%, tr:  96.32%, tr_best:  96.53%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  2.209268/ 11.863584, val:  66.25%, val_best:  73.75%, tr:  95.30%, tr_best:  96.53%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  2.156986/ 11.412278, val:  70.83%, val_best:  73.75%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  2.299986/ 11.186337, val:  70.00%, val_best:  73.75%, tr:  95.61%, tr_best:  97.24%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  1.855132/ 10.873833, val:  72.08%, val_best:  73.75%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  1.892005/ 12.289576, val:  64.58%, val_best:  73.75%, tr:  97.65%, tr_best:  98.16%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  1.880628/ 12.550871, val:  68.33%, val_best:  73.75%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  2.153861/ 12.105399, val:  69.17%, val_best:  73.75%, tr:  96.53%, tr_best:  98.37%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  1.990365/ 11.995812, val:  69.17%, val_best:  73.75%, tr:  97.45%, tr_best:  98.37%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  2.010362/ 12.638893, val:  67.92%, val_best:  73.75%, tr:  97.96%, tr_best:  98.37%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  2.147582/ 13.397220, val:  65.42%, val_best:  73.75%, tr:  97.55%, tr_best:  98.37%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  1.682378/ 13.052221, val:  70.42%, val_best:  73.75%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  2.014869/ 13.336113, val:  69.17%, val_best:  73.75%, tr:  97.85%, tr_best:  99.28%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  1.971669/ 12.998545, val:  72.92%, val_best:  73.75%, tr:  96.83%, tr_best:  99.28%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  1.803738/ 13.323996, val:  71.67%, val_best:  73.75%, tr:  98.67%, tr_best:  99.28%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  1.705339/ 14.270063, val:  66.25%, val_best:  73.75%, tr:  98.88%, tr_best:  99.28%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  2.124750/ 13.730031, val:  70.83%, val_best:  73.75%, tr:  97.55%, tr_best:  99.28%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  1.966944/ 13.570789, val:  68.75%, val_best:  73.75%, tr:  98.57%, tr_best:  99.28%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  1.673325/ 13.890814, val:  70.00%, val_best:  73.75%, tr:  99.18%, tr_best:  99.28%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  1.738249/ 15.155579, val:  70.42%, val_best:  73.75%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  1.987034/ 14.278414, val:  71.67%, val_best:  73.75%, tr:  97.04%, tr_best:  99.28%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  1.527714/ 14.155625, val:  70.83%, val_best:  73.75%, tr:  99.08%, tr_best:  99.28%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  1.780487/ 14.549748, val:  71.67%, val_best:  73.75%, tr:  99.08%, tr_best:  99.28%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  1.695564/ 14.494345, val:  72.50%, val_best:  73.75%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  1.481129/ 14.934054, val:  70.00%, val_best:  73.75%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  1.437507/ 14.600348, val:  72.92%, val_best:  73.75%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  1.652606/ 14.872039, val:  72.92%, val_best:  73.75%, tr:  98.98%, tr_best:  99.69%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  1.703502/ 15.799743, val:  68.33%, val_best:  73.75%, tr:  98.57%, tr_best:  99.69%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  1.686193/ 15.486025, val:  71.67%, val_best:  73.75%, tr:  98.47%, tr_best:  99.69%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  1.635712/ 15.148136, val:  72.08%, val_best:  73.75%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  1.496146/ 15.317532, val:  75.42%, val_best:  75.42%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  1.505322/ 15.910439, val:  70.42%, val_best:  75.42%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  1.623882/ 15.808568, val:  74.17%, val_best:  75.42%, tr:  99.08%, tr_best:  99.69%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  2.083889/ 17.953190, val:  71.67%, val_best:  75.42%, tr:  97.45%, tr_best:  99.69%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  1.600910/ 16.630701, val:  70.83%, val_best:  75.42%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  1.440893/ 16.834806, val:  69.58%, val_best:  75.42%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  1.642953/ 16.814892, val:  71.25%, val_best:  75.42%, tr:  98.77%, tr_best:  99.69%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  1.694113/ 17.637657, val:  69.17%, val_best:  75.42%, tr:  98.57%, tr_best:  99.69%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  1.791222/ 18.031000, val:  70.83%, val_best:  75.42%, tr:  98.37%, tr_best:  99.69%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  1.515104/ 17.380629, val:  67.08%, val_best:  75.42%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  1.562432/ 17.117083, val:  71.67%, val_best:  75.42%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  1.637555/ 17.986252, val:  67.92%, val_best:  75.42%, tr:  98.57%, tr_best:  99.69%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  1.426297/ 16.914232, val:  72.08%, val_best:  75.42%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  1.334471/ 18.842405, val:  65.83%, val_best:  75.42%, tr:  99.18%, tr_best:  99.69%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  1.338108/ 17.317417, val:  72.50%, val_best:  75.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  1.257049/ 18.362997, val:  70.42%, val_best:  75.42%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  1.319416/ 18.180986, val:  70.42%, val_best:  75.42%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  1.339180/ 18.183817, val:  71.67%, val_best:  75.42%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  1.213713/ 18.263901, val:  70.00%, val_best:  75.42%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  1.186795/ 18.438301, val:  73.33%, val_best:  75.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  1.657526/ 18.643818, val:  70.00%, val_best:  75.42%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  1.320250/ 19.798775, val:  69.58%, val_best:  75.42%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  1.306133/ 19.091415, val:  70.42%, val_best:  75.42%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  1.204281/ 18.946693, val:  69.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  1.390991/ 19.473606, val:  71.25%, val_best:  75.42%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  1.185562/ 18.895166, val:  71.67%, val_best:  75.42%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  1.155210/ 19.097841, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  1.093521/ 19.305996, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  1.177671/ 19.115532, val:  72.92%, val_best:  75.42%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  1.291091/ 18.960793, val:  75.00%, val_best:  75.42%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  1.376190/ 19.399258, val:  73.33%, val_best:  75.42%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  1.268663/ 20.147434, val:  70.83%, val_best:  75.42%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  1.273790/ 19.990870, val:  70.42%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  1.317934/ 19.848600, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  1.279805/ 20.847198, val:  69.58%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  1.076879/ 20.587986, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  1.139063/ 20.368248, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  1.111178/ 20.808077, val:  71.25%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  1.185459/ 20.961565, val:  70.83%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  1.222052/ 21.374262, val:  70.83%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  1.121935/ 21.071665, val:  69.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  1.060596/ 21.663769, val:  69.17%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  1.631384/ 21.452362, val:  71.67%, val_best:  75.42%, tr:  98.88%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  1.240531/ 22.684189, val:  70.83%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  1.195870/ 21.845655, val:  72.08%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  1.255057/ 22.312025, val:  71.67%, val_best:  75.42%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  1.083400/ 21.657425, val:  75.00%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  1.058535/ 22.330826, val:  70.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  1.101350/ 22.616047, val:  70.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  1.147113/ 23.048590, val:  70.42%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  1.191693/ 23.013903, val:  70.42%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  1.025221/ 24.455610, val:  67.92%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  1.190940/ 23.351091, val:  73.33%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  1.261065/ 23.274614, val:  73.33%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  1.083065/ 24.162928, val:  70.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  1.269307/ 23.905943, val:  71.25%, val_best:  75.42%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.969324/ 24.228413, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  1.028290/ 25.188034, val:  71.25%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.928111/ 24.285192, val:  73.75%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  1.053096/ 23.674706, val:  75.00%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  1.079350/ 24.078329, val:  72.50%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  1.049072/ 24.582880, val:  71.25%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  1.166543/ 24.858002, val:  70.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.904822/ 24.881294, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.940492/ 25.348732, val:  72.50%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  1.028233/ 26.680021, val:  69.17%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  1.028216/ 25.559093, val:  70.83%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.995448/ 25.563305, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.916386/ 25.558380, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.976045/ 26.229010, val:  70.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.970587/ 25.344296, val:  74.17%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  1.080608/ 26.850006, val:  67.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  1.048360/ 25.837797, val:  74.58%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.989646/ 26.391890, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.869332/ 26.681038, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.834782/ 27.739132, val:  71.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  1.086044/ 26.870804, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.850618/ 26.446859, val:  75.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.898085/ 27.048050, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.903251/ 27.387823, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.997037/ 27.301056, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  1.080654/ 27.349342, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.916144/ 27.288532, val:  71.25%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.863144/ 27.282232, val:  71.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  1.020790/ 27.910015, val:  70.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  1.019704/ 27.666206, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  1.118157/ 28.318483, val:  71.25%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.899550/ 27.539076, val:  71.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.969412/ 27.916801, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  1.014805/ 29.130178, val:  68.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.972746/ 28.672123, val:  70.42%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  1.037642/ 29.766209, val:  68.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.937176/ 28.596191, val:  71.25%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.872474/ 27.923582, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.807103/ 28.850203, val:  71.25%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.944183/ 29.267656, val:  71.25%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.947863/ 30.173693, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.887328/ 29.913406, val:  70.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.887082/ 29.591528, val:  70.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  1.033202/ 29.048845, val:  72.92%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.701984/ 29.507294, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.774673/ 28.918825, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.831622/ 29.530254, val:  70.83%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.798369/ 30.170532, val:  70.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.883350/ 29.073505, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.775136/ 29.857529, val:  71.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.731566/ 30.521156, val:  71.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.877983/ 30.121031, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.876675/ 31.093576, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.928115/ 30.180681, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.886522/ 30.312956, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.955501/ 30.542246, val:  73.75%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.971730/ 31.120243, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  1.093209/ 31.183458, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.874083/ 31.454712, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.773565/ 31.298254, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.770373/ 32.089725, val:  69.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.690645/ 31.886522, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.849767/ 31.828176, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.812440/ 31.822081, val:  74.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.704297/ 33.267986, val:  71.25%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.782350/ 31.858809, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.638351/ 31.765493, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.803312/ 32.287273, val:  72.08%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.718539/ 32.145958, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b1be9c861e4748a227ceb81e55030c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▃▃▅▅▇█▇▇█▆████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▂▁▁▃▃▅▅▇▆▅▅▆▆▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▆▇▇▇█▇▇</td></tr><tr><td>tr_acc</td><td>▁▂▃▅▆▆▇██▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▅▇█▅▅▅▄▃▃▄▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▄▅▆▇▇▇██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▂▁▁▃▃▅▅▇▆▅▅▆▆▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▆▇▇▇█▇▇</td></tr><tr><td>val_loss</td><td>▁▁▂▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.71854</td></tr><tr><td>val_acc_best</td><td>0.75417</td></tr><tr><td>val_acc_now</td><td>0.7375</td></tr><tr><td>val_loss</td><td>32.14596</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-sweep-37</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cpf2t43y' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cpf2t43y</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_063056-cpf2t43y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q4auhvvs with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_064359-q4auhvvs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/q4auhvvs' target=\"_blank\">twilight-sweep-38</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/q4auhvvs' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/q4auhvvs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 14.519691/ 21.706875, val:  42.08%, val_best:  42.08%, tr:  30.23%, tr_best:  30.23%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 18.938950/ 11.223166, val:  48.33%, val_best:  48.33%, tr:  39.94%, tr_best:  39.94%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 27.047190/ 19.679293, val:  42.92%, val_best:  48.33%, tr:  42.59%, tr_best:  42.59%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 12.168914/ 19.277773, val:  28.75%, val_best:  48.33%, tr:  55.36%, tr_best:  55.36%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 18.876715/ 19.653952, val:  46.67%, val_best:  48.33%, tr:  52.09%, tr_best:  55.36%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 12.261816/ 26.338327, val:  44.17%, val_best:  48.33%, tr:  58.73%, tr_best:  58.73%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 12.905189/ 14.514026, val:  60.83%, val_best:  60.83%, tr:  61.90%, tr_best:  61.90%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 10.380850/ 21.631704, val:  39.17%, val_best:  60.83%, tr:  63.64%, tr_best:  63.64%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 10.495001/ 11.166053, val:  57.92%, val_best:  60.83%, tr:  66.80%, tr_best:  66.80%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 10.593919/ 17.602867, val:  49.17%, val_best:  60.83%, tr:  68.03%, tr_best:  68.03%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 10.589455/ 14.311939, val:  52.92%, val_best:  60.83%, tr:  67.82%, tr_best:  68.03%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  8.578986/ 15.604609, val:  56.25%, val_best:  60.83%, tr:  72.22%, tr_best:  72.22%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  7.811725/ 15.537984, val:  58.33%, val_best:  60.83%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  9.206714/ 19.036533, val:  66.25%, val_best:  66.25%, tr:  74.97%, tr_best:  76.30%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  7.835674/ 18.354147, val:  59.58%, val_best:  66.25%, tr:  81.41%, tr_best:  81.41%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  3.908631/ 14.435089, val:  61.67%, val_best:  66.25%, tr:  89.38%, tr_best:  89.38%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  4.584395/ 12.939661, val:  71.25%, val_best:  71.25%, tr:  88.05%, tr_best:  89.38%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  3.880737/ 14.977591, val:  63.75%, val_best:  71.25%, tr:  90.60%, tr_best:  90.60%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  2.434661/ 12.637718, val:  69.17%, val_best:  71.25%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  1.812429/ 10.984590, val:  75.83%, val_best:  75.83%, tr:  96.63%, tr_best:  96.63%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  1.740991/ 14.150731, val:  67.08%, val_best:  75.83%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  2.105483/ 16.914097, val:  61.25%, val_best:  75.83%, tr:  95.71%, tr_best:  97.34%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  2.106182/ 12.123307, val:  75.42%, val_best:  75.83%, tr:  96.22%, tr_best:  97.34%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  2.075255/ 13.781091, val:  69.17%, val_best:  75.83%, tr:  95.20%, tr_best:  97.34%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  1.327910/ 11.837994, val:  77.08%, val_best:  77.08%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  1.435899/ 13.393121, val:  71.67%, val_best:  77.08%, tr:  96.83%, tr_best:  97.85%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.895066/ 13.234814, val:  76.67%, val_best:  77.08%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.608194/ 12.570403, val:  75.00%, val_best:  77.08%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.536655/ 11.624758, val:  77.92%, val_best:  77.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.550718/ 12.625537, val:  72.92%, val_best:  77.92%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.340449/ 12.139471, val:  77.50%, val_best:  77.92%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.367885/ 12.548539, val:  75.42%, val_best:  77.92%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.730534/ 13.711029, val:  72.50%, val_best:  77.92%, tr:  98.57%, tr_best:  99.90%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.394066/ 13.878367, val:  75.42%, val_best:  77.92%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.387395/ 13.720234, val:  74.58%, val_best:  77.92%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.236130/ 14.293335, val:  71.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.116367/ 13.565512, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.138516/ 12.847153, val:  76.67%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.133168/ 12.569454, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.101838/ 13.262397, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.135989/ 13.012140, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.075670/ 12.849674, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.084645/ 13.008260, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.094602/ 13.049325, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.106941/ 12.495551, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.091531/ 13.343064, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.049061/ 12.937813, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.051943/ 12.540119, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.062825/ 13.247078, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.066167/ 12.437374, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.046948/ 12.759647, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.075245/ 13.840142, val:  73.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.112018/ 13.328140, val:  76.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.078627/ 13.118336, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.042362/ 12.932076, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.072341/ 13.219603, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.060612/ 12.763906, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.029104/ 12.813143, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.023725/ 12.922101, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.013469/ 13.116891, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.022302/ 12.845256, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.020930/ 13.249630, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.018398/ 12.686313, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.010710/ 12.819876, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.014918/ 13.017389, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.020480/ 12.765643, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.016870/ 12.872094, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.043831/ 13.358604, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.014160/ 12.734305, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.009503/ 12.667599, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.012835/ 12.755584, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.027844/ 12.890051, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.133774/ 13.680000, val:  73.75%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.024153/ 13.254209, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.027560/ 13.248115, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.014297/ 12.914533, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.015005/ 12.987332, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.020900/ 13.865614, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.017442/ 13.053196, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.010302/ 13.474929, val:  75.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.010273/ 13.335838, val:  75.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.004817/ 13.086504, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.005256/ 13.533762, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.006881/ 13.123418, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.004747/ 13.213984, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.001704/ 13.310038, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.002465/ 13.369120, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.002463/ 13.757352, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.004235/ 13.114999, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.002507/ 13.369159, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.007629/ 13.405391, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.003098/ 13.460201, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.002308/ 13.594323, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.002062/ 13.416998, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000971/ 13.487423, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.002265/ 13.719243, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.001370/ 13.560989, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.004935/ 13.819008, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.001485/ 13.677938, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000279/ 13.566245, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss:  0.001504/ 13.568480, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss:  0.001613/ 13.420724, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss:  0.002963/ 13.454277, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss:  0.001228/ 13.566374, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss:  0.000167/ 13.624503, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss:  0.000696/ 13.550910, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss:  0.001849/ 13.751430, val:  77.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss:  0.000636/ 13.606683, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss:  0.000021/ 13.638514, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss:  0.000015/ 13.640181, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss:  0.000010/ 13.609086, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss:  0.000009/ 13.615191, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss:  0.000008/ 13.629530, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss:  0.000007/ 13.642154, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss:  0.000007/ 13.640347, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss:  0.000005/ 13.636091, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss:  0.000005/ 13.654444, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss:  0.000004/ 13.651357, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss:  0.000004/ 13.650317, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss:  0.000004/ 13.646908, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.638268, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.639431, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss:  0.000004/ 13.640390, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.645255, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.662827, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.649469, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.650546, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.644538, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.643924, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.646464, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.651382, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.633496, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.624697, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.625321, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.628332, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.626176, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.629180, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.633650, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.635760, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.635276, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.612663, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.619527, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.619893, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.616066, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.607560, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.610124, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.605247, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss:  0.000055/ 13.595411, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss:  0.001387/ 14.015852, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss:  0.001034/ 13.902252, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss:  0.000341/ 13.674539, val:  76.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss:  0.000171/ 13.630555, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss:  0.001399/ 13.639147, val:  77.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss:  0.000770/ 13.805317, val:  76.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss:  0.000184/ 13.694636, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss:  0.000615/ 13.636755, val:  76.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss:  0.000057/ 13.707218, val:  77.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss:  0.001065/ 13.704341, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss:  0.001533/ 13.680029, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss:  0.000479/ 13.569159, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss:  0.000036/ 13.527816, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss:  0.000006/ 13.520227, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss:  0.000006/ 13.517983, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss:  0.000005/ 13.516345, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss:  0.000005/ 13.520423, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss:  0.000005/ 13.516696, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss:  0.000004/ 13.519016, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss:  0.000004/ 13.514983, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.518196, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.508305, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.509085, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.506365, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.503225, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.507487, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.499921, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.507241, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.508795, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.509450, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.517399, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.506881, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.486570, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.484404, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.481343, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.490484, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.486221, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.486736, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.489534, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss:  0.000003/ 13.475868, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.475406, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.475716, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.471088, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.471396, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.480466, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.481992, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.482177, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.478095, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.484234, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.481632, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.484769, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss:  0.000002/ 13.484846, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f826d1236772457b85922c339e7a27dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄█▇███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▂▁▂▄▇▆▆▇▇▇█████▇▇████▇█▇██████▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>tr_acc</td><td>▁▃▄▇████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▄▅▇▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>val_acc_now</td><td>▂▁▂▄▇▆▆▇▇▇█████▇▇████▇█▇██████▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss</td><td>▁█▄▃▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0</td></tr><tr><td>val_acc_best</td><td>0.82917</td></tr><tr><td>val_acc_now</td><td>0.7875</td></tr><tr><td>val_loss</td><td>13.48485</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twilight-sweep-38</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/q4auhvvs' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/q4auhvvs</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_064359-q4auhvvs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 39b2fqkl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_065708-39b2fqkl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/39b2fqkl' target=\"_blank\">light-sweep-39</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/39b2fqkl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/39b2fqkl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 16.576878/ 34.161186, val:  41.25%, val_best:  41.25%, tr:  28.91%, tr_best:  28.91%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 27.651197/ 29.092161, val:  30.42%, val_best:  41.25%, tr:  44.13%, tr_best:  44.13%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 20.754782/ 17.754885, val:  46.67%, val_best:  46.67%, tr:  51.17%, tr_best:  51.17%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 18.393574/ 26.484392, val:  48.33%, val_best:  48.33%, tr:  55.26%, tr_best:  55.26%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 20.464710/ 25.939140, val:  54.58%, val_best:  54.58%, tr:  54.65%, tr_best:  55.26%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 19.478561/ 25.737934, val:  51.67%, val_best:  54.58%, tr:  59.35%, tr_best:  59.35%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 18.139559/ 29.971922, val:  52.92%, val_best:  54.58%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 15.424008/ 27.970053, val:  47.92%, val_best:  54.58%, tr:  66.19%, tr_best:  66.19%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 11.328127/ 17.881283, val:  55.83%, val_best:  55.83%, tr:  73.03%, tr_best:  73.03%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 15.082337/ 24.833729, val:  58.75%, val_best:  58.75%, tr:  71.91%, tr_best:  73.03%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 10.129914/ 29.050234, val:  49.58%, val_best:  58.75%, tr:  78.45%, tr_best:  78.45%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  8.061479/ 25.187996, val:  45.83%, val_best:  58.75%, tr:  81.92%, tr_best:  81.92%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  8.198809/ 17.721216, val:  63.75%, val_best:  63.75%, tr:  80.18%, tr_best:  81.92%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 10.950075/ 24.178734, val:  62.50%, val_best:  63.75%, tr:  81.31%, tr_best:  81.92%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  6.188339/ 20.459133, val:  72.50%, val_best:  72.50%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  6.247995/ 21.598866, val:  68.75%, val_best:  72.50%, tr:  89.48%, tr_best:  89.48%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  5.295350/ 21.596239, val:  70.00%, val_best:  72.50%, tr:  90.40%, tr_best:  90.40%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  3.445939/ 17.272787, val:  73.33%, val_best:  73.33%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  2.669326/ 18.977182, val:  72.08%, val_best:  73.33%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  3.609906/ 18.192030, val:  74.17%, val_best:  74.17%, tr:  94.79%, tr_best:  96.32%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  3.322739/ 21.725626, val:  67.08%, val_best:  74.17%, tr:  94.69%, tr_best:  96.32%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  3.576657/ 24.822441, val:  64.58%, val_best:  74.17%, tr:  95.40%, tr_best:  96.32%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  2.839248/ 19.380390, val:  75.42%, val_best:  75.42%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  2.018069/ 20.867294, val:  74.58%, val_best:  75.42%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.964979/ 19.387100, val:  75.00%, val_best:  75.42%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  1.039366/ 20.350370, val:  73.75%, val_best:  75.42%, tr:  99.08%, tr_best:  99.59%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.988657/ 18.926584, val:  77.92%, val_best:  77.92%, tr:  99.18%, tr_best:  99.59%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.598872/ 18.526791, val:  75.42%, val_best:  77.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.714115/ 19.371311, val:  74.17%, val_best:  77.92%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.639488/ 19.349054, val:  74.17%, val_best:  77.92%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.436056/ 17.887209, val:  77.08%, val_best:  77.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.298920/ 18.623310, val:  78.75%, val_best:  78.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.327309/ 20.226662, val:  74.17%, val_best:  78.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.720854/ 19.081722, val:  80.00%, val_best:  80.00%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.516823/ 20.363768, val:  73.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.446555/ 19.516523, val:  77.92%, val_best:  80.00%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.360467/ 19.405590, val:  80.00%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.239228/ 19.322758, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.167260/ 19.537312, val:  76.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.216527/ 21.517986, val:  76.25%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.335733/ 21.001211, val:  76.25%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.187465/ 19.587536, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.224269/ 20.103975, val:  77.08%, val_best:  80.00%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.381800/ 20.904890, val:  77.08%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.273924/ 19.817471, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.143101/ 20.018829, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.100887/ 20.103033, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.082654/ 20.614737, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.069578/ 20.147667, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.109452/ 21.075243, val:  75.42%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.076611/ 20.250391, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.046826/ 20.115303, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.049898/ 19.894772, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.038607/ 20.870815, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.038307/ 20.597492, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.055227/ 20.361118, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.030822/ 20.472759, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.025229/ 20.049999, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.024019/ 20.607964, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.025757/ 20.327665, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.016501/ 20.735012, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.008697/ 20.334505, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.027906/ 20.619020, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.037282/ 20.235533, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.022230/ 20.162012, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.026012/ 20.361795, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.033569/ 21.199581, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.020339/ 20.972807, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.022276/ 20.800241, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.039617/ 20.913860, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.029627/ 21.483250, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.048715/ 22.086119, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.710334/ 21.741514, val:  79.17%, val_best:  81.25%, tr:  99.28%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.089041/ 22.079451, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.055644/ 22.084101, val:  75.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.041118/ 21.975710, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.023716/ 22.138521, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.031789/ 21.738359, val:  76.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.008632/ 21.268753, val:  76.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.006369/ 21.088148, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.008060/ 21.440580, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.016138/ 21.576693, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.012385/ 21.864096, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.007468/ 21.490101, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.010495/ 21.986267, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.009471/ 21.597519, val:  76.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.013545/ 21.234669, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.006226/ 21.501520, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.003552/ 21.240545, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.004476/ 21.378918, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.009300/ 21.188580, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.018499/ 21.321077, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.016157/ 21.405718, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.013156/ 21.176342, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.017248/ 21.674814, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.014500/ 21.330563, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.008453/ 21.562332, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.006316/ 21.685860, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.007546/ 21.643003, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.002236/ 21.518896, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss:  0.007079/ 21.396023, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss:  0.001960/ 21.459816, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss:  0.005798/ 21.340099, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss:  0.004679/ 21.429974, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss:  0.007867/ 21.404661, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss:  0.002314/ 21.032148, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss:  0.011454/ 21.923002, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss:  0.009416/ 21.306629, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss:  0.005630/ 21.756292, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss:  0.000797/ 21.626999, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss:  0.003237/ 21.941196, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss:  0.001718/ 21.636671, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss:  0.003112/ 21.525093, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss:  0.041073/ 22.017042, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss:  0.012568/ 22.024624, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss:  0.014711/ 21.528952, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss:  0.014367/ 21.554689, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss:  0.015535/ 21.616537, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss:  0.010672/ 21.666847, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss:  0.006167/ 21.890051, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss:  0.006552/ 21.888458, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss:  0.011969/ 22.265509, val:  76.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss:  0.010145/ 21.479965, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss:  0.016710/ 22.331820, val:  77.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss:  0.012065/ 21.861607, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss:  0.006996/ 21.730997, val:  77.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss:  0.003613/ 21.640148, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss:  0.007040/ 21.662724, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss:  0.015954/ 22.602600, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss:  0.006609/ 22.025061, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss:  0.018435/ 21.919674, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss:  0.007324/ 22.082455, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss:  0.012740/ 21.169378, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss:  0.011010/ 22.011282, val:  77.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss:  0.004622/ 21.607735, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss:  0.006922/ 21.618374, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss:  0.004445/ 21.785509, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss:  0.008544/ 21.516134, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss:  0.004338/ 21.855577, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss:  0.009066/ 21.486845, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss:  0.007800/ 21.770908, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss:  0.002292/ 22.077038, val:  76.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss:  0.004032/ 22.129646, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss:  0.003450/ 21.977003, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss:  0.001765/ 21.837101, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss:  0.002782/ 21.891342, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss:  0.003298/ 21.814386, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss:  0.001926/ 21.431599, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss:  0.003332/ 21.580402, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss:  0.000080/ 21.589813, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss:  0.000406/ 21.743050, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss:  0.000003/ 21.748472, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss:  0.000002/ 21.747597, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss:  0.000002/ 21.752579, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.753016, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.745354, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.745975, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.741379, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.743610, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss:  0.000003/ 21.767212, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss:  0.000002/ 21.762352, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss:  0.000002/ 21.777010, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss:  0.000002/ 21.780907, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss:  0.000002/ 21.785158, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss:  0.000002/ 21.787170, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.776779, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.785065, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.792768, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.789890, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss:  0.000004/ 21.771708, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.781254, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.787701, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.786665, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.787222, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss:  0.000314/ 21.781967, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss:  0.007409/ 21.829443, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss:  0.000012/ 21.746878, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss:  0.000004/ 21.741102, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss:  0.000003/ 21.735022, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss:  0.000002/ 21.740080, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss:  0.000003/ 21.738895, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss:  0.000003/ 21.731474, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss:  0.000002/ 21.729698, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss:  0.000003/ 21.727871, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss:  0.000002/ 21.736849, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss:  0.000002/ 21.737814, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss:  0.000006/ 21.737465, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss:  0.000002/ 21.713678, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.721039, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.717253, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.716415, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.722109, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.713217, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.709362, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.714565, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.714199, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.714571, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.709518, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.708559, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss:  0.000001/ 21.711180, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44332b31464d4900a74da27ea7219fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▆▆▆███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▅▆▇▇▇█▇████████▇██████████▇███████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▇▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▆▇▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▅▆▇▇▇█▇████████▇██████████▇███████████</td></tr><tr><td>val_loss</td><td>█▆▅▃▁▂▂▂▃▂▂▃▃▂▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0</td></tr><tr><td>val_acc_best</td><td>0.8125</td></tr><tr><td>val_acc_now</td><td>0.8</td></tr><tr><td>val_loss</td><td>21.71118</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">light-sweep-39</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/39b2fqkl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/39b2fqkl</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_065708-39b2fqkl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i7itwrxr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_071016-i7itwrxr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i7itwrxr' target=\"_blank\">sage-sweep-40</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i7itwrxr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i7itwrxr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:100.790016/ 58.385887, val:  28.33%, val_best:  28.33%, tr:  16.96%, tr_best:  16.96%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 42.847469/ 28.157467, val:  35.83%, val_best:  35.83%, tr:  30.75%, tr_best:  30.75%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 26.493330/ 31.250675, val:  43.33%, val_best:  43.33%, tr:  43.51%, tr_best:  43.51%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 24.602795/ 30.589729, val:  47.08%, val_best:  47.08%, tr:  45.76%, tr_best:  45.76%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 26.138914/ 44.160389, val:  40.00%, val_best:  47.08%, tr:  47.40%, tr_best:  47.40%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 27.312155/ 30.455851, val:  44.17%, val_best:  47.08%, tr:  51.28%, tr_best:  51.28%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 21.438593/ 42.181969, val:  36.25%, val_best:  47.08%, tr:  57.92%, tr_best:  57.92%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 23.221054/ 29.913691, val:  46.67%, val_best:  47.08%, tr:  55.57%, tr_best:  57.92%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 16.661829/ 30.341541, val:  52.08%, val_best:  52.08%, tr:  65.99%, tr_best:  65.99%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 18.642134/ 28.483953, val:  55.42%, val_best:  55.42%, tr:  66.19%, tr_best:  66.19%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 15.268597/ 28.812952, val:  62.50%, val_best:  62.50%, tr:  70.38%, tr_best:  70.38%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 16.207952/ 35.642887, val:  52.08%, val_best:  62.50%, tr:  75.18%, tr_best:  75.18%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 16.439657/ 31.862377, val:  64.58%, val_best:  64.58%, tr:  74.57%, tr_best:  75.18%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 10.768883/ 33.532200, val:  49.58%, val_best:  64.58%, tr:  84.58%, tr_best:  84.58%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 10.093359/ 31.052992, val:  68.33%, val_best:  68.33%, tr:  86.31%, tr_best:  86.31%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  6.250628/ 35.225731, val:  58.33%, val_best:  68.33%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  6.855795/ 37.853123, val:  62.50%, val_best:  68.33%, tr:  91.42%, tr_best:  94.18%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  5.131559/ 34.449654, val:  64.58%, val_best:  68.33%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  4.629938/ 36.480869, val:  65.00%, val_best:  68.33%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  3.614251/ 37.720825, val:  62.08%, val_best:  68.33%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  3.385297/ 34.153477, val:  69.17%, val_best:  69.17%, tr:  97.34%, tr_best:  97.85%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  2.941805/ 47.153446, val:  60.83%, val_best:  69.17%, tr:  97.65%, tr_best:  97.85%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  3.497891/ 37.852825, val:  66.25%, val_best:  69.17%, tr:  97.14%, tr_best:  97.85%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  2.345721/ 35.724648, val:  70.00%, val_best:  70.00%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  1.640444/ 37.387653, val:  69.58%, val_best:  70.00%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  2.422894/ 40.094460, val:  67.50%, val_best:  70.00%, tr:  98.57%, tr_best:  99.49%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  1.742192/ 39.954510, val:  71.25%, val_best:  71.25%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  1.691151/ 40.681103, val:  70.42%, val_best:  71.25%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  1.694647/ 39.801819, val:  67.50%, val_best:  71.25%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  1.136260/ 40.609825, val:  65.42%, val_best:  71.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.952096/ 41.837009, val:  67.92%, val_best:  71.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  1.084599/ 41.457390, val:  67.08%, val_best:  71.25%, tr:  99.28%, tr_best:  99.80%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.815296/ 41.924946, val:  68.75%, val_best:  71.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.662605/ 43.457970, val:  66.25%, val_best:  71.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.605472/ 41.465565, val:  70.42%, val_best:  71.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.498609/ 43.360214, val:  68.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.431578/ 42.816040, val:  70.83%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.405217/ 43.403587, val:  67.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.348759/ 44.752117, val:  67.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.261380/ 42.878460, val:  68.75%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.255518/ 42.917721, val:  71.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.248059/ 44.386337, val:  68.75%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.336373/ 45.300957, val:  67.08%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.243760/ 42.945175, val:  70.42%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.170575/ 43.386227, val:  70.42%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.144435/ 43.381741, val:  69.58%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.144234/ 43.614113, val:  69.17%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.155079/ 43.836040, val:  70.00%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.091826/ 45.268936, val:  68.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.145766/ 45.519089, val:  67.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.121276/ 44.271660, val:  67.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.151360/ 44.006649, val:  69.17%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.069342/ 44.979866, val:  66.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.093022/ 44.361629, val:  67.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.073128/ 44.523716, val:  71.67%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.084702/ 44.995842, val:  68.75%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.050980/ 45.467922, val:  70.42%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.108647/ 43.134300, val:  72.50%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.058492/ 43.885239, val:  72.50%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.106219/ 45.404919, val:  71.25%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.050226/ 45.222183, val:  70.00%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.056778/ 44.476406, val:  70.83%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.032435/ 44.192760, val:  69.17%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.036165/ 44.232231, val:  67.50%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.030786/ 43.926559, val:  69.17%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.010997/ 44.096088, val:  70.83%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.027311/ 45.045635, val:  69.17%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.023644/ 46.434536, val:  65.42%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.010305/ 45.422379, val:  66.25%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.007561/ 45.607761, val:  67.50%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.026967/ 44.865658, val:  67.08%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.059279/ 45.330990, val:  67.92%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.043502/ 45.427387, val:  69.17%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.048787/ 46.040691, val:  65.42%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.029815/ 44.652489, val:  70.00%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.019135/ 46.336014, val:  66.25%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.003207/ 46.029701, val:  65.00%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.006924/ 45.886429, val:  68.33%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.015980/ 45.771503, val:  68.75%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.025541/ 46.263927, val:  68.33%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.030820/ 44.315605, val:  69.58%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.005888/ 44.414967, val:  69.58%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.032181/ 44.055153, val:  72.92%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.017110/ 44.114731, val:  70.83%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.012855/ 44.435631, val:  67.92%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.002453/ 44.899845, val:  68.75%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000859/ 44.533566, val:  67.50%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.004454/ 44.534603, val:  67.92%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.012321/ 44.957352, val:  69.58%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.015635/ 44.689880, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.003381/ 44.638416, val:  69.58%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.002464/ 44.844223, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.001493/ 44.943348, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.012986/ 45.193188, val:  68.33%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.005704/ 45.111595, val:  71.25%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.019071/ 45.428539, val:  71.25%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.012772/ 45.229637, val:  70.83%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.005164/ 45.001629, val:  70.83%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.018345/ 44.880657, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.014796/ 44.496460, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss:  0.011326/ 44.621292, val:  71.67%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss:  0.025492/ 45.045540, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss:  0.051232/ 45.937222, val:  69.58%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss:  0.024093/ 45.253017, val:  71.25%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss:  0.001803/ 45.734306, val:  72.92%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss:  0.003145/ 45.762222, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss:  0.008708/ 45.467762, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss:  0.009326/ 45.027515, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss:  0.000003/ 45.022179, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss:  0.000003/ 45.024258, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss:  0.000003/ 45.026062, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.027660, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.029472, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.031044, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.033707, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.034958, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.036079, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.044025, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.045204, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.046230, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.072868, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.073738, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.074711, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.075626, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.076584, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.077389, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.077671, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.078968, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.079357, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.080517, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.081161, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.081825, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.082485, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss:  0.000002/ 45.082882, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.083717, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.084255, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.084778, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.085320, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.059902, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.054249, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.054810, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.063274, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.063637, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.064060, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.064457, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.064949, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.070930, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.071213, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.058170, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.058414, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.058891, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.059174, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.059410, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.059883, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.060043, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.060448, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.060703, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.060917, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.061211, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.061607, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.061813, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.031143, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.031361, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.031399, val:  70.42%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.044521, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.044964, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.047752, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.047897, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.048073, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.048462, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.048603, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.048584, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.048981, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.049175, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.049236, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.049580, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.049820, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.045780, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.046043, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss:  0.000003/ 45.056087, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.057190, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.057091, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.057255, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.057278, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.057465, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.051380, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.051678, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss:  0.000001/ 44.993587, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss:  0.000001/ 44.993759, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss:  0.000001/ 44.993828, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss:  0.000001/ 44.994102, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss:  0.000001/ 44.994267, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss:  0.000001/ 44.994495, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss:  0.000001/ 44.972736, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss:  0.000001/ 44.972889, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss:  0.000001/ 44.972950, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss:  0.000001/ 44.973114, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss:  0.000001/ 44.973351, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.006382, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss:  0.000001/ 45.006622, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e569a43f40b498886be1407d31201e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▅▅▆▇▇███▇███▇▇▇▇██████████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▅▇████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▅▇▇▇██████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▅▅▆▇▇███▇███▇▇▇▇██████████████████████</td></tr><tr><td>val_loss</td><td>▁▂▁▄▅▆▆▇▇▇▇▇█▇███▇▇█▇█▇▇██████████████▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0</td></tr><tr><td>val_acc_best</td><td>0.72917</td></tr><tr><td>val_acc_now</td><td>0.7</td></tr><tr><td>val_loss</td><td>45.00662</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sage-sweep-40</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i7itwrxr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/i7itwrxr</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_071016-i7itwrxr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jekqs8ec with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4362d46cee504b979ead2e9a63d2459b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113420765226086, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_072329-jekqs8ec</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jekqs8ec' target=\"_blank\">driven-sweep-41</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jekqs8ec' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jekqs8ec</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.303334/  2.303068, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.303294/  2.303046, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.303226/  2.302972, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.11%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.303112/  2.302921, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  2.303272/  2.302818, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  10.21%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  2.302740/  2.302740, val:  10.42%, val_best:  10.42%, tr:  10.01%, tr_best:  10.21%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  2.302306/  2.302378, val:  10.42%, val_best:  10.42%, tr:  11.03%, tr_best:  11.03%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  2.301851/  2.301831, val:  10.00%, val_best:  10.42%, tr:  11.75%, tr_best:  11.75%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  2.300469/  2.301176, val:  10.83%, val_best:  10.83%, tr:  12.56%, tr_best:  12.56%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  2.298324/  2.299015, val:  11.67%, val_best:  11.67%, tr:  13.99%, tr_best:  13.99%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  2.295412/  2.296060, val:  13.33%, val_best:  13.33%, tr:  13.99%, tr_best:  13.99%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  2.289759/  2.291385, val:  12.92%, val_best:  13.33%, tr:  15.93%, tr_best:  15.93%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  2.280814/  2.284092, val:  14.17%, val_best:  14.17%, tr:  17.47%, tr_best:  17.47%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  2.268046/  2.271704, val:  16.25%, val_best:  16.25%, tr:  17.47%, tr_best:  17.47%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  2.249854/  2.256901, val:  19.17%, val_best:  19.17%, tr:  18.28%, tr_best:  18.28%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  2.228404/  2.237332, val:  22.08%, val_best:  22.08%, tr:  19.82%, tr_best:  19.82%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  2.202608/  2.215072, val:  23.33%, val_best:  23.33%, tr:  20.74%, tr_best:  20.74%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  2.174319/  2.191453, val:  26.25%, val_best:  26.25%, tr:  24.31%, tr_best:  24.31%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  2.138661/  2.160641, val:  30.83%, val_best:  30.83%, tr:  28.70%, tr_best:  28.70%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  2.098031/  2.123400, val:  37.92%, val_best:  37.92%, tr:  32.18%, tr_best:  32.18%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  2.055474/  2.081497, val:  36.25%, val_best:  37.92%, tr:  37.69%, tr_best:  37.69%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  2.002956/  2.033107, val:  36.67%, val_best:  37.92%, tr:  39.94%, tr_best:  39.94%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  1.949745/  1.983985, val:  42.92%, val_best:  42.92%, tr:  41.37%, tr_best:  41.37%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  1.896288/  1.930063, val:  44.17%, val_best:  44.17%, tr:  43.82%, tr_best:  43.82%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  1.843356/  1.880202, val:  47.50%, val_best:  47.50%, tr:  44.94%, tr_best:  44.94%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  1.795705/  1.838100, val:  48.75%, val_best:  48.75%, tr:  46.48%, tr_best:  46.48%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  1.745079/  1.798405, val:  52.08%, val_best:  52.08%, tr:  48.62%, tr_best:  48.62%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  1.699985/  1.763120, val:  50.83%, val_best:  52.08%, tr:  50.77%, tr_best:  50.77%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  1.665161/  1.731172, val:  48.33%, val_best:  52.08%, tr:  52.09%, tr_best:  52.09%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  1.620366/  1.701149, val:  50.00%, val_best:  52.08%, tr:  53.42%, tr_best:  53.42%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  1.592405/  1.671600, val:  51.25%, val_best:  52.08%, tr:  54.75%, tr_best:  54.75%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  1.553486/  1.643400, val:  51.67%, val_best:  52.08%, tr:  55.77%, tr_best:  55.77%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  1.524265/  1.622625, val:  51.67%, val_best:  52.08%, tr:  56.79%, tr_best:  56.79%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  1.505376/  1.604725, val:  50.42%, val_best:  52.08%, tr:  57.71%, tr_best:  57.71%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  1.479965/  1.586032, val:  53.75%, val_best:  53.75%, tr:  57.20%, tr_best:  57.71%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  1.448208/  1.562342, val:  51.25%, val_best:  53.75%, tr:  58.53%, tr_best:  58.53%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  1.432400/  1.546996, val:  51.25%, val_best:  53.75%, tr:  57.81%, tr_best:  58.53%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  1.411651/  1.536434, val:  53.33%, val_best:  53.75%, tr:  57.92%, tr_best:  58.53%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  1.394823/  1.516539, val:  52.50%, val_best:  53.75%, tr:  58.94%, tr_best:  58.94%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  1.377167/  1.501395, val:  55.83%, val_best:  55.83%, tr:  60.57%, tr_best:  60.57%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  1.354165/  1.490252, val:  57.08%, val_best:  57.08%, tr:  60.57%, tr_best:  60.57%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  1.342757/  1.474507, val:  55.00%, val_best:  57.08%, tr:  60.88%, tr_best:  60.88%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  1.321448/  1.465226, val:  57.50%, val_best:  57.50%, tr:  62.51%, tr_best:  62.51%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  1.308761/  1.454435, val:  54.58%, val_best:  57.50%, tr:  62.31%, tr_best:  62.51%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  1.292269/  1.446846, val:  56.67%, val_best:  57.50%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  1.278312/  1.438253, val:  57.08%, val_best:  57.50%, tr:  62.10%, tr_best:  62.72%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  1.261197/  1.429712, val:  59.17%, val_best:  59.17%, tr:  62.10%, tr_best:  62.72%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  1.255087/  1.423045, val:  56.67%, val_best:  59.17%, tr:  62.00%, tr_best:  62.72%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  1.241813/  1.415236, val:  59.17%, val_best:  59.17%, tr:  63.84%, tr_best:  63.84%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  1.232008/  1.406383, val:  58.75%, val_best:  59.17%, tr:  62.10%, tr_best:  63.84%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  1.220281/  1.401743, val:  58.75%, val_best:  59.17%, tr:  64.35%, tr_best:  64.35%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  1.210545/  1.394337, val:  57.50%, val_best:  59.17%, tr:  63.53%, tr_best:  64.35%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  1.196679/  1.385792, val:  62.50%, val_best:  62.50%, tr:  64.56%, tr_best:  64.56%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  1.185365/  1.380282, val:  62.50%, val_best:  62.50%, tr:  65.68%, tr_best:  65.68%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  1.176207/  1.373838, val:  59.17%, val_best:  62.50%, tr:  67.52%, tr_best:  67.52%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  1.163783/  1.368082, val:  57.50%, val_best:  62.50%, tr:  67.21%, tr_best:  67.52%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  1.147799/  1.365740, val:  58.75%, val_best:  62.50%, tr:  67.52%, tr_best:  67.52%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  1.145411/  1.363315, val:  60.00%, val_best:  62.50%, tr:  69.05%, tr_best:  69.05%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  1.130782/  1.358015, val:  61.67%, val_best:  62.50%, tr:  67.31%, tr_best:  69.05%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  1.130288/  1.350402, val:  64.58%, val_best:  64.58%, tr:  69.25%, tr_best:  69.25%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  1.120316/  1.343743, val:  62.08%, val_best:  64.58%, tr:  65.37%, tr_best:  69.25%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  1.119068/  1.343302, val:  63.75%, val_best:  64.58%, tr:  68.03%, tr_best:  69.25%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  1.107586/  1.340095, val:  62.92%, val_best:  64.58%, tr:  69.25%, tr_best:  69.25%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  1.090308/  1.336192, val:  64.17%, val_best:  64.58%, tr:  70.28%, tr_best:  70.28%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  1.083023/  1.330554, val:  62.50%, val_best:  64.58%, tr:  69.05%, tr_best:  70.28%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  1.081122/  1.328892, val:  65.00%, val_best:  65.00%, tr:  68.44%, tr_best:  70.28%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  1.075205/  1.326558, val:  65.00%, val_best:  65.00%, tr:  71.60%, tr_best:  71.60%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  1.070335/  1.323624, val:  64.58%, val_best:  65.00%, tr:  70.99%, tr_best:  71.60%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  1.054626/  1.323423, val:  60.00%, val_best:  65.00%, tr:  69.05%, tr_best:  71.60%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  1.050373/  1.315583, val:  67.92%, val_best:  67.92%, tr:  72.42%, tr_best:  72.42%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  1.044180/  1.312813, val:  66.67%, val_best:  67.92%, tr:  72.11%, tr_best:  72.42%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  1.037043/  1.309441, val:  67.08%, val_best:  67.92%, tr:  70.58%, tr_best:  72.42%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  1.025778/  1.309521, val:  66.67%, val_best:  67.92%, tr:  69.46%, tr_best:  72.42%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  1.025131/  1.304343, val:  69.58%, val_best:  69.58%, tr:  71.81%, tr_best:  72.42%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  1.016591/  1.305338, val:  62.50%, val_best:  69.58%, tr:  70.68%, tr_best:  72.42%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  1.015000/  1.303624, val:  63.33%, val_best:  69.58%, tr:  71.60%, tr_best:  72.42%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  1.005052/  1.296306, val:  64.58%, val_best:  69.58%, tr:  70.68%, tr_best:  72.42%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  1.007647/  1.296383, val:  65.00%, val_best:  69.58%, tr:  72.11%, tr_best:  72.42%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  0.997028/  1.291890, val:  62.50%, val_best:  69.58%, tr:  72.93%, tr_best:  72.93%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  0.997035/  1.291154, val:  65.00%, val_best:  69.58%, tr:  73.34%, tr_best:  73.34%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  0.982309/  1.290706, val:  68.75%, val_best:  69.58%, tr:  71.60%, tr_best:  73.34%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  0.981601/  1.288390, val:  63.75%, val_best:  69.58%, tr:  74.16%, tr_best:  74.16%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  0.972629/  1.287184, val:  61.25%, val_best:  69.58%, tr:  74.46%, tr_best:  74.46%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  0.975866/  1.279978, val:  66.67%, val_best:  69.58%, tr:  75.69%, tr_best:  75.69%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  0.972010/  1.282975, val:  63.33%, val_best:  69.58%, tr:  75.08%, tr_best:  75.69%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  0.966138/  1.279272, val:  66.25%, val_best:  69.58%, tr:  74.77%, tr_best:  75.69%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  0.954781/  1.277414, val:  64.58%, val_best:  69.58%, tr:  73.34%, tr_best:  75.69%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  0.951657/  1.276232, val:  68.33%, val_best:  69.58%, tr:  77.02%, tr_best:  77.02%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  0.948019/  1.279055, val:  64.17%, val_best:  69.58%, tr:  74.67%, tr_best:  77.02%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  0.937497/  1.278445, val:  66.25%, val_best:  69.58%, tr:  74.06%, tr_best:  77.02%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  0.941085/  1.271256, val:  67.50%, val_best:  69.58%, tr:  75.79%, tr_best:  77.02%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  0.925459/  1.268237, val:  65.42%, val_best:  69.58%, tr:  76.51%, tr_best:  77.02%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  0.930795/  1.268497, val:  65.00%, val_best:  69.58%, tr:  73.75%, tr_best:  77.02%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  0.928519/  1.270229, val:  65.00%, val_best:  69.58%, tr:  78.14%, tr_best:  78.14%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  0.919009/  1.262818, val:  67.92%, val_best:  69.58%, tr:  77.02%, tr_best:  78.14%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  0.910712/  1.261892, val:  67.50%, val_best:  69.58%, tr:  75.79%, tr_best:  78.14%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  0.907532/  1.263898, val:  66.25%, val_best:  69.58%, tr:  78.55%, tr_best:  78.55%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  0.899344/  1.260513, val:  66.67%, val_best:  69.58%, tr:  77.02%, tr_best:  78.55%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  0.903989/  1.256480, val:  65.00%, val_best:  69.58%, tr:  76.10%, tr_best:  78.55%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  0.895697/  1.260797, val:  65.42%, val_best:  69.58%, tr:  77.63%, tr_best:  78.55%\n",
      "epoch-100 lr=['0.0001000'], tr/val_loss:  0.885898/  1.252939, val:  69.17%, val_best:  69.58%, tr:  76.71%, tr_best:  78.55%\n",
      "epoch-101 lr=['0.0001000'], tr/val_loss:  0.881041/  1.249051, val:  68.33%, val_best:  69.58%, tr:  79.06%, tr_best:  79.06%\n",
      "epoch-102 lr=['0.0001000'], tr/val_loss:  0.881621/  1.251838, val:  68.75%, val_best:  69.58%, tr:  79.26%, tr_best:  79.26%\n",
      "epoch-103 lr=['0.0001000'], tr/val_loss:  0.874495/  1.251320, val:  67.08%, val_best:  69.58%, tr:  78.04%, tr_best:  79.26%\n",
      "epoch-104 lr=['0.0001000'], tr/val_loss:  0.866763/  1.248106, val:  66.25%, val_best:  69.58%, tr:  80.18%, tr_best:  80.18%\n",
      "epoch-105 lr=['0.0001000'], tr/val_loss:  0.874573/  1.250462, val:  67.92%, val_best:  69.58%, tr:  79.57%, tr_best:  80.18%\n",
      "epoch-106 lr=['0.0001000'], tr/val_loss:  0.866221/  1.246002, val:  66.25%, val_best:  69.58%, tr:  80.80%, tr_best:  80.80%\n",
      "epoch-107 lr=['0.0001000'], tr/val_loss:  0.860793/  1.249106, val:  67.50%, val_best:  69.58%, tr:  79.57%, tr_best:  80.80%\n",
      "epoch-108 lr=['0.0001000'], tr/val_loss:  0.852856/  1.250190, val:  68.75%, val_best:  69.58%, tr:  80.29%, tr_best:  80.80%\n",
      "epoch-109 lr=['0.0001000'], tr/val_loss:  0.848907/  1.252556, val:  67.92%, val_best:  69.58%, tr:  80.49%, tr_best:  80.80%\n",
      "epoch-110 lr=['0.0001000'], tr/val_loss:  0.841382/  1.246142, val:  67.08%, val_best:  69.58%, tr:  81.41%, tr_best:  81.41%\n",
      "epoch-111 lr=['0.0001000'], tr/val_loss:  0.839299/  1.246767, val:  67.50%, val_best:  69.58%, tr:  81.51%, tr_best:  81.51%\n",
      "epoch-112 lr=['0.0001000'], tr/val_loss:  0.835175/  1.245965, val:  67.50%, val_best:  69.58%, tr:  81.82%, tr_best:  81.82%\n",
      "epoch-113 lr=['0.0001000'], tr/val_loss:  0.839526/  1.253605, val:  67.92%, val_best:  69.58%, tr:  80.08%, tr_best:  81.82%\n",
      "epoch-114 lr=['0.0001000'], tr/val_loss:  0.836385/  1.247631, val:  70.42%, val_best:  70.42%, tr:  82.33%, tr_best:  82.33%\n",
      "epoch-115 lr=['0.0001000'], tr/val_loss:  0.825949/  1.249918, val:  67.92%, val_best:  70.42%, tr:  82.02%, tr_best:  82.33%\n",
      "epoch-116 lr=['0.0001000'], tr/val_loss:  0.815929/  1.251667, val:  68.75%, val_best:  70.42%, tr:  81.92%, tr_best:  82.33%\n",
      "epoch-117 lr=['0.0001000'], tr/val_loss:  0.817699/  1.254921, val:  67.08%, val_best:  70.42%, tr:  82.74%, tr_best:  82.74%\n",
      "epoch-118 lr=['0.0001000'], tr/val_loss:  0.814771/  1.255405, val:  69.58%, val_best:  70.42%, tr:  81.10%, tr_best:  82.74%\n",
      "epoch-119 lr=['0.0001000'], tr/val_loss:  0.807851/  1.251546, val:  67.92%, val_best:  70.42%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-120 lr=['0.0001000'], tr/val_loss:  0.799410/  1.255215, val:  68.75%, val_best:  70.42%, tr:  84.68%, tr_best:  84.68%\n",
      "epoch-121 lr=['0.0001000'], tr/val_loss:  0.803072/  1.254358, val:  67.92%, val_best:  70.42%, tr:  82.23%, tr_best:  84.68%\n",
      "epoch-122 lr=['0.0001000'], tr/val_loss:  0.806246/  1.254362, val:  67.92%, val_best:  70.42%, tr:  82.74%, tr_best:  84.68%\n",
      "epoch-123 lr=['0.0001000'], tr/val_loss:  0.800354/  1.254722, val:  68.33%, val_best:  70.42%, tr:  82.84%, tr_best:  84.68%\n",
      "epoch-124 lr=['0.0001000'], tr/val_loss:  0.786837/  1.255100, val:  70.42%, val_best:  70.42%, tr:  83.25%, tr_best:  84.68%\n",
      "epoch-125 lr=['0.0001000'], tr/val_loss:  0.785656/  1.253963, val:  69.58%, val_best:  70.42%, tr:  85.19%, tr_best:  85.19%\n",
      "epoch-126 lr=['0.0001000'], tr/val_loss:  0.797095/  1.253414, val:  67.08%, val_best:  70.42%, tr:  86.41%, tr_best:  86.41%\n",
      "epoch-127 lr=['0.0001000'], tr/val_loss:  0.782468/  1.255947, val:  70.00%, val_best:  70.42%, tr:  84.98%, tr_best:  86.41%\n",
      "epoch-128 lr=['0.0001000'], tr/val_loss:  0.774587/  1.251246, val:  68.75%, val_best:  70.42%, tr:  86.52%, tr_best:  86.52%\n",
      "epoch-129 lr=['0.0001000'], tr/val_loss:  0.771276/  1.248268, val:  69.17%, val_best:  70.42%, tr:  86.21%, tr_best:  86.52%\n",
      "epoch-130 lr=['0.0001000'], tr/val_loss:  0.777849/  1.247199, val:  68.33%, val_best:  70.42%, tr:  86.72%, tr_best:  86.72%\n",
      "epoch-131 lr=['0.0001000'], tr/val_loss:  0.762343/  1.247810, val:  69.58%, val_best:  70.42%, tr:  85.70%, tr_best:  86.72%\n",
      "epoch-132 lr=['0.0001000'], tr/val_loss:  0.760864/  1.252078, val:  69.17%, val_best:  70.42%, tr:  84.58%, tr_best:  86.72%\n",
      "epoch-133 lr=['0.0001000'], tr/val_loss:  0.758669/  1.251193, val:  66.67%, val_best:  70.42%, tr:  87.13%, tr_best:  87.13%\n",
      "epoch-134 lr=['0.0001000'], tr/val_loss:  0.751376/  1.252490, val:  71.25%, val_best:  71.25%, tr:  86.62%, tr_best:  87.13%\n",
      "epoch-135 lr=['0.0001000'], tr/val_loss:  0.745599/  1.255452, val:  68.75%, val_best:  71.25%, tr:  86.93%, tr_best:  87.13%\n",
      "epoch-136 lr=['0.0001000'], tr/val_loss:  0.752647/  1.254973, val:  70.83%, val_best:  71.25%, tr:  87.44%, tr_best:  87.44%\n",
      "epoch-137 lr=['0.0001000'], tr/val_loss:  0.753058/  1.256453, val:  70.42%, val_best:  71.25%, tr:  86.93%, tr_best:  87.44%\n",
      "epoch-138 lr=['0.0001000'], tr/val_loss:  0.733540/  1.254723, val:  69.17%, val_best:  71.25%, tr:  87.13%, tr_best:  87.44%\n",
      "epoch-139 lr=['0.0001000'], tr/val_loss:  0.735779/  1.259186, val:  72.50%, val_best:  72.50%, tr:  87.23%, tr_best:  87.44%\n",
      "epoch-140 lr=['0.0001000'], tr/val_loss:  0.739436/  1.250678, val:  67.08%, val_best:  72.50%, tr:  86.62%, tr_best:  87.44%\n",
      "epoch-141 lr=['0.0001000'], tr/val_loss:  0.725148/  1.255390, val:  70.83%, val_best:  72.50%, tr:  88.46%, tr_best:  88.46%\n",
      "epoch-142 lr=['0.0001000'], tr/val_loss:  0.722295/  1.258190, val:  70.42%, val_best:  72.50%, tr:  88.15%, tr_best:  88.46%\n",
      "epoch-143 lr=['0.0001000'], tr/val_loss:  0.722360/  1.253969, val:  70.42%, val_best:  72.50%, tr:  89.68%, tr_best:  89.68%\n",
      "epoch-144 lr=['0.0001000'], tr/val_loss:  0.716667/  1.262074, val:  70.83%, val_best:  72.50%, tr:  88.56%, tr_best:  89.68%\n",
      "epoch-145 lr=['0.0001000'], tr/val_loss:  0.714980/  1.259762, val:  67.50%, val_best:  72.50%, tr:  88.05%, tr_best:  89.68%\n",
      "epoch-146 lr=['0.0001000'], tr/val_loss:  0.710043/  1.254701, val:  72.08%, val_best:  72.50%, tr:  88.56%, tr_best:  89.68%\n",
      "epoch-147 lr=['0.0001000'], tr/val_loss:  0.718221/  1.254221, val:  72.08%, val_best:  72.50%, tr:  87.95%, tr_best:  89.68%\n",
      "epoch-148 lr=['0.0001000'], tr/val_loss:  0.700233/  1.254409, val:  72.08%, val_best:  72.50%, tr:  88.66%, tr_best:  89.68%\n",
      "epoch-149 lr=['0.0001000'], tr/val_loss:  0.699201/  1.254339, val:  69.58%, val_best:  72.50%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-150 lr=['0.0001000'], tr/val_loss:  0.700908/  1.258150, val:  70.00%, val_best:  72.50%, tr:  90.09%, tr_best:  90.09%\n",
      "epoch-151 lr=['0.0001000'], tr/val_loss:  0.695863/  1.259278, val:  72.08%, val_best:  72.50%, tr:  88.56%, tr_best:  90.09%\n",
      "epoch-152 lr=['0.0001000'], tr/val_loss:  0.686821/  1.262188, val:  71.25%, val_best:  72.50%, tr:  87.95%, tr_best:  90.09%\n",
      "epoch-153 lr=['0.0001000'], tr/val_loss:  0.693019/  1.262314, val:  71.67%, val_best:  72.50%, tr:  89.89%, tr_best:  90.09%\n",
      "epoch-154 lr=['0.0001000'], tr/val_loss:  0.686284/  1.260547, val:  72.50%, val_best:  72.50%, tr:  89.17%, tr_best:  90.09%\n",
      "epoch-155 lr=['0.0001000'], tr/val_loss:  0.682853/  1.262935, val:  70.83%, val_best:  72.50%, tr:  90.81%, tr_best:  90.81%\n",
      "epoch-156 lr=['0.0001000'], tr/val_loss:  0.682364/  1.260688, val:  71.25%, val_best:  72.50%, tr:  88.66%, tr_best:  90.81%\n",
      "epoch-157 lr=['0.0001000'], tr/val_loss:  0.671356/  1.266865, val:  71.25%, val_best:  72.50%, tr:  90.19%, tr_best:  90.81%\n",
      "epoch-158 lr=['0.0001000'], tr/val_loss:  0.683183/  1.264482, val:  70.83%, val_best:  72.50%, tr:  90.19%, tr_best:  90.81%\n",
      "epoch-159 lr=['0.0001000'], tr/val_loss:  0.674266/  1.270829, val:  72.08%, val_best:  72.50%, tr:  89.79%, tr_best:  90.81%\n",
      "epoch-160 lr=['0.0001000'], tr/val_loss:  0.669139/  1.259245, val:  71.25%, val_best:  72.50%, tr:  90.60%, tr_best:  90.81%\n",
      "epoch-161 lr=['0.0001000'], tr/val_loss:  0.662731/  1.264771, val:  72.92%, val_best:  72.92%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-162 lr=['0.0001000'], tr/val_loss:  0.664705/  1.265195, val:  72.50%, val_best:  72.92%, tr:  91.22%, tr_best:  91.62%\n",
      "epoch-163 lr=['0.0001000'], tr/val_loss:  0.658273/  1.257669, val:  72.92%, val_best:  72.92%, tr:  91.01%, tr_best:  91.62%\n",
      "epoch-164 lr=['0.0001000'], tr/val_loss:  0.656461/  1.270302, val:  72.08%, val_best:  72.92%, tr:  90.91%, tr_best:  91.62%\n",
      "epoch-165 lr=['0.0001000'], tr/val_loss:  0.657464/  1.269876, val:  72.50%, val_best:  72.92%, tr:  90.40%, tr_best:  91.62%\n",
      "epoch-166 lr=['0.0001000'], tr/val_loss:  0.648235/  1.271428, val:  73.75%, val_best:  73.75%, tr:  90.50%, tr_best:  91.62%\n",
      "epoch-167 lr=['0.0001000'], tr/val_loss:  0.649535/  1.271157, val:  72.92%, val_best:  73.75%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-168 lr=['0.0001000'], tr/val_loss:  0.647191/  1.270469, val:  73.33%, val_best:  73.75%, tr:  89.99%, tr_best:  91.62%\n",
      "epoch-169 lr=['0.0001000'], tr/val_loss:  0.635073/  1.265484, val:  72.08%, val_best:  73.75%, tr:  91.42%, tr_best:  91.62%\n",
      "epoch-170 lr=['0.0001000'], tr/val_loss:  0.640617/  1.273795, val:  72.92%, val_best:  73.75%, tr:  91.22%, tr_best:  91.62%\n",
      "epoch-171 lr=['0.0001000'], tr/val_loss:  0.638106/  1.267889, val:  73.75%, val_best:  73.75%, tr:  91.73%, tr_best:  91.73%\n",
      "epoch-172 lr=['0.0001000'], tr/val_loss:  0.632430/  1.273851, val:  72.92%, val_best:  73.75%, tr:  91.93%, tr_best:  91.93%\n",
      "epoch-173 lr=['0.0001000'], tr/val_loss:  0.628933/  1.278455, val:  71.67%, val_best:  73.75%, tr:  92.44%, tr_best:  92.44%\n",
      "epoch-174 lr=['0.0001000'], tr/val_loss:  0.624155/  1.276084, val:  72.50%, val_best:  73.75%, tr:  91.73%, tr_best:  92.44%\n",
      "epoch-175 lr=['0.0001000'], tr/val_loss:  0.619451/  1.276467, val:  72.92%, val_best:  73.75%, tr:  92.03%, tr_best:  92.44%\n",
      "epoch-176 lr=['0.0001000'], tr/val_loss:  0.614551/  1.280029, val:  73.75%, val_best:  73.75%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-177 lr=['0.0001000'], tr/val_loss:  0.626761/  1.270570, val:  74.17%, val_best:  74.17%, tr:  91.73%, tr_best:  92.95%\n",
      "epoch-178 lr=['0.0001000'], tr/val_loss:  0.615681/  1.275340, val:  75.42%, val_best:  75.42%, tr:  92.13%, tr_best:  92.95%\n",
      "epoch-179 lr=['0.0001000'], tr/val_loss:  0.616502/  1.268316, val:  74.17%, val_best:  75.42%, tr:  92.65%, tr_best:  92.95%\n",
      "epoch-180 lr=['0.0001000'], tr/val_loss:  0.604677/  1.271758, val:  74.58%, val_best:  75.42%, tr:  93.26%, tr_best:  93.26%\n",
      "epoch-181 lr=['0.0001000'], tr/val_loss:  0.608008/  1.272447, val:  74.58%, val_best:  75.42%, tr:  92.54%, tr_best:  93.26%\n",
      "epoch-182 lr=['0.0001000'], tr/val_loss:  0.602446/  1.272260, val:  72.50%, val_best:  75.42%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-183 lr=['0.0001000'], tr/val_loss:  0.603974/  1.277271, val:  74.17%, val_best:  75.42%, tr:  92.75%, tr_best:  93.36%\n",
      "epoch-184 lr=['0.0001000'], tr/val_loss:  0.600758/  1.278628, val:  74.17%, val_best:  75.42%, tr:  93.67%, tr_best:  93.67%\n",
      "epoch-185 lr=['0.0001000'], tr/val_loss:  0.593763/  1.284418, val:  72.92%, val_best:  75.42%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-186 lr=['0.0001000'], tr/val_loss:  0.594530/  1.285468, val:  75.42%, val_best:  75.42%, tr:  94.18%, tr_best:  94.48%\n",
      "epoch-187 lr=['0.0001000'], tr/val_loss:  0.594425/  1.285318, val:  72.92%, val_best:  75.42%, tr:  94.59%, tr_best:  94.59%\n",
      "epoch-188 lr=['0.0001000'], tr/val_loss:  0.583370/  1.281461, val:  73.75%, val_best:  75.42%, tr:  93.97%, tr_best:  94.59%\n",
      "epoch-189 lr=['0.0001000'], tr/val_loss:  0.580371/  1.287667, val:  75.00%, val_best:  75.42%, tr:  93.67%, tr_best:  94.59%\n",
      "epoch-190 lr=['0.0001000'], tr/val_loss:  0.579836/  1.289461, val:  74.58%, val_best:  75.42%, tr:  94.38%, tr_best:  94.59%\n",
      "epoch-191 lr=['0.0001000'], tr/val_loss:  0.583955/  1.295246, val:  73.75%, val_best:  75.42%, tr:  94.18%, tr_best:  94.59%\n",
      "epoch-192 lr=['0.0001000'], tr/val_loss:  0.572883/  1.288767, val:  75.00%, val_best:  75.42%, tr:  93.46%, tr_best:  94.59%\n",
      "epoch-193 lr=['0.0001000'], tr/val_loss:  0.571543/  1.294992, val:  74.17%, val_best:  75.42%, tr:  94.38%, tr_best:  94.59%\n",
      "epoch-194 lr=['0.0001000'], tr/val_loss:  0.570046/  1.303941, val:  73.75%, val_best:  75.42%, tr:  93.67%, tr_best:  94.59%\n",
      "epoch-195 lr=['0.0001000'], tr/val_loss:  0.571963/  1.296271, val:  72.92%, val_best:  75.42%, tr:  92.95%, tr_best:  94.59%\n",
      "epoch-196 lr=['0.0001000'], tr/val_loss:  0.562644/  1.295503, val:  74.58%, val_best:  75.42%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-197 lr=['0.0001000'], tr/val_loss:  0.560599/  1.302000, val:  74.58%, val_best:  75.42%, tr:  94.48%, tr_best:  94.99%\n",
      "epoch-198 lr=['0.0001000'], tr/val_loss:  0.562521/  1.301576, val:  74.17%, val_best:  75.42%, tr:  94.79%, tr_best:  94.99%\n",
      "epoch-199 lr=['0.0001000'], tr/val_loss:  0.550256/  1.306278, val:  73.33%, val_best:  75.42%, tr:  95.51%, tr_best:  95.51%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2590837f154de3b8320b65ede6f1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▁▃▄▆▅▅▆▆▇▅▅▆▆▆▆▇▅▇▇▅▇▅▆█▆███▆▇▇▆███▇██</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▂▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██████████</td></tr><tr><td>tr_acc</td><td>▁▁▁▂▃▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█▇██████████</td></tr><tr><td>tr_epoch_loss</td><td>████▇▆▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▂▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▁▂▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██████████</td></tr><tr><td>val_loss</td><td>████▇▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.95506</td></tr><tr><td>tr_epoch_loss</td><td>0.55026</td></tr><tr><td>val_acc_best</td><td>0.75417</td></tr><tr><td>val_acc_now</td><td>0.73333</td></tr><tr><td>val_loss</td><td>1.30628</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">driven-sweep-41</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jekqs8ec' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jekqs8ec</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_072329-jekqs8ec/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5a2itu3r with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_073501-5a2itu3r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5a2itu3r' target=\"_blank\">efficient-sweep-42</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5a2itu3r' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5a2itu3r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  2.309651/  2.371933, val:  41.25%, val_best:  41.25%, tr:  33.71%, tr_best:  33.71%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  1.892996/  2.210163, val:  35.00%, val_best:  41.25%, tr:  43.51%, tr_best:  43.51%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  2.101151/  2.205286, val:  29.58%, val_best:  41.25%, tr:  41.57%, tr_best:  43.51%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  2.077538/  1.963589, val:  36.67%, val_best:  41.25%, tr:  40.25%, tr_best:  43.51%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  1.902734/  1.812126, val:  43.33%, val_best:  43.33%, tr:  39.84%, tr_best:  43.51%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  1.865278/  1.916100, val:  38.33%, val_best:  43.33%, tr:  43.21%, tr_best:  43.51%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  2.135408/  2.553150, val:  40.42%, val_best:  43.33%, tr:  43.62%, tr_best:  43.62%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  2.370673/  2.529002, val:  27.50%, val_best:  43.33%, tr:  37.49%, tr_best:  43.62%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  2.123761/  2.173666, val:  45.83%, val_best:  45.83%, tr:  44.84%, tr_best:  44.84%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  1.906903/  1.950260, val:  44.17%, val_best:  45.83%, tr:  49.95%, tr_best:  49.95%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  1.835596/  2.767279, val:  50.00%, val_best:  50.00%, tr:  48.72%, tr_best:  49.95%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  2.007742/  2.408342, val:  41.25%, val_best:  50.00%, tr:  48.01%, tr_best:  49.95%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  2.979155/  2.605025, val:  35.42%, val_best:  50.00%, tr:  36.87%, tr_best:  49.95%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  2.740795/  3.234202, val:  29.17%, val_best:  50.00%, tr:  31.77%, tr_best:  49.95%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  3.899067/  3.329965, val:  25.83%, val_best:  50.00%, tr:  26.05%, tr_best:  49.95%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  3.029008/  2.512116, val:  35.42%, val_best:  50.00%, tr:  31.15%, tr_best:  49.95%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  2.416129/  2.521827, val:  42.92%, val_best:  50.00%, tr:  36.57%, tr_best:  49.95%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  2.626525/  2.689417, val:  32.92%, val_best:  50.00%, tr:  38.82%, tr_best:  49.95%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  2.553153/  2.479197, val:  42.08%, val_best:  50.00%, tr:  38.61%, tr_best:  49.95%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  2.335437/  2.924146, val:  30.42%, val_best:  50.00%, tr:  36.16%, tr_best:  49.95%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  3.087409/  3.254728, val:  30.42%, val_best:  50.00%, tr:  32.18%, tr_best:  49.95%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  3.451432/  5.431575, val:  27.08%, val_best:  50.00%, tr:  34.63%, tr_best:  49.95%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  3.454280/  3.621294, val:  32.08%, val_best:  50.00%, tr:  29.83%, tr_best:  49.95%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  2.703999/  2.783346, val:  38.75%, val_best:  50.00%, tr:  36.87%, tr_best:  49.95%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  2.468498/  2.755967, val:  41.67%, val_best:  50.00%, tr:  33.71%, tr_best:  49.95%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  2.339079/  2.598813, val:  36.25%, val_best:  50.00%, tr:  35.55%, tr_best:  49.95%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  2.335222/  2.803854, val:  31.67%, val_best:  50.00%, tr:  37.59%, tr_best:  49.95%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  2.434202/  2.661946, val:  30.83%, val_best:  50.00%, tr:  31.66%, tr_best:  49.95%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  3.353474/  3.772998, val:  22.08%, val_best:  50.00%, tr:  34.42%, tr_best:  49.95%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  2.832342/  3.794562, val:  22.92%, val_best:  50.00%, tr:  30.95%, tr_best:  49.95%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  2.922532/  4.554250, val:  28.33%, val_best:  50.00%, tr:  30.64%, tr_best:  49.95%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  3.415275/  4.119846, val:  25.83%, val_best:  50.00%, tr:  31.46%, tr_best:  49.95%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  3.902890/  3.818986, val:  27.50%, val_best:  50.00%, tr:  34.93%, tr_best:  49.95%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  3.155377/  2.302288, val:  30.00%, val_best:  50.00%, tr:  33.30%, tr_best:  49.95%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  3.609692/  7.626876, val:  30.83%, val_best:  50.00%, tr:  36.47%, tr_best:  49.95%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  3.853001/  3.714761, val:  41.67%, val_best:  50.00%, tr:  37.28%, tr_best:  49.95%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  2.933926/  4.332044, val:  33.33%, val_best:  50.00%, tr:  36.57%, tr_best:  49.95%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  3.281627/  3.546498, val:  40.00%, val_best:  50.00%, tr:  36.87%, tr_best:  49.95%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  2.696508/  3.014079, val:  32.92%, val_best:  50.00%, tr:  37.28%, tr_best:  49.95%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  2.358576/  3.158999, val:  42.50%, val_best:  50.00%, tr:  36.57%, tr_best:  49.95%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  2.344741/  2.562507, val:  42.08%, val_best:  50.00%, tr:  40.86%, tr_best:  49.95%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  2.500901/  2.810021, val:  45.83%, val_best:  50.00%, tr:  37.39%, tr_best:  49.95%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  3.439561/  5.398108, val:  26.25%, val_best:  50.00%, tr:  32.38%, tr_best:  49.95%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  3.820425/  4.372698, val:  36.25%, val_best:  50.00%, tr:  28.50%, tr_best:  49.95%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  2.961511/  3.392959, val:  32.08%, val_best:  50.00%, tr:  39.22%, tr_best:  49.95%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  2.637387/  3.016594, val:  38.75%, val_best:  50.00%, tr:  36.16%, tr_best:  49.95%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  2.596839/  3.152120, val:  25.83%, val_best:  50.00%, tr:  39.22%, tr_best:  49.95%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  2.609349/  2.910992, val:  36.25%, val_best:  50.00%, tr:  36.26%, tr_best:  49.95%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  2.812575/  2.742653, val:  35.00%, val_best:  50.00%, tr:  39.53%, tr_best:  49.95%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  4.350666/  5.014990, val:  37.50%, val_best:  50.00%, tr:  33.30%, tr_best:  49.95%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  3.578619/  4.023936, val:  25.83%, val_best:  50.00%, tr:  34.01%, tr_best:  49.95%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  3.345071/  4.305904, val:  35.83%, val_best:  50.00%, tr:  31.77%, tr_best:  49.95%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  4.572739/  3.119644, val:  31.67%, val_best:  50.00%, tr:  29.42%, tr_best:  49.95%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  3.105191/  3.543300, val:  39.58%, val_best:  50.00%, tr:  29.93%, tr_best:  49.95%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  3.452740/  2.952306, val:  31.67%, val_best:  50.00%, tr:  29.72%, tr_best:  49.95%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  3.513333/  4.594648, val:  37.92%, val_best:  50.00%, tr:  31.77%, tr_best:  49.95%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  4.984930/  3.290758, val:  36.67%, val_best:  50.00%, tr:  24.92%, tr_best:  49.95%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  3.002303/  2.129570, val:  33.75%, val_best:  50.00%, tr:  30.95%, tr_best:  49.95%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  3.066455/  2.705970, val:  45.00%, val_best:  50.00%, tr:  35.14%, tr_best:  49.95%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  2.822031/  3.780466, val:  23.75%, val_best:  50.00%, tr:  34.22%, tr_best:  49.95%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  3.434009/  4.717141, val:  19.17%, val_best:  50.00%, tr:  30.44%, tr_best:  49.95%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  4.056717/  6.858591, val:  25.00%, val_best:  50.00%, tr:  31.87%, tr_best:  49.95%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  3.873247/  2.172607, val:  28.33%, val_best:  50.00%, tr:  21.55%, tr_best:  49.95%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  4.408182/  9.903385, val:  10.83%, val_best:  50.00%, tr:  20.53%, tr_best:  49.95%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  3.670759/  2.370192, val:  27.50%, val_best:  50.00%, tr:  18.28%, tr_best:  49.95%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  2.673478/  3.939387, val:  19.58%, val_best:  50.00%, tr:  24.31%, tr_best:  49.95%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  5.306218/  7.960094, val:  16.67%, val_best:  50.00%, tr:  20.53%, tr_best:  49.95%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  4.852936/  3.033152, val:  18.75%, val_best:  50.00%, tr:  19.51%, tr_best:  49.95%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  3.124268/  4.169807, val:  20.83%, val_best:  50.00%, tr:  21.96%, tr_best:  49.95%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  3.307168/  3.381969, val:  10.00%, val_best:  50.00%, tr:  21.04%, tr_best:  49.95%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  4.179620/  8.038900, val:  17.08%, val_best:  50.00%, tr:  16.55%, tr_best:  49.95%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  4.109404/  7.350714, val:  14.58%, val_best:  50.00%, tr:  21.35%, tr_best:  49.95%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  3.871931/  4.947275, val:  24.17%, val_best:  50.00%, tr:  18.59%, tr_best:  49.95%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  3.203529/  2.889107, val:  25.42%, val_best:  50.00%, tr:  20.02%, tr_best:  49.95%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  2.976022/  2.938606, val:  18.75%, val_best:  50.00%, tr:  20.33%, tr_best:  49.95%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  4.312726/  7.154338, val:  15.00%, val_best:  50.00%, tr:  17.47%, tr_best:  49.95%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  3.462696/  3.089916, val:  19.17%, val_best:  50.00%, tr:  19.41%, tr_best:  49.95%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  3.061294/  2.759712, val:  10.00%, val_best:  50.00%, tr:  23.29%, tr_best:  49.95%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  3.045736/  3.349073, val:  18.33%, val_best:  50.00%, tr:  18.49%, tr_best:  49.95%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  3.189611/  3.557660, val:  25.42%, val_best:  50.00%, tr:  19.82%, tr_best:  49.95%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  2.808166/  5.347689, val:  20.83%, val_best:  50.00%, tr:  19.82%, tr_best:  49.95%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  3.046237/  4.909540, val:   9.58%, val_best:  50.00%, tr:  18.18%, tr_best:  49.95%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  8.173377/  5.090016, val:  10.00%, val_best:  50.00%, tr:  13.18%, tr_best:  49.95%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  3.570490/  2.738482, val:  10.00%, val_best:  50.00%, tr:   8.07%, tr_best:  49.95%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  3.484914/  3.363408, val:  10.00%, val_best:  50.00%, tr:   9.09%, tr_best:  49.95%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  3.032781/  3.000935, val:  10.00%, val_best:  50.00%, tr:   8.89%, tr_best:  49.95%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  3.396212/  3.571966, val:  10.00%, val_best:  50.00%, tr:  10.93%, tr_best:  49.95%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  3.070702/  3.759353, val:  10.00%, val_best:  50.00%, tr:   9.19%, tr_best:  49.95%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  3.190752/  3.214213, val:  10.00%, val_best:  50.00%, tr:   8.89%, tr_best:  49.95%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  3.097854/  3.383489, val:  10.00%, val_best:  50.00%, tr:  10.83%, tr_best:  49.95%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  3.491304/  2.518705, val:  10.00%, val_best:  50.00%, tr:   9.19%, tr_best:  49.95%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  3.083914/  4.609434, val:  10.00%, val_best:  50.00%, tr:  10.21%, tr_best:  49.95%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  3.290790/  3.090060, val:  10.00%, val_best:  50.00%, tr:  11.95%, tr_best:  49.95%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  3.159177/  2.641039, val:  10.00%, val_best:  50.00%, tr:  11.64%, tr_best:  49.95%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  3.260813/  2.859068, val:  10.00%, val_best:  50.00%, tr:   9.40%, tr_best:  49.95%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  3.129823/  2.886476, val:  10.00%, val_best:  50.00%, tr:   8.17%, tr_best:  49.95%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  3.225531/  3.837268, val:  10.00%, val_best:  50.00%, tr:   8.89%, tr_best:  49.95%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  3.470411/  3.681008, val:  10.00%, val_best:  50.00%, tr:   9.91%, tr_best:  49.95%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  3.190673/  4.099853, val:  10.00%, val_best:  50.00%, tr:  10.21%, tr_best:  49.95%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  3.743044/  3.642048, val:  10.00%, val_best:  50.00%, tr:   9.60%, tr_best:  49.95%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss:  3.147409/  3.624687, val:  10.00%, val_best:  50.00%, tr:  11.64%, tr_best:  49.95%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss:  3.660490/  2.882769, val:  10.00%, val_best:  50.00%, tr:   7.97%, tr_best:  49.95%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss:  3.309631/  3.095098, val:  10.00%, val_best:  50.00%, tr:   8.58%, tr_best:  49.95%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss:  3.215627/  3.547600, val:  10.00%, val_best:  50.00%, tr:   9.70%, tr_best:  49.95%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss:  3.018291/  3.163059, val:  10.00%, val_best:  50.00%, tr:  11.24%, tr_best:  49.95%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss:  2.973455/  3.785682, val:  10.00%, val_best:  50.00%, tr:  11.03%, tr_best:  49.95%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss:  3.505667/  3.177588, val:  10.00%, val_best:  50.00%, tr:   8.58%, tr_best:  49.95%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss:  3.098235/  4.190776, val:  10.00%, val_best:  50.00%, tr:  10.73%, tr_best:  49.95%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss:  3.158118/  2.966154, val:  10.00%, val_best:  50.00%, tr:   9.70%, tr_best:  49.95%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss:  3.147397/  3.039690, val:  10.00%, val_best:  50.00%, tr:  10.11%, tr_best:  49.95%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss:  3.191593/  2.970252, val:  10.00%, val_best:  50.00%, tr:   9.81%, tr_best:  49.95%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss:  3.208387/  3.380471, val:  10.00%, val_best:  50.00%, tr:  11.03%, tr_best:  49.95%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss:  3.544855/  3.687844, val:  10.00%, val_best:  50.00%, tr:   9.70%, tr_best:  49.95%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss:  3.544472/  4.298853, val:  10.00%, val_best:  50.00%, tr:  11.34%, tr_best:  49.95%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss:  3.329778/  3.029476, val:  10.00%, val_best:  50.00%, tr:  10.01%, tr_best:  49.95%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss:  2.989503/  3.685799, val:  10.00%, val_best:  50.00%, tr:  10.42%, tr_best:  49.95%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss:  3.215421/  3.125413, val:  10.00%, val_best:  50.00%, tr:  11.03%, tr_best:  49.95%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss:  3.291806/  3.819887, val:  10.00%, val_best:  50.00%, tr:   9.81%, tr_best:  49.95%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss:  3.267531/  3.715175, val:  10.00%, val_best:  50.00%, tr:  10.52%, tr_best:  49.95%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss:  3.281986/  3.102506, val:  10.00%, val_best:  50.00%, tr:   9.60%, tr_best:  49.95%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss:  3.270720/  3.566185, val:  10.00%, val_best:  50.00%, tr:   9.50%, tr_best:  49.95%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss:  3.280184/  3.216098, val:  10.00%, val_best:  50.00%, tr:  10.93%, tr_best:  49.95%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss:  3.053430/  3.485949, val:  10.00%, val_best:  50.00%, tr:  10.73%, tr_best:  49.95%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss:  3.453391/  3.256119, val:  10.00%, val_best:  50.00%, tr:   9.70%, tr_best:  49.95%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss:  3.226178/  3.458553, val:  10.00%, val_best:  50.00%, tr:   9.81%, tr_best:  49.95%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss:  3.357664/  3.333619, val:  10.00%, val_best:  50.00%, tr:  10.01%, tr_best:  49.95%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss:  2.991305/  4.860574, val:  10.00%, val_best:  50.00%, tr:  10.11%, tr_best:  49.95%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss:  3.501901/  3.369855, val:  10.00%, val_best:  50.00%, tr:  10.32%, tr_best:  49.95%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss:  3.291756/  3.079489, val:  10.00%, val_best:  50.00%, tr:  11.75%, tr_best:  49.95%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss:  3.448062/  3.108333, val:  10.00%, val_best:  50.00%, tr:   9.19%, tr_best:  49.95%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss:  3.575102/  3.013510, val:  10.00%, val_best:  50.00%, tr:   9.70%, tr_best:  49.95%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss:  3.393742/  3.196354, val:  10.00%, val_best:  50.00%, tr:   9.19%, tr_best:  49.95%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss:  3.166198/  4.279317, val:  10.00%, val_best:  50.00%, tr:  10.93%, tr_best:  49.95%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss:  3.395464/  3.177552, val:  10.00%, val_best:  50.00%, tr:  10.21%, tr_best:  49.95%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss:  3.102921/  3.318307, val:  10.00%, val_best:  50.00%, tr:  10.42%, tr_best:  49.95%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss:  3.158167/  4.207388, val:  10.00%, val_best:  50.00%, tr:   9.30%, tr_best:  49.95%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss:  3.484016/  2.942734, val:  10.00%, val_best:  50.00%, tr:   9.40%, tr_best:  49.95%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss:  3.003422/  3.361588, val:  10.00%, val_best:  50.00%, tr:  10.01%, tr_best:  49.95%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss:  3.107949/  3.802453, val:  10.00%, val_best:  50.00%, tr:  10.01%, tr_best:  49.95%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss:  2.963072/  3.163704, val:  10.00%, val_best:  50.00%, tr:  10.32%, tr_best:  49.95%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss:  3.299043/  2.858620, val:  10.00%, val_best:  50.00%, tr:   8.78%, tr_best:  49.95%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss:  3.466122/  3.422559, val:  10.00%, val_best:  50.00%, tr:   8.38%, tr_best:  49.95%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss:  3.159337/  3.657190, val:  10.00%, val_best:  50.00%, tr:   9.30%, tr_best:  49.95%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss:  3.380982/  5.344846, val:  10.00%, val_best:  50.00%, tr:  10.21%, tr_best:  49.95%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss:  3.533164/  3.489843, val:  10.00%, val_best:  50.00%, tr:  11.85%, tr_best:  49.95%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss:  3.179142/  3.766281, val:  10.00%, val_best:  50.00%, tr:   9.50%, tr_best:  49.95%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss:  3.278296/  3.572898, val:  10.00%, val_best:  50.00%, tr:   9.09%, tr_best:  49.95%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss:  2.960815/  4.398381, val:  10.00%, val_best:  50.00%, tr:  10.32%, tr_best:  49.95%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss:  3.689458/  4.017083, val:  10.00%, val_best:  50.00%, tr:  10.32%, tr_best:  49.95%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss:  3.598801/  4.047739, val:  10.00%, val_best:  50.00%, tr:  10.52%, tr_best:  49.95%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss:  3.423418/  4.642348, val:  10.00%, val_best:  50.00%, tr:   8.89%, tr_best:  49.95%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss:  3.430884/  3.057625, val:  10.00%, val_best:  50.00%, tr:   9.60%, tr_best:  49.95%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss:  3.390317/  5.305638, val:  10.00%, val_best:  50.00%, tr:  10.73%, tr_best:  49.95%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss:  3.320919/  4.477574, val:  10.00%, val_best:  50.00%, tr:   9.91%, tr_best:  49.95%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss:  3.444381/  3.494192, val:  10.00%, val_best:  50.00%, tr:   8.89%, tr_best:  49.95%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss:  3.172119/  3.556423, val:  10.00%, val_best:  50.00%, tr:  10.52%, tr_best:  49.95%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss:  3.370904/  3.791719, val:  10.00%, val_best:  50.00%, tr:  10.21%, tr_best:  49.95%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss:  3.374846/  4.358787, val:  10.00%, val_best:  50.00%, tr:   8.27%, tr_best:  49.95%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss:  3.141847/  3.374601, val:  10.00%, val_best:  50.00%, tr:   9.91%, tr_best:  49.95%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss:  3.185458/  3.209186, val:  10.00%, val_best:  50.00%, tr:  10.21%, tr_best:  49.95%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss:  3.166918/  2.772368, val:  10.00%, val_best:  50.00%, tr:  10.42%, tr_best:  49.95%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss:  3.023106/  2.707034, val:  10.00%, val_best:  50.00%, tr:   9.30%, tr_best:  49.95%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss:  3.145911/  4.477373, val:  10.00%, val_best:  50.00%, tr:  10.01%, tr_best:  49.95%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss:  3.158449/  3.807534, val:  10.00%, val_best:  50.00%, tr:  11.13%, tr_best:  49.95%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss:  3.520727/  3.429536, val:  10.00%, val_best:  50.00%, tr:  10.73%, tr_best:  49.95%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss:  3.168890/  3.636706, val:  10.00%, val_best:  50.00%, tr:   9.30%, tr_best:  49.95%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss:  3.543999/  3.899716, val:  10.00%, val_best:  50.00%, tr:   9.30%, tr_best:  49.95%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss:  3.406322/  3.471546, val:  10.00%, val_best:  50.00%, tr:   9.60%, tr_best:  49.95%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss:  3.374718/  3.248224, val:  10.00%, val_best:  50.00%, tr:   9.50%, tr_best:  49.95%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss:  3.512528/  2.982980, val:  10.00%, val_best:  50.00%, tr:   9.30%, tr_best:  49.95%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss:  3.318402/  2.523504, val:  10.00%, val_best:  50.00%, tr:   8.68%, tr_best:  49.95%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss:  3.246454/  2.852340, val:  10.00%, val_best:  50.00%, tr:  10.32%, tr_best:  49.95%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss:  2.985152/  3.107773, val:  10.00%, val_best:  50.00%, tr:   8.48%, tr_best:  49.95%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss:  3.202154/  3.863792, val:  10.00%, val_best:  50.00%, tr:   7.56%, tr_best:  49.95%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss:  3.327231/  3.748068, val:  10.00%, val_best:  50.00%, tr:  10.52%, tr_best:  49.95%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss:  3.023015/  3.534107, val:  10.00%, val_best:  50.00%, tr:   9.09%, tr_best:  49.95%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss:  3.242347/  3.033058, val:  10.00%, val_best:  50.00%, tr:   9.50%, tr_best:  49.95%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss:  3.537479/  3.604165, val:  10.00%, val_best:  50.00%, tr:   7.87%, tr_best:  49.95%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss:  3.199092/  3.755471, val:  10.00%, val_best:  50.00%, tr:  10.42%, tr_best:  49.95%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss:  3.263157/  3.424629, val:  10.00%, val_best:  50.00%, tr:  10.01%, tr_best:  49.95%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss:  2.884272/  2.548523, val:  10.00%, val_best:  50.00%, tr:  10.73%, tr_best:  49.95%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss:  3.210443/  4.948627, val:  10.00%, val_best:  50.00%, tr:  11.03%, tr_best:  49.95%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss:  3.736871/  3.913162, val:  10.00%, val_best:  50.00%, tr:  11.75%, tr_best:  49.95%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss:  3.268713/  3.008670, val:  10.00%, val_best:  50.00%, tr:   9.60%, tr_best:  49.95%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss:  3.120319/  2.797342, val:  10.00%, val_best:  50.00%, tr:   9.30%, tr_best:  49.95%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss:  3.136763/  3.476711, val:  10.00%, val_best:  50.00%, tr:  10.52%, tr_best:  49.95%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss:  3.386845/  3.829824, val:  10.00%, val_best:  50.00%, tr:  10.32%, tr_best:  49.95%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss:  3.299558/  2.799933, val:  10.00%, val_best:  50.00%, tr:   8.68%, tr_best:  49.95%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss:  3.166450/  3.200822, val:  10.00%, val_best:  50.00%, tr:  10.01%, tr_best:  49.95%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss:  3.411447/  3.869906, val:  10.00%, val_best:  50.00%, tr:   9.40%, tr_best:  49.95%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss:  3.161962/  3.368325, val:  10.00%, val_best:  50.00%, tr:  10.93%, tr_best:  49.95%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss:  3.451601/  4.486787, val:  10.00%, val_best:  50.00%, tr:   8.78%, tr_best:  49.95%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss:  3.313795/  3.314269, val:  10.00%, val_best:  50.00%, tr:  10.73%, tr_best:  49.95%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss:  3.426551/  3.832760, val:  10.00%, val_best:  50.00%, tr:   9.50%, tr_best:  49.95%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss:  2.920837/  4.396354, val:  10.00%, val_best:  50.00%, tr:   8.78%, tr_best:  49.95%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss:  3.219856/  3.440561, val:  10.00%, val_best:  50.00%, tr:  10.62%, tr_best:  49.95%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss:  3.084810/  3.184163, val:  10.00%, val_best:  50.00%, tr:  10.21%, tr_best:  49.95%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss:  3.111179/  2.766832, val:  10.00%, val_best:  50.00%, tr:   9.19%, tr_best:  49.95%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss:  3.083113/  2.731156, val:  10.00%, val_best:  50.00%, tr:  11.64%, tr_best:  49.95%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss:  3.096156/  4.036353, val:  10.00%, val_best:  50.00%, tr:   8.89%, tr_best:  49.95%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5f5f8b56cd4d93a0956d5e6d0ebdc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆█▄▅▃▆▆▆▄▆▅▄▅▅▂▄▃▄▂▂▂▂▃▂▄▂▃▃▂▃▂▃▂▃▂▂▂▂▁▂</td></tr><tr><td>summary_val_acc</td><td>▆▇█▆▅▆▄▆█▆▄▅▃▃▂▂▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tr_acc</td><td>▇▇█▅▆▆▅▆▆▆▅▅▅▄▃▃▃▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tr_epoch_loss</td><td>▁▁▁▄▂▂▄▄▂▄▆▆▅▃▇█▅▄▅▅▅▆▅▅▅▅▅▅▆▅▅▅▅▆▅▅▄▅▆▅</td></tr><tr><td>val_acc_best</td><td>▁▃▅█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▆▇█▆▅▆▄▆█▆▄▅▃▃▂▂▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▁▁▂▂▂▃▄▂▃▄▂▅▄██▃▂▃▃▃▃▂▃▃▃▃▄▃▃▂▃▂▄▂▂▂▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.08887</td></tr><tr><td>tr_epoch_loss</td><td>3.09616</td></tr><tr><td>val_acc_best</td><td>0.5</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>4.03635</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-sweep-42</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5a2itu3r' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5a2itu3r</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_073501-5a2itu3r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: di585ty8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60e3a8b5f8741eaaae6a7af575c85de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112866969779134, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_074651-di585ty8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/di585ty8' target=\"_blank\">grateful-sweep-43</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/di585ty8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/di585ty8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  3.173728/  5.847661, val:  37.92%, val_best:  37.92%, tr:  34.73%, tr_best:  34.73%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  4.061583/  3.918132, val:  47.08%, val_best:  47.08%, tr:  46.27%, tr_best:  46.27%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  4.686545/  6.680345, val:  45.42%, val_best:  47.08%, tr:  54.55%, tr_best:  54.55%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  3.542699/  4.576387, val:  53.33%, val_best:  53.33%, tr:  61.49%, tr_best:  61.49%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  3.250935/  3.149457, val:  60.83%, val_best:  60.83%, tr:  62.92%, tr_best:  62.92%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.509311/  6.155590, val:  45.83%, val_best:  60.83%, tr:  68.23%, tr_best:  68.23%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.743471/  3.269891, val:  63.33%, val_best:  63.33%, tr:  75.18%, tr_best:  75.18%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.684102/  5.461554, val:  45.83%, val_best:  63.33%, tr:  76.81%, tr_best:  76.81%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.449179/  3.175139, val:  64.17%, val_best:  64.17%, tr:  81.61%, tr_best:  81.61%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.218494/  4.137254, val:  62.92%, val_best:  64.17%, tr:  81.51%, tr_best:  81.61%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.685176/  3.432703, val:  72.50%, val_best:  72.50%, tr:  86.72%, tr_best:  86.72%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.704230/  4.216754, val:  65.00%, val_best:  72.50%, tr:  87.03%, tr_best:  87.03%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.453939/  4.154433, val:  69.58%, val_best:  72.50%, tr:  88.97%, tr_best:  88.97%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.092797/  4.672170, val:  67.08%, val_best:  72.50%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.950081/  4.076905, val:  75.00%, val_best:  75.00%, tr:  93.26%, tr_best:  93.26%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.667212/  4.620582, val:  68.33%, val_best:  75.00%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.612488/  4.757144, val:  72.08%, val_best:  75.00%, tr:  96.83%, tr_best:  97.34%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.625808/  4.961798, val:  72.50%, val_best:  75.00%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.366667/  4.467943, val:  77.50%, val_best:  77.50%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.326145/  4.549664, val:  78.33%, val_best:  78.33%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.246732/  4.616159, val:  78.75%, val_best:  78.75%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.187984/  5.045065, val:  74.17%, val_best:  78.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.159913/  5.116596, val:  75.83%, val_best:  78.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.134351/  4.971635, val:  80.83%, val_best:  80.83%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.115835/  5.244699, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.116262/  5.333580, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.099487/  5.345318, val:  77.08%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.072954/  5.360748, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.067001/  5.481648, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.061416/  5.681935, val:  76.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.057398/  5.864509, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.048353/  5.661053, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.042465/  5.669039, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.038307/  5.714015, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.038435/  5.747601, val:  76.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.038721/  5.972870, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.034327/  5.793589, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.033494/  5.957476, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.028169/  6.222402, val:  75.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.024578/  6.090276, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.029993/  5.959130, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.028580/  6.036320, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.023318/  6.080657, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.018288/  6.015078, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.012662/  6.275677, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.014002/  6.208011, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.013105/  6.219547, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.014336/  6.205323, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.011151/  6.193948, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.007990/  6.284461, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.013331/  6.496430, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.007333/  6.380576, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.008007/  6.368557, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.008896/  6.211658, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.009710/  6.383714, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.017174/  6.429983, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.010198/  6.321775, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.011071/  6.488127, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.014902/  6.506095, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.010401/  6.351114, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.008107/  6.323163, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.005992/  6.446833, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.006601/  6.396875, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.005614/  6.505606, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.004475/  6.541361, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.003444/  6.480365, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.005302/  6.512992, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.003860/  6.461707, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.005526/  6.551620, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.004711/  6.530926, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.003873/  6.507685, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.006028/  6.563923, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.006052/  6.523612, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.005697/  6.612905, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.006335/  6.467981, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.002242/  6.562936, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.001407/  6.523856, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.002163/  6.519794, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.002153/  6.520571, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.001239/  6.456119, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.001005/  6.437550, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.002859/  6.528361, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.002349/  6.490686, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.001474/  6.484905, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.001600/  6.580136, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.000977/  6.575365, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.001170/  6.675971, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001418/  6.572380, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.001926/  6.574856, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.002202/  6.625556, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.002088/  6.692695, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000463/  6.676898, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000464/  6.621427, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.001058/  6.516422, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.003044/  6.661420, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.001697/  6.605251, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.003010/  6.604858, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000892/  6.514187, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.001718/  6.572309, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.001931/  6.668748, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.000818/  6.581323, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.001308/  6.683237, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.000884/  6.590878, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.000614/  6.601526, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.000576/  6.589227, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.000360/  6.579412, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.000432/  6.590031, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.000305/  6.585712, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.000187/  6.607311, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.000242/  6.592831, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.000160/  6.597335, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.000351/  6.587529, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.000349/  6.592518, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.000244/  6.588743, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.000325/  6.554809, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.000278/  6.578827, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.001408/  6.507680, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.000843/  6.604426, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.001536/  6.570287, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.001050/  6.553586, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.003765/  6.564752, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.002597/  6.685914, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.001603/  6.623096, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.001493/  6.627830, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.002346/  6.638641, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.000556/  6.582027, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.000927/  6.641489, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.001082/  6.605518, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.001695/  6.654520, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.001615/  6.702747, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.000840/  6.668201, val:  77.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.001653/  6.628557, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.000705/  6.616938, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.000459/  6.639805, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.000630/  6.567590, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.000363/  6.601192, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.000264/  6.569227, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.000360/  6.567277, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.000511/  6.647621, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.000259/  6.619267, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.000396/  6.627354, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.000275/  6.642106, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.000298/  6.643863, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.000203/  6.629715, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.001298/  6.652962, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.000933/  6.582615, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.000919/  6.685823, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.000930/  6.654363, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.000281/  6.680803, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.002323/  6.696195, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.000181/  6.682160, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.000535/  6.635410, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.001038/  6.702907, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.000774/  6.746323, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.000825/  6.660980, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.000416/  6.623622, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.000285/  6.600424, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.000201/  6.585046, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.000200/  6.587955, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.000188/  6.602108, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.000818/  6.577698, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.000392/  6.630932, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.001763/  6.628944, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.001055/  6.683815, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.001035/  6.647322, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.001325/  6.675393, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.004104/  6.596685, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.004913/  6.712121, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.001384/  6.662458, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.005034/  6.848414, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.004105/  6.746060, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.001537/  6.672443, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.001780/  6.738218, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.002039/  6.649772, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.001327/  6.782315, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.003128/  6.649069, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.001300/  6.696492, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.000671/  6.737513, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.000477/  6.723989, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.000400/  6.730917, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.000400/  6.693850, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.000773/  6.661886, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.000171/  6.712039, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.000130/  6.702433, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.000146/  6.691792, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.000103/  6.702522, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.000145/  6.689497, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.000136/  6.684032, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.000354/  6.752356, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.000167/  6.754088, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.000286/  6.776920, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.000465/  6.689453, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.000180/  6.722407, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.000537/  6.761927, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.001418/  6.769842, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.000796/  6.828412, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.001046/  6.775249, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.000635/  6.790260, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.001062/  6.780623, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.000853/  6.718075, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1968fe899b524e4e96574d1d616232d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▄▅█▇▇▇█▇▇▇█▇███████████▇██████████████</td></tr><tr><td>tr_acc</td><td>▁▄▆█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▇▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▄▅█▇▇▇█▇▇▇█▇███████████▇██████████████</td></tr><tr><td>val_loss</td><td>▁▆▂▃▃▄▅▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇██▇█▇██▇▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00085</td></tr><tr><td>val_acc_best</td><td>0.8125</td></tr><tr><td>val_acc_now</td><td>0.79583</td></tr><tr><td>val_loss</td><td>6.71808</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-sweep-43</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/di585ty8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/di585ty8</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_074651-di585ty8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y9mzx9i0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_080010-y9mzx9i0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y9mzx9i0' target=\"_blank\">serene-sweep-44</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y9mzx9i0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y9mzx9i0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.303353/  2.303047, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.303309/  2.302968, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.303253/  2.302910, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.303190/  2.302812, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  2.303148/  2.302564, val:  10.83%, val_best:  10.83%, tr:  10.93%, tr_best:  10.93%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  2.301729/  2.301426, val:  12.08%, val_best:  12.08%, tr:  12.97%, tr_best:  12.97%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  2.299195/  2.298213, val:  12.08%, val_best:  12.08%, tr:  13.28%, tr_best:  13.28%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  2.291955/  2.287342, val:  15.42%, val_best:  15.42%, tr:  13.69%, tr_best:  13.69%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  2.271152/  2.261273, val:  15.42%, val_best:  15.42%, tr:  13.18%, tr_best:  13.69%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  2.226244/  2.212501, val:  20.83%, val_best:  20.83%, tr:  18.28%, tr_best:  18.28%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  2.160856/  2.148658, val:  24.17%, val_best:  24.17%, tr:  22.88%, tr_best:  22.88%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  2.081878/  2.083671, val:  27.08%, val_best:  27.08%, tr:  25.13%, tr_best:  25.13%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  2.004355/  2.018250, val:  37.92%, val_best:  37.92%, tr:  30.75%, tr_best:  30.75%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  1.931864/  1.951834, val:  40.83%, val_best:  40.83%, tr:  35.44%, tr_best:  35.44%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  1.856521/  1.891581, val:  45.42%, val_best:  45.42%, tr:  42.19%, tr_best:  42.19%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  1.784443/  1.840114, val:  44.58%, val_best:  45.42%, tr:  49.44%, tr_best:  49.44%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  1.735309/  1.798832, val:  48.75%, val_best:  48.75%, tr:  49.34%, tr_best:  49.44%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  1.689108/  1.765762, val:  50.00%, val_best:  50.00%, tr:  54.24%, tr_best:  54.24%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  1.643938/  1.742528, val:  50.83%, val_best:  50.83%, tr:  54.34%, tr_best:  54.34%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  1.610696/  1.717457, val:  50.83%, val_best:  50.83%, tr:  54.55%, tr_best:  54.55%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  1.579686/  1.694072, val:  51.67%, val_best:  51.67%, tr:  57.81%, tr_best:  57.81%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  1.555190/  1.677564, val:  49.58%, val_best:  51.67%, tr:  57.51%, tr_best:  57.81%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  1.535210/  1.666557, val:  51.67%, val_best:  51.67%, tr:  57.81%, tr_best:  57.81%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  1.511258/  1.650439, val:  51.67%, val_best:  51.67%, tr:  60.16%, tr_best:  60.16%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  1.493640/  1.640906, val:  53.75%, val_best:  53.75%, tr:  59.65%, tr_best:  60.16%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  1.482951/  1.634025, val:  53.33%, val_best:  53.75%, tr:  60.37%, tr_best:  60.37%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  1.466835/  1.628738, val:  55.00%, val_best:  55.00%, tr:  60.27%, tr_best:  60.37%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  1.453756/  1.616778, val:  55.42%, val_best:  55.42%, tr:  61.18%, tr_best:  61.18%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  1.449565/  1.607248, val:  55.83%, val_best:  55.83%, tr:  62.51%, tr_best:  62.51%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  1.427397/  1.606721, val:  57.08%, val_best:  57.08%, tr:  61.80%, tr_best:  62.51%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  1.424898/  1.602538, val:  56.25%, val_best:  57.08%, tr:  62.61%, tr_best:  62.61%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  1.412566/  1.600857, val:  55.00%, val_best:  57.08%, tr:  61.08%, tr_best:  62.61%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  1.404515/  1.597133, val:  57.50%, val_best:  57.50%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  1.406695/  1.602111, val:  56.67%, val_best:  57.50%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  1.389188/  1.589326, val:  59.58%, val_best:  59.58%, tr:  63.74%, tr_best:  64.04%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  1.377225/  1.587539, val:  59.17%, val_best:  59.58%, tr:  63.02%, tr_best:  64.04%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  1.376716/  1.588454, val:  57.92%, val_best:  59.58%, tr:  64.15%, tr_best:  64.15%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  1.367405/  1.588654, val:  57.50%, val_best:  59.58%, tr:  64.35%, tr_best:  64.35%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  1.364854/  1.588690, val:  57.50%, val_best:  59.58%, tr:  64.96%, tr_best:  64.96%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  1.359916/  1.590860, val:  61.25%, val_best:  61.25%, tr:  64.25%, tr_best:  64.96%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  1.348556/  1.590686, val:  60.83%, val_best:  61.25%, tr:  63.64%, tr_best:  64.96%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  1.350938/  1.587321, val:  60.00%, val_best:  61.25%, tr:  66.91%, tr_best:  66.91%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  1.332637/  1.584820, val:  59.17%, val_best:  61.25%, tr:  65.78%, tr_best:  66.91%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  1.332459/  1.574342, val:  57.50%, val_best:  61.25%, tr:  67.62%, tr_best:  67.62%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  1.324092/  1.573550, val:  60.00%, val_best:  61.25%, tr:  65.17%, tr_best:  67.62%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  1.320506/  1.570142, val:  59.17%, val_best:  61.25%, tr:  67.21%, tr_best:  67.62%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  1.309541/  1.572553, val:  60.83%, val_best:  61.25%, tr:  66.09%, tr_best:  67.62%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  1.316571/  1.570692, val:  58.75%, val_best:  61.25%, tr:  65.78%, tr_best:  67.62%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  1.307824/  1.565314, val:  60.00%, val_best:  61.25%, tr:  68.03%, tr_best:  68.03%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  1.304996/  1.565676, val:  62.50%, val_best:  62.50%, tr:  67.93%, tr_best:  68.03%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  1.298591/  1.560013, val:  60.42%, val_best:  62.50%, tr:  66.80%, tr_best:  68.03%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  1.296293/  1.563760, val:  61.67%, val_best:  62.50%, tr:  68.64%, tr_best:  68.64%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  1.288575/  1.552332, val:  60.42%, val_best:  62.50%, tr:  67.42%, tr_best:  68.64%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  1.282843/  1.558514, val:  61.67%, val_best:  62.50%, tr:  68.74%, tr_best:  68.74%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  1.280541/  1.560846, val:  59.17%, val_best:  62.50%, tr:  71.20%, tr_best:  71.20%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  1.276508/  1.561510, val:  60.00%, val_best:  62.50%, tr:  69.66%, tr_best:  71.20%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  1.265335/  1.564762, val:  63.33%, val_best:  63.33%, tr:  71.60%, tr_best:  71.60%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  1.268821/  1.566696, val:  60.83%, val_best:  63.33%, tr:  69.25%, tr_best:  71.60%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  1.257932/  1.567964, val:  63.33%, val_best:  63.33%, tr:  68.74%, tr_best:  71.60%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  1.264013/  1.562552, val:  63.33%, val_best:  63.33%, tr:  69.87%, tr_best:  71.60%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  1.262749/  1.565485, val:  61.67%, val_best:  63.33%, tr:  68.44%, tr_best:  71.60%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  1.261458/  1.570888, val:  64.17%, val_best:  64.17%, tr:  69.15%, tr_best:  71.60%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  1.256273/  1.568869, val:  62.08%, val_best:  64.17%, tr:  70.48%, tr_best:  71.60%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  1.238891/  1.564477, val:  61.25%, val_best:  64.17%, tr:  72.11%, tr_best:  72.11%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  1.234093/  1.562087, val:  63.75%, val_best:  64.17%, tr:  71.91%, tr_best:  72.11%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  1.236586/  1.563033, val:  63.75%, val_best:  64.17%, tr:  70.58%, tr_best:  72.11%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  1.231159/  1.561305, val:  60.00%, val_best:  64.17%, tr:  72.63%, tr_best:  72.63%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  1.230751/  1.571572, val:  63.33%, val_best:  64.17%, tr:  73.34%, tr_best:  73.34%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  1.218587/  1.571409, val:  62.08%, val_best:  64.17%, tr:  69.25%, tr_best:  73.34%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  1.216304/  1.574015, val:  61.25%, val_best:  64.17%, tr:  73.14%, tr_best:  73.34%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  1.209774/  1.577664, val:  63.75%, val_best:  64.17%, tr:  73.34%, tr_best:  73.34%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  1.211007/  1.578251, val:  60.00%, val_best:  64.17%, tr:  72.01%, tr_best:  73.34%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  1.201639/  1.573281, val:  65.42%, val_best:  65.42%, tr:  71.71%, tr_best:  73.34%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  1.204836/  1.566062, val:  64.17%, val_best:  65.42%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  1.194952/  1.573144, val:  60.83%, val_best:  65.42%, tr:  73.24%, tr_best:  74.26%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  1.203347/  1.585539, val:  63.75%, val_best:  65.42%, tr:  73.14%, tr_best:  74.26%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  1.187572/  1.588841, val:  62.92%, val_best:  65.42%, tr:  74.06%, tr_best:  74.26%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  1.195896/  1.592822, val:  64.17%, val_best:  65.42%, tr:  74.46%, tr_best:  74.46%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  1.188030/  1.580097, val:  63.33%, val_best:  65.42%, tr:  74.57%, tr_best:  74.57%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  1.192601/  1.594038, val:  65.42%, val_best:  65.42%, tr:  74.06%, tr_best:  74.57%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  1.183225/  1.589199, val:  64.17%, val_best:  65.42%, tr:  73.75%, tr_best:  74.57%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  1.180521/  1.591815, val:  62.50%, val_best:  65.42%, tr:  75.28%, tr_best:  75.28%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  1.176629/  1.588209, val:  63.75%, val_best:  65.42%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  1.178070/  1.588733, val:  65.42%, val_best:  65.42%, tr:  76.92%, tr_best:  76.92%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  1.176641/  1.597672, val:  65.00%, val_best:  65.42%, tr:  77.22%, tr_best:  77.22%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  1.168399/  1.596654, val:  63.75%, val_best:  65.42%, tr:  75.79%, tr_best:  77.22%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  1.162986/  1.599309, val:  64.17%, val_best:  65.42%, tr:  74.06%, tr_best:  77.22%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  1.167488/  1.605166, val:  67.92%, val_best:  67.92%, tr:  78.45%, tr_best:  78.45%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  1.161619/  1.615841, val:  65.83%, val_best:  67.92%, tr:  75.89%, tr_best:  78.45%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  1.153026/  1.622148, val:  63.33%, val_best:  67.92%, tr:  77.43%, tr_best:  78.45%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  1.159897/  1.623825, val:  67.08%, val_best:  67.92%, tr:  76.40%, tr_best:  78.45%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  1.140037/  1.612128, val:  62.50%, val_best:  67.92%, tr:  78.04%, tr_best:  78.45%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  1.148610/  1.616543, val:  66.25%, val_best:  67.92%, tr:  75.49%, tr_best:  78.45%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  1.157556/  1.622911, val:  65.83%, val_best:  67.92%, tr:  78.24%, tr_best:  78.45%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  1.144426/  1.621920, val:  67.08%, val_best:  67.92%, tr:  77.83%, tr_best:  78.45%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  1.142516/  1.627208, val:  65.42%, val_best:  67.92%, tr:  78.24%, tr_best:  78.45%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  1.138037/  1.631016, val:  66.25%, val_best:  67.92%, tr:  79.78%, tr_best:  79.78%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  1.133631/  1.627340, val:  66.25%, val_best:  67.92%, tr:  77.43%, tr_best:  79.78%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  1.135318/  1.638843, val:  66.25%, val_best:  67.92%, tr:  77.94%, tr_best:  79.78%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  1.132354/  1.639239, val:  64.58%, val_best:  67.92%, tr:  78.45%, tr_best:  79.78%\n",
      "epoch-100 lr=['0.0001000'], tr/val_loss:  1.126266/  1.636166, val:  69.58%, val_best:  69.58%, tr:  78.14%, tr_best:  79.78%\n",
      "epoch-101 lr=['0.0001000'], tr/val_loss:  1.115570/  1.640083, val:  67.08%, val_best:  69.58%, tr:  79.98%, tr_best:  79.98%\n",
      "epoch-102 lr=['0.0001000'], tr/val_loss:  1.120079/  1.648257, val:  65.83%, val_best:  69.58%, tr:  80.49%, tr_best:  80.49%\n",
      "epoch-103 lr=['0.0001000'], tr/val_loss:  1.119369/  1.649656, val:  65.00%, val_best:  69.58%, tr:  78.35%, tr_best:  80.49%\n",
      "epoch-104 lr=['0.0001000'], tr/val_loss:  1.111765/  1.643049, val:  66.25%, val_best:  69.58%, tr:  81.31%, tr_best:  81.31%\n",
      "epoch-105 lr=['0.0001000'], tr/val_loss:  1.130286/  1.649017, val:  65.42%, val_best:  69.58%, tr:  78.14%, tr_best:  81.31%\n",
      "epoch-106 lr=['0.0001000'], tr/val_loss:  1.121139/  1.648043, val:  67.50%, val_best:  69.58%, tr:  82.33%, tr_best:  82.33%\n",
      "epoch-107 lr=['0.0001000'], tr/val_loss:  1.119485/  1.654388, val:  67.50%, val_best:  69.58%, tr:  80.59%, tr_best:  82.33%\n",
      "epoch-108 lr=['0.0001000'], tr/val_loss:  1.111901/  1.657026, val:  67.50%, val_best:  69.58%, tr:  81.92%, tr_best:  82.33%\n",
      "epoch-109 lr=['0.0001000'], tr/val_loss:  1.107487/  1.662787, val:  67.50%, val_best:  69.58%, tr:  82.02%, tr_best:  82.33%\n",
      "epoch-110 lr=['0.0001000'], tr/val_loss:  1.097456/  1.659495, val:  67.92%, val_best:  69.58%, tr:  81.72%, tr_best:  82.33%\n",
      "epoch-111 lr=['0.0001000'], tr/val_loss:  1.103791/  1.668169, val:  66.67%, val_best:  69.58%, tr:  82.43%, tr_best:  82.43%\n",
      "epoch-112 lr=['0.0001000'], tr/val_loss:  1.099117/  1.675597, val:  66.67%, val_best:  69.58%, tr:  84.17%, tr_best:  84.17%\n",
      "epoch-113 lr=['0.0001000'], tr/val_loss:  1.109065/  1.667298, val:  67.08%, val_best:  69.58%, tr:  81.51%, tr_best:  84.17%\n",
      "epoch-114 lr=['0.0001000'], tr/val_loss:  1.110694/  1.670631, val:  66.25%, val_best:  69.58%, tr:  81.72%, tr_best:  84.17%\n",
      "epoch-115 lr=['0.0001000'], tr/val_loss:  1.101926/  1.685574, val:  67.92%, val_best:  69.58%, tr:  84.17%, tr_best:  84.17%\n",
      "epoch-116 lr=['0.0001000'], tr/val_loss:  1.093905/  1.693744, val:  68.75%, val_best:  69.58%, tr:  80.49%, tr_best:  84.17%\n",
      "epoch-117 lr=['0.0001000'], tr/val_loss:  1.095894/  1.698430, val:  67.92%, val_best:  69.58%, tr:  83.15%, tr_best:  84.17%\n",
      "epoch-118 lr=['0.0001000'], tr/val_loss:  1.092098/  1.702219, val:  67.50%, val_best:  69.58%, tr:  82.64%, tr_best:  84.17%\n",
      "epoch-119 lr=['0.0001000'], tr/val_loss:  1.090492/  1.697101, val:  67.50%, val_best:  69.58%, tr:  83.55%, tr_best:  84.17%\n",
      "epoch-120 lr=['0.0001000'], tr/val_loss:  1.087542/  1.699404, val:  66.67%, val_best:  69.58%, tr:  86.01%, tr_best:  86.01%\n",
      "epoch-121 lr=['0.0001000'], tr/val_loss:  1.090651/  1.700224, val:  68.75%, val_best:  69.58%, tr:  83.04%, tr_best:  86.01%\n",
      "epoch-122 lr=['0.0001000'], tr/val_loss:  1.096934/  1.705725, val:  67.92%, val_best:  69.58%, tr:  82.64%, tr_best:  86.01%\n",
      "epoch-123 lr=['0.0001000'], tr/val_loss:  1.099536/  1.713587, val:  67.50%, val_best:  69.58%, tr:  83.86%, tr_best:  86.01%\n",
      "epoch-124 lr=['0.0001000'], tr/val_loss:  1.074682/  1.703848, val:  67.08%, val_best:  69.58%, tr:  84.37%, tr_best:  86.01%\n",
      "epoch-125 lr=['0.0001000'], tr/val_loss:  1.076584/  1.711595, val:  67.50%, val_best:  69.58%, tr:  84.68%, tr_best:  86.01%\n",
      "epoch-126 lr=['0.0001000'], tr/val_loss:  1.095187/  1.711107, val:  69.58%, val_best:  69.58%, tr:  85.80%, tr_best:  86.01%\n",
      "epoch-127 lr=['0.0001000'], tr/val_loss:  1.076566/  1.723961, val:  67.92%, val_best:  69.58%, tr:  84.58%, tr_best:  86.01%\n",
      "epoch-128 lr=['0.0001000'], tr/val_loss:  1.073155/  1.710133, val:  69.58%, val_best:  69.58%, tr:  85.50%, tr_best:  86.01%\n",
      "epoch-129 lr=['0.0001000'], tr/val_loss:  1.072219/  1.716784, val:  70.00%, val_best:  70.00%, tr:  86.31%, tr_best:  86.31%\n",
      "epoch-130 lr=['0.0001000'], tr/val_loss:  1.081658/  1.721530, val:  68.33%, val_best:  70.00%, tr:  84.58%, tr_best:  86.31%\n",
      "epoch-131 lr=['0.0001000'], tr/val_loss:  1.065588/  1.727712, val:  68.75%, val_best:  70.00%, tr:  84.58%, tr_best:  86.31%\n",
      "epoch-132 lr=['0.0001000'], tr/val_loss:  1.065744/  1.740877, val:  68.75%, val_best:  70.00%, tr:  83.45%, tr_best:  86.31%\n",
      "epoch-133 lr=['0.0001000'], tr/val_loss:  1.059073/  1.730138, val:  68.75%, val_best:  70.00%, tr:  86.62%, tr_best:  86.62%\n",
      "epoch-134 lr=['0.0001000'], tr/val_loss:  1.058785/  1.743793, val:  68.33%, val_best:  70.00%, tr:  86.11%, tr_best:  86.62%\n",
      "epoch-135 lr=['0.0001000'], tr/val_loss:  1.054638/  1.736553, val:  68.33%, val_best:  70.00%, tr:  86.41%, tr_best:  86.62%\n",
      "epoch-136 lr=['0.0001000'], tr/val_loss:  1.065468/  1.760212, val:  68.33%, val_best:  70.00%, tr:  87.13%, tr_best:  87.13%\n",
      "epoch-137 lr=['0.0001000'], tr/val_loss:  1.074566/  1.757257, val:  67.92%, val_best:  70.00%, tr:  85.80%, tr_best:  87.13%\n",
      "epoch-138 lr=['0.0001000'], tr/val_loss:  1.057763/  1.756168, val:  69.17%, val_best:  70.00%, tr:  85.80%, tr_best:  87.13%\n",
      "epoch-139 lr=['0.0001000'], tr/val_loss:  1.063040/  1.763757, val:  69.58%, val_best:  70.00%, tr:  86.72%, tr_best:  87.13%\n",
      "epoch-140 lr=['0.0001000'], tr/val_loss:  1.061666/  1.774469, val:  69.58%, val_best:  70.00%, tr:  87.23%, tr_best:  87.23%\n",
      "epoch-141 lr=['0.0001000'], tr/val_loss:  1.046440/  1.774550, val:  69.58%, val_best:  70.00%, tr:  86.41%, tr_best:  87.23%\n",
      "epoch-142 lr=['0.0001000'], tr/val_loss:  1.043723/  1.776441, val:  69.17%, val_best:  70.00%, tr:  87.64%, tr_best:  87.64%\n",
      "epoch-143 lr=['0.0001000'], tr/val_loss:  1.048660/  1.780902, val:  69.58%, val_best:  70.00%, tr:  86.93%, tr_best:  87.64%\n",
      "epoch-144 lr=['0.0001000'], tr/val_loss:  1.049231/  1.795081, val:  67.92%, val_best:  70.00%, tr:  88.87%, tr_best:  88.87%\n",
      "epoch-145 lr=['0.0001000'], tr/val_loss:  1.045486/  1.801826, val:  65.00%, val_best:  70.00%, tr:  87.44%, tr_best:  88.87%\n",
      "epoch-146 lr=['0.0001000'], tr/val_loss:  1.042458/  1.790790, val:  69.58%, val_best:  70.00%, tr:  87.54%, tr_best:  88.87%\n",
      "epoch-147 lr=['0.0001000'], tr/val_loss:  1.053126/  1.803039, val:  71.25%, val_best:  71.25%, tr:  87.64%, tr_best:  88.87%\n",
      "epoch-148 lr=['0.0001000'], tr/val_loss:  1.039119/  1.810742, val:  67.92%, val_best:  71.25%, tr:  88.25%, tr_best:  88.87%\n",
      "epoch-149 lr=['0.0001000'], tr/val_loss:  1.043203/  1.817372, val:  68.33%, val_best:  71.25%, tr:  88.66%, tr_best:  88.87%\n",
      "epoch-150 lr=['0.0001000'], tr/val_loss:  1.048694/  1.827129, val:  66.67%, val_best:  71.25%, tr:  88.87%, tr_best:  88.87%\n",
      "epoch-151 lr=['0.0001000'], tr/val_loss:  1.040030/  1.826102, val:  67.92%, val_best:  71.25%, tr:  87.95%, tr_best:  88.87%\n",
      "epoch-152 lr=['0.0001000'], tr/val_loss:  1.031806/  1.830246, val:  68.33%, val_best:  71.25%, tr:  88.15%, tr_best:  88.87%\n",
      "epoch-153 lr=['0.0001000'], tr/val_loss:  1.046855/  1.857664, val:  68.33%, val_best:  71.25%, tr:  88.25%, tr_best:  88.87%\n",
      "epoch-154 lr=['0.0001000'], tr/val_loss:  1.031747/  1.841039, val:  69.58%, val_best:  71.25%, tr:  89.07%, tr_best:  89.07%\n",
      "epoch-155 lr=['0.0001000'], tr/val_loss:  1.037771/  1.844709, val:  68.75%, val_best:  71.25%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-156 lr=['0.0001000'], tr/val_loss:  1.037729/  1.840863, val:  67.50%, val_best:  71.25%, tr:  88.36%, tr_best:  89.99%\n",
      "epoch-157 lr=['0.0001000'], tr/val_loss:  1.025022/  1.862147, val:  68.75%, val_best:  71.25%, tr:  89.38%, tr_best:  89.99%\n",
      "epoch-158 lr=['0.0001000'], tr/val_loss:  1.038601/  1.846929, val:  69.17%, val_best:  71.25%, tr:  88.66%, tr_best:  89.99%\n",
      "epoch-159 lr=['0.0001000'], tr/val_loss:  1.033367/  1.867190, val:  69.17%, val_best:  71.25%, tr:  90.19%, tr_best:  90.19%\n",
      "epoch-160 lr=['0.0001000'], tr/val_loss:  1.026907/  1.863224, val:  71.25%, val_best:  71.25%, tr:  90.09%, tr_best:  90.19%\n",
      "epoch-161 lr=['0.0001000'], tr/val_loss:  1.024995/  1.860165, val:  70.00%, val_best:  71.25%, tr:  90.70%, tr_best:  90.70%\n",
      "epoch-162 lr=['0.0001000'], tr/val_loss:  1.034556/  1.871147, val:  71.25%, val_best:  71.25%, tr:  90.09%, tr_best:  90.70%\n",
      "epoch-163 lr=['0.0001000'], tr/val_loss:  1.029979/  1.878439, val:  70.42%, val_best:  71.25%, tr:  90.30%, tr_best:  90.70%\n",
      "epoch-164 lr=['0.0001000'], tr/val_loss:  1.025518/  1.883790, val:  69.58%, val_best:  71.25%, tr:  91.32%, tr_best:  91.32%\n",
      "epoch-165 lr=['0.0001000'], tr/val_loss:  1.034772/  1.883384, val:  68.33%, val_best:  71.25%, tr:  90.40%, tr_best:  91.32%\n",
      "epoch-166 lr=['0.0001000'], tr/val_loss:  1.025663/  1.887641, val:  69.58%, val_best:  71.25%, tr:  90.40%, tr_best:  91.32%\n",
      "epoch-167 lr=['0.0001000'], tr/val_loss:  1.028548/  1.896294, val:  68.33%, val_best:  71.25%, tr:  90.30%, tr_best:  91.32%\n",
      "epoch-168 lr=['0.0001000'], tr/val_loss:  1.027408/  1.893114, val:  69.17%, val_best:  71.25%, tr:  89.99%, tr_best:  91.32%\n",
      "epoch-169 lr=['0.0001000'], tr/val_loss:  1.020504/  1.917780, val:  70.83%, val_best:  71.25%, tr:  90.70%, tr_best:  91.32%\n",
      "epoch-170 lr=['0.0001000'], tr/val_loss:  1.023055/  1.915484, val:  69.17%, val_best:  71.25%, tr:  90.19%, tr_best:  91.32%\n",
      "epoch-171 lr=['0.0001000'], tr/val_loss:  1.028336/  1.919254, val:  70.00%, val_best:  71.25%, tr:  91.01%, tr_best:  91.32%\n",
      "epoch-172 lr=['0.0001000'], tr/val_loss:  1.024330/  1.932867, val:  70.00%, val_best:  71.25%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-173 lr=['0.0001000'], tr/val_loss:  1.021419/  1.949336, val:  67.92%, val_best:  71.25%, tr:  90.91%, tr_best:  91.42%\n",
      "epoch-174 lr=['0.0001000'], tr/val_loss:  1.014005/  1.950030, val:  67.92%, val_best:  71.25%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-175 lr=['0.0001000'], tr/val_loss:  1.010397/  1.955913, val:  70.42%, val_best:  71.25%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-176 lr=['0.0001000'], tr/val_loss:  1.009595/  1.956195, val:  70.42%, val_best:  71.25%, tr:  92.34%, tr_best:  92.34%\n",
      "epoch-177 lr=['0.0001000'], tr/val_loss:  1.024668/  1.968218, val:  70.00%, val_best:  71.25%, tr:  90.60%, tr_best:  92.34%\n",
      "epoch-178 lr=['0.0001000'], tr/val_loss:  1.012040/  1.967621, val:  71.25%, val_best:  71.25%, tr:  90.91%, tr_best:  92.34%\n",
      "epoch-179 lr=['0.0001000'], tr/val_loss:  1.017467/  1.965262, val:  70.42%, val_best:  71.25%, tr:  91.11%, tr_best:  92.34%\n",
      "epoch-180 lr=['0.0001000'], tr/val_loss:  1.005965/  1.984316, val:  69.17%, val_best:  71.25%, tr:  90.60%, tr_best:  92.34%\n",
      "epoch-181 lr=['0.0001000'], tr/val_loss:  1.013373/  1.989992, val:  67.92%, val_best:  71.25%, tr:  91.73%, tr_best:  92.34%\n",
      "epoch-182 lr=['0.0001000'], tr/val_loss:  1.003854/  2.000439, val:  68.33%, val_best:  71.25%, tr:  91.32%, tr_best:  92.34%\n",
      "epoch-183 lr=['0.0001000'], tr/val_loss:  1.011492/  1.997777, val:  70.00%, val_best:  71.25%, tr:  91.42%, tr_best:  92.34%\n",
      "epoch-184 lr=['0.0001000'], tr/val_loss:  1.013059/  2.001908, val:  72.08%, val_best:  72.08%, tr:  91.42%, tr_best:  92.34%\n",
      "epoch-185 lr=['0.0001000'], tr/val_loss:  1.007102/  2.009763, val:  68.75%, val_best:  72.08%, tr:  91.93%, tr_best:  92.34%\n",
      "epoch-186 lr=['0.0001000'], tr/val_loss:  1.008590/  2.013302, val:  67.92%, val_best:  72.08%, tr:  92.75%, tr_best:  92.75%\n",
      "epoch-187 lr=['0.0001000'], tr/val_loss:  1.011500/  2.017583, val:  67.92%, val_best:  72.08%, tr:  92.13%, tr_best:  92.75%\n",
      "epoch-188 lr=['0.0001000'], tr/val_loss:  1.011765/  2.011560, val:  68.75%, val_best:  72.08%, tr:  90.81%, tr_best:  92.75%\n",
      "epoch-189 lr=['0.0001000'], tr/val_loss:  0.998926/  2.027121, val:  68.33%, val_best:  72.08%, tr:  92.13%, tr_best:  92.75%\n",
      "epoch-190 lr=['0.0001000'], tr/val_loss:  1.004511/  2.041471, val:  66.67%, val_best:  72.08%, tr:  92.44%, tr_best:  92.75%\n",
      "epoch-191 lr=['0.0001000'], tr/val_loss:  1.013189/  2.052843, val:  67.08%, val_best:  72.08%, tr:  91.42%, tr_best:  92.75%\n",
      "epoch-192 lr=['0.0001000'], tr/val_loss:  0.999439/  2.037482, val:  70.00%, val_best:  72.08%, tr:  91.62%, tr_best:  92.75%\n",
      "epoch-193 lr=['0.0001000'], tr/val_loss:  0.994288/  2.060467, val:  70.42%, val_best:  72.08%, tr:  93.56%, tr_best:  93.56%\n",
      "epoch-194 lr=['0.0001000'], tr/val_loss:  1.000304/  2.083328, val:  67.50%, val_best:  72.08%, tr:  91.11%, tr_best:  93.56%\n",
      "epoch-195 lr=['0.0001000'], tr/val_loss:  1.000498/  2.074292, val:  68.33%, val_best:  72.08%, tr:  91.22%, tr_best:  93.56%\n",
      "epoch-196 lr=['0.0001000'], tr/val_loss:  1.003147/  2.066411, val:  70.00%, val_best:  72.08%, tr:  93.16%, tr_best:  93.56%\n",
      "epoch-197 lr=['0.0001000'], tr/val_loss:  0.992321/  2.084681, val:  70.42%, val_best:  72.08%, tr:  92.54%, tr_best:  93.56%\n",
      "epoch-198 lr=['0.0001000'], tr/val_loss:  0.993756/  2.073743, val:  69.17%, val_best:  72.08%, tr:  92.65%, tr_best:  93.56%\n",
      "epoch-199 lr=['0.0001000'], tr/val_loss:  0.980708/  2.087972, val:  69.58%, val_best:  72.08%, tr:  93.56%, tr_best:  93.56%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea25626100442d2bb5497ec4a223c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▁▄▄▅▅▅▇▇▆▅▆▅▆▇▆▇▅▇█▆▇▅▅▇▆███▆▇▇████▇██</td></tr><tr><td>summary_val_acc</td><td>▁▁▂▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████▇████▇██████████</td></tr><tr><td>tr_acc</td><td>▁▁▂▄▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>tr_epoch_loss</td><td>███▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▂▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇██████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▂▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████▇████▇██████████</td></tr><tr><td>val_loss</td><td>██▇▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.93565</td></tr><tr><td>tr_epoch_loss</td><td>0.98071</td></tr><tr><td>val_acc_best</td><td>0.72083</td></tr><tr><td>val_acc_now</td><td>0.69583</td></tr><tr><td>val_loss</td><td>2.08797</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">serene-sweep-44</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y9mzx9i0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y9mzx9i0</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_080010-y9mzx9i0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gb1oi6lv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_081323-gb1oi6lv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gb1oi6lv' target=\"_blank\">exalted-sweep-45</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gb1oi6lv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gb1oi6lv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.303365/  2.303072, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.303305/  2.302870, val:  10.83%, val_best:  10.83%, tr:  10.62%, tr_best:  10.62%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.298999/  2.294487, val:  15.42%, val_best:  15.42%, tr:  14.91%, tr_best:  14.91%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.283077/  2.274230, val:  14.58%, val_best:  15.42%, tr:  15.63%, tr_best:  15.63%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  2.243052/  2.223698, val:  17.50%, val_best:  17.50%, tr:  13.79%, tr_best:  15.63%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  2.165542/  2.146511, val:  22.92%, val_best:  22.92%, tr:  24.51%, tr_best:  24.51%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  2.066669/  2.043312, val:  30.00%, val_best:  30.00%, tr:  30.13%, tr_best:  30.13%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  1.942449/  1.939585, val:  38.33%, val_best:  38.33%, tr:  35.85%, tr_best:  35.85%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  1.837240/  1.853279, val:  43.33%, val_best:  43.33%, tr:  43.11%, tr_best:  43.11%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  1.744442/  1.779928, val:  46.25%, val_best:  46.25%, tr:  45.45%, tr_best:  45.45%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  1.676486/  1.721160, val:  49.17%, val_best:  49.17%, tr:  52.50%, tr_best:  52.50%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  1.617114/  1.672831, val:  48.75%, val_best:  49.17%, tr:  53.93%, tr_best:  53.93%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  1.564360/  1.631475, val:  51.25%, val_best:  51.25%, tr:  56.08%, tr_best:  56.08%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  1.524612/  1.599016, val:  52.08%, val_best:  52.08%, tr:  57.92%, tr_best:  57.92%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  1.482623/  1.576780, val:  53.33%, val_best:  53.33%, tr:  58.94%, tr_best:  58.94%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  1.448020/  1.553829, val:  54.17%, val_best:  54.17%, tr:  59.86%, tr_best:  59.86%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  1.413649/  1.538088, val:  53.33%, val_best:  54.17%, tr:  60.16%, tr_best:  60.16%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  1.393849/  1.519889, val:  54.58%, val_best:  54.58%, tr:  62.92%, tr_best:  62.92%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  1.367412/  1.508326, val:  55.42%, val_best:  55.42%, tr:  63.74%, tr_best:  63.74%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  1.345012/  1.496933, val:  55.00%, val_best:  55.42%, tr:  62.00%, tr_best:  63.74%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  1.324297/  1.486348, val:  58.75%, val_best:  58.75%, tr:  63.02%, tr_best:  63.74%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  1.306080/  1.475520, val:  59.58%, val_best:  59.58%, tr:  65.68%, tr_best:  65.68%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  1.290483/  1.467499, val:  57.92%, val_best:  59.58%, tr:  65.47%, tr_best:  65.68%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  1.266630/  1.458648, val:  60.42%, val_best:  60.42%, tr:  67.42%, tr_best:  67.42%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  1.255344/  1.451228, val:  59.58%, val_best:  60.42%, tr:  67.11%, tr_best:  67.42%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  1.243612/  1.447288, val:  62.92%, val_best:  62.92%, tr:  68.64%, tr_best:  68.64%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  1.228994/  1.437187, val:  63.75%, val_best:  63.75%, tr:  67.72%, tr_best:  68.64%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  1.215344/  1.433996, val:  65.00%, val_best:  65.00%, tr:  68.85%, tr_best:  68.85%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  1.213678/  1.423755, val:  63.33%, val_best:  65.00%, tr:  68.74%, tr_best:  68.85%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  1.195264/  1.418196, val:  63.33%, val_best:  65.00%, tr:  69.15%, tr_best:  69.15%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  1.182186/  1.416373, val:  62.08%, val_best:  65.00%, tr:  68.74%, tr_best:  69.15%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  1.171627/  1.410001, val:  62.08%, val_best:  65.00%, tr:  67.93%, tr_best:  69.15%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  1.165075/  1.410152, val:  61.67%, val_best:  65.00%, tr:  70.17%, tr_best:  70.17%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  1.163763/  1.406527, val:  64.17%, val_best:  65.00%, tr:  70.58%, tr_best:  70.58%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  1.142412/  1.396154, val:  63.75%, val_best:  65.00%, tr:  70.48%, tr_best:  70.58%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  1.136074/  1.389012, val:  65.00%, val_best:  65.00%, tr:  70.17%, tr_best:  70.58%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  1.132284/  1.385949, val:  64.17%, val_best:  65.00%, tr:  72.22%, tr_best:  72.22%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  1.119462/  1.382063, val:  65.00%, val_best:  65.00%, tr:  71.20%, tr_best:  72.22%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  1.115415/  1.376150, val:  65.00%, val_best:  65.00%, tr:  71.81%, tr_best:  72.22%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  1.106817/  1.375093, val:  66.67%, val_best:  66.67%, tr:  73.85%, tr_best:  73.85%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  1.094048/  1.373622, val:  65.00%, val_best:  66.67%, tr:  72.11%, tr_best:  73.85%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  1.100402/  1.362475, val:  69.17%, val_best:  69.17%, tr:  74.36%, tr_best:  74.36%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  1.080898/  1.364909, val:  66.25%, val_best:  69.17%, tr:  74.36%, tr_best:  74.36%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  1.079047/  1.358713, val:  66.25%, val_best:  69.17%, tr:  75.79%, tr_best:  75.79%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  1.069864/  1.352144, val:  64.58%, val_best:  69.17%, tr:  73.54%, tr_best:  75.79%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  1.063018/  1.349965, val:  65.42%, val_best:  69.17%, tr:  75.08%, tr_best:  75.79%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  1.055015/  1.348325, val:  67.92%, val_best:  69.17%, tr:  74.26%, tr_best:  75.79%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  1.054591/  1.346014, val:  67.92%, val_best:  69.17%, tr:  73.24%, tr_best:  75.79%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  1.045547/  1.346125, val:  65.83%, val_best:  69.17%, tr:  76.71%, tr_best:  76.71%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  1.038934/  1.345426, val:  63.75%, val_best:  69.17%, tr:  75.59%, tr_best:  76.71%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  1.033425/  1.340067, val:  67.50%, val_best:  69.17%, tr:  76.30%, tr_best:  76.71%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  1.028553/  1.340512, val:  68.75%, val_best:  69.17%, tr:  76.40%, tr_best:  76.71%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  1.021913/  1.340844, val:  67.92%, val_best:  69.17%, tr:  77.53%, tr_best:  77.53%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  1.016116/  1.336254, val:  67.08%, val_best:  69.17%, tr:  77.83%, tr_best:  77.83%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  1.008222/  1.338047, val:  69.17%, val_best:  69.17%, tr:  79.06%, tr_best:  79.06%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  1.005745/  1.335512, val:  69.58%, val_best:  69.58%, tr:  78.96%, tr_best:  79.06%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  0.992790/  1.336457, val:  72.08%, val_best:  72.08%, tr:  81.72%, tr_best:  81.72%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  0.990399/  1.339727, val:  67.08%, val_best:  72.08%, tr:  79.78%, tr_best:  81.72%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  0.979602/  1.339861, val:  67.08%, val_best:  72.08%, tr:  79.37%, tr_best:  81.72%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  0.981052/  1.334659, val:  66.25%, val_best:  72.08%, tr:  81.00%, tr_best:  81.72%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  0.974325/  1.331345, val:  67.92%, val_best:  72.08%, tr:  77.94%, tr_best:  81.72%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  0.972602/  1.330987, val:  67.50%, val_best:  72.08%, tr:  80.49%, tr_best:  81.72%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  0.966111/  1.327981, val:  67.92%, val_best:  72.08%, tr:  81.10%, tr_best:  81.72%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  0.952767/  1.327638, val:  70.83%, val_best:  72.08%, tr:  83.45%, tr_best:  83.45%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  0.945397/  1.330584, val:  68.33%, val_best:  72.08%, tr:  84.07%, tr_best:  84.07%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  0.947800/  1.332003, val:  72.50%, val_best:  72.50%, tr:  82.12%, tr_best:  84.07%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  0.942580/  1.323609, val:  71.25%, val_best:  72.50%, tr:  83.66%, tr_best:  84.07%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  0.941547/  1.329083, val:  70.42%, val_best:  72.50%, tr:  84.68%, tr_best:  84.68%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  0.930590/  1.327336, val:  70.83%, val_best:  72.50%, tr:  82.33%, tr_best:  84.68%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  0.925310/  1.328016, val:  68.33%, val_best:  72.50%, tr:  84.88%, tr_best:  84.88%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  0.918930/  1.325592, val:  74.58%, val_best:  74.58%, tr:  85.70%, tr_best:  85.70%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  0.915933/  1.330619, val:  69.17%, val_best:  74.58%, tr:  84.27%, tr_best:  85.70%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  0.906752/  1.330767, val:  71.25%, val_best:  74.58%, tr:  84.37%, tr_best:  85.70%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  0.904533/  1.323609, val:  73.33%, val_best:  74.58%, tr:  85.70%, tr_best:  85.70%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  0.899245/  1.327990, val:  71.25%, val_best:  74.58%, tr:  85.29%, tr_best:  85.70%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  0.896995/  1.328469, val:  71.67%, val_best:  74.58%, tr:  86.93%, tr_best:  86.93%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  0.883688/  1.336440, val:  69.58%, val_best:  74.58%, tr:  87.44%, tr_best:  87.44%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  0.889108/  1.330948, val:  71.67%, val_best:  74.58%, tr:  87.64%, tr_best:  87.64%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  0.880793/  1.330036, val:  71.67%, val_best:  74.58%, tr:  87.13%, tr_best:  87.64%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  0.877872/  1.334246, val:  73.75%, val_best:  74.58%, tr:  86.93%, tr_best:  87.64%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  0.869530/  1.334678, val:  70.42%, val_best:  74.58%, tr:  87.03%, tr_best:  87.64%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  0.866503/  1.333080, val:  70.00%, val_best:  74.58%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  0.860085/  1.335389, val:  72.08%, val_best:  74.58%, tr:  88.36%, tr_best:  88.56%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  0.860658/  1.335689, val:  72.08%, val_best:  74.58%, tr:  89.07%, tr_best:  89.07%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  0.857893/  1.341430, val:  68.33%, val_best:  74.58%, tr:  89.27%, tr_best:  89.27%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  0.852940/  1.340077, val:  71.25%, val_best:  74.58%, tr:  89.48%, tr_best:  89.48%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  0.843293/  1.338855, val:  73.75%, val_best:  74.58%, tr:  88.46%, tr_best:  89.48%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  0.842924/  1.339151, val:  72.08%, val_best:  74.58%, tr:  89.68%, tr_best:  89.68%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  0.833221/  1.344723, val:  69.58%, val_best:  74.58%, tr:  90.40%, tr_best:  90.40%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  0.826275/  1.356261, val:  71.25%, val_best:  74.58%, tr:  90.40%, tr_best:  90.40%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  0.827378/  1.343805, val:  72.50%, val_best:  74.58%, tr:  91.11%, tr_best:  91.11%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  0.813456/  1.348723, val:  72.92%, val_best:  74.58%, tr:  90.70%, tr_best:  91.11%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  0.816057/  1.349170, val:  67.50%, val_best:  74.58%, tr:  91.11%, tr_best:  91.11%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  0.815214/  1.345262, val:  72.08%, val_best:  74.58%, tr:  92.03%, tr_best:  92.03%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  0.809079/  1.343837, val:  72.92%, val_best:  74.58%, tr:  90.91%, tr_best:  92.03%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  0.799458/  1.345295, val:  70.83%, val_best:  74.58%, tr:  91.62%, tr_best:  92.03%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  0.800433/  1.348427, val:  71.25%, val_best:  74.58%, tr:  92.03%, tr_best:  92.03%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  0.791876/  1.346117, val:  74.17%, val_best:  74.58%, tr:  90.60%, tr_best:  92.03%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  0.791526/  1.343747, val:  72.92%, val_best:  74.58%, tr:  91.42%, tr_best:  92.03%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  0.788958/  1.348124, val:  70.83%, val_best:  74.58%, tr:  91.32%, tr_best:  92.03%\n",
      "epoch-100 lr=['0.0001000'], tr/val_loss:  0.780880/  1.341631, val:  73.75%, val_best:  74.58%, tr:  92.13%, tr_best:  92.13%\n",
      "epoch-101 lr=['0.0001000'], tr/val_loss:  0.772873/  1.346654, val:  72.50%, val_best:  74.58%, tr:  93.56%, tr_best:  93.56%\n",
      "epoch-102 lr=['0.0001000'], tr/val_loss:  0.771976/  1.347807, val:  72.50%, val_best:  74.58%, tr:  92.95%, tr_best:  93.56%\n",
      "epoch-103 lr=['0.0001000'], tr/val_loss:  0.770992/  1.348041, val:  73.33%, val_best:  74.58%, tr:  92.65%, tr_best:  93.56%\n",
      "epoch-104 lr=['0.0001000'], tr/val_loss:  0.764678/  1.341781, val:  73.75%, val_best:  74.58%, tr:  92.85%, tr_best:  93.56%\n",
      "epoch-105 lr=['0.0001000'], tr/val_loss:  0.773342/  1.350052, val:  71.67%, val_best:  74.58%, tr:  92.34%, tr_best:  93.56%\n",
      "epoch-106 lr=['0.0001000'], tr/val_loss:  0.763685/  1.340515, val:  72.50%, val_best:  74.58%, tr:  92.65%, tr_best:  93.56%\n",
      "epoch-107 lr=['0.0001000'], tr/val_loss:  0.756422/  1.356255, val:  70.42%, val_best:  74.58%, tr:  92.95%, tr_best:  93.56%\n",
      "epoch-108 lr=['0.0001000'], tr/val_loss:  0.749827/  1.355455, val:  72.08%, val_best:  74.58%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-109 lr=['0.0001000'], tr/val_loss:  0.744663/  1.355674, val:  71.25%, val_best:  74.58%, tr:  93.56%, tr_best:  93.77%\n",
      "epoch-110 lr=['0.0001000'], tr/val_loss:  0.738319/  1.356156, val:  72.08%, val_best:  74.58%, tr:  93.36%, tr_best:  93.77%\n",
      "epoch-111 lr=['0.0001000'], tr/val_loss:  0.735472/  1.351273, val:  72.08%, val_best:  74.58%, tr:  94.38%, tr_best:  94.38%\n",
      "epoch-112 lr=['0.0001000'], tr/val_loss:  0.730003/  1.355521, val:  71.25%, val_best:  74.58%, tr:  94.08%, tr_best:  94.38%\n",
      "epoch-113 lr=['0.0001000'], tr/val_loss:  0.733169/  1.360125, val:  73.33%, val_best:  74.58%, tr:  94.08%, tr_best:  94.38%\n",
      "epoch-114 lr=['0.0001000'], tr/val_loss:  0.731874/  1.350295, val:  75.00%, val_best:  75.00%, tr:  94.08%, tr_best:  94.38%\n",
      "epoch-115 lr=['0.0001000'], tr/val_loss:  0.721478/  1.354714, val:  70.00%, val_best:  75.00%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-116 lr=['0.0001000'], tr/val_loss:  0.713983/  1.357025, val:  74.17%, val_best:  75.00%, tr:  93.77%, tr_best:  95.10%\n",
      "epoch-117 lr=['0.0001000'], tr/val_loss:  0.716623/  1.357335, val:  73.75%, val_best:  75.00%, tr:  94.89%, tr_best:  95.10%\n",
      "epoch-118 lr=['0.0001000'], tr/val_loss:  0.710485/  1.370552, val:  71.67%, val_best:  75.00%, tr:  94.59%, tr_best:  95.10%\n",
      "epoch-119 lr=['0.0001000'], tr/val_loss:  0.705830/  1.365121, val:  73.75%, val_best:  75.00%, tr:  94.79%, tr_best:  95.10%\n",
      "epoch-120 lr=['0.0001000'], tr/val_loss:  0.697396/  1.363002, val:  71.25%, val_best:  75.00%, tr:  94.79%, tr_best:  95.10%\n",
      "epoch-121 lr=['0.0001000'], tr/val_loss:  0.699183/  1.363808, val:  72.92%, val_best:  75.00%, tr:  94.48%, tr_best:  95.10%\n",
      "epoch-122 lr=['0.0001000'], tr/val_loss:  0.704605/  1.360758, val:  75.00%, val_best:  75.00%, tr:  94.59%, tr_best:  95.10%\n",
      "epoch-123 lr=['0.0001000'], tr/val_loss:  0.697455/  1.370753, val:  70.42%, val_best:  75.00%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-124 lr=['0.0001000'], tr/val_loss:  0.682661/  1.369772, val:  73.33%, val_best:  75.00%, tr:  95.20%, tr_best:  95.51%\n",
      "epoch-125 lr=['0.0001000'], tr/val_loss:  0.683045/  1.370840, val:  72.50%, val_best:  75.00%, tr:  94.99%, tr_best:  95.51%\n",
      "epoch-126 lr=['0.0001000'], tr/val_loss:  0.694614/  1.381523, val:  71.25%, val_best:  75.00%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-127 lr=['0.0001000'], tr/val_loss:  0.682021/  1.382281, val:  72.08%, val_best:  75.00%, tr:  95.10%, tr_best:  95.71%\n",
      "epoch-128 lr=['0.0001000'], tr/val_loss:  0.675185/  1.377176, val:  74.17%, val_best:  75.00%, tr:  95.51%, tr_best:  95.71%\n",
      "epoch-129 lr=['0.0001000'], tr/val_loss:  0.672669/  1.376630, val:  74.58%, val_best:  75.00%, tr:  95.51%, tr_best:  95.71%\n",
      "epoch-130 lr=['0.0001000'], tr/val_loss:  0.677830/  1.377210, val:  74.58%, val_best:  75.00%, tr:  95.51%, tr_best:  95.71%\n",
      "epoch-131 lr=['0.0001000'], tr/val_loss:  0.664949/  1.377730, val:  75.00%, val_best:  75.00%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-132 lr=['0.0001000'], tr/val_loss:  0.659906/  1.389668, val:  73.75%, val_best:  75.00%, tr:  95.61%, tr_best:  95.91%\n",
      "epoch-133 lr=['0.0001000'], tr/val_loss:  0.653236/  1.384945, val:  74.17%, val_best:  75.00%, tr:  95.61%, tr_best:  95.91%\n",
      "epoch-134 lr=['0.0001000'], tr/val_loss:  0.651146/  1.386113, val:  74.58%, val_best:  75.00%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-135 lr=['0.0001000'], tr/val_loss:  0.650043/  1.383677, val:  75.42%, val_best:  75.42%, tr:  95.30%, tr_best:  95.91%\n",
      "epoch-136 lr=['0.0001000'], tr/val_loss:  0.651484/  1.397574, val:  73.33%, val_best:  75.42%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-137 lr=['0.0001000'], tr/val_loss:  0.655020/  1.390609, val:  75.42%, val_best:  75.42%, tr:  95.91%, tr_best:  96.12%\n",
      "epoch-138 lr=['0.0001000'], tr/val_loss:  0.637268/  1.391063, val:  72.92%, val_best:  75.42%, tr:  96.02%, tr_best:  96.12%\n",
      "epoch-139 lr=['0.0001000'], tr/val_loss:  0.637870/  1.399194, val:  74.17%, val_best:  75.42%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-140 lr=['0.0001000'], tr/val_loss:  0.636614/  1.407658, val:  73.75%, val_best:  75.42%, tr:  95.91%, tr_best:  96.22%\n",
      "epoch-141 lr=['0.0001000'], tr/val_loss:  0.630567/  1.408102, val:  75.00%, val_best:  75.42%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-142 lr=['0.0001000'], tr/val_loss:  0.623834/  1.410258, val:  74.17%, val_best:  75.42%, tr:  96.22%, tr_best:  96.32%\n",
      "epoch-143 lr=['0.0001000'], tr/val_loss:  0.627496/  1.404532, val:  75.83%, val_best:  75.83%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-144 lr=['0.0001000'], tr/val_loss:  0.622728/  1.417952, val:  73.75%, val_best:  75.83%, tr:  96.63%, tr_best:  96.63%\n",
      "epoch-145 lr=['0.0001000'], tr/val_loss:  0.623234/  1.420210, val:  72.08%, val_best:  75.83%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-146 lr=['0.0001000'], tr/val_loss:  0.619368/  1.416831, val:  74.58%, val_best:  75.83%, tr:  96.42%, tr_best:  97.04%\n",
      "epoch-147 lr=['0.0001000'], tr/val_loss:  0.623487/  1.415884, val:  75.00%, val_best:  75.83%, tr:  96.73%, tr_best:  97.04%\n",
      "epoch-148 lr=['0.0001000'], tr/val_loss:  0.610221/  1.427136, val:  73.75%, val_best:  75.83%, tr:  96.94%, tr_best:  97.04%\n",
      "epoch-149 lr=['0.0001000'], tr/val_loss:  0.606174/  1.428146, val:  73.33%, val_best:  75.83%, tr:  96.42%, tr_best:  97.04%\n",
      "epoch-150 lr=['0.0001000'], tr/val_loss:  0.606835/  1.416963, val:  74.17%, val_best:  75.83%, tr:  96.94%, tr_best:  97.04%\n",
      "epoch-151 lr=['0.0001000'], tr/val_loss:  0.601295/  1.432297, val:  75.00%, val_best:  75.83%, tr:  96.83%, tr_best:  97.04%\n",
      "epoch-152 lr=['0.0001000'], tr/val_loss:  0.596163/  1.420023, val:  74.17%, val_best:  75.83%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-153 lr=['0.0001000'], tr/val_loss:  0.601189/  1.435815, val:  75.00%, val_best:  75.83%, tr:  97.24%, tr_best:  97.34%\n",
      "epoch-154 lr=['0.0001000'], tr/val_loss:  0.592125/  1.429270, val:  75.00%, val_best:  75.83%, tr:  97.24%, tr_best:  97.34%\n",
      "epoch-155 lr=['0.0001000'], tr/val_loss:  0.590502/  1.434188, val:  73.75%, val_best:  75.83%, tr:  96.83%, tr_best:  97.34%\n",
      "epoch-156 lr=['0.0001000'], tr/val_loss:  0.590324/  1.437136, val:  75.42%, val_best:  75.83%, tr:  97.14%, tr_best:  97.34%\n",
      "epoch-157 lr=['0.0001000'], tr/val_loss:  0.582575/  1.442418, val:  72.08%, val_best:  75.83%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-158 lr=['0.0001000'], tr/val_loss:  0.592728/  1.438009, val:  74.17%, val_best:  75.83%, tr:  97.24%, tr_best:  97.34%\n",
      "epoch-159 lr=['0.0001000'], tr/val_loss:  0.582028/  1.438233, val:  76.25%, val_best:  76.25%, tr:  97.24%, tr_best:  97.34%\n",
      "epoch-160 lr=['0.0001000'], tr/val_loss:  0.575904/  1.446737, val:  75.83%, val_best:  76.25%, tr:  97.24%, tr_best:  97.34%\n",
      "epoch-161 lr=['0.0001000'], tr/val_loss:  0.574365/  1.446598, val:  76.25%, val_best:  76.25%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-162 lr=['0.0001000'], tr/val_loss:  0.576695/  1.451664, val:  76.67%, val_best:  76.67%, tr:  97.34%, tr_best:  97.45%\n",
      "epoch-163 lr=['0.0001000'], tr/val_loss:  0.571896/  1.448011, val:  76.25%, val_best:  76.67%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-164 lr=['0.0001000'], tr/val_loss:  0.568947/  1.456340, val:  75.83%, val_best:  76.67%, tr:  97.45%, tr_best:  97.96%\n",
      "epoch-165 lr=['0.0001000'], tr/val_loss:  0.570662/  1.453115, val:  75.42%, val_best:  76.67%, tr:  97.65%, tr_best:  97.96%\n",
      "epoch-166 lr=['0.0001000'], tr/val_loss:  0.560147/  1.459187, val:  75.83%, val_best:  76.67%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-167 lr=['0.0001000'], tr/val_loss:  0.560987/  1.461589, val:  76.67%, val_best:  76.67%, tr:  97.55%, tr_best:  97.96%\n",
      "epoch-168 lr=['0.0001000'], tr/val_loss:  0.559610/  1.459413, val:  76.25%, val_best:  76.67%, tr:  97.65%, tr_best:  97.96%\n",
      "epoch-169 lr=['0.0001000'], tr/val_loss:  0.551419/  1.462378, val:  75.83%, val_best:  76.67%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-170 lr=['0.0001000'], tr/val_loss:  0.552395/  1.469908, val:  76.25%, val_best:  76.67%, tr:  97.65%, tr_best:  97.96%\n",
      "epoch-171 lr=['0.0001000'], tr/val_loss:  0.552473/  1.467414, val:  77.92%, val_best:  77.92%, tr:  97.55%, tr_best:  97.96%\n",
      "epoch-172 lr=['0.0001000'], tr/val_loss:  0.545460/  1.476395, val:  76.67%, val_best:  77.92%, tr:  97.65%, tr_best:  97.96%\n",
      "epoch-173 lr=['0.0001000'], tr/val_loss:  0.545031/  1.479427, val:  76.67%, val_best:  77.92%, tr:  97.75%, tr_best:  97.96%\n",
      "epoch-174 lr=['0.0001000'], tr/val_loss:  0.538695/  1.473517, val:  77.08%, val_best:  77.92%, tr:  97.75%, tr_best:  97.96%\n",
      "epoch-175 lr=['0.0001000'], tr/val_loss:  0.538325/  1.477801, val:  76.67%, val_best:  77.92%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-176 lr=['0.0001000'], tr/val_loss:  0.538133/  1.474560, val:  75.00%, val_best:  77.92%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-177 lr=['0.0001000'], tr/val_loss:  0.542408/  1.475093, val:  76.67%, val_best:  77.92%, tr:  98.16%, tr_best:  98.26%\n",
      "epoch-178 lr=['0.0001000'], tr/val_loss:  0.536821/  1.479945, val:  75.42%, val_best:  77.92%, tr:  97.75%, tr_best:  98.26%\n",
      "epoch-179 lr=['0.0001000'], tr/val_loss:  0.535594/  1.484581, val:  76.25%, val_best:  77.92%, tr:  98.06%, tr_best:  98.26%\n",
      "epoch-180 lr=['0.0001000'], tr/val_loss:  0.521291/  1.488203, val:  76.25%, val_best:  77.92%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-181 lr=['0.0001000'], tr/val_loss:  0.526421/  1.493320, val:  75.00%, val_best:  77.92%, tr:  98.16%, tr_best:  98.37%\n",
      "epoch-182 lr=['0.0001000'], tr/val_loss:  0.523690/  1.500245, val:  75.42%, val_best:  77.92%, tr:  98.26%, tr_best:  98.37%\n",
      "epoch-183 lr=['0.0001000'], tr/val_loss:  0.522339/  1.493209, val:  76.67%, val_best:  77.92%, tr:  98.16%, tr_best:  98.37%\n",
      "epoch-184 lr=['0.0001000'], tr/val_loss:  0.519031/  1.494536, val:  77.50%, val_best:  77.92%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-185 lr=['0.0001000'], tr/val_loss:  0.516053/  1.496979, val:  76.67%, val_best:  77.92%, tr:  98.16%, tr_best:  98.47%\n",
      "epoch-186 lr=['0.0001000'], tr/val_loss:  0.518054/  1.502870, val:  77.08%, val_best:  77.92%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-187 lr=['0.0001000'], tr/val_loss:  0.512419/  1.507799, val:  77.08%, val_best:  77.92%, tr:  98.37%, tr_best:  98.67%\n",
      "epoch-188 lr=['0.0001000'], tr/val_loss:  0.509277/  1.501734, val:  76.67%, val_best:  77.92%, tr:  98.47%, tr_best:  98.67%\n",
      "epoch-189 lr=['0.0001000'], tr/val_loss:  0.502281/  1.509312, val:  77.08%, val_best:  77.92%, tr:  98.57%, tr_best:  98.67%\n",
      "epoch-190 lr=['0.0001000'], tr/val_loss:  0.505099/  1.510922, val:  76.25%, val_best:  77.92%, tr:  98.37%, tr_best:  98.67%\n",
      "epoch-191 lr=['0.0001000'], tr/val_loss:  0.505935/  1.506422, val:  77.92%, val_best:  77.92%, tr:  98.57%, tr_best:  98.67%\n",
      "epoch-192 lr=['0.0001000'], tr/val_loss:  0.496934/  1.504373, val:  77.08%, val_best:  77.92%, tr:  98.26%, tr_best:  98.67%\n",
      "epoch-193 lr=['0.0001000'], tr/val_loss:  0.495136/  1.504749, val:  76.67%, val_best:  77.92%, tr:  98.57%, tr_best:  98.67%\n",
      "epoch-194 lr=['0.0001000'], tr/val_loss:  0.492709/  1.515761, val:  77.50%, val_best:  77.92%, tr:  98.37%, tr_best:  98.67%\n",
      "epoch-195 lr=['0.0001000'], tr/val_loss:  0.491403/  1.511217, val:  77.50%, val_best:  77.92%, tr:  98.37%, tr_best:  98.67%\n",
      "epoch-196 lr=['0.0001000'], tr/val_loss:  0.489711/  1.518883, val:  75.42%, val_best:  77.92%, tr:  98.37%, tr_best:  98.67%\n",
      "epoch-197 lr=['0.0001000'], tr/val_loss:  0.485657/  1.510592, val:  76.25%, val_best:  77.92%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-198 lr=['0.0001000'], tr/val_loss:  0.489309/  1.513419, val:  75.00%, val_best:  77.92%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-199 lr=['0.0001000'], tr/val_loss:  0.474680/  1.529316, val:  76.67%, val_best:  77.92%, tr:  98.77%, tr_best:  98.77%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b93d5b5f6ea47799d932cd26c27712a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▃▅▄▅▅▇▆▆▅▅▆▇▇█▇█▇████▆▇███████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▅▆▆▆▆▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇█▇████▇██████████</td></tr><tr><td>tr_acc</td><td>▁▂▄▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▆▅▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▅▆▆▆▇▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▅▆▆▆▆▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇█▇████▇██████████</td></tr><tr><td>val_loss</td><td>█▇▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98774</td></tr><tr><td>tr_epoch_loss</td><td>0.47468</td></tr><tr><td>val_acc_best</td><td>0.77917</td></tr><tr><td>val_acc_now</td><td>0.76667</td></tr><tr><td>val_loss</td><td>1.52932</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-sweep-45</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gb1oi6lv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gb1oi6lv</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_081323-gb1oi6lv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qoso35j0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_082634-qoso35j0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qoso35j0' target=\"_blank\">cerulean-sweep-46</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qoso35j0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qoso35j0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.303382/  2.303132, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.303225/  2.302214, val:  11.67%, val_best:  11.67%, tr:  10.83%, tr_best:  10.83%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.297444/  2.290537, val:  14.17%, val_best:  14.17%, tr:  14.61%, tr_best:  14.61%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.266434/  2.246359, val:  15.00%, val_best:  15.00%, tr:  14.40%, tr_best:  14.61%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  2.178614/  2.139522, val:  22.50%, val_best:  22.50%, tr:  19.41%, tr_best:  19.41%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  2.028106/  2.001099, val:  37.08%, val_best:  37.08%, tr:  29.32%, tr_best:  29.32%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  1.872483/  1.862229, val:  40.42%, val_best:  40.42%, tr:  39.43%, tr_best:  39.43%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  1.721922/  1.759253, val:  41.25%, val_best:  41.25%, tr:  47.40%, tr_best:  47.40%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  1.622042/  1.676338, val:  47.08%, val_best:  47.08%, tr:  50.97%, tr_best:  50.97%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  1.536817/  1.611838, val:  47.50%, val_best:  47.50%, tr:  53.52%, tr_best:  53.52%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  1.472512/  1.560198, val:  51.67%, val_best:  51.67%, tr:  57.30%, tr_best:  57.30%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  1.419843/  1.519324, val:  51.67%, val_best:  51.67%, tr:  58.43%, tr_best:  58.43%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  1.380463/  1.493334, val:  50.42%, val_best:  51.67%, tr:  59.65%, tr_best:  59.65%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  1.344212/  1.472414, val:  52.92%, val_best:  52.92%, tr:  61.59%, tr_best:  61.59%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  1.305204/  1.453113, val:  55.83%, val_best:  55.83%, tr:  63.13%, tr_best:  63.13%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  1.275473/  1.434852, val:  56.25%, val_best:  56.25%, tr:  62.92%, tr_best:  63.13%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  1.243590/  1.417712, val:  57.08%, val_best:  57.08%, tr:  63.84%, tr_best:  63.84%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  1.230014/  1.407654, val:  57.08%, val_best:  57.08%, tr:  64.25%, tr_best:  64.25%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  1.206864/  1.400279, val:  57.50%, val_best:  57.50%, tr:  65.47%, tr_best:  65.47%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  1.186415/  1.389224, val:  57.92%, val_best:  57.92%, tr:  64.35%, tr_best:  65.47%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  1.165530/  1.379525, val:  59.58%, val_best:  59.58%, tr:  66.80%, tr_best:  66.80%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  1.147698/  1.375567, val:  61.25%, val_best:  61.25%, tr:  66.19%, tr_best:  66.80%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  1.132217/  1.378057, val:  58.75%, val_best:  61.25%, tr:  66.09%, tr_best:  66.80%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  1.117267/  1.369043, val:  58.75%, val_best:  61.25%, tr:  69.87%, tr_best:  69.87%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  1.106468/  1.365104, val:  60.83%, val_best:  61.25%, tr:  69.56%, tr_best:  69.87%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  1.089782/  1.364174, val:  60.00%, val_best:  61.25%, tr:  70.58%, tr_best:  70.58%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  1.078291/  1.350760, val:  60.83%, val_best:  61.25%, tr:  68.54%, tr_best:  70.58%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  1.061885/  1.346232, val:  60.42%, val_best:  61.25%, tr:  72.32%, tr_best:  72.32%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  1.068382/  1.333494, val:  64.17%, val_best:  64.17%, tr:  70.38%, tr_best:  72.32%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  1.046329/  1.336873, val:  65.83%, val_best:  65.83%, tr:  71.50%, tr_best:  72.32%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  1.035426/  1.336843, val:  62.50%, val_best:  65.83%, tr:  72.52%, tr_best:  72.52%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  1.023172/  1.327861, val:  60.83%, val_best:  65.83%, tr:  71.30%, tr_best:  72.52%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  1.010118/  1.330742, val:  62.92%, val_best:  65.83%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  1.011967/  1.329998, val:  63.33%, val_best:  65.83%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  0.994423/  1.317282, val:  65.83%, val_best:  65.83%, tr:  75.28%, tr_best:  75.28%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  0.982102/  1.316993, val:  64.17%, val_best:  65.83%, tr:  74.46%, tr_best:  75.28%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  0.974445/  1.317164, val:  64.17%, val_best:  65.83%, tr:  73.34%, tr_best:  75.28%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  0.965227/  1.310574, val:  65.83%, val_best:  65.83%, tr:  75.28%, tr_best:  75.28%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  0.964672/  1.320603, val:  65.00%, val_best:  65.83%, tr:  76.92%, tr_best:  76.92%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  0.958004/  1.319064, val:  64.58%, val_best:  65.83%, tr:  77.02%, tr_best:  77.02%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  0.938915/  1.318045, val:  66.25%, val_best:  66.25%, tr:  76.61%, tr_best:  77.02%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  0.939869/  1.315748, val:  66.67%, val_best:  66.67%, tr:  79.78%, tr_best:  79.78%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  0.919930/  1.304771, val:  68.33%, val_best:  68.33%, tr:  79.67%, tr_best:  79.78%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  0.917817/  1.310871, val:  65.42%, val_best:  68.33%, tr:  78.75%, tr_best:  79.78%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  0.907147/  1.310059, val:  65.00%, val_best:  68.33%, tr:  78.75%, tr_best:  79.78%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  0.901629/  1.311241, val:  66.67%, val_best:  68.33%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  0.889750/  1.315528, val:  65.42%, val_best:  68.33%, tr:  81.92%, tr_best:  81.92%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  0.895491/  1.311031, val:  65.00%, val_best:  68.33%, tr:  78.65%, tr_best:  81.92%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  0.879733/  1.307606, val:  65.42%, val_best:  68.33%, tr:  82.12%, tr_best:  82.12%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  0.873007/  1.304902, val:  65.00%, val_best:  68.33%, tr:  80.90%, tr_best:  82.12%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  0.867636/  1.311212, val:  65.00%, val_best:  68.33%, tr:  83.25%, tr_best:  83.25%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  0.857803/  1.309469, val:  67.08%, val_best:  68.33%, tr:  81.10%, tr_best:  83.25%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  0.849964/  1.316954, val:  70.00%, val_best:  70.00%, tr:  83.76%, tr_best:  83.76%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  0.842856/  1.317445, val:  68.75%, val_best:  70.00%, tr:  83.76%, tr_best:  83.76%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  0.836078/  1.326324, val:  68.33%, val_best:  70.00%, tr:  85.09%, tr_best:  85.09%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  0.831354/  1.324689, val:  66.25%, val_best:  70.00%, tr:  85.50%, tr_best:  85.50%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  0.811129/  1.326104, val:  68.75%, val_best:  70.00%, tr:  86.31%, tr_best:  86.31%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  0.813035/  1.338969, val:  67.08%, val_best:  70.00%, tr:  86.01%, tr_best:  86.31%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  0.804106/  1.340608, val:  65.83%, val_best:  70.00%, tr:  85.80%, tr_best:  86.31%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  0.802026/  1.333451, val:  67.92%, val_best:  70.00%, tr:  86.21%, tr_best:  86.31%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  0.795814/  1.335698, val:  68.75%, val_best:  70.00%, tr:  86.01%, tr_best:  86.31%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  0.793758/  1.334912, val:  69.58%, val_best:  70.00%, tr:  87.84%, tr_best:  87.84%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  0.787273/  1.337984, val:  67.50%, val_best:  70.00%, tr:  87.74%, tr_best:  87.84%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  0.780113/  1.333569, val:  69.17%, val_best:  70.00%, tr:  88.36%, tr_best:  88.36%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  0.768682/  1.347835, val:  70.00%, val_best:  70.00%, tr:  89.27%, tr_best:  89.27%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  0.767133/  1.350006, val:  65.42%, val_best:  70.00%, tr:  88.15%, tr_best:  89.27%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  0.763678/  1.350257, val:  67.92%, val_best:  70.00%, tr:  89.48%, tr_best:  89.48%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  0.762038/  1.361773, val:  68.75%, val_best:  70.00%, tr:  89.58%, tr_best:  89.58%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  0.748385/  1.363944, val:  67.08%, val_best:  70.00%, tr:  88.05%, tr_best:  89.58%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  0.739710/  1.365551, val:  69.58%, val_best:  70.00%, tr:  89.68%, tr_best:  89.68%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  0.734566/  1.369692, val:  68.33%, val_best:  70.00%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  0.731861/  1.373981, val:  70.00%, val_best:  70.00%, tr:  90.19%, tr_best:  90.19%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  0.720416/  1.389256, val:  68.75%, val_best:  70.00%, tr:  90.81%, tr_best:  90.81%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  0.718741/  1.381262, val:  66.67%, val_best:  70.00%, tr:  91.32%, tr_best:  91.32%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  0.710891/  1.387109, val:  69.58%, val_best:  70.00%, tr:  90.91%, tr_best:  91.32%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  0.710715/  1.393453, val:  70.00%, val_best:  70.00%, tr:  91.11%, tr_best:  91.32%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  0.699534/  1.400539, val:  70.00%, val_best:  70.00%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  0.707396/  1.412755, val:  68.75%, val_best:  70.00%, tr:  90.30%, tr_best:  91.83%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  0.702048/  1.404834, val:  67.92%, val_best:  70.00%, tr:  91.32%, tr_best:  91.83%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  0.692477/  1.416440, val:  70.00%, val_best:  70.00%, tr:  90.81%, tr_best:  91.83%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  0.688384/  1.409424, val:  68.33%, val_best:  70.00%, tr:  91.32%, tr_best:  91.83%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  0.686467/  1.419257, val:  68.33%, val_best:  70.00%, tr:  92.24%, tr_best:  92.24%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  0.678658/  1.426061, val:  69.58%, val_best:  70.00%, tr:  92.03%, tr_best:  92.24%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  0.682792/  1.426440, val:  69.17%, val_best:  70.00%, tr:  92.65%, tr_best:  92.65%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  0.672908/  1.434986, val:  66.25%, val_best:  70.00%, tr:  93.46%, tr_best:  93.46%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  0.672615/  1.436803, val:  71.25%, val_best:  71.25%, tr:  92.34%, tr_best:  93.46%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  0.660622/  1.445535, val:  72.50%, val_best:  72.50%, tr:  92.95%, tr_best:  93.46%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  0.656580/  1.439866, val:  69.58%, val_best:  72.50%, tr:  93.46%, tr_best:  93.46%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  0.652793/  1.450587, val:  68.75%, val_best:  72.50%, tr:  93.67%, tr_best:  93.67%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  0.644026/  1.477417, val:  70.00%, val_best:  72.50%, tr:  93.05%, tr_best:  93.67%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  0.649747/  1.475335, val:  69.17%, val_best:  72.50%, tr:  93.67%, tr_best:  93.67%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  0.634109/  1.473095, val:  72.08%, val_best:  72.50%, tr:  93.87%, tr_best:  93.87%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  0.637669/  1.482193, val:  69.58%, val_best:  72.50%, tr:  93.97%, tr_best:  93.97%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  0.634752/  1.481805, val:  70.00%, val_best:  72.50%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  0.632990/  1.485822, val:  69.17%, val_best:  72.50%, tr:  94.08%, tr_best:  94.48%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  0.620763/  1.494893, val:  69.58%, val_best:  72.50%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  0.620876/  1.501029, val:  67.92%, val_best:  72.50%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  0.615139/  1.498078, val:  69.58%, val_best:  72.50%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  0.612024/  1.516759, val:  69.58%, val_best:  72.50%, tr:  94.79%, tr_best:  94.99%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  0.609937/  1.524903, val:  70.00%, val_best:  72.50%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-100 lr=['0.0001000'], tr/val_loss:  0.601249/  1.529679, val:  69.17%, val_best:  72.50%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-101 lr=['0.0001000'], tr/val_loss:  0.596797/  1.535131, val:  69.17%, val_best:  72.50%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-102 lr=['0.0001000'], tr/val_loss:  0.594808/  1.561146, val:  69.17%, val_best:  72.50%, tr:  95.40%, tr_best:  96.32%\n",
      "epoch-103 lr=['0.0001000'], tr/val_loss:  0.596064/  1.551020, val:  70.00%, val_best:  72.50%, tr:  96.12%, tr_best:  96.32%\n",
      "epoch-104 lr=['0.0001000'], tr/val_loss:  0.588939/  1.550430, val:  68.33%, val_best:  72.50%, tr:  96.12%, tr_best:  96.32%\n",
      "epoch-105 lr=['0.0001000'], tr/val_loss:  0.597763/  1.557930, val:  70.00%, val_best:  72.50%, tr:  95.81%, tr_best:  96.32%\n",
      "epoch-106 lr=['0.0001000'], tr/val_loss:  0.588401/  1.555403, val:  68.33%, val_best:  72.50%, tr:  96.12%, tr_best:  96.32%\n",
      "epoch-107 lr=['0.0001000'], tr/val_loss:  0.580303/  1.585340, val:  67.50%, val_best:  72.50%, tr:  96.12%, tr_best:  96.32%\n",
      "epoch-108 lr=['0.0001000'], tr/val_loss:  0.577164/  1.586586, val:  69.58%, val_best:  72.50%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-109 lr=['0.0001000'], tr/val_loss:  0.572915/  1.596086, val:  68.33%, val_best:  72.50%, tr:  96.12%, tr_best:  96.73%\n",
      "epoch-110 lr=['0.0001000'], tr/val_loss:  0.574671/  1.591184, val:  69.17%, val_best:  72.50%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-111 lr=['0.0001000'], tr/val_loss:  0.565305/  1.604272, val:  68.75%, val_best:  72.50%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-112 lr=['0.0001000'], tr/val_loss:  0.562165/  1.614453, val:  68.75%, val_best:  72.50%, tr:  96.94%, tr_best:  96.94%\n",
      "epoch-113 lr=['0.0001000'], tr/val_loss:  0.563322/  1.625286, val:  70.83%, val_best:  72.50%, tr:  96.83%, tr_best:  96.94%\n",
      "epoch-114 lr=['0.0001000'], tr/val_loss:  0.565848/  1.621659, val:  67.92%, val_best:  72.50%, tr:  96.73%, tr_best:  96.94%\n",
      "epoch-115 lr=['0.0001000'], tr/val_loss:  0.551369/  1.643470, val:  68.33%, val_best:  72.50%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-116 lr=['0.0001000'], tr/val_loss:  0.547854/  1.654374, val:  69.17%, val_best:  72.50%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-117 lr=['0.0001000'], tr/val_loss:  0.549270/  1.651224, val:  68.33%, val_best:  72.50%, tr:  96.83%, tr_best:  97.24%\n",
      "epoch-118 lr=['0.0001000'], tr/val_loss:  0.550394/  1.675192, val:  67.92%, val_best:  72.50%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-119 lr=['0.0001000'], tr/val_loss:  0.548507/  1.678479, val:  68.33%, val_best:  72.50%, tr:  96.94%, tr_best:  97.34%\n",
      "epoch-120 lr=['0.0001000'], tr/val_loss:  0.540671/  1.685360, val:  68.75%, val_best:  72.50%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-121 lr=['0.0001000'], tr/val_loss:  0.540834/  1.685033, val:  67.08%, val_best:  72.50%, tr:  96.63%, tr_best:  97.55%\n",
      "epoch-122 lr=['0.0001000'], tr/val_loss:  0.543732/  1.673997, val:  68.75%, val_best:  72.50%, tr:  97.24%, tr_best:  97.55%\n",
      "epoch-123 lr=['0.0001000'], tr/val_loss:  0.539942/  1.689976, val:  67.08%, val_best:  72.50%, tr:  97.45%, tr_best:  97.55%\n",
      "epoch-124 lr=['0.0001000'], tr/val_loss:  0.529070/  1.696707, val:  68.75%, val_best:  72.50%, tr:  97.45%, tr_best:  97.55%\n",
      "epoch-125 lr=['0.0001000'], tr/val_loss:  0.528686/  1.696156, val:  68.33%, val_best:  72.50%, tr:  97.34%, tr_best:  97.55%\n",
      "epoch-126 lr=['0.0001000'], tr/val_loss:  0.540715/  1.710389, val:  68.75%, val_best:  72.50%, tr:  97.34%, tr_best:  97.55%\n",
      "epoch-127 lr=['0.0001000'], tr/val_loss:  0.521558/  1.724778, val:  68.75%, val_best:  72.50%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-128 lr=['0.0001000'], tr/val_loss:  0.526620/  1.714823, val:  70.00%, val_best:  72.50%, tr:  97.55%, tr_best:  97.65%\n",
      "epoch-129 lr=['0.0001000'], tr/val_loss:  0.524035/  1.734618, val:  66.67%, val_best:  72.50%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-130 lr=['0.0001000'], tr/val_loss:  0.529316/  1.733317, val:  68.75%, val_best:  72.50%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-131 lr=['0.0001000'], tr/val_loss:  0.515273/  1.747670, val:  68.33%, val_best:  72.50%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-132 lr=['0.0001000'], tr/val_loss:  0.510919/  1.767999, val:  70.83%, val_best:  72.50%, tr:  97.65%, tr_best:  97.96%\n",
      "epoch-133 lr=['0.0001000'], tr/val_loss:  0.513705/  1.759827, val:  68.75%, val_best:  72.50%, tr:  97.65%, tr_best:  97.96%\n",
      "epoch-134 lr=['0.0001000'], tr/val_loss:  0.512443/  1.777237, val:  68.75%, val_best:  72.50%, tr:  97.85%, tr_best:  97.96%\n",
      "epoch-135 lr=['0.0001000'], tr/val_loss:  0.509429/  1.763753, val:  67.92%, val_best:  72.50%, tr:  97.55%, tr_best:  97.96%\n",
      "epoch-136 lr=['0.0001000'], tr/val_loss:  0.512883/  1.772493, val:  69.58%, val_best:  72.50%, tr:  97.85%, tr_best:  97.96%\n",
      "epoch-137 lr=['0.0001000'], tr/val_loss:  0.515150/  1.778693, val:  69.17%, val_best:  72.50%, tr:  97.85%, tr_best:  97.96%\n",
      "epoch-138 lr=['0.0001000'], tr/val_loss:  0.501827/  1.792462, val:  71.25%, val_best:  72.50%, tr:  97.85%, tr_best:  97.96%\n",
      "epoch-139 lr=['0.0001000'], tr/val_loss:  0.501301/  1.806567, val:  70.42%, val_best:  72.50%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-140 lr=['0.0001000'], tr/val_loss:  0.498772/  1.802858, val:  69.17%, val_best:  72.50%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-141 lr=['0.0001000'], tr/val_loss:  0.493576/  1.808803, val:  68.75%, val_best:  72.50%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-142 lr=['0.0001000'], tr/val_loss:  0.487032/  1.817253, val:  70.83%, val_best:  72.50%, tr:  97.96%, tr_best:  98.16%\n",
      "epoch-143 lr=['0.0001000'], tr/val_loss:  0.485957/  1.823360, val:  68.75%, val_best:  72.50%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-144 lr=['0.0001000'], tr/val_loss:  0.482499/  1.846753, val:  71.67%, val_best:  72.50%, tr:  98.16%, tr_best:  98.26%\n",
      "epoch-145 lr=['0.0001000'], tr/val_loss:  0.479690/  1.853287, val:  69.17%, val_best:  72.50%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-146 lr=['0.0001000'], tr/val_loss:  0.475294/  1.852393, val:  70.83%, val_best:  72.50%, tr:  98.16%, tr_best:  98.26%\n",
      "epoch-147 lr=['0.0001000'], tr/val_loss:  0.475531/  1.864842, val:  70.83%, val_best:  72.50%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-148 lr=['0.0001000'], tr/val_loss:  0.469829/  1.873672, val:  69.17%, val_best:  72.50%, tr:  98.16%, tr_best:  98.26%\n",
      "epoch-149 lr=['0.0001000'], tr/val_loss:  0.471473/  1.883534, val:  70.42%, val_best:  72.50%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-150 lr=['0.0001000'], tr/val_loss:  0.464516/  1.880628, val:  68.33%, val_best:  72.50%, tr:  98.16%, tr_best:  98.47%\n",
      "epoch-151 lr=['0.0001000'], tr/val_loss:  0.465165/  1.890586, val:  69.17%, val_best:  72.50%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-152 lr=['0.0001000'], tr/val_loss:  0.462516/  1.906488, val:  70.42%, val_best:  72.50%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-153 lr=['0.0001000'], tr/val_loss:  0.465688/  1.902251, val:  69.17%, val_best:  72.50%, tr:  98.47%, tr_best:  98.57%\n",
      "epoch-154 lr=['0.0001000'], tr/val_loss:  0.459700/  1.917403, val:  70.00%, val_best:  72.50%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-155 lr=['0.0001000'], tr/val_loss:  0.453628/  1.924865, val:  69.17%, val_best:  72.50%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-156 lr=['0.0001000'], tr/val_loss:  0.454804/  1.940655, val:  70.00%, val_best:  72.50%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-157 lr=['0.0001000'], tr/val_loss:  0.453023/  1.960075, val:  69.17%, val_best:  72.50%, tr:  98.37%, tr_best:  98.67%\n",
      "epoch-158 lr=['0.0001000'], tr/val_loss:  0.454960/  1.958830, val:  68.75%, val_best:  72.50%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-159 lr=['0.0001000'], tr/val_loss:  0.445878/  1.965284, val:  68.75%, val_best:  72.50%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-160 lr=['0.0001000'], tr/val_loss:  0.445187/  1.971855, val:  69.17%, val_best:  72.50%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-161 lr=['0.0001000'], tr/val_loss:  0.433947/  1.953539, val:  69.17%, val_best:  72.50%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-162 lr=['0.0001000'], tr/val_loss:  0.448394/  1.978869, val:  68.75%, val_best:  72.50%, tr:  98.57%, tr_best:  98.77%\n",
      "epoch-163 lr=['0.0001000'], tr/val_loss:  0.434027/  1.976049, val:  70.00%, val_best:  72.50%, tr:  98.67%, tr_best:  98.77%\n",
      "epoch-164 lr=['0.0001000'], tr/val_loss:  0.436347/  1.991972, val:  69.17%, val_best:  72.50%, tr:  98.57%, tr_best:  98.77%\n",
      "epoch-165 lr=['0.0001000'], tr/val_loss:  0.431828/  2.000365, val:  69.17%, val_best:  72.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-166 lr=['0.0001000'], tr/val_loss:  0.426750/  2.002121, val:  68.75%, val_best:  72.50%, tr:  98.67%, tr_best:  98.98%\n",
      "epoch-167 lr=['0.0001000'], tr/val_loss:  0.426909/  2.015903, val:  69.17%, val_best:  72.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-168 lr=['0.0001000'], tr/val_loss:  0.428693/  2.012146, val:  68.75%, val_best:  72.50%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-169 lr=['0.0001000'], tr/val_loss:  0.414004/  2.020649, val:  69.17%, val_best:  72.50%, tr:  98.98%, tr_best:  99.08%\n",
      "epoch-170 lr=['0.0001000'], tr/val_loss:  0.420794/  2.036657, val:  69.17%, val_best:  72.50%, tr:  98.98%, tr_best:  99.08%\n",
      "epoch-171 lr=['0.0001000'], tr/val_loss:  0.419340/  2.035332, val:  69.58%, val_best:  72.50%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-172 lr=['0.0001000'], tr/val_loss:  0.418901/  2.065793, val:  69.58%, val_best:  72.50%, tr:  98.98%, tr_best:  99.08%\n",
      "epoch-173 lr=['0.0001000'], tr/val_loss:  0.413192/  2.059950, val:  70.42%, val_best:  72.50%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-174 lr=['0.0001000'], tr/val_loss:  0.412248/  2.071901, val:  72.08%, val_best:  72.50%, tr:  98.98%, tr_best:  99.08%\n",
      "epoch-175 lr=['0.0001000'], tr/val_loss:  0.411321/  2.068394, val:  71.25%, val_best:  72.50%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-176 lr=['0.0001000'], tr/val_loss:  0.406150/  2.073595, val:  70.83%, val_best:  72.50%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-177 lr=['0.0001000'], tr/val_loss:  0.423053/  2.084731, val:  70.42%, val_best:  72.50%, tr:  99.28%, tr_best:  99.39%\n",
      "epoch-178 lr=['0.0001000'], tr/val_loss:  0.411354/  2.090915, val:  70.00%, val_best:  72.50%, tr:  98.77%, tr_best:  99.39%\n",
      "epoch-179 lr=['0.0001000'], tr/val_loss:  0.405951/  2.087781, val:  72.08%, val_best:  72.50%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-180 lr=['0.0001000'], tr/val_loss:  0.403934/  2.093892, val:  71.67%, val_best:  72.50%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-181 lr=['0.0001000'], tr/val_loss:  0.405013/  2.096179, val:  70.83%, val_best:  72.50%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-182 lr=['0.0001000'], tr/val_loss:  0.396883/  2.108675, val:  70.83%, val_best:  72.50%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-183 lr=['0.0001000'], tr/val_loss:  0.395011/  2.100326, val:  70.42%, val_best:  72.50%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-184 lr=['0.0001000'], tr/val_loss:  0.396608/  2.126539, val:  70.00%, val_best:  72.50%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-185 lr=['0.0001000'], tr/val_loss:  0.394372/  2.132279, val:  70.83%, val_best:  72.50%, tr:  99.28%, tr_best:  99.39%\n",
      "epoch-186 lr=['0.0001000'], tr/val_loss:  0.396526/  2.145541, val:  70.00%, val_best:  72.50%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-187 lr=['0.0001000'], tr/val_loss:  0.391942/  2.158173, val:  71.67%, val_best:  72.50%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-188 lr=['0.0001000'], tr/val_loss:  0.388710/  2.153974, val:  70.83%, val_best:  72.50%, tr:  99.28%, tr_best:  99.39%\n",
      "epoch-189 lr=['0.0001000'], tr/val_loss:  0.379253/  2.182991, val:  71.25%, val_best:  72.50%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-190 lr=['0.0001000'], tr/val_loss:  0.383398/  2.184601, val:  70.42%, val_best:  72.50%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-191 lr=['0.0001000'], tr/val_loss:  0.387135/  2.193881, val:  70.83%, val_best:  72.50%, tr:  99.28%, tr_best:  99.39%\n",
      "epoch-192 lr=['0.0001000'], tr/val_loss:  0.378830/  2.186970, val:  70.83%, val_best:  72.50%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-193 lr=['0.0001000'], tr/val_loss:  0.380903/  2.215048, val:  70.42%, val_best:  72.50%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-194 lr=['0.0001000'], tr/val_loss:  0.383802/  2.214175, val:  72.08%, val_best:  72.50%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-195 lr=['0.0001000'], tr/val_loss:  0.384856/  2.216142, val:  72.50%, val_best:  72.50%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-196 lr=['0.0001000'], tr/val_loss:  0.386136/  2.232859, val:  71.25%, val_best:  72.50%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-197 lr=['0.0001000'], tr/val_loss:  0.373370/  2.227017, val:  70.83%, val_best:  72.50%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-198 lr=['0.0001000'], tr/val_loss:  0.375073/  2.240434, val:  70.42%, val_best:  72.50%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-199 lr=['0.0001000'], tr/val_loss:  0.370347/  2.238653, val:  72.08%, val_best:  72.50%, tr:  99.59%, tr_best:  99.59%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad535fb2256484d84f94380563d4bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄▆▄▅▅▇▆█▅▆▆███▇█▇██████▇██████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▅▆▆▇▇▇▇▇▇██▇█████▇███████▇████████████</td></tr><tr><td>tr_acc</td><td>▁▂▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▅▆▆▇▇▇▇███████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▅▆▆▇▇▇▇▇▇██▇█████▇███████▇████████████</td></tr><tr><td>val_loss</td><td>█▆▃▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99591</td></tr><tr><td>tr_epoch_loss</td><td>0.37035</td></tr><tr><td>val_acc_best</td><td>0.725</td></tr><tr><td>val_acc_now</td><td>0.72083</td></tr><tr><td>val_loss</td><td>2.23865</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cerulean-sweep-46</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qoso35j0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qoso35j0</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_082634-qoso35j0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0uvl6vu6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_083922-0uvl6vu6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0uvl6vu6' target=\"_blank\">polished-sweep-47</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0uvl6vu6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0uvl6vu6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325702/  2.309367, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:   8.89%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.323408/  2.309339, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:   9.40%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.319913/  2.316170, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:   9.40%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.322621/  2.318868, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.40%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.323630/  2.316624, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:   9.40%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.314643/  2.312522, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  2.322383/  2.317189, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  2.323500/  2.311199, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.91%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  2.318820/  2.316814, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:   9.91%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.325358/  2.328855, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:   9.91%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  2.318062/  2.313319, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:   9.91%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  2.321499/  2.314362, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  2.318739/  2.304731, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.91%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  2.323381/  2.312831, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  10.52%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  2.318632/  2.314031, val:  10.00%, val_best:  10.00%, tr:  11.24%, tr_best:  11.24%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  2.334141/  2.312476, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  2.325246/  2.309235, val:  10.00%, val_best:  10.00%, tr:  10.73%, tr_best:  11.24%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  2.313498/  2.304715, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  11.24%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  2.317416/  2.315703, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  11.24%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  2.328925/  2.309338, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  11.24%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  2.325712/  2.311423, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  2.317473/  2.314049, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  11.24%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  2.321084/  2.316247, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  2.323519/  2.311760, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  11.24%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  2.315132/  2.309812, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  11.24%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  2.321132/  2.308563, val:  10.00%, val_best:  10.00%, tr:   7.56%, tr_best:  11.24%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  2.324926/  2.309820, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  11.24%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  2.315943/  2.313346, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  11.24%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  2.320931/  2.314629, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  11.24%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  2.316856/  2.309039, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  11.24%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  2.317568/  2.312462, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  11.24%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  2.322536/  2.308460, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  11.24%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  2.317184/  2.325424, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  11.24%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  2.323347/  2.308666, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  11.24%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  2.324209/  2.309332, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  11.24%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  2.317179/  2.315681, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  11.24%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  2.326358/  2.314513, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  11.24%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  2.318460/  2.316844, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  11.24%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  2.317593/  2.308844, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  11.24%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  2.322765/  2.307623, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  11.24%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  2.325134/  2.308829, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  11.24%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  2.314856/  2.311282, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  11.24%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  2.320183/  2.309368, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  11.24%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  2.319872/  2.311711, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  11.24%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  2.319850/  2.314886, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  11.24%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  2.316781/  2.307194, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  11.24%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  2.317213/  2.309639, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  11.24%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  2.316024/  2.321777, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  11.24%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  2.322300/  2.308692, val:  10.00%, val_best:  10.00%, tr:   7.76%, tr_best:  11.24%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  2.322022/  2.309639, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  11.24%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  2.320133/  2.311189, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  11.24%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  2.318374/  2.309343, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  11.24%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  2.323557/  2.312201, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  11.24%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  2.325249/  2.318944, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  11.24%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  2.322435/  2.310744, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  11.24%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  2.319107/  2.311890, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  11.24%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  2.318825/  2.309790, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  11.24%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  2.324973/  2.306278, val:  10.00%, val_best:  10.00%, tr:   7.76%, tr_best:  11.24%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  2.316925/  2.315704, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  11.24%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  2.322272/  2.310759, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  11.24%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  2.319633/  2.313704, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  11.24%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  2.319135/  2.306101, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  11.24%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  2.323344/  2.307318, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  11.24%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  2.323301/  2.312440, val:  10.00%, val_best:  10.00%, tr:   7.15%, tr_best:  11.24%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  2.314006/  2.310386, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  11.24%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  2.318532/  2.315302, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  2.318847/  2.308046, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  11.24%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  2.315717/  2.322596, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  11.24%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  2.325798/  2.310406, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  11.24%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  2.317781/  2.314536, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  11.24%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  2.320998/  2.312184, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  11.24%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  2.323415/  2.319147, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  11.24%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  2.323264/  2.311337, val:  10.00%, val_best:  10.00%, tr:   7.76%, tr_best:  11.24%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  2.322191/  2.311279, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  11.24%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  2.321045/  2.317480, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  11.24%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  2.327291/  2.311272, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  11.24%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  2.319391/  2.315876, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  11.24%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  2.319038/  2.321109, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  11.24%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  2.318532/  2.311095, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  11.24%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  2.320800/  2.310123, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  11.24%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  2.322366/  2.309981, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  11.24%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  2.318656/  2.305710, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  11.24%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  2.321045/  2.309697, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  2.324742/  2.315970, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  11.24%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  2.317994/  2.312845, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  11.24%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  2.316085/  2.316632, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  11.24%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  2.322831/  2.324681, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  11.24%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  2.318426/  2.310144, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  2.319360/  2.316563, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  11.24%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  2.322934/  2.320799, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  11.24%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  2.320748/  2.312796, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  11.24%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  2.318540/  2.310683, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  11.24%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  2.314051/  2.315422, val:  10.00%, val_best:  10.00%, tr:  10.73%, tr_best:  11.24%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  2.321696/  2.314612, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  11.24%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  2.323909/  2.306177, val:  10.00%, val_best:  10.00%, tr:   7.15%, tr_best:  11.24%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  2.321943/  2.307636, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  11.24%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  2.317030/  2.319769, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  11.24%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  2.320492/  2.315406, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  11.24%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  2.323087/  2.321030, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  11.24%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  2.324332/  2.313588, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  11.24%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  2.326483/  2.313702, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  11.24%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  2.311864/  2.314276, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  11.24%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  2.316943/  2.311819, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  11.24%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  2.321003/  2.312660, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  11.24%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  2.325311/  2.314935, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  11.24%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  2.321927/  2.310999, val:  10.00%, val_best:  10.00%, tr:   7.15%, tr_best:  11.24%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  2.319391/  2.311135, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  2.323736/  2.311384, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  11.24%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  2.325066/  2.306257, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  11.24%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  2.322867/  2.310258, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  11.24%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  2.321428/  2.307564, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  11.24%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  2.318439/  2.311863, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  11.24%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  2.318721/  2.317565, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  11.24%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  2.321588/  2.309795, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  11.24%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  2.322235/  2.308270, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  11.24%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  2.316953/  2.312957, val:  10.00%, val_best:  10.00%, tr:   7.56%, tr_best:  11.24%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  2.325571/  2.313039, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  11.24%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  2.319710/  2.306851, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  11.24%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  2.319279/  2.317208, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  11.24%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  2.321485/  2.308218, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  11.24%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  2.316335/  2.312444, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  11.24%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  2.323276/  2.307178, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  11.24%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  2.324637/  2.313655, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  11.24%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  2.318306/  2.313654, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  11.24%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  2.314917/  2.321694, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  11.24%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  2.323692/  2.305553, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  11.24%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  2.318526/  2.314168, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  11.24%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  2.322460/  2.323959, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  11.24%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  2.319284/  2.309883, val:  10.00%, val_best:  10.00%, tr:   7.46%, tr_best:  11.24%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  2.321027/  2.308323, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  11.24%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  2.323728/  2.313300, val:  10.00%, val_best:  10.00%, tr:   7.46%, tr_best:  11.24%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  2.317241/  2.311491, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  11.24%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  2.323107/  2.318122, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  11.24%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  2.325568/  2.311228, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  11.24%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  2.317678/  2.309959, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  2.318274/  2.315550, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  11.24%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  2.325375/  2.314965, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  2.321816/  2.312149, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  11.24%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  2.319939/  2.308301, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  2.325231/  2.305268, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  11.24%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  2.311435/  2.316545, val:  10.00%, val_best:  10.00%, tr:  11.24%, tr_best:  11.24%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  2.319032/  2.306899, val:  10.00%, val_best:  10.00%, tr:   7.56%, tr_best:  11.24%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  2.319704/  2.312706, val:  10.00%, val_best:  10.00%, tr:   7.25%, tr_best:  11.24%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  2.324745/  2.316265, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  11.24%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  2.327381/  2.305187, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  11.24%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  2.316803/  2.317176, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  11.24%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  2.322843/  2.311920, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  11.24%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  2.318967/  2.319079, val:  10.00%, val_best:  10.00%, tr:  10.32%, tr_best:  11.24%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  2.317206/  2.306808, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  11.24%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  2.310966/  2.322780, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  11.24%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  2.326067/  2.315670, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  11.24%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  2.326809/  2.310260, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  11.24%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  2.321504/  2.307180, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  11.24%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  2.318009/  2.311144, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  11.24%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  2.323520/  2.309907, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:  11.24%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  2.316762/  2.311496, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  11.24%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  2.328894/  2.307794, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  11.24%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  2.318425/  2.312682, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  11.24%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  2.324462/  2.307942, val:  10.00%, val_best:  10.00%, tr:   7.56%, tr_best:  11.24%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  2.321324/  2.307292, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  11.24%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  2.318024/  2.319574, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  11.24%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  2.327491/  2.307292, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  11.24%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  2.320818/  2.308200, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  11.24%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  2.321950/  2.306418, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  11.24%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  2.317059/  2.314639, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  11.24%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  2.325961/  2.316806, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  11.24%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  2.315991/  2.310505, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  2.321127/  2.312586, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  11.24%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  2.323570/  2.312821, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  11.24%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  2.320534/  2.312976, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  11.24%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  2.318478/  2.309511, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  11.24%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  2.323277/  2.307205, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  11.24%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  2.315225/  2.307589, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  11.24%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  2.318511/  2.313071, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  11.24%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  2.323114/  2.315778, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  11.24%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  2.320550/  2.305221, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  11.24%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  2.319540/  2.319617, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  11.24%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  2.317014/  2.315995, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  11.24%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  2.321255/  2.308828, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  2.318433/  2.312435, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  11.24%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  2.319237/  2.308631, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  11.24%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  2.323183/  2.312139, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  11.24%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  2.323282/  2.312134, val:  10.00%, val_best:  10.00%, tr:  10.62%, tr_best:  11.24%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  2.323847/  2.309359, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  11.24%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  2.326561/  2.307478, val:  10.00%, val_best:  10.00%, tr:   6.54%, tr_best:  11.24%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  2.313739/  2.309763, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  11.24%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  2.321340/  2.316335, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  11.24%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  2.323972/  2.311065, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  11.24%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  2.316950/  2.314579, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  11.24%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  2.317556/  2.307307, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  11.24%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  2.321548/  2.308718, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  11.24%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  2.316713/  2.311215, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  11.24%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  2.317126/  2.313702, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  11.24%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  2.319426/  2.306279, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  11.24%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  2.318103/  2.317538, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  11.24%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  2.325517/  2.315340, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  11.24%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  2.323713/  2.310107, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  11.24%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  2.321098/  2.313953, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  11.24%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  2.324970/  2.308429, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  11.24%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  2.318478/  2.307961, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  11.24%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f57c3da214a40ffa4041d2f7209c09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▃▃▁▃▆█▁▆█▁▆▁▁▆▃▃▁▃▆▃▃█▆▁▃█▃▃█▃█▁█▆█▆▃▁▃</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tr_acc</td><td>▆▆▇▄█▁▄▄▅▂▇▅▅▄▅▄▆▅▆▅▇▄▅▃▆▆▅▃▁▅▅▂▄▄▆▆▇▅▂▇</td></tr><tr><td>tr_epoch_loss</td><td>▄▁▅█▆▃▂▅▅▃▃▄▃▂▄▆▃▂▄▂▅▃▃▅▂▁▂▂▃▂▅▂▂▁▂▃▃▃▂▅</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂▃█▃▂▂▂▃▂▄▂▂▃▄▅▂▂▄▅▅▃▂▁▃▃▆▂▄▁▄▂▂▅▂▂▅▂▄▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.09499</td></tr><tr><td>tr_epoch_loss</td><td>2.31848</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.30796</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polished-sweep-47</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0uvl6vu6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0uvl6vu6</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_083922-0uvl6vu6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5ve3c697 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_085054-5ve3c697</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ve3c697' target=\"_blank\">winter-sweep-48</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ve3c697' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ve3c697</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.305365/  2.302841, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:   8.17%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.304994/  2.302639, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:   8.17%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.305079/  2.302667, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:   8.99%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.304731/  2.302685, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:   8.99%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  2.303970/  2.300286, val:  12.92%, val_best:  12.92%, tr:   8.78%, tr_best:   8.99%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  2.266850/  2.223145, val:  21.67%, val_best:  21.67%, tr:  12.77%, tr_best:  12.77%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  2.151572/  2.110335, val:  31.25%, val_best:  31.25%, tr:  21.76%, tr_best:  21.76%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  1.939990/  1.885307, val:  39.17%, val_best:  39.17%, tr:  37.08%, tr_best:  37.08%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  1.646641/  1.652577, val:  47.92%, val_best:  47.92%, tr:  48.31%, tr_best:  48.31%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  1.453878/  1.513590, val:  52.50%, val_best:  52.50%, tr:  56.38%, tr_best:  56.38%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  1.321565/  1.472058, val:  53.33%, val_best:  53.33%, tr:  57.61%, tr_best:  57.61%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  1.219938/  1.402524, val:  56.67%, val_best:  56.67%, tr:  60.78%, tr_best:  60.78%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  1.167811/  1.399071, val:  60.00%, val_best:  60.00%, tr:  63.74%, tr_best:  63.74%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  1.113411/  1.390927, val:  62.08%, val_best:  62.08%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  1.028220/  1.467012, val:  59.58%, val_best:  62.08%, tr:  66.80%, tr_best:  66.80%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.993782/  1.445762, val:  60.83%, val_best:  62.08%, tr:  69.77%, tr_best:  69.77%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.973963/  1.380201, val:  61.25%, val_best:  62.08%, tr:  69.36%, tr_best:  69.77%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.898587/  1.433164, val:  62.92%, val_best:  62.92%, tr:  74.46%, tr_best:  74.46%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.859039/  1.482218, val:  59.58%, val_best:  62.92%, tr:  74.26%, tr_best:  74.46%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.836308/  1.482054, val:  61.25%, val_best:  62.92%, tr:  74.57%, tr_best:  74.57%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.780082/  1.534104, val:  63.75%, val_best:  63.75%, tr:  76.61%, tr_best:  76.61%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.747825/  1.508027, val:  64.58%, val_best:  64.58%, tr:  77.63%, tr_best:  77.63%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.752574/  1.504900, val:  65.42%, val_best:  65.42%, tr:  78.14%, tr_best:  78.14%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.695559/  1.570274, val:  70.42%, val_best:  70.42%, tr:  79.26%, tr_best:  79.26%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.660923/  1.634242, val:  63.33%, val_best:  70.42%, tr:  82.53%, tr_best:  82.53%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.633465/  1.582800, val:  68.33%, val_best:  70.42%, tr:  83.66%, tr_best:  83.66%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.621471/  1.697049, val:  70.83%, val_best:  70.83%, tr:  82.43%, tr_best:  83.66%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.590634/  1.838978, val:  67.50%, val_best:  70.83%, tr:  84.68%, tr_best:  84.68%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.587593/  1.715264, val:  67.50%, val_best:  70.83%, tr:  85.29%, tr_best:  85.29%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.545049/  1.886021, val:  63.33%, val_best:  70.83%, tr:  87.03%, tr_best:  87.03%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.526052/  1.863875, val:  67.50%, val_best:  70.83%, tr:  88.36%, tr_best:  88.36%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.524272/  1.978001, val:  68.75%, val_best:  70.83%, tr:  88.36%, tr_best:  88.36%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.512025/  1.984848, val:  67.08%, val_best:  70.83%, tr:  89.27%, tr_best:  89.27%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.469550/  1.889508, val:  67.92%, val_best:  70.83%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.466441/  2.122459, val:  60.42%, val_best:  70.83%, tr:  93.05%, tr_best:  93.05%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.455602/  2.137835, val:  66.67%, val_best:  70.83%, tr:  90.50%, tr_best:  93.05%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.396273/  2.043257, val:  73.33%, val_best:  73.33%, tr:  93.56%, tr_best:  93.56%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.379854/  2.190035, val:  64.17%, val_best:  73.33%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.387463/  2.202960, val:  67.92%, val_best:  73.33%, tr:  95.40%, tr_best:  96.02%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.361975/  2.260381, val:  68.75%, val_best:  73.33%, tr:  95.20%, tr_best:  96.02%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.335043/  2.234946, val:  74.58%, val_best:  74.58%, tr:  94.89%, tr_best:  96.02%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.291443/  2.452466, val:  68.75%, val_best:  74.58%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.288521/  2.363687, val:  69.58%, val_best:  74.58%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.280412/  2.506076, val:  67.08%, val_best:  74.58%, tr:  97.24%, tr_best:  98.67%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.276817/  2.513804, val:  66.25%, val_best:  74.58%, tr:  97.45%, tr_best:  98.67%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.262132/  2.513437, val:  72.50%, val_best:  74.58%, tr:  98.57%, tr_best:  98.67%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.245306/  2.633677, val:  75.00%, val_best:  75.00%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.220030/  2.642635, val:  72.50%, val_best:  75.00%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.246614/  2.623805, val:  73.33%, val_best:  75.00%, tr:  99.28%, tr_best:  99.59%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.204675/  2.713542, val:  72.92%, val_best:  75.00%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.172289/  2.761939, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.169991/  2.814834, val:  70.83%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.140889/  2.832815, val:  74.58%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.134607/  2.971972, val:  71.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.141561/  2.934206, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.143650/  3.019687, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.118625/  3.055392, val:  75.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.111935/  3.100556, val:  75.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.105747/  3.122471, val:  76.25%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.089845/  3.271757, val:  75.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.080128/  3.283470, val:  75.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.089994/  3.294154, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.088433/  3.335949, val:  75.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.063911/  3.378436, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.066177/  3.379838, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.060534/  3.362495, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.057633/  3.524351, val:  74.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.060329/  3.568663, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.064228/  3.483548, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.051749/  3.559905, val:  73.33%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.041654/  3.470634, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.041840/  3.610543, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.047559/  3.623110, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.045073/  3.525622, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.042243/  3.644428, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.042691/  3.596664, val:  77.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.033742/  3.734175, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.028277/  3.663353, val:  77.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.039617/  3.776391, val:  77.92%, val_best:  77.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.035491/  3.819743, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.040714/  3.702626, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.027903/  3.781718, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.038269/  3.810431, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.028531/  3.812777, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.034069/  3.873062, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.029454/  3.902336, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.038241/  3.903589, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.032266/  3.766485, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.038972/  3.898949, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.031770/  3.947400, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.050826/  3.902719, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.028473/  3.885158, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.021009/  3.888304, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.016330/  3.910305, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.013415/  4.048976, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.014304/  4.021394, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.012186/  4.021070, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.010869/  4.033346, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.006432/  4.049345, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.010818/  4.045818, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.011175/  3.980423, val:  76.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.007173/  4.056706, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.011287/  4.038012, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.009019/  4.090678, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.007379/  4.063439, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.007369/  4.039653, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.006953/  4.080262, val:  75.42%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.004488/  4.065847, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.005016/  4.054896, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.009041/  4.094360, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.004718/  4.107202, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.005360/  4.098221, val:  76.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.004773/  4.137838, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.004466/  4.141489, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.005224/  4.167794, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.004468/  4.113510, val:  76.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.006462/  4.176788, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.005549/  4.215069, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.010057/  4.255794, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.009463/  4.173500, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.008880/  4.211660, val:  76.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.011503/  4.247411, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.014955/  4.225119, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.010372/  4.235908, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.009314/  4.236164, val:  76.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.006605/  4.204913, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.004991/  4.304730, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.004348/  4.230110, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.003910/  4.208610, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.004057/  4.163028, val:  76.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.004496/  4.204024, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.005533/  4.277884, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.004237/  4.303996, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.004597/  4.219560, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.005224/  4.242129, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.003482/  4.281514, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.004248/  4.284051, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.004831/  4.252589, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.007047/  4.282875, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.003573/  4.263484, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.003452/  4.315338, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.004306/  4.314410, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.005222/  4.363679, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.003403/  4.315396, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.003729/  4.365239, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.003120/  4.383971, val:  76.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.004855/  4.324378, val:  76.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.004149/  4.397760, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.003841/  4.356993, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.002587/  4.393988, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.002991/  4.372023, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.004098/  4.369360, val:  76.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.002331/  4.403607, val:  75.00%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.002779/  4.338994, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.002598/  4.413292, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.001198/  4.310833, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.002388/  4.363133, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.002903/  4.341470, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.002313/  4.325203, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.002202/  4.384604, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.003421/  4.369251, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.004141/  4.414803, val:  75.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.003012/  4.356690, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.004950/  4.359868, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.005445/  4.326803, val:  75.42%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.003469/  4.340208, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.004590/  4.429925, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.003825/  4.391157, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.007279/  4.436591, val:  75.42%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.018140/  4.369452, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.009628/  4.398518, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.008896/  4.369342, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.007945/  4.416399, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.006219/  4.433115, val:  75.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.006002/  4.400837, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.003953/  4.458015, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.004350/  4.473605, val:  75.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.003039/  4.505597, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.003371/  4.503234, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.004315/  4.450475, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.003440/  4.524400, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.004019/  4.559035, val:  76.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.002843/  4.467451, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.002222/  4.464971, val:  78.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.001963/  4.478316, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.003654/  4.466098, val:  75.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.003989/  4.520431, val:  75.42%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.003456/  4.533731, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.002036/  4.526762, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.001573/  4.496771, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.002031/  4.541069, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.003857/  4.537063, val:  74.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.002433/  4.537046, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.002883/  4.457292, val:  75.42%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.002777/  4.485418, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.003672/  4.511113, val:  77.50%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.003666/  4.518373, val:  75.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.005791/  4.521295, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.024628/  4.471119, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.009723/  4.457214, val:  77.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856d6fc34af247a3b1afba5cda132fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▄▅▅▇▇█████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▅▆▆▇▆▇█▇█▇██████▇█████████████████████</td></tr><tr><td>tr_acc</td><td>▁▁▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▅▆▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▅▆▆▇▆▇█▇█▇██████▇█████████████████████</td></tr><tr><td>val_loss</td><td>▃▃▁▁▁▁▂▂▃▃▄▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██▇████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00972</td></tr><tr><td>val_acc_best</td><td>0.79583</td></tr><tr><td>val_acc_now</td><td>0.77917</td></tr><tr><td>val_loss</td><td>4.45721</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-sweep-48</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ve3c697' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5ve3c697</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_085054-5ve3c697/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gkw26u5h with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_090217-gkw26u5h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gkw26u5h' target=\"_blank\">stellar-sweep-49</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gkw26u5h' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gkw26u5h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  2.088779/  2.021866, val:  36.67%, val_best:  36.67%, tr:  28.29%, tr_best:  28.29%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  1.903006/  1.945471, val:  37.08%, val_best:  37.08%, tr:  39.63%, tr_best:  39.63%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  1.879182/  2.273692, val:  25.00%, val_best:  37.08%, tr:  43.62%, tr_best:  43.62%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  1.947310/  1.924642, val:  40.83%, val_best:  40.83%, tr:  39.63%, tr_best:  43.62%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  1.759367/  2.172381, val:  38.33%, val_best:  40.83%, tr:  43.92%, tr_best:  43.92%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  1.723237/  2.112999, val:  35.83%, val_best:  40.83%, tr:  48.11%, tr_best:  48.11%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  1.851484/  1.747218, val:  45.42%, val_best:  45.42%, tr:  44.13%, tr_best:  48.11%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  1.708589/  2.464251, val:  34.58%, val_best:  45.42%, tr:  43.21%, tr_best:  48.11%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  1.930428/  2.248789, val:  37.92%, val_best:  45.42%, tr:  42.70%, tr_best:  48.11%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  1.830513/  2.304411, val:  38.33%, val_best:  45.42%, tr:  48.52%, tr_best:  48.52%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  2.037548/  1.978862, val:  43.75%, val_best:  45.42%, tr:  39.43%, tr_best:  48.52%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  1.955088/  2.316175, val:  37.92%, val_best:  45.42%, tr:  45.45%, tr_best:  48.52%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  1.840669/  2.269144, val:  38.75%, val_best:  45.42%, tr:  46.88%, tr_best:  48.52%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  1.854053/  2.497334, val:  38.33%, val_best:  45.42%, tr:  45.05%, tr_best:  48.52%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  1.896780/  2.708181, val:  35.42%, val_best:  45.42%, tr:  44.84%, tr_best:  48.52%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  1.997520/  2.275124, val:  45.83%, val_best:  45.83%, tr:  43.41%, tr_best:  48.52%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  1.906340/  2.736231, val:  25.00%, val_best:  45.83%, tr:  44.74%, tr_best:  48.52%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  2.035183/  1.963600, val:  29.58%, val_best:  45.83%, tr:  43.62%, tr_best:  48.52%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  1.945431/  2.394845, val:  51.25%, val_best:  51.25%, tr:  42.59%, tr_best:  48.52%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  1.958378/  2.294755, val:  39.17%, val_best:  51.25%, tr:  45.76%, tr_best:  48.52%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  2.059477/  2.184545, val:  39.17%, val_best:  51.25%, tr:  41.37%, tr_best:  48.52%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  2.133295/  1.857815, val:  51.67%, val_best:  51.67%, tr:  45.05%, tr_best:  48.52%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  1.931087/  2.206364, val:  34.58%, val_best:  51.67%, tr:  44.02%, tr_best:  48.52%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  1.804116/  1.950130, val:  45.42%, val_best:  51.67%, tr:  42.39%, tr_best:  48.52%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  1.720775/  2.091007, val:  44.58%, val_best:  51.67%, tr:  47.80%, tr_best:  48.52%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  1.979812/  2.113467, val:  44.58%, val_best:  51.67%, tr:  45.97%, tr_best:  48.52%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  1.991713/  2.318467, val:  44.58%, val_best:  51.67%, tr:  43.62%, tr_best:  48.52%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  1.859126/  2.495069, val:  42.50%, val_best:  51.67%, tr:  45.05%, tr_best:  48.52%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  2.215287/  2.497286, val:  38.75%, val_best:  51.67%, tr:  44.13%, tr_best:  48.52%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  1.871149/  2.420006, val:  36.67%, val_best:  51.67%, tr:  46.27%, tr_best:  48.52%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  1.845153/  1.611999, val:  50.83%, val_best:  51.67%, tr:  46.07%, tr_best:  48.52%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  1.846603/  3.058625, val:  36.67%, val_best:  51.67%, tr:  49.03%, tr_best:  49.03%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  2.568104/  3.030927, val:  40.42%, val_best:  51.67%, tr:  40.96%, tr_best:  49.03%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  2.498963/  1.919559, val:  39.58%, val_best:  51.67%, tr:  36.77%, tr_best:  49.03%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  2.471672/  1.984069, val:  41.67%, val_best:  51.67%, tr:  39.53%, tr_best:  49.03%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  1.980024/  1.837123, val:  47.08%, val_best:  51.67%, tr:  42.49%, tr_best:  49.03%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  1.868011/  3.035423, val:  42.92%, val_best:  51.67%, tr:  41.47%, tr_best:  49.03%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  2.108378/  3.059458, val:  35.42%, val_best:  51.67%, tr:  44.02%, tr_best:  49.03%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  2.222232/  2.699331, val:  43.33%, val_best:  51.67%, tr:  41.68%, tr_best:  49.03%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  2.272193/  2.271321, val:  48.75%, val_best:  51.67%, tr:  44.33%, tr_best:  49.03%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  1.779455/  2.458906, val:  41.67%, val_best:  51.67%, tr:  44.74%, tr_best:  49.03%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  1.923129/  2.357692, val:  40.42%, val_best:  51.67%, tr:  45.45%, tr_best:  49.03%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  1.757064/  2.095505, val:  43.33%, val_best:  51.67%, tr:  49.85%, tr_best:  49.85%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  1.912389/  3.088949, val:  34.58%, val_best:  51.67%, tr:  46.88%, tr_best:  49.85%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  1.833736/  1.969753, val:  44.17%, val_best:  51.67%, tr:  46.78%, tr_best:  49.85%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  1.847748/  2.259720, val:  37.50%, val_best:  51.67%, tr:  46.17%, tr_best:  49.85%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  2.259249/  2.839581, val:  28.33%, val_best:  51.67%, tr:  43.62%, tr_best:  49.85%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  2.026417/  2.800684, val:  31.67%, val_best:  51.67%, tr:  44.94%, tr_best:  49.85%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  2.614978/  2.627345, val:  36.67%, val_best:  51.67%, tr:  41.98%, tr_best:  49.85%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  2.007020/  2.122706, val:  45.00%, val_best:  51.67%, tr:  45.66%, tr_best:  49.85%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  1.855784/  2.317734, val:  52.50%, val_best:  52.50%, tr:  45.45%, tr_best:  49.85%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  1.997080/  2.167174, val:  39.58%, val_best:  52.50%, tr:  45.86%, tr_best:  49.85%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  1.981859/  2.209716, val:  45.42%, val_best:  52.50%, tr:  47.09%, tr_best:  49.85%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  1.894320/  2.871747, val:  44.58%, val_best:  52.50%, tr:  48.83%, tr_best:  49.85%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  1.990001/  2.522889, val:  35.83%, val_best:  52.50%, tr:  46.58%, tr_best:  49.85%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  1.929742/  2.235392, val:  47.50%, val_best:  52.50%, tr:  48.01%, tr_best:  49.85%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  2.135290/  2.731834, val:  39.17%, val_best:  52.50%, tr:  43.31%, tr_best:  49.85%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  2.417137/  2.295092, val:  36.67%, val_best:  52.50%, tr:  39.63%, tr_best:  49.85%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  1.763681/  2.668612, val:  41.25%, val_best:  52.50%, tr:  50.46%, tr_best:  50.46%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  1.901349/  2.599230, val:  34.17%, val_best:  52.50%, tr:  48.01%, tr_best:  50.46%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  1.853968/  2.856594, val:  37.92%, val_best:  52.50%, tr:  52.30%, tr_best:  52.30%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  2.060013/  2.180577, val:  45.42%, val_best:  52.50%, tr:  47.09%, tr_best:  52.30%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  2.105958/  2.281893, val:  37.08%, val_best:  52.50%, tr:  45.76%, tr_best:  52.30%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  2.261934/  2.198456, val:  40.83%, val_best:  52.50%, tr:  45.86%, tr_best:  52.30%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  2.346760/  2.233011, val:  43.75%, val_best:  52.50%, tr:  45.45%, tr_best:  52.30%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  2.573808/  2.724441, val:  51.25%, val_best:  52.50%, tr:  44.84%, tr_best:  52.30%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  2.545242/  2.779627, val:  33.33%, val_best:  52.50%, tr:  44.23%, tr_best:  52.30%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  2.751703/  2.188649, val:  45.00%, val_best:  52.50%, tr:  42.19%, tr_best:  52.30%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  2.151205/  2.337040, val:  41.25%, val_best:  52.50%, tr:  44.64%, tr_best:  52.30%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  2.102865/  1.960665, val:  45.00%, val_best:  52.50%, tr:  45.15%, tr_best:  52.30%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  2.357208/  3.568178, val:  33.75%, val_best:  52.50%, tr:  45.45%, tr_best:  52.30%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  2.451852/  2.301961, val:  41.25%, val_best:  52.50%, tr:  45.15%, tr_best:  52.30%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  2.210229/  2.364330, val:  33.75%, val_best:  52.50%, tr:  45.45%, tr_best:  52.30%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  2.105700/  2.599304, val:  42.08%, val_best:  52.50%, tr:  48.01%, tr_best:  52.30%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  2.139666/  2.434422, val:  40.00%, val_best:  52.50%, tr:  45.15%, tr_best:  52.30%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  2.331498/  3.246395, val:  35.00%, val_best:  52.50%, tr:  44.02%, tr_best:  52.30%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  2.699536/  3.026623, val:  25.83%, val_best:  52.50%, tr:  43.11%, tr_best:  52.30%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  2.409802/  3.054324, val:  36.67%, val_best:  52.50%, tr:  45.56%, tr_best:  52.30%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  2.991494/  3.222817, val:  30.00%, val_best:  52.50%, tr:  40.35%, tr_best:  52.30%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  2.583158/  2.934145, val:  34.58%, val_best:  52.50%, tr:  43.82%, tr_best:  52.30%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  2.242652/  2.887736, val:  46.25%, val_best:  52.50%, tr:  44.54%, tr_best:  52.30%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  2.239909/  2.737564, val:  27.50%, val_best:  52.50%, tr:  46.27%, tr_best:  52.30%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  2.590058/  3.343438, val:  47.92%, val_best:  52.50%, tr:  44.43%, tr_best:  52.30%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  2.410859/  2.499456, val:  42.92%, val_best:  52.50%, tr:  44.02%, tr_best:  52.30%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  2.506128/  2.584974, val:  42.50%, val_best:  52.50%, tr:  47.91%, tr_best:  52.30%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  2.140047/  2.012757, val:  42.92%, val_best:  52.50%, tr:  44.23%, tr_best:  52.30%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  1.929879/  2.201686, val:  44.58%, val_best:  52.50%, tr:  50.15%, tr_best:  52.30%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  1.903576/  2.543387, val:  33.33%, val_best:  52.50%, tr:  45.35%, tr_best:  52.30%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  2.118676/  3.222632, val:  34.58%, val_best:  52.50%, tr:  45.66%, tr_best:  52.30%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  2.443451/  2.578307, val:  51.67%, val_best:  52.50%, tr:  41.68%, tr_best:  52.30%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  2.197333/  2.223413, val:  36.67%, val_best:  52.50%, tr:  47.70%, tr_best:  52.30%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  2.086862/  2.950253, val:  22.08%, val_best:  52.50%, tr:  47.80%, tr_best:  52.30%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  2.225522/  2.024699, val:  43.75%, val_best:  52.50%, tr:  42.08%, tr_best:  52.30%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  2.120096/  2.762335, val:  29.58%, val_best:  52.50%, tr:  46.17%, tr_best:  52.30%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  2.421705/  2.724622, val:  35.00%, val_best:  52.50%, tr:  43.92%, tr_best:  52.30%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  2.423444/  2.474193, val:  40.42%, val_best:  52.50%, tr:  46.78%, tr_best:  52.30%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  2.303834/  2.490368, val:  42.08%, val_best:  52.50%, tr:  44.43%, tr_best:  52.30%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  2.416387/  2.893585, val:  37.08%, val_best:  52.50%, tr:  44.02%, tr_best:  52.30%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  2.590346/  3.483140, val:  32.92%, val_best:  52.50%, tr:  40.45%, tr_best:  52.30%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  2.654935/  2.706220, val:  42.08%, val_best:  52.50%, tr:  44.43%, tr_best:  52.30%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss:  2.300885/  2.355150, val:  50.83%, val_best:  52.50%, tr:  46.48%, tr_best:  52.30%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss:  2.673776/  2.661848, val:  45.00%, val_best:  52.50%, tr:  43.92%, tr_best:  52.30%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss:  1.989290/  2.632003, val:  40.42%, val_best:  52.50%, tr:  47.29%, tr_best:  52.30%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss:  2.378960/  3.142910, val:  43.75%, val_best:  52.50%, tr:  44.33%, tr_best:  52.30%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss:  2.284109/  3.240091, val:  35.00%, val_best:  52.50%, tr:  45.15%, tr_best:  52.30%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss:  2.066099/  2.248626, val:  42.08%, val_best:  52.50%, tr:  46.07%, tr_best:  52.30%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss:  2.540305/  3.233294, val:  21.25%, val_best:  52.50%, tr:  43.00%, tr_best:  52.30%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss:  2.385836/  2.935800, val:  41.67%, val_best:  52.50%, tr:  39.22%, tr_best:  52.30%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss:  2.238856/  2.212680, val:  37.50%, val_best:  52.50%, tr:  44.54%, tr_best:  52.30%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss:  2.106085/  2.375162, val:  37.50%, val_best:  52.50%, tr:  44.74%, tr_best:  52.30%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss:  2.088795/  2.569402, val:  40.42%, val_best:  52.50%, tr:  43.82%, tr_best:  52.30%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss:  2.077073/  2.423108, val:  34.17%, val_best:  52.50%, tr:  46.78%, tr_best:  52.30%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss:  2.144658/  2.437033, val:  46.67%, val_best:  52.50%, tr:  43.51%, tr_best:  52.30%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss:  2.482359/  2.894244, val:  29.17%, val_best:  52.50%, tr:  45.35%, tr_best:  52.30%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss:  2.128523/  2.883195, val:  48.33%, val_best:  52.50%, tr:  43.92%, tr_best:  52.30%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss:  2.347038/  2.330319, val:  42.50%, val_best:  52.50%, tr:  44.74%, tr_best:  52.30%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss:  2.130696/  2.675526, val:  41.67%, val_best:  52.50%, tr:  46.37%, tr_best:  52.30%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss:  2.388308/  3.248915, val:  41.25%, val_best:  52.50%, tr:  43.72%, tr_best:  52.30%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss:  1.987765/  2.617293, val:  30.42%, val_best:  52.50%, tr:  47.09%, tr_best:  52.30%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss:  2.363453/  2.556441, val:  34.58%, val_best:  52.50%, tr:  44.23%, tr_best:  52.30%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss:  2.475152/  2.966576, val:  41.25%, val_best:  52.50%, tr:  44.13%, tr_best:  52.30%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss:  2.351819/  3.873649, val:  37.08%, val_best:  52.50%, tr:  45.76%, tr_best:  52.30%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss:  2.410713/  1.987400, val:  44.17%, val_best:  52.50%, tr:  42.90%, tr_best:  52.30%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss:  2.674276/  3.274340, val:  30.00%, val_best:  52.50%, tr:  42.49%, tr_best:  52.30%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss:  2.359433/  2.168937, val:  44.58%, val_best:  52.50%, tr:  44.02%, tr_best:  52.30%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss:  2.143692/  2.850933, val:  36.25%, val_best:  52.50%, tr:  48.42%, tr_best:  52.30%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss:  2.093721/  3.283291, val:  40.83%, val_best:  52.50%, tr:  47.80%, tr_best:  52.30%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss:  2.577388/  2.288738, val:  44.17%, val_best:  52.50%, tr:  46.68%, tr_best:  52.30%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss:  2.054503/  2.956902, val:  28.33%, val_best:  52.50%, tr:  44.23%, tr_best:  52.30%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss:  2.215266/  2.024446, val:  43.75%, val_best:  52.50%, tr:  45.76%, tr_best:  52.30%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss:  2.180898/  2.613750, val:  44.58%, val_best:  52.50%, tr:  45.15%, tr_best:  52.30%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss:  2.467680/  3.945901, val:  33.33%, val_best:  52.50%, tr:  42.80%, tr_best:  52.30%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss:  2.230367/  2.891768, val:  43.33%, val_best:  52.50%, tr:  45.05%, tr_best:  52.30%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss:  2.016867/  2.825475, val:  34.58%, val_best:  52.50%, tr:  46.88%, tr_best:  52.30%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss:  2.437004/  3.924733, val:  12.08%, val_best:  52.50%, tr:  43.62%, tr_best:  52.30%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss:  2.384724/  3.336403, val:  29.17%, val_best:  52.50%, tr:  43.41%, tr_best:  52.30%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss:  2.524386/  2.289290, val:  45.42%, val_best:  52.50%, tr:  42.90%, tr_best:  52.30%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss:  2.008068/  2.887749, val:  35.00%, val_best:  52.50%, tr:  48.11%, tr_best:  52.30%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss:  2.029256/  2.131960, val:  41.25%, val_best:  52.50%, tr:  49.74%, tr_best:  52.30%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss:  1.754696/  2.163548, val:  48.75%, val_best:  52.50%, tr:  50.56%, tr_best:  52.30%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss:  2.232694/  2.720617, val:  36.25%, val_best:  52.50%, tr:  47.70%, tr_best:  52.30%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss:  2.121068/  2.862813, val:  39.58%, val_best:  52.50%, tr:  45.56%, tr_best:  52.30%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss:  2.047566/  2.610336, val:  37.50%, val_best:  52.50%, tr:  48.31%, tr_best:  52.30%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss:  2.483127/  3.664866, val:  26.67%, val_best:  52.50%, tr:  42.29%, tr_best:  52.30%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss:  2.289655/  2.576972, val:  34.58%, val_best:  52.50%, tr:  43.92%, tr_best:  52.30%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss:  1.779765/  3.120584, val:  35.00%, val_best:  52.50%, tr:  48.01%, tr_best:  52.30%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss:  1.980838/  2.542975, val:  41.67%, val_best:  52.50%, tr:  50.66%, tr_best:  52.30%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss:  2.108560/  3.301373, val:  32.50%, val_best:  52.50%, tr:  46.78%, tr_best:  52.30%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss:  2.741326/  3.753518, val:  36.67%, val_best:  52.50%, tr:  43.11%, tr_best:  52.30%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss:  2.338442/  2.905939, val:  40.00%, val_best:  52.50%, tr:  46.78%, tr_best:  52.30%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss:  2.457521/  3.417078, val:  30.42%, val_best:  52.50%, tr:  46.58%, tr_best:  52.30%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss:  2.759877/  3.443554, val:  38.33%, val_best:  52.50%, tr:  42.59%, tr_best:  52.30%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss:  2.637111/  2.222327, val:  46.25%, val_best:  52.50%, tr:  43.51%, tr_best:  52.30%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss:  2.021863/  2.318687, val:  40.83%, val_best:  52.50%, tr:  46.58%, tr_best:  52.30%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss:  2.158277/  2.433727, val:  39.17%, val_best:  52.50%, tr:  48.11%, tr_best:  52.30%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss:  2.651412/  3.791657, val:  24.17%, val_best:  52.50%, tr:  43.31%, tr_best:  52.30%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss:  2.571200/  3.317276, val:  38.75%, val_best:  52.50%, tr:  45.15%, tr_best:  52.30%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss:  2.258778/  3.111088, val:  30.83%, val_best:  52.50%, tr:  48.72%, tr_best:  52.30%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss:  2.401241/  2.652030, val:  44.17%, val_best:  52.50%, tr:  45.66%, tr_best:  52.30%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss:  2.148967/  2.426877, val:  42.08%, val_best:  52.50%, tr:  45.05%, tr_best:  52.30%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss:  2.265465/  2.154646, val:  40.00%, val_best:  52.50%, tr:  44.43%, tr_best:  52.30%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss:  2.021594/  2.366249, val:  48.75%, val_best:  52.50%, tr:  45.15%, tr_best:  52.30%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss:  2.072429/  2.500554, val:  36.67%, val_best:  52.50%, tr:  47.91%, tr_best:  52.30%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss:  2.140984/  2.903439, val:  36.67%, val_best:  52.50%, tr:  47.80%, tr_best:  52.30%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss:  2.179338/  2.823170, val:  37.08%, val_best:  52.50%, tr:  46.27%, tr_best:  52.30%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss:  2.048105/  4.602641, val:  47.08%, val_best:  52.50%, tr:  47.70%, tr_best:  52.30%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss:  2.207889/  3.441527, val:  31.67%, val_best:  52.50%, tr:  49.23%, tr_best:  52.30%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss:  2.454049/  2.602858, val:  40.00%, val_best:  52.50%, tr:  45.25%, tr_best:  52.30%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss:  1.960721/  2.016803, val:  42.08%, val_best:  52.50%, tr:  48.72%, tr_best:  52.30%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss:  2.159114/  2.359649, val:  42.08%, val_best:  52.50%, tr:  48.42%, tr_best:  52.30%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss:  2.217419/  2.036203, val:  44.17%, val_best:  52.50%, tr:  45.66%, tr_best:  52.30%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss:  2.199927/  2.110225, val:  42.92%, val_best:  52.50%, tr:  43.92%, tr_best:  52.30%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss:  1.911530/  2.655024, val:  41.67%, val_best:  52.50%, tr:  48.21%, tr_best:  52.30%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss:  2.444726/  2.908475, val:  27.92%, val_best:  52.50%, tr:  46.48%, tr_best:  52.30%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss:  2.456370/  3.079109, val:  44.17%, val_best:  52.50%, tr:  44.74%, tr_best:  52.30%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss:  2.157413/  2.910826, val:  30.83%, val_best:  52.50%, tr:  49.74%, tr_best:  52.30%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss:  2.203426/  2.044544, val:  45.00%, val_best:  52.50%, tr:  48.01%, tr_best:  52.30%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss:  2.188956/  3.398869, val:  34.58%, val_best:  52.50%, tr:  45.86%, tr_best:  52.30%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss:  2.175463/  2.435144, val:  43.33%, val_best:  52.50%, tr:  48.21%, tr_best:  52.30%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss:  2.070384/  2.896828, val:  29.58%, val_best:  52.50%, tr:  47.09%, tr_best:  52.30%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss:  2.083710/  2.493857, val:  43.33%, val_best:  52.50%, tr:  45.86%, tr_best:  52.30%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss:  1.910600/  2.712992, val:  45.42%, val_best:  52.50%, tr:  50.36%, tr_best:  52.30%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss:  2.333386/  2.435058, val:  39.17%, val_best:  52.50%, tr:  46.27%, tr_best:  52.30%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss:  2.004970/  2.968165, val:  42.08%, val_best:  52.50%, tr:  47.60%, tr_best:  52.30%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss:  2.142500/  2.397253, val:  44.17%, val_best:  52.50%, tr:  50.77%, tr_best:  52.30%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss:  2.235430/  2.045203, val:  42.08%, val_best:  52.50%, tr:  45.97%, tr_best:  52.30%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss:  2.071113/  3.428130, val:  38.75%, val_best:  52.50%, tr:  48.01%, tr_best:  52.30%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss:  2.434304/  2.625097, val:  39.58%, val_best:  52.50%, tr:  49.95%, tr_best:  52.30%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss:  2.247349/  2.996780, val:  40.00%, val_best:  52.50%, tr:  47.09%, tr_best:  52.30%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss:  2.555756/  2.471520, val:  49.17%, val_best:  52.50%, tr:  46.37%, tr_best:  52.30%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss:  2.495572/  2.290520, val:  50.42%, val_best:  52.50%, tr:  46.68%, tr_best:  52.30%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss:  2.093717/  3.462602, val:  28.33%, val_best:  52.50%, tr:  46.48%, tr_best:  52.30%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss:  2.119161/  3.280775, val:  32.50%, val_best:  52.50%, tr:  50.46%, tr_best:  52.30%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss:  2.329649/  3.215442, val:  32.50%, val_best:  52.50%, tr:  47.40%, tr_best:  52.30%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss:  1.965403/  2.419454, val:  43.75%, val_best:  52.50%, tr:  48.83%, tr_best:  52.30%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss:  1.978541/  2.932748, val:  32.50%, val_best:  52.50%, tr:  47.29%, tr_best:  52.30%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss:  2.014913/  2.038236, val:  42.50%, val_best:  52.50%, tr:  48.83%, tr_best:  52.30%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss:  2.403344/  2.425800, val:  42.50%, val_best:  52.50%, tr:  47.70%, tr_best:  52.30%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss:  2.356292/  2.608204, val:  47.50%, val_best:  52.50%, tr:  54.03%, tr_best:  54.03%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss:  1.824157/  3.046510, val:  30.83%, val_best:  52.50%, tr:  50.97%, tr_best:  54.03%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41da49b8ed044b0843e571286fc6305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▅▂▃▂▃▇▃▄▅▃▄▃▃▃▅▆▅▅▅▃▆▄▁▅▅▃▃▅▆▄▃▃▄▄▃█▅▆▅</td></tr><tr><td>summary_val_acc</td><td>▅▄▅▇▅▆▄▆▆▆█▄▅█▅▄▄▆█▆█▁▅▆▅▆▄▃▅▄▅▂▅▃▆▆▆▅▃▄</td></tr><tr><td>tr_acc</td><td>▁▆▆▃▄▄▅▂▄▅▄▅█▄▄▃▃▄▂▄▅▃▃▅▃▃▃▃▄▆▃▃▄▆▄▆▄▆▅▅</td></tr><tr><td>tr_epoch_loss</td><td>▂▁▂▃▃▃▂▂▁▂▂▃▂▇▆▅▇▄▆▅▅▇▃▄▆▅▆▅▄▁█▇▅▄▄▄▃▃▄▃</td></tr><tr><td>val_acc_best</td><td>▁▃▅▅▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▅▄▅▇▅▆▄▆▆▆█▄▅█▅▄▄▆█▆█▁▅▆▅▆▄▃▅▄▅▂▅▃▆▆▆▅▃▄</td></tr><tr><td>val_loss</td><td>▁▂▂▂▂▂▃▅▃▁▂▃▄▄▂▆▄▁▃▃▂▆▃▄▅▂█▆▄▅▆▇▂▆▁▁▃▆▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.66667</td></tr><tr><td>tr_acc</td><td>0.5097</td></tr><tr><td>tr_epoch_loss</td><td>1.82416</td></tr><tr><td>val_acc_best</td><td>0.525</td></tr><tr><td>val_acc_now</td><td>0.30833</td></tr><tr><td>val_loss</td><td>3.04651</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-sweep-49</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gkw26u5h' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gkw26u5h</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_090217-gkw26u5h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oni89efi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_091400-oni89efi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oni89efi' target=\"_blank\">effortless-sweep-50</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oni89efi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oni89efi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.304233/  2.286559, val:  14.58%, val_best:  14.58%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.001899/  1.737806, val:  45.42%, val_best:  45.42%, tr:  28.40%, tr_best:  28.40%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  1.552850/  1.612273, val:  49.58%, val_best:  49.58%, tr:  54.24%, tr_best:  54.24%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  1.403562/  1.654387, val:  53.75%, val_best:  53.75%, tr:  59.04%, tr_best:  59.04%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  1.328148/  1.580718, val:  57.08%, val_best:  57.08%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  1.291855/  1.553385, val:  62.92%, val_best:  62.92%, tr:  61.49%, tr_best:  62.72%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  1.212697/  1.570528, val:  60.83%, val_best:  62.92%, tr:  68.03%, tr_best:  68.03%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  1.211426/  1.529983, val:  63.75%, val_best:  63.75%, tr:  66.50%, tr_best:  68.03%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  1.189406/  1.612938, val:  60.42%, val_best:  63.75%, tr:  67.31%, tr_best:  68.03%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  1.150115/  1.611932, val:  61.67%, val_best:  63.75%, tr:  70.07%, tr_best:  70.07%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  1.145952/  1.778243, val:  53.33%, val_best:  63.75%, tr:  69.46%, tr_best:  70.07%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  1.079249/  1.638896, val:  65.83%, val_best:  65.83%, tr:  75.08%, tr_best:  75.08%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  1.074987/  1.722047, val:  61.25%, val_best:  65.83%, tr:  79.26%, tr_best:  79.26%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  1.054360/  1.720701, val:  65.83%, val_best:  65.83%, tr:  77.32%, tr_best:  79.26%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.998616/  1.894596, val:  63.33%, val_best:  65.83%, tr:  81.82%, tr_best:  81.82%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.987418/  1.797219, val:  65.00%, val_best:  65.83%, tr:  82.23%, tr_best:  82.23%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.979480/  1.829488, val:  71.67%, val_best:  71.67%, tr:  84.88%, tr_best:  84.88%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.937662/  1.876654, val:  70.83%, val_best:  71.67%, tr:  89.07%, tr_best:  89.07%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.900109/  2.041997, val:  66.25%, val_best:  71.67%, tr:  86.52%, tr_best:  89.07%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.902899/  1.991402, val:  67.50%, val_best:  71.67%, tr:  87.64%, tr_best:  89.07%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.867717/  2.125051, val:  65.00%, val_best:  71.67%, tr:  90.30%, tr_best:  90.30%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.854123/  2.255191, val:  63.75%, val_best:  71.67%, tr:  89.79%, tr_best:  90.30%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.838343/  2.166083, val:  69.17%, val_best:  71.67%, tr:  92.65%, tr_best:  92.65%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.845078/  2.204492, val:  70.00%, val_best:  71.67%, tr:  91.52%, tr_best:  92.65%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.779442/  2.288874, val:  70.00%, val_best:  71.67%, tr:  94.08%, tr_best:  94.08%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.776339/  2.301807, val:  69.58%, val_best:  71.67%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.816317/  2.318299, val:  73.33%, val_best:  73.33%, tr:  91.62%, tr_best:  95.71%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.762065/  2.453540, val:  72.08%, val_best:  73.33%, tr:  95.61%, tr_best:  95.71%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.738251/  2.439097, val:  73.75%, val_best:  73.75%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.728739/  2.579864, val:  70.42%, val_best:  73.75%, tr:  96.94%, tr_best:  96.94%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.705117/  2.580995, val:  74.58%, val_best:  74.58%, tr:  96.63%, tr_best:  96.94%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.712789/  2.688944, val:  73.75%, val_best:  74.58%, tr:  94.89%, tr_best:  96.94%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.703009/  2.803926, val:  71.25%, val_best:  74.58%, tr:  96.22%, tr_best:  96.94%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.701144/  2.822955, val:  73.33%, val_best:  74.58%, tr:  96.83%, tr_best:  96.94%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.659077/  2.962182, val:  72.50%, val_best:  74.58%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.667404/  3.025178, val:  72.08%, val_best:  74.58%, tr:  97.24%, tr_best:  97.34%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.648942/  3.030083, val:  73.75%, val_best:  74.58%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.631624/  3.073642, val:  72.92%, val_best:  74.58%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.619989/  3.107031, val:  73.75%, val_best:  74.58%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.608819/  3.258867, val:  71.67%, val_best:  74.58%, tr:  98.57%, tr_best:  98.88%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.597543/  3.253006, val:  73.75%, val_best:  74.58%, tr:  98.57%, tr_best:  98.88%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.572897/  3.320281, val:  74.17%, val_best:  74.58%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.558235/  3.353172, val:  73.33%, val_best:  74.58%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.544824/  3.483445, val:  72.92%, val_best:  74.58%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.532168/  3.527164, val:  74.58%, val_best:  74.58%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.516921/  3.649839, val:  71.25%, val_best:  74.58%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.512399/  3.667058, val:  75.83%, val_best:  75.83%, tr:  99.18%, tr_best:  99.69%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.501910/  3.743462, val:  73.75%, val_best:  75.83%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.488192/  3.729984, val:  77.08%, val_best:  77.08%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.494641/  3.837039, val:  76.25%, val_best:  77.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.463735/  3.973335, val:  74.58%, val_best:  77.08%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.465238/  3.982031, val:  73.75%, val_best:  77.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.463676/  4.113928, val:  72.50%, val_best:  77.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.443444/  4.134723, val:  72.08%, val_best:  77.08%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.430909/  4.132760, val:  76.67%, val_best:  77.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.431699/  4.212795, val:  74.17%, val_best:  77.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.412723/  4.267275, val:  73.75%, val_best:  77.08%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.432462/  4.377543, val:  73.75%, val_best:  77.08%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.416008/  4.382640, val:  74.17%, val_best:  77.08%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.400379/  4.403052, val:  74.58%, val_best:  77.08%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.394217/  4.455887, val:  75.00%, val_best:  77.08%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.386417/  4.568676, val:  75.00%, val_best:  77.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.391744/  4.600526, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.375404/  4.617136, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.365344/  4.835687, val:  73.75%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.358649/  4.818075, val:  75.00%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.358154/  4.881776, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.364249/  4.895625, val:  75.00%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.357723/  4.978583, val:  75.83%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.336184/  5.041739, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.337275/  5.137389, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.358151/  5.143650, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.338792/  5.231913, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.319834/  5.265898, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.319019/  5.306074, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.316494/  5.377285, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.316612/  5.433226, val:  75.42%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.317600/  5.422287, val:  73.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.309702/  5.522178, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.306912/  5.510085, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.294713/  5.641297, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.300403/  5.614460, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.277768/  5.696762, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.287546/  5.708262, val:  73.75%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.301575/  5.752846, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.298843/  5.818757, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.274810/  5.870907, val:  74.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.276039/  5.941195, val:  74.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.261623/  5.935782, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.253319/  6.024382, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.264701/  6.012939, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.245393/  6.131953, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.224942/  6.170932, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.249711/  6.121662, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.243131/  6.220688, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.238956/  6.271734, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.223744/  6.264242, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.225686/  6.317052, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.204653/  6.378449, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.208802/  6.454746, val:  76.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.209482/  6.406520, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.207620/  6.477661, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.186211/  6.508540, val:  73.75%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.183969/  6.557726, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.183699/  6.639915, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.184291/  6.687816, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.183965/  6.779928, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.204285/  6.733306, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.197683/  6.824244, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.197078/  6.835514, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.189041/  6.852885, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.177389/  6.940911, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.172624/  6.933221, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.179609/  7.009758, val:  77.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.199693/  7.094480, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.188385/  7.038495, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.174427/  7.072366, val:  77.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.179699/  7.110827, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.188873/  7.150414, val:  74.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.176585/  7.263273, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.158502/  7.250398, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.151373/  7.254837, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.155604/  7.293443, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.153521/  7.383986, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.165007/  7.453209, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.162975/  7.400598, val:  77.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.162582/  7.392336, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.160122/  7.489778, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.143492/  7.454638, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.148866/  7.524021, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.147987/  7.627228, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.143599/  7.657704, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.146162/  7.734425, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.130557/  7.776792, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.131871/  7.835832, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.143976/  7.845028, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.124543/  7.857695, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.116306/  7.892504, val:  76.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.132766/  8.018843, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.116956/  8.020292, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.118287/  8.048429, val:  73.75%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.127711/  8.025700, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.125476/  8.085531, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.108745/  8.059751, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.116725/  8.131563, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.110979/  8.096906, val:  73.75%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.113326/  8.167829, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.105904/  8.207634, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.116678/  8.311770, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.113470/  8.289235, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.116711/  8.337391, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.112634/  8.358187, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.113707/  8.431458, val:  74.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.108653/  8.374547, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.089965/  8.367057, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.093696/  8.406839, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.099862/  8.502730, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.094967/  8.443469, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.098875/  8.478949, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.111360/  8.503116, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.105442/  8.595232, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.083188/  8.562593, val:  74.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.099349/  8.585976, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.106404/  8.545649, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.102360/  8.552125, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.097862/  8.594501, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.095543/  8.696264, val:  74.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.108784/  8.591139, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.092413/  8.679029, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.097504/  8.708202, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.090477/  8.838465, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.091184/  8.731315, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.094984/  8.733691, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.091674/  8.870573, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.089556/  8.821731, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.097501/  8.798861, val:  76.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.098066/  8.830089, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.095653/  8.900749, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.092975/  9.019692, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.093194/  9.010777, val:  74.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.092647/  9.038480, val:  73.33%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.084618/  9.011460, val:  73.75%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.089546/  9.041728, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.093752/  9.179291, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.105406/  9.174500, val:  75.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.114746/  9.091070, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.089479/  9.120835, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.098371/  9.131038, val:  76.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.104357/  9.142629, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.085390/  9.159142, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.082378/  9.217501, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.079925/  9.249545, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.080582/  9.264529, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.084150/  9.229640, val:  76.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.081795/  9.320813, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.083826/  9.242753, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.078994/  9.320986, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.087282/  9.291771, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.086712/  9.304553, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.085413/  9.273829, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d32ab0d7ddb40f7a3a207f2f144b90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▂▆▆███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▅▅▆▆▆▇▇▇▇█▇▇▇█▇████████▇█▇█▇▇██▇██▇███</td></tr><tr><td>tr_acc</td><td>▁▄▅▆▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▅▅▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▅▅▆▆▆▇▇▇▇█▇▇▇█▇████████▇█▇█▇▇██▇██▇███</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.08541</td></tr><tr><td>val_acc_best</td><td>0.77917</td></tr><tr><td>val_acc_now</td><td>0.775</td></tr><tr><td>val_loss</td><td>9.27383</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">effortless-sweep-50</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oni89efi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oni89efi</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_091400-oni89efi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ci9vu2fu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1164893e0374b71b71273cf9b36114d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113151245647006, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_092720-ci9vu2fu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ci9vu2fu' target=\"_blank\">vague-sweep-51</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ci9vu2fu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ci9vu2fu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 26.920477/ 39.142139, val:  35.00%, val_best:  35.00%, tr:  25.84%, tr_best:  25.84%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 17.009497/ 18.214151, val:  37.50%, val_best:  37.50%, tr:  41.37%, tr_best:  41.37%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 12.021788/ 10.545276, val:  43.75%, val_best:  43.75%, tr:  52.71%, tr_best:  52.71%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 12.861349/ 16.946808, val:  38.33%, val_best:  43.75%, tr:  50.56%, tr_best:  52.71%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 10.598364/  9.028966, val:  56.25%, val_best:  56.25%, tr:  54.65%, tr_best:  54.65%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  8.622342/ 13.300245, val:  49.58%, val_best:  56.25%, tr:  57.71%, tr_best:  57.71%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  7.897657/ 11.635532, val:  51.67%, val_best:  56.25%, tr:  61.08%, tr_best:  61.08%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 10.745133/ 15.748152, val:  46.67%, val_best:  56.25%, tr:  57.41%, tr_best:  61.08%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 11.428802/ 13.889071, val:  47.92%, val_best:  56.25%, tr:  61.39%, tr_best:  61.39%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 10.054190/ 14.280080, val:  52.08%, val_best:  56.25%, tr:  65.78%, tr_best:  65.78%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 10.323772/ 14.521496, val:  53.33%, val_best:  56.25%, tr:  63.74%, tr_best:  65.78%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  5.587976/ 16.600176, val:  51.67%, val_best:  56.25%, tr:  70.28%, tr_best:  70.28%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  8.204268/ 11.142815, val:  52.92%, val_best:  56.25%, tr:  68.74%, tr_best:  70.28%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  8.783658/ 16.567604, val:  44.58%, val_best:  56.25%, tr:  67.52%, tr_best:  70.28%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  5.418488/ 16.616461, val:  46.67%, val_best:  56.25%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  6.498123/ 22.362740, val:  44.17%, val_best:  56.25%, tr:  77.12%, tr_best:  77.12%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  7.375091/ 16.665905, val:  50.00%, val_best:  56.25%, tr:  75.59%, tr_best:  77.12%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  3.721961/  9.176435, val:  66.67%, val_best:  66.67%, tr:  83.86%, tr_best:  83.86%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  3.117954/ 13.398007, val:  52.92%, val_best:  66.67%, tr:  87.54%, tr_best:  87.54%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  2.453950/  9.560272, val:  72.50%, val_best:  72.50%, tr:  91.22%, tr_best:  91.22%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  2.791913/ 14.105499, val:  60.83%, val_best:  72.50%, tr:  89.48%, tr_best:  91.22%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  3.367839/ 15.538897, val:  51.25%, val_best:  72.50%, tr:  88.15%, tr_best:  91.22%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  2.957486/ 12.158981, val:  62.50%, val_best:  72.50%, tr:  88.87%, tr_best:  91.22%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  2.138600/ 11.220448, val:  67.50%, val_best:  72.50%, tr:  92.54%, tr_best:  92.54%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  1.296963/ 10.934174, val:  64.17%, val_best:  72.50%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  1.099362/ 11.845812, val:  63.75%, val_best:  72.50%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  1.119620/ 10.561602, val:  69.17%, val_best:  72.50%, tr:  96.53%, tr_best:  97.04%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.747192/ 11.142183, val:  66.25%, val_best:  72.50%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.807857/ 10.105723, val:  70.00%, val_best:  72.50%, tr:  98.16%, tr_best:  98.88%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.844910/  9.953430, val:  69.17%, val_best:  72.50%, tr:  98.67%, tr_best:  98.88%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.497717/ 11.205092, val:  65.83%, val_best:  72.50%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.424861/ 11.990350, val:  66.67%, val_best:  72.50%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.508582/ 10.982217, val:  68.75%, val_best:  72.50%, tr:  98.67%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.348284/  9.676955, val:  73.75%, val_best:  73.75%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.251018/ 11.020493, val:  67.92%, val_best:  73.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.256585/  9.805664, val:  71.25%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.176794/ 10.680404, val:  67.50%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.131493/ 10.696644, val:  71.25%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.153987/ 10.656415, val:  71.25%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.122118/ 10.588972, val:  72.92%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.090518/ 10.298216, val:  72.50%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.128898/ 10.403531, val:  70.42%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.180157/ 12.248672, val:  62.50%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.178092/ 10.663352, val:  72.50%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.111934/ 11.105457, val:  72.92%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.080831/ 10.220295, val:  73.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.076385/ 11.042776, val:  73.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.051362/ 10.047404, val:  76.67%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.048249/ 10.928761, val:  71.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.028966/ 10.514673, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.041444/ 11.205941, val:  71.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.047422/  9.659499, val:  77.08%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.038527/ 10.144139, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.017025/ 10.637011, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.041232/ 10.706190, val:  72.50%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.071223/ 11.382662, val:  69.58%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.021851/ 10.654887, val:  71.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.026052/ 10.981065, val:  72.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.036537/ 10.765294, val:  70.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.037034/ 10.694371, val:  73.33%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.014908/ 10.690572, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.011840/ 10.788804, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.015947/ 10.900965, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.008357/ 11.467023, val:  70.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.017368/ 10.735082, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.014620/ 10.699081, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.013896/ 11.364135, val:  73.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.024932/ 11.058877, val:  72.50%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.015359/ 10.940621, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.008667/ 10.730919, val:  73.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.011146/ 10.809431, val:  73.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.017799/ 10.815617, val:  72.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.078738/ 11.158702, val:  72.50%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.024303/ 10.277157, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.011401/ 10.486282, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.010579/ 11.020331, val:  70.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.011406/  9.673614, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.012899/ 10.511712, val:  73.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.012003/ 10.389849, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.014316/ 10.267774, val:  70.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.013536/  9.867259, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.008562/ 10.212933, val:  72.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.002725/ 10.004948, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.002904/ 10.020791, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.007920/ 10.731624, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.005518/ 10.197735, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.005979/ 11.027596, val:  71.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.001049/ 10.457945, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.003005/ 10.769747, val:  71.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.006777/ 10.401339, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.001596/ 10.148558, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.001223/ 10.316120, val:  77.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.001164/ 10.386992, val:  77.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000032/ 10.390399, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000032/ 10.340934, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000023/ 10.359426, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000017/ 10.380981, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000016/ 10.387048, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000012/ 10.407118, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000009/ 10.416425, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss:  0.000009/ 10.422643, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss:  0.000008/ 10.417883, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss:  0.000007/ 10.410074, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss:  0.000008/ 10.413404, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss:  0.000006/ 10.423563, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss:  0.000007/ 10.424953, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss:  0.000006/ 10.433106, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss:  0.000007/ 10.444711, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss:  0.000005/ 10.438450, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss:  0.000007/ 10.441028, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss:  0.000006/ 10.439394, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss:  0.000005/ 10.444633, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.444333, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.449741, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss:  0.000004/ 10.447747, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss:  0.000005/ 10.443936, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss:  0.000005/ 10.436133, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.421510, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.426653, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.435640, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.437422, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.426780, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.427697, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.444221, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.450834, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.445535, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.457704, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.458353, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.453870, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.454715, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.473627, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.474216, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.496836, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.497189, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.501225, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.501989, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.502417, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.503851, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.505010, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.493507, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.488186, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.504953, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.513565, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.519762, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.520363, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.534755, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.535386, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.523460, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.518667, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.516528, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.518751, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.518275, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.511867, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.515035, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.517885, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.511048, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss:  0.000003/ 10.511077, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.516327, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.524355, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.524834, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.525284, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.528659, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.520406, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.525430, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.529947, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.525440, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.529608, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.537741, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.534382, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.536928, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.537081, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.542956, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.542831, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.540190, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.542709, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.542823, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.544718, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.540052, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.540537, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.536637, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.535167, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.536922, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.529096, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.529551, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.529904, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.529036, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.542626, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.539303, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.543483, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.543906, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.544118, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.542768, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.547002, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss:  0.000001/ 10.552134, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss:  0.000001/ 10.553001, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss:  0.000001/ 10.552398, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.561677, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss:  0.000001/ 10.561856, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss:  0.000002/ 10.562263, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss:  0.000001/ 10.562645, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e9ba4ea86146d8a4a52d68bae10093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▁▅▅███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▂▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▅▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▄▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▂▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_loss</td><td>▆▃▄█▁▂▁▂▁▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0</td></tr><tr><td>val_acc_best</td><td>0.77917</td></tr><tr><td>val_acc_now</td><td>0.76667</td></tr><tr><td>val_loss</td><td>10.56264</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vague-sweep-51</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ci9vu2fu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ci9vu2fu</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_092720-ci9vu2fu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g50l5b8p with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a5878ad2d144f7bdb707be07a7fcc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111306724552479, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_094024-g50l5b8p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g50l5b8p' target=\"_blank\">azure-sweep-52</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g50l5b8p' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g50l5b8p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): Feedback_Receiver()\n",
      "      (6): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (7): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (8): Feedback_Receiver()\n",
      "      (9): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.185672/  3.135265, val:  43.75%, val_best:  43.75%, tr:  24.51%, tr_best:  24.51%\n",
      "[module.layers.5] weight_fb parameter count: 2,000\n",
      "[module.layers.8] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.727983/  3.452796, val:  44.58%, val_best:  44.58%, tr:  47.29%, tr_best:  47.29%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  3.466449/  4.151895, val:  49.58%, val_best:  49.58%, tr:  54.95%, tr_best:  54.95%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.987830/  4.326616, val:  50.83%, val_best:  50.83%, tr:  59.96%, tr_best:  59.96%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  3.268484/  5.095343, val:  60.42%, val_best:  60.42%, tr:  59.35%, tr_best:  59.96%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  3.823736/  4.394239, val:  55.00%, val_best:  60.42%, tr:  60.16%, tr_best:  60.16%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  3.362637/  6.444280, val:  52.50%, val_best:  60.42%, tr:  64.66%, tr_best:  64.66%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  3.144562/  6.541986, val:  47.08%, val_best:  60.42%, tr:  69.46%, tr_best:  69.46%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  3.635197/  5.179648, val:  57.92%, val_best:  60.42%, tr:  67.72%, tr_best:  69.46%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  5.109889/  6.972566, val:  57.08%, val_best:  60.42%, tr:  67.52%, tr_best:  69.46%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  4.887625/  5.805880, val:  69.17%, val_best:  69.17%, tr:  69.15%, tr_best:  69.46%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  3.823090/  8.207549, val:  57.50%, val_best:  69.17%, tr:  76.81%, tr_best:  76.81%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  3.715055/  7.578650, val:  64.58%, val_best:  69.17%, tr:  76.30%, tr_best:  76.81%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  3.539791/  8.017151, val:  63.33%, val_best:  69.17%, tr:  80.18%, tr_best:  80.18%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  3.423644/ 10.371109, val:  60.00%, val_best:  69.17%, tr:  83.04%, tr_best:  83.04%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  3.282366/  9.762388, val:  63.33%, val_best:  69.17%, tr:  82.84%, tr_best:  83.04%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  3.537028/ 11.096187, val:  59.58%, val_best:  69.17%, tr:  82.64%, tr_best:  83.04%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  3.488847/ 10.504980, val:  62.92%, val_best:  69.17%, tr:  82.84%, tr_best:  83.04%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  3.412648/ 10.801031, val:  65.00%, val_best:  69.17%, tr:  85.50%, tr_best:  85.50%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  3.137074/ 10.302694, val:  66.67%, val_best:  69.17%, tr:  89.79%, tr_best:  89.79%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  3.986171/ 12.347505, val:  55.00%, val_best:  69.17%, tr:  87.23%, tr_best:  89.79%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  3.368416/ 11.351441, val:  65.42%, val_best:  69.17%, tr:  89.27%, tr_best:  89.79%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  2.471675/ 11.857289, val:  66.25%, val_best:  69.17%, tr:  94.38%, tr_best:  94.38%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  2.439137/ 12.284650, val:  66.67%, val_best:  69.17%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  2.280523/ 13.524927, val:  61.67%, val_best:  69.17%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  2.328591/ 12.460161, val:  69.58%, val_best:  69.58%, tr:  95.61%, tr_best:  97.14%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  2.289543/ 13.643294, val:  65.42%, val_best:  69.58%, tr:  96.12%, tr_best:  97.14%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  2.106767/ 13.382436, val:  66.67%, val_best:  69.58%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  2.032416/ 13.033504, val:  73.33%, val_best:  73.33%, tr:  97.65%, tr_best:  98.06%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  2.241929/ 14.019205, val:  67.08%, val_best:  73.33%, tr:  97.65%, tr_best:  98.06%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  2.210010/ 13.557959, val:  74.17%, val_best:  74.17%, tr:  97.14%, tr_best:  98.06%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  2.238221/ 14.614470, val:  71.67%, val_best:  74.17%, tr:  96.83%, tr_best:  98.06%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  2.487134/ 15.435700, val:  68.75%, val_best:  74.17%, tr:  96.12%, tr_best:  98.06%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  2.044291/ 16.194019, val:  67.50%, val_best:  74.17%, tr:  97.85%, tr_best:  98.06%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  2.041308/ 16.241106, val:  68.75%, val_best:  74.17%, tr:  97.75%, tr_best:  98.06%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  1.746379/ 16.371162, val:  72.08%, val_best:  74.17%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  1.721604/ 16.901970, val:  72.50%, val_best:  74.17%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  1.709422/ 17.132818, val:  68.75%, val_best:  74.17%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  1.526539/ 17.553886, val:  75.00%, val_best:  75.00%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  1.550998/ 17.891439, val:  69.58%, val_best:  75.00%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  1.552255/ 17.964071, val:  72.92%, val_best:  75.00%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  1.492641/ 18.504410, val:  71.67%, val_best:  75.00%, tr:  99.18%, tr_best:  99.69%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  1.493559/ 19.080858, val:  71.25%, val_best:  75.00%, tr:  99.08%, tr_best:  99.69%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  1.585534/ 19.023779, val:  72.50%, val_best:  75.00%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  1.417259/ 20.799055, val:  65.83%, val_best:  75.00%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  1.424844/ 19.937992, val:  70.42%, val_best:  75.00%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  1.236262/ 20.503824, val:  71.25%, val_best:  75.00%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  1.271212/ 20.626320, val:  72.08%, val_best:  75.00%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  1.384406/ 21.242521, val:  69.17%, val_best:  75.00%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  1.298256/ 20.905310, val:  72.50%, val_best:  75.00%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  1.162158/ 21.411203, val:  72.50%, val_best:  75.00%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  1.089898/ 22.002748, val:  70.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  1.182843/ 21.590904, val:  74.17%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  1.256324/ 22.527344, val:  71.67%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  1.177568/ 22.746597, val:  71.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  1.003625/ 22.559113, val:  71.67%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.964245/ 23.125826, val:  75.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  1.213898/ 23.157558, val:  72.08%, val_best:  75.00%, tr:  99.49%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  1.064386/ 24.244633, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.942948/ 23.395910, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.875498/ 24.098032, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  1.035755/ 25.409283, val:  68.75%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.932279/ 24.527155, val:  74.58%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.911371/ 24.437134, val:  74.58%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.848145/ 24.702667, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.850198/ 25.147732, val:  71.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.808825/ 25.928768, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.843802/ 25.758549, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.876429/ 25.893223, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.928674/ 25.799713, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.714493/ 25.711260, val:  74.58%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.844886/ 26.448309, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.824150/ 26.867342, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.772023/ 27.018810, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.644469/ 27.021271, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.667516/ 27.639040, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.640351/ 28.080578, val:  73.33%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.622267/ 28.352749, val:  72.08%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.612951/ 28.503094, val:  72.92%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.589561/ 28.290279, val:  73.33%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.695546/ 28.879780, val:  72.92%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.553532/ 28.700974, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.540955/ 28.754665, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.515207/ 29.375160, val:  72.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.572639/ 29.241081, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.695266/ 29.843941, val:  74.17%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.591556/ 30.647434, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.588983/ 30.019865, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.581551/ 30.275442, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.548893/ 30.874706, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.596774/ 30.896072, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.615204/ 31.280848, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.511811/ 31.350336, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.472088/ 31.343452, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.481399/ 31.985134, val:  74.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.488499/ 32.676376, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.554979/ 32.233662, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.493729/ 32.352676, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.528768/ 33.061211, val:  72.92%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.515630/ 32.767471, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.493107/ 33.181519, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.436366/ 32.898071, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.439297/ 33.644814, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.514138/ 33.217464, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.507681/ 33.639782, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.526718/ 33.462776, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.483438/ 33.990879, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.467718/ 34.416122, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.426826/ 34.157928, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.438652/ 34.350048, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.428771/ 34.342674, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.416346/ 33.835880, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.321021/ 35.042717, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.409041/ 34.508266, val:  76.25%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.433223/ 34.063808, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.456232/ 35.664715, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.411791/ 35.597054, val:  74.17%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.505063/ 34.787346, val:  77.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.394383/ 35.825073, val:  72.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.396815/ 35.769058, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.360279/ 35.869488, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.417138/ 35.776779, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.327676/ 36.127895, val:  76.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.352414/ 36.316574, val:  75.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.310735/ 35.991577, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.349693/ 36.409798, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.338135/ 36.629681, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.302811/ 37.020435, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.353051/ 37.217289, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.337737/ 37.502327, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.346587/ 36.963848, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.391951/ 37.504395, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.340674/ 37.962997, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.323638/ 37.634895, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.308036/ 38.272251, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.340226/ 38.278133, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.302942/ 37.747780, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.252181/ 38.050007, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.306795/ 37.781326, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.321132/ 38.039070, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.292635/ 38.084389, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.389088/ 38.478497, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.343773/ 38.698078, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.347987/ 38.372124, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.348459/ 38.219940, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.336801/ 38.383369, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.272612/ 39.100727, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.262937/ 38.617870, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.250928/ 38.519985, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.192501/ 38.880577, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.225416/ 38.897148, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.286462/ 39.111496, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.312126/ 38.779625, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.351711/ 39.608692, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.233554/ 39.652817, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.297645/ 40.040562, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.225272/ 39.894249, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.289726/ 40.447594, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.242473/ 40.154968, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.308154/ 40.564896, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.273897/ 40.936108, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.274865/ 40.964436, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.247203/ 40.815472, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.306851/ 40.797089, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.231960/ 40.974445, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.225286/ 41.560814, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.274806/ 41.257248, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.227194/ 41.520103, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.183742/ 41.448875, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.197214/ 41.317417, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.222028/ 42.207573, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.202911/ 42.183144, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.182883/ 42.102520, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.178415/ 42.247791, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.196570/ 42.562943, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.156732/ 42.360542, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.167998/ 42.944073, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.209852/ 42.398777, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.267530/ 43.098747, val:  76.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.197334/ 42.548470, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.222258/ 42.975834, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.199124/ 42.812393, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.174758/ 43.329956, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.167224/ 43.341156, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.172694/ 42.895779, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.124369/ 43.472530, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.238924/ 43.162807, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.181829/ 43.280197, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.184195/ 42.772598, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.147235/ 43.113560, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.159167/ 42.877308, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.200270/ 43.301453, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.226076/ 43.588253, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.190103/ 43.588463, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.128700/ 43.878521, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.192632/ 43.524673, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.148520/ 43.832539, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.216498/ 43.658646, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.199300/ 43.738308, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.163657/ 43.928780, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a666bd3d2948b8bd681bf7057ac023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▄▇▅▇▇█████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▅▆▆▆▇▇▅▇▆▇▆▇▇▇▇█▇▇▇▇▇██▇█████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▆▇▇██████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▅▆█▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▅▆▆▆▇▇▅▇▆▇▆▇▇▇▇█▇▇▇▇▇██▇█████████████</td></tr><tr><td>val_loss</td><td>▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.16366</td></tr><tr><td>val_acc_best</td><td>0.79167</td></tr><tr><td>val_acc_now</td><td>0.78333</td></tr><tr><td>val_loss</td><td>43.92878</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">azure-sweep-52</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g50l5b8p' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g50l5b8p</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_094024-g50l5b8p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9b5k6ygy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_095335-9b5k6ygy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9b5k6ygy' target=\"_blank\">magic-sweep-53</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9b5k6ygy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9b5k6ygy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  2.447581/  3.204786, val:  47.50%, val_best:  47.50%, tr:  30.64%, tr_best:  30.64%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  2.150635/  1.823341, val:  45.42%, val_best:  47.50%, tr:  43.00%, tr_best:  43.00%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  1.844977/  1.999849, val:  38.75%, val_best:  47.50%, tr:  44.02%, tr_best:  44.02%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  1.907530/  2.355238, val:  35.42%, val_best:  47.50%, tr:  43.31%, tr_best:  44.02%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  1.772810/  2.364043, val:  47.08%, val_best:  47.50%, tr:  48.72%, tr_best:  48.72%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  1.844425/  2.050901, val:  36.67%, val_best:  47.50%, tr:  46.88%, tr_best:  48.72%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  1.969822/  1.854955, val:  42.50%, val_best:  47.50%, tr:  44.23%, tr_best:  48.72%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  1.928276/  2.307671, val:  30.42%, val_best:  47.50%, tr:  42.80%, tr_best:  48.72%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  1.893769/  2.392225, val:  43.75%, val_best:  47.50%, tr:  45.45%, tr_best:  48.72%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  2.013196/  2.482102, val:  31.67%, val_best:  47.50%, tr:  49.03%, tr_best:  49.03%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  1.850277/  2.463533, val:  44.58%, val_best:  47.50%, tr:  48.42%, tr_best:  49.03%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  2.178582/  2.193037, val:  44.17%, val_best:  47.50%, tr:  49.64%, tr_best:  49.64%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  1.945871/  1.945261, val:  45.83%, val_best:  47.50%, tr:  48.72%, tr_best:  49.64%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  1.952147/  2.843400, val:  39.17%, val_best:  47.50%, tr:  47.60%, tr_best:  49.64%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  2.396668/  4.715521, val:  50.00%, val_best:  50.00%, tr:  50.36%, tr_best:  50.36%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  2.117533/  3.715712, val:  37.50%, val_best:  50.00%, tr:  49.64%, tr_best:  50.36%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  2.368417/  4.312494, val:  48.33%, val_best:  50.00%, tr:  50.05%, tr_best:  50.36%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  2.169337/  2.543855, val:  39.17%, val_best:  50.00%, tr:  48.31%, tr_best:  50.36%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  1.976526/  2.564967, val:  42.08%, val_best:  50.00%, tr:  50.56%, tr_best:  50.56%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  1.885689/  2.223529, val:  45.42%, val_best:  50.00%, tr:  51.17%, tr_best:  51.17%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  2.186813/  2.316218, val:  50.83%, val_best:  50.83%, tr:  52.60%, tr_best:  52.60%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  2.048632/  2.856420, val:  47.08%, val_best:  50.83%, tr:  51.28%, tr_best:  52.60%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  1.891357/  2.250588, val:  42.08%, val_best:  50.83%, tr:  50.46%, tr_best:  52.60%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  1.916003/  2.365054, val:  55.42%, val_best:  55.42%, tr:  52.40%, tr_best:  52.60%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  2.085239/  2.744290, val:  39.58%, val_best:  55.42%, tr:  51.48%, tr_best:  52.60%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  2.208917/  2.266667, val:  46.67%, val_best:  55.42%, tr:  48.42%, tr_best:  52.60%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  2.085830/  2.626388, val:  50.42%, val_best:  55.42%, tr:  49.44%, tr_best:  52.60%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  2.220907/  2.695752, val:  37.92%, val_best:  55.42%, tr:  51.58%, tr_best:  52.60%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  1.926547/  2.104555, val:  52.08%, val_best:  55.42%, tr:  51.69%, tr_best:  52.60%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  1.946178/  2.637340, val:  42.92%, val_best:  55.42%, tr:  55.16%, tr_best:  55.16%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  2.785403/  3.171165, val:  45.83%, val_best:  55.42%, tr:  46.68%, tr_best:  55.16%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  1.975598/  2.600852, val:  47.92%, val_best:  55.42%, tr:  55.67%, tr_best:  55.67%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  2.302827/  2.565585, val:  49.17%, val_best:  55.42%, tr:  52.09%, tr_best:  55.67%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  2.179420/  2.350649, val:  44.58%, val_best:  55.42%, tr:  50.36%, tr_best:  55.67%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  1.942188/  2.099890, val:  46.67%, val_best:  55.42%, tr:  47.70%, tr_best:  55.67%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  2.167253/  3.284366, val:  45.00%, val_best:  55.42%, tr:  50.15%, tr_best:  55.67%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  2.110632/  2.519334, val:  52.08%, val_best:  55.42%, tr:  49.23%, tr_best:  55.67%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  2.535895/  2.529791, val:  40.83%, val_best:  55.42%, tr:  50.87%, tr_best:  55.67%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  2.518256/  2.408702, val:  49.58%, val_best:  55.42%, tr:  50.26%, tr_best:  55.67%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  2.215926/  3.289530, val:  46.67%, val_best:  55.42%, tr:  53.12%, tr_best:  55.67%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  2.153128/  2.726650, val:  46.25%, val_best:  55.42%, tr:  54.55%, tr_best:  55.67%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  2.709376/  2.772475, val:  43.33%, val_best:  55.42%, tr:  51.38%, tr_best:  55.67%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  2.788357/  2.243626, val:  41.67%, val_best:  55.42%, tr:  44.74%, tr_best:  55.67%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  2.293668/  2.527784, val:  52.08%, val_best:  55.42%, tr:  42.80%, tr_best:  55.67%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  2.438447/  2.308901, val:  47.08%, val_best:  55.42%, tr:  45.35%, tr_best:  55.67%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  2.138759/  2.794668, val:  45.42%, val_best:  55.42%, tr:  47.40%, tr_best:  55.67%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  2.873943/  2.847480, val:  43.75%, val_best:  55.42%, tr:  46.37%, tr_best:  55.67%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  2.455356/  2.869298, val:  36.25%, val_best:  55.42%, tr:  45.05%, tr_best:  55.67%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  2.232028/  2.578602, val:  51.25%, val_best:  55.42%, tr:  49.44%, tr_best:  55.67%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  2.418070/  3.159146, val:  40.42%, val_best:  55.42%, tr:  49.34%, tr_best:  55.67%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  2.877635/  3.095373, val:  42.92%, val_best:  55.42%, tr:  45.76%, tr_best:  55.67%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  2.205553/  2.272795, val:  37.08%, val_best:  55.42%, tr:  49.03%, tr_best:  55.67%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  2.220817/  2.060336, val:  49.17%, val_best:  55.42%, tr:  39.43%, tr_best:  55.67%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  2.142883/  2.787867, val:  43.33%, val_best:  55.42%, tr:  44.94%, tr_best:  55.67%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  2.594020/  2.196533, val:  47.92%, val_best:  55.42%, tr:  48.31%, tr_best:  55.67%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  2.185610/  4.258265, val:  53.33%, val_best:  55.42%, tr:  48.52%, tr_best:  55.67%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  2.777902/  3.155546, val:  38.75%, val_best:  55.42%, tr:  48.31%, tr_best:  55.67%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  3.094373/  2.982386, val:  36.67%, val_best:  55.42%, tr:  47.91%, tr_best:  55.67%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  2.355644/  2.426251, val:  52.92%, val_best:  55.42%, tr:  51.89%, tr_best:  55.67%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  1.998660/  2.803087, val:  37.92%, val_best:  55.42%, tr:  52.30%, tr_best:  55.67%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  2.923179/  2.731891, val:  48.75%, val_best:  55.42%, tr:  48.31%, tr_best:  55.67%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  2.319191/  2.917676, val:  50.42%, val_best:  55.42%, tr:  54.75%, tr_best:  55.67%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  2.911217/  2.398083, val:  50.42%, val_best:  55.42%, tr:  50.77%, tr_best:  55.67%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  3.440930/  4.228928, val:  48.33%, val_best:  55.42%, tr:  46.68%, tr_best:  55.67%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  3.558383/  4.989270, val:  42.50%, val_best:  55.42%, tr:  47.60%, tr_best:  55.67%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  3.587679/  4.490898, val:  47.08%, val_best:  55.42%, tr:  48.21%, tr_best:  55.67%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  2.504757/  2.655189, val:  48.33%, val_best:  55.42%, tr:  48.93%, tr_best:  55.67%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  2.702194/  3.938689, val:  46.25%, val_best:  55.42%, tr:  46.37%, tr_best:  55.67%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  4.211529/  3.419197, val:  46.67%, val_best:  55.42%, tr:  49.64%, tr_best:  55.67%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  3.172069/  6.414093, val:  49.58%, val_best:  55.42%, tr:  50.87%, tr_best:  55.67%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  3.633386/  2.152252, val:  45.00%, val_best:  55.42%, tr:  46.88%, tr_best:  55.67%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  2.793884/  2.868366, val:  37.50%, val_best:  55.42%, tr:  53.93%, tr_best:  55.67%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  2.806021/  4.298110, val:  46.25%, val_best:  55.42%, tr:  46.88%, tr_best:  55.67%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  3.122238/  4.307427, val:  46.25%, val_best:  55.42%, tr:  51.58%, tr_best:  55.67%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  3.023772/  2.325810, val:  52.08%, val_best:  55.42%, tr:  46.17%, tr_best:  55.67%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  3.381907/  2.715171, val:  46.67%, val_best:  55.42%, tr:  51.38%, tr_best:  55.67%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  3.125743/  3.261479, val:  41.25%, val_best:  55.42%, tr:  46.68%, tr_best:  55.67%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  2.882519/  2.606403, val:  43.75%, val_best:  55.42%, tr:  47.60%, tr_best:  55.67%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  3.499853/  2.655448, val:  40.00%, val_best:  55.42%, tr:  46.17%, tr_best:  55.67%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  3.286053/  6.170470, val:  42.50%, val_best:  55.42%, tr:  46.17%, tr_best:  55.67%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  3.664962/  3.994585, val:  46.25%, val_best:  55.42%, tr:  47.50%, tr_best:  55.67%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  3.110475/  3.629219, val:  38.75%, val_best:  55.42%, tr:  49.85%, tr_best:  55.67%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  3.251417/  3.534111, val:  53.33%, val_best:  55.42%, tr:  44.43%, tr_best:  55.67%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  3.523029/  3.554804, val:  37.50%, val_best:  55.42%, tr:  47.70%, tr_best:  55.67%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  2.510277/  3.026680, val:  46.25%, val_best:  55.42%, tr:  47.09%, tr_best:  55.67%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  2.567295/  4.114982, val:  32.50%, val_best:  55.42%, tr:  47.91%, tr_best:  55.67%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  2.920868/  3.875188, val:  44.58%, val_best:  55.42%, tr:  45.45%, tr_best:  55.67%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  2.926054/  3.387454, val:  40.83%, val_best:  55.42%, tr:  46.99%, tr_best:  55.67%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  2.544187/  3.470498, val:  47.50%, val_best:  55.42%, tr:  46.99%, tr_best:  55.67%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  2.927720/  3.093290, val:  41.25%, val_best:  55.42%, tr:  45.56%, tr_best:  55.67%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  2.991739/  2.942563, val:  43.33%, val_best:  55.42%, tr:  39.73%, tr_best:  55.67%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  2.556321/  2.888541, val:  41.67%, val_best:  55.42%, tr:  44.33%, tr_best:  55.67%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  2.314120/  2.660338, val:  45.00%, val_best:  55.42%, tr:  47.60%, tr_best:  55.67%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  3.324535/  3.400393, val:  38.75%, val_best:  55.42%, tr:  41.68%, tr_best:  55.67%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  2.520474/  6.200815, val:  46.25%, val_best:  55.42%, tr:  45.66%, tr_best:  55.67%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  3.334141/  3.542863, val:  47.92%, val_best:  55.42%, tr:  41.88%, tr_best:  55.67%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  2.446016/  3.980841, val:  43.75%, val_best:  55.42%, tr:  48.52%, tr_best:  55.67%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  2.249366/  3.070009, val:  41.25%, val_best:  55.42%, tr:  49.34%, tr_best:  55.67%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  2.751423/  5.663824, val:  39.58%, val_best:  55.42%, tr:  44.13%, tr_best:  55.67%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  4.539032/  4.608170, val:  48.75%, val_best:  55.42%, tr:  41.06%, tr_best:  55.67%\n",
      "epoch-100 lr=['0.1000000'], tr/val_loss:  2.931418/  3.887146, val:  50.83%, val_best:  55.42%, tr:  47.80%, tr_best:  55.67%\n",
      "epoch-101 lr=['0.1000000'], tr/val_loss:  2.689077/  4.364946, val:  37.08%, val_best:  55.42%, tr:  45.45%, tr_best:  55.67%\n",
      "epoch-102 lr=['0.1000000'], tr/val_loss:  5.254220/  8.453604, val:  42.92%, val_best:  55.42%, tr:  44.23%, tr_best:  55.67%\n",
      "epoch-103 lr=['0.1000000'], tr/val_loss:  3.776188/  3.712744, val:  28.33%, val_best:  55.42%, tr:  44.74%, tr_best:  55.67%\n",
      "epoch-104 lr=['0.1000000'], tr/val_loss:  2.893034/  3.008188, val:  44.17%, val_best:  55.42%, tr:  45.66%, tr_best:  55.67%\n",
      "epoch-105 lr=['0.1000000'], tr/val_loss:  3.053441/  7.641209, val:  45.42%, val_best:  55.42%, tr:  48.62%, tr_best:  55.67%\n",
      "epoch-106 lr=['0.1000000'], tr/val_loss:  3.562770/  6.514539, val:  32.92%, val_best:  55.42%, tr:  48.11%, tr_best:  55.67%\n",
      "epoch-107 lr=['0.1000000'], tr/val_loss:  4.159862/  5.594218, val:  42.50%, val_best:  55.42%, tr:  44.74%, tr_best:  55.67%\n",
      "epoch-108 lr=['0.1000000'], tr/val_loss:  2.510690/  4.679102, val:  42.92%, val_best:  55.42%, tr:  45.97%, tr_best:  55.67%\n",
      "epoch-109 lr=['0.1000000'], tr/val_loss:  4.204865/  3.890756, val:  33.75%, val_best:  55.42%, tr:  46.68%, tr_best:  55.67%\n",
      "epoch-110 lr=['0.1000000'], tr/val_loss:  2.647793/  2.762628, val:  49.17%, val_best:  55.42%, tr:  44.33%, tr_best:  55.67%\n",
      "epoch-111 lr=['0.1000000'], tr/val_loss:  2.710538/  5.463230, val:  47.50%, val_best:  55.42%, tr:  48.52%, tr_best:  55.67%\n",
      "epoch-112 lr=['0.1000000'], tr/val_loss:  4.298713/  2.353980, val:  45.42%, val_best:  55.42%, tr:  43.72%, tr_best:  55.67%\n",
      "epoch-113 lr=['0.1000000'], tr/val_loss:  2.153873/  2.575327, val:  42.50%, val_best:  55.42%, tr:  50.66%, tr_best:  55.67%\n",
      "epoch-114 lr=['0.1000000'], tr/val_loss:  2.669032/  9.158900, val:  35.83%, val_best:  55.42%, tr:  48.52%, tr_best:  55.67%\n",
      "epoch-115 lr=['0.1000000'], tr/val_loss: 10.365158/  3.400212, val:  26.25%, val_best:  55.42%, tr:  34.32%, tr_best:  55.67%\n",
      "epoch-116 lr=['0.1000000'], tr/val_loss:  3.014631/  2.803094, val:  40.83%, val_best:  55.42%, tr:  33.50%, tr_best:  55.67%\n",
      "epoch-117 lr=['0.1000000'], tr/val_loss:  2.974117/  3.427172, val:  35.83%, val_best:  55.42%, tr:  35.04%, tr_best:  55.67%\n",
      "epoch-118 lr=['0.1000000'], tr/val_loss:  3.287052/  7.831794, val:  29.17%, val_best:  55.42%, tr:  32.18%, tr_best:  55.67%\n",
      "epoch-119 lr=['0.1000000'], tr/val_loss:  3.218874/  2.264421, val:  40.42%, val_best:  55.42%, tr:  32.07%, tr_best:  55.67%\n",
      "epoch-120 lr=['0.1000000'], tr/val_loss:  2.562063/  2.568587, val:  33.75%, val_best:  55.42%, tr:  35.96%, tr_best:  55.67%\n",
      "epoch-121 lr=['0.1000000'], tr/val_loss:  2.593048/  3.467626, val:  33.33%, val_best:  55.42%, tr:  35.96%, tr_best:  55.67%\n",
      "epoch-122 lr=['0.1000000'], tr/val_loss:  3.053775/  3.079201, val:  31.67%, val_best:  55.42%, tr:  32.99%, tr_best:  55.67%\n",
      "epoch-123 lr=['0.1000000'], tr/val_loss:  5.686259/  3.482271, val:  36.25%, val_best:  55.42%, tr:  31.56%, tr_best:  55.67%\n",
      "epoch-124 lr=['0.1000000'], tr/val_loss:  4.287678/  4.739009, val:  35.00%, val_best:  55.42%, tr:  35.24%, tr_best:  55.67%\n",
      "epoch-125 lr=['0.1000000'], tr/val_loss:  6.797735/  5.922288, val:  24.17%, val_best:  55.42%, tr:  33.91%, tr_best:  55.67%\n",
      "epoch-126 lr=['0.1000000'], tr/val_loss:  3.898898/  3.682544, val:  37.50%, val_best:  55.42%, tr:  33.91%, tr_best:  55.67%\n",
      "epoch-127 lr=['0.1000000'], tr/val_loss:  3.549408/  4.896835, val:  38.33%, val_best:  55.42%, tr:  36.47%, tr_best:  55.67%\n",
      "epoch-128 lr=['0.1000000'], tr/val_loss:  4.275610/  8.746262, val:  14.17%, val_best:  55.42%, tr:  28.19%, tr_best:  55.67%\n",
      "epoch-129 lr=['0.1000000'], tr/val_loss:  4.326717/  5.584307, val:  19.17%, val_best:  55.42%, tr:  25.84%, tr_best:  55.67%\n",
      "epoch-130 lr=['0.1000000'], tr/val_loss:  4.821890/  4.899109, val:  22.50%, val_best:  55.42%, tr:  23.19%, tr_best:  55.67%\n",
      "epoch-131 lr=['0.1000000'], tr/val_loss:  3.988039/  8.146687, val:  27.92%, val_best:  55.42%, tr:  31.56%, tr_best:  55.67%\n",
      "epoch-132 lr=['0.1000000'], tr/val_loss: 10.387770/ 12.112978, val:   9.58%, val_best:  55.42%, tr:  27.48%, tr_best:  55.67%\n",
      "epoch-133 lr=['0.1000000'], tr/val_loss:  5.442015/  4.177281, val:  21.67%, val_best:  55.42%, tr:  17.06%, tr_best:  55.67%\n",
      "epoch-134 lr=['0.1000000'], tr/val_loss:  3.695879/  3.909948, val:  13.33%, val_best:  55.42%, tr:  17.36%, tr_best:  55.67%\n",
      "epoch-135 lr=['0.1000000'], tr/val_loss:  3.776012/  4.974046, val:  15.00%, val_best:  55.42%, tr:  14.91%, tr_best:  55.67%\n",
      "epoch-136 lr=['0.1000000'], tr/val_loss:  3.614902/  2.489387, val:  29.17%, val_best:  55.42%, tr:  19.92%, tr_best:  55.67%\n",
      "epoch-137 lr=['0.1000000'], tr/val_loss:  3.016623/  4.173381, val:  27.08%, val_best:  55.42%, tr:  17.67%, tr_best:  55.67%\n",
      "epoch-138 lr=['0.1000000'], tr/val_loss:  3.632091/  3.416604, val:  20.00%, val_best:  55.42%, tr:  23.60%, tr_best:  55.67%\n",
      "epoch-139 lr=['0.1000000'], tr/val_loss:  3.042600/  3.033933, val:  26.67%, val_best:  55.42%, tr:  19.61%, tr_best:  55.67%\n",
      "epoch-140 lr=['0.1000000'], tr/val_loss:  3.048256/  2.927903, val:  16.25%, val_best:  55.42%, tr:  21.86%, tr_best:  55.67%\n",
      "epoch-141 lr=['0.1000000'], tr/val_loss:  3.284380/  3.703568, val:  17.92%, val_best:  55.42%, tr:  19.20%, tr_best:  55.67%\n",
      "epoch-142 lr=['0.1000000'], tr/val_loss:  2.998428/  3.420359, val:  21.67%, val_best:  55.42%, tr:  22.57%, tr_best:  55.67%\n",
      "epoch-143 lr=['0.1000000'], tr/val_loss:  3.229452/  4.421212, val:  23.75%, val_best:  55.42%, tr:  25.23%, tr_best:  55.67%\n",
      "epoch-144 lr=['0.1000000'], tr/val_loss:  3.504797/  5.317955, val:  21.67%, val_best:  55.42%, tr:  24.00%, tr_best:  55.67%\n",
      "epoch-145 lr=['0.1000000'], tr/val_loss:  3.626245/  4.878497, val:  14.17%, val_best:  55.42%, tr:  22.17%, tr_best:  55.67%\n",
      "epoch-146 lr=['0.1000000'], tr/val_loss:  3.437948/  3.380687, val:  31.25%, val_best:  55.42%, tr:  23.80%, tr_best:  55.67%\n",
      "epoch-147 lr=['0.1000000'], tr/val_loss:  4.668675/  3.867796, val:  18.33%, val_best:  55.42%, tr:  23.49%, tr_best:  55.67%\n",
      "epoch-148 lr=['0.1000000'], tr/val_loss:  4.641478/  5.402482, val:  20.83%, val_best:  55.42%, tr:  17.36%, tr_best:  55.67%\n",
      "epoch-149 lr=['0.1000000'], tr/val_loss:  3.752640/  4.445288, val:  13.75%, val_best:  55.42%, tr:  20.33%, tr_best:  55.67%\n",
      "epoch-150 lr=['0.1000000'], tr/val_loss:  3.629967/  5.107660, val:  22.08%, val_best:  55.42%, tr:  21.35%, tr_best:  55.67%\n",
      "epoch-151 lr=['0.1000000'], tr/val_loss:  3.940050/  4.449521, val:  23.75%, val_best:  55.42%, tr:  25.03%, tr_best:  55.67%\n",
      "epoch-152 lr=['0.1000000'], tr/val_loss:  3.993600/  4.409940, val:  22.50%, val_best:  55.42%, tr:  16.24%, tr_best:  55.67%\n",
      "epoch-153 lr=['0.1000000'], tr/val_loss:  3.384196/  5.410208, val:  27.08%, val_best:  55.42%, tr:  23.19%, tr_best:  55.67%\n",
      "epoch-154 lr=['0.1000000'], tr/val_loss:  4.089379/  4.319680, val:  29.58%, val_best:  55.42%, tr:  26.35%, tr_best:  55.67%\n",
      "epoch-155 lr=['0.1000000'], tr/val_loss:  4.742444/  3.871440, val:  23.33%, val_best:  55.42%, tr:  24.82%, tr_best:  55.67%\n",
      "epoch-156 lr=['0.1000000'], tr/val_loss:  3.328476/  4.571846, val:  16.25%, val_best:  55.42%, tr:  29.32%, tr_best:  55.67%\n",
      "epoch-157 lr=['0.1000000'], tr/val_loss:  3.971684/  4.476359, val:  23.75%, val_best:  55.42%, tr:  26.56%, tr_best:  55.67%\n",
      "epoch-158 lr=['0.1000000'], tr/val_loss:  3.338950/  3.049862, val:  26.25%, val_best:  55.42%, tr:  29.11%, tr_best:  55.67%\n",
      "epoch-159 lr=['0.1000000'], tr/val_loss:  2.851612/  2.918845, val:  33.33%, val_best:  55.42%, tr:  27.78%, tr_best:  55.67%\n",
      "epoch-160 lr=['0.1000000'], tr/val_loss:  3.124395/  3.610197, val:  34.58%, val_best:  55.42%, tr:  26.86%, tr_best:  55.67%\n",
      "epoch-161 lr=['0.1000000'], tr/val_loss:  3.957159/  3.877482, val:  20.83%, val_best:  55.42%, tr:  28.40%, tr_best:  55.67%\n",
      "epoch-162 lr=['0.1000000'], tr/val_loss:  3.053615/  3.061948, val:  14.17%, val_best:  55.42%, tr:  29.01%, tr_best:  55.67%\n",
      "epoch-163 lr=['0.1000000'], tr/val_loss:  2.898256/  3.501969, val:  33.75%, val_best:  55.42%, tr:  26.76%, tr_best:  55.67%\n",
      "epoch-164 lr=['0.1000000'], tr/val_loss:  3.278348/  3.740602, val:  14.17%, val_best:  55.42%, tr:  23.70%, tr_best:  55.67%\n",
      "epoch-165 lr=['0.1000000'], tr/val_loss:  3.076759/  3.519337, val:  15.00%, val_best:  55.42%, tr:  27.99%, tr_best:  55.67%\n",
      "epoch-166 lr=['0.1000000'], tr/val_loss:  3.258926/  4.710557, val:  22.50%, val_best:  55.42%, tr:  27.78%, tr_best:  55.67%\n",
      "epoch-167 lr=['0.1000000'], tr/val_loss:  3.429124/  3.801766, val:  20.00%, val_best:  55.42%, tr:  26.97%, tr_best:  55.67%\n",
      "epoch-168 lr=['0.1000000'], tr/val_loss:  3.226898/  3.818674, val:  32.50%, val_best:  55.42%, tr:  27.68%, tr_best:  55.67%\n",
      "epoch-169 lr=['0.1000000'], tr/val_loss:  3.583766/  2.914320, val:  26.25%, val_best:  55.42%, tr:  26.35%, tr_best:  55.67%\n",
      "epoch-170 lr=['0.1000000'], tr/val_loss:  2.518044/  2.521449, val:  23.33%, val_best:  55.42%, tr:  31.05%, tr_best:  55.67%\n",
      "epoch-171 lr=['0.1000000'], tr/val_loss:  3.027818/  3.410167, val:  34.17%, val_best:  55.42%, tr:  30.64%, tr_best:  55.67%\n",
      "epoch-172 lr=['0.1000000'], tr/val_loss:  2.464137/  2.546092, val:  25.00%, val_best:  55.42%, tr:  30.03%, tr_best:  55.67%\n",
      "epoch-173 lr=['0.1000000'], tr/val_loss:  2.989759/  3.889819, val:  34.17%, val_best:  55.42%, tr:  29.32%, tr_best:  55.67%\n",
      "epoch-174 lr=['0.1000000'], tr/val_loss:  2.822371/  2.278570, val:  31.67%, val_best:  55.42%, tr:  33.81%, tr_best:  55.67%\n",
      "epoch-175 lr=['0.1000000'], tr/val_loss:  2.965135/  3.317335, val:  36.67%, val_best:  55.42%, tr:  28.60%, tr_best:  55.67%\n",
      "epoch-176 lr=['0.1000000'], tr/val_loss:  4.462993/  3.015285, val:  22.92%, val_best:  55.42%, tr:  23.90%, tr_best:  55.67%\n",
      "epoch-177 lr=['0.1000000'], tr/val_loss:  4.181159/  5.268836, val:  31.25%, val_best:  55.42%, tr:  23.90%, tr_best:  55.67%\n",
      "epoch-178 lr=['0.1000000'], tr/val_loss:  3.416464/  3.654943, val:  35.00%, val_best:  55.42%, tr:  29.93%, tr_best:  55.67%\n",
      "epoch-179 lr=['0.1000000'], tr/val_loss:  3.101007/  3.063493, val:  29.17%, val_best:  55.42%, tr:  38.00%, tr_best:  55.67%\n",
      "epoch-180 lr=['0.1000000'], tr/val_loss:  2.606910/  2.731870, val:  36.67%, val_best:  55.42%, tr:  34.93%, tr_best:  55.67%\n",
      "epoch-181 lr=['0.1000000'], tr/val_loss:  3.596677/  9.101272, val:  30.00%, val_best:  55.42%, tr:  34.32%, tr_best:  55.67%\n",
      "epoch-182 lr=['0.1000000'], tr/val_loss:  4.372567/  3.160073, val:  36.67%, val_best:  55.42%, tr:  32.79%, tr_best:  55.67%\n",
      "epoch-183 lr=['0.1000000'], tr/val_loss:  3.046403/  3.023070, val:  20.42%, val_best:  55.42%, tr:  32.28%, tr_best:  55.67%\n",
      "epoch-184 lr=['0.1000000'], tr/val_loss:  2.911320/  2.906758, val:  17.08%, val_best:  55.42%, tr:  32.38%, tr_best:  55.67%\n",
      "epoch-185 lr=['0.1000000'], tr/val_loss:  3.725091/  5.823261, val:  25.42%, val_best:  55.42%, tr:  30.03%, tr_best:  55.67%\n",
      "epoch-186 lr=['0.1000000'], tr/val_loss:  3.608836/  3.249475, val:  45.42%, val_best:  55.42%, tr:  30.64%, tr_best:  55.67%\n",
      "epoch-187 lr=['0.1000000'], tr/val_loss:  4.050716/  5.183563, val:  19.58%, val_best:  55.42%, tr:  29.52%, tr_best:  55.67%\n",
      "epoch-188 lr=['0.1000000'], tr/val_loss:  5.352451/  3.402295, val:  18.33%, val_best:  55.42%, tr:  27.48%, tr_best:  55.67%\n",
      "epoch-189 lr=['0.1000000'], tr/val_loss:  2.832919/  3.141017, val:  22.50%, val_best:  55.42%, tr:  33.61%, tr_best:  55.67%\n",
      "epoch-190 lr=['0.1000000'], tr/val_loss:  3.031579/  2.820947, val:  42.08%, val_best:  55.42%, tr:  36.06%, tr_best:  55.67%\n",
      "epoch-191 lr=['0.1000000'], tr/val_loss:  4.203926/  6.033660, val:  25.83%, val_best:  55.42%, tr:  33.30%, tr_best:  55.67%\n",
      "epoch-192 lr=['0.1000000'], tr/val_loss:  3.075677/  3.419960, val:  27.92%, val_best:  55.42%, tr:  34.93%, tr_best:  55.67%\n",
      "epoch-193 lr=['0.1000000'], tr/val_loss:  2.609871/  4.620953, val:  18.75%, val_best:  55.42%, tr:  35.04%, tr_best:  55.67%\n",
      "epoch-194 lr=['0.1000000'], tr/val_loss:  3.270309/  3.048893, val:  40.42%, val_best:  55.42%, tr:  32.58%, tr_best:  55.67%\n",
      "epoch-195 lr=['0.1000000'], tr/val_loss:  2.810966/  3.159653, val:  25.42%, val_best:  55.42%, tr:  34.12%, tr_best:  55.67%\n",
      "epoch-196 lr=['0.1000000'], tr/val_loss:  2.711118/  3.655339, val:  31.25%, val_best:  55.42%, tr:  34.22%, tr_best:  55.67%\n",
      "epoch-197 lr=['0.1000000'], tr/val_loss:  3.593499/  4.026816, val:  37.92%, val_best:  55.42%, tr:  31.87%, tr_best:  55.67%\n",
      "epoch-198 lr=['0.1000000'], tr/val_loss:  3.725847/  3.936804, val:  41.67%, val_best:  55.42%, tr:  34.53%, tr_best:  55.67%\n",
      "epoch-199 lr=['0.1000000'], tr/val_loss:  3.030820/  3.281695, val:  36.67%, val_best:  55.42%, tr:  30.54%, tr_best:  55.67%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f7e05637ab4db595718a75bbf68a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▇▃▇▅▆▇▅▇█▇▇▆▆▅▅▅▅▅▅▆▇▅▃▆▆▃▄▄▃▂▄▃▄▄▂▅▃▁▆</td></tr><tr><td>summary_val_acc</td><td>▇▅▄▅▇▇▆█▇▇▆▇▇▇▅▇▆▄▆▆█▄▇▆▅▅▄▁▂▁▃▃▅▃▃▃▅▇▃▃</td></tr><tr><td>tr_acc</td><td>▆▇▇▇▇▇█▇█▆▆▇▇▇█▇▆▇▆▇▇▇▆▄▅▅▄▁▂▂▃▃▃▃▄▃▄▄▄▄</td></tr><tr><td>tr_epoch_loss</td><td>▂▁▁▂▁▂▁▂▂▂▃▃▄▅▃▅▄▃▄▂▄▅▃▄▃▇▆▆▄▅▆█▄▄▃▇▃▅▇▃</td></tr><tr><td>val_acc_best</td><td>▁▁▁▃▃███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▇▅▄▅▇▇▆█▇▇▆▇▇▇▅▇▆▄▆▆█▄▇▆▅▅▄▁▂▁▃▃▅▃▃▃▅▇▃▃</td></tr><tr><td>val_loss</td><td>▁▁▂▃▁▁▂▂▂▂▂▁▂▄▂▂▆▄▂▃▃▆▂▂▂▄█▄▃▄▄▃▃▄▂▂▂▃▆▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>0.33333</td></tr><tr><td>tr_acc</td><td>0.30541</td></tr><tr><td>tr_epoch_loss</td><td>3.03082</td></tr><tr><td>val_acc_best</td><td>0.55417</td></tr><tr><td>val_acc_now</td><td>0.36667</td></tr><tr><td>val_loss</td><td>3.2817</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">magic-sweep-53</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9b5k6ygy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9b5k6ygy</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_095335-9b5k6ygy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u53uqjvr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a90b57c8284b03873e2b78d1e90772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111352895386517, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_100556-u53uqjvr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/u53uqjvr' target=\"_blank\">lively-sweep-54</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/u53uqjvr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/u53uqjvr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = 63cc597115630ec173f3099200061e53\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.305368/  2.302869, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:   8.17%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.304892/  2.302000, val:  10.42%, val_best:  10.42%, tr:   7.87%, tr_best:   8.17%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.299430/  2.290066, val:  18.33%, val_best:  18.33%, tr:  11.95%, tr_best:  11.95%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.238634/  2.226519, val:  24.17%, val_best:  24.17%, tr:  21.25%, tr_best:  21.25%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  2.094419/  2.033407, val:  37.50%, val_best:  37.50%, tr:  32.28%, tr_best:  32.28%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  1.797986/  1.799708, val:  47.50%, val_best:  47.50%, tr:  48.01%, tr_best:  48.01%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  1.603552/  1.693462, val:  52.08%, val_best:  52.08%, tr:  56.18%, tr_best:  56.18%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  1.484099/  1.607617, val:  56.67%, val_best:  56.67%, tr:  58.43%, tr_best:  58.43%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  1.406806/  1.578844, val:  61.25%, val_best:  61.25%, tr:  60.47%, tr_best:  60.47%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  1.364876/  1.551933, val:  57.08%, val_best:  61.25%, tr:  65.07%, tr_best:  65.07%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  1.321471/  1.573084, val:  54.58%, val_best:  61.25%, tr:  62.41%, tr_best:  65.07%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  1.275922/  1.512830, val:  60.42%, val_best:  61.25%, tr:  65.17%, tr_best:  65.17%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  1.242256/  1.530711, val:  57.50%, val_best:  61.25%, tr:  65.78%, tr_best:  65.78%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  1.223451/  1.534886, val:  61.25%, val_best:  61.25%, tr:  65.27%, tr_best:  65.78%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  1.158677/  1.612909, val:  59.17%, val_best:  61.25%, tr:  67.11%, tr_best:  67.11%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  1.153170/  1.543279, val:  60.42%, val_best:  61.25%, tr:  67.72%, tr_best:  67.72%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  1.159417/  1.501258, val:  60.42%, val_best:  61.25%, tr:  67.93%, tr_best:  67.93%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  1.086221/  1.525276, val:  60.83%, val_best:  61.25%, tr:  71.09%, tr_best:  71.09%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  1.087479/  1.594368, val:  60.00%, val_best:  61.25%, tr:  70.89%, tr_best:  71.09%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  1.089171/  1.544665, val:  60.00%, val_best:  61.25%, tr:  72.42%, tr_best:  72.42%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  1.066055/  1.570709, val:  58.75%, val_best:  61.25%, tr:  70.89%, tr_best:  72.42%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  1.036328/  1.582716, val:  64.58%, val_best:  64.58%, tr:  71.60%, tr_best:  72.42%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  1.062261/  1.562400, val:  62.50%, val_best:  64.58%, tr:  73.44%, tr_best:  73.44%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  1.020671/  1.664889, val:  61.25%, val_best:  64.58%, tr:  72.42%, tr_best:  73.44%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.986609/  1.703657, val:  63.33%, val_best:  64.58%, tr:  74.16%, tr_best:  74.16%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.985681/  1.642836, val:  65.00%, val_best:  65.00%, tr:  73.95%, tr_best:  74.16%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.981603/  1.704366, val:  63.33%, val_best:  65.00%, tr:  78.35%, tr_best:  78.35%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.948347/  1.754084, val:  61.67%, val_best:  65.00%, tr:  75.38%, tr_best:  78.35%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.957027/  1.741414, val:  63.75%, val_best:  65.00%, tr:  78.04%, tr_best:  78.35%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.951742/  1.945077, val:  59.58%, val_best:  65.00%, tr:  76.20%, tr_best:  78.35%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.924858/  1.911206, val:  62.50%, val_best:  65.00%, tr:  79.26%, tr_best:  79.26%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.942797/  1.958374, val:  62.50%, val_best:  65.00%, tr:  77.94%, tr_best:  79.26%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.960088/  1.945555, val:  63.75%, val_best:  65.00%, tr:  75.59%, tr_best:  79.26%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.926368/  1.985071, val:  62.50%, val_best:  65.00%, tr:  79.78%, tr_best:  79.78%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.898206/  2.070739, val:  63.33%, val_best:  65.00%, tr:  80.08%, tr_best:  80.08%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.921515/  2.138203, val:  60.42%, val_best:  65.00%, tr:  76.61%, tr_best:  80.08%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.867794/  1.996840, val:  64.17%, val_best:  65.00%, tr:  79.57%, tr_best:  80.08%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.897214/  2.140013, val:  62.08%, val_best:  65.00%, tr:  78.24%, tr_best:  80.08%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.900691/  2.181399, val:  61.25%, val_best:  65.00%, tr:  79.88%, tr_best:  80.08%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.871656/  2.083051, val:  62.92%, val_best:  65.00%, tr:  80.90%, tr_best:  80.90%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.867035/  2.115260, val:  68.33%, val_best:  68.33%, tr:  80.29%, tr_best:  80.90%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.862158/  2.236630, val:  60.83%, val_best:  68.33%, tr:  81.10%, tr_best:  81.10%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.820495/  2.124679, val:  65.83%, val_best:  68.33%, tr:  85.09%, tr_best:  85.09%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.848870/  2.255899, val:  62.50%, val_best:  68.33%, tr:  81.82%, tr_best:  85.09%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.823732/  2.307934, val:  65.42%, val_best:  68.33%, tr:  84.68%, tr_best:  85.09%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.802084/  2.312616, val:  65.00%, val_best:  68.33%, tr:  84.68%, tr_best:  85.09%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.801004/  2.347652, val:  68.33%, val_best:  68.33%, tr:  84.07%, tr_best:  85.09%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.773244/  2.452251, val:  67.50%, val_best:  68.33%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.804380/  2.432188, val:  67.50%, val_best:  68.33%, tr:  86.62%, tr_best:  88.56%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.759709/  2.393728, val:  72.50%, val_best:  72.50%, tr:  86.11%, tr_best:  88.56%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.734865/  2.565832, val:  62.92%, val_best:  72.50%, tr:  89.48%, tr_best:  89.48%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.765158/  2.492463, val:  70.42%, val_best:  72.50%, tr:  85.50%, tr_best:  89.48%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.704757/  2.531806, val:  67.50%, val_best:  72.50%, tr:  90.91%, tr_best:  90.91%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.708029/  2.634238, val:  65.83%, val_best:  72.50%, tr:  91.93%, tr_best:  91.93%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.694157/  2.518107, val:  71.25%, val_best:  72.50%, tr:  91.62%, tr_best:  91.93%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.722338/  2.661786, val:  65.42%, val_best:  72.50%, tr:  89.79%, tr_best:  91.93%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.662430/  2.795660, val:  62.92%, val_best:  72.50%, tr:  91.22%, tr_best:  91.93%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.735880/  2.687298, val:  66.25%, val_best:  72.50%, tr:  90.70%, tr_best:  91.93%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.678832/  2.575662, val:  74.17%, val_best:  74.17%, tr:  88.87%, tr_best:  91.93%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.687332/  2.785206, val:  68.33%, val_best:  74.17%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.604112/  2.851977, val:  66.67%, val_best:  74.17%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.642100/  2.828583, val:  63.75%, val_best:  74.17%, tr:  92.44%, tr_best:  92.95%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.620596/  2.852996, val:  67.50%, val_best:  74.17%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.577011/  2.863337, val:  72.92%, val_best:  74.17%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.574576/  2.935291, val:  69.17%, val_best:  74.17%, tr:  95.91%, tr_best:  96.32%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.562547/  2.863042, val:  74.58%, val_best:  74.58%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.561080/  2.946073, val:  67.92%, val_best:  74.58%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.556816/  2.977272, val:  75.00%, val_best:  75.00%, tr:  96.63%, tr_best:  96.73%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.541548/  2.971014, val:  70.42%, val_best:  75.00%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.538620/  3.050876, val:  69.17%, val_best:  75.00%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.501187/  2.987916, val:  73.75%, val_best:  75.00%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.585623/  3.065032, val:  69.58%, val_best:  75.00%, tr:  96.94%, tr_best:  97.65%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.536847/  3.127987, val:  70.83%, val_best:  75.00%, tr:  97.34%, tr_best:  97.65%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.500595/  3.280281, val:  67.92%, val_best:  75.00%, tr:  97.14%, tr_best:  97.65%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.459511/  3.148300, val:  70.00%, val_best:  75.00%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.459577/  3.245216, val:  68.75%, val_best:  75.00%, tr:  98.06%, tr_best:  98.47%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.457073/  3.318089, val:  67.92%, val_best:  75.00%, tr:  98.37%, tr_best:  98.47%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.473266/  3.288131, val:  76.25%, val_best:  76.25%, tr:  97.14%, tr_best:  98.47%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.494142/  3.366304, val:  69.17%, val_best:  76.25%, tr:  97.14%, tr_best:  98.47%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.491904/  3.299234, val:  72.92%, val_best:  76.25%, tr:  97.45%, tr_best:  98.47%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.435890/  3.253596, val:  72.08%, val_best:  76.25%, tr:  98.37%, tr_best:  98.47%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.451198/  3.348400, val:  75.83%, val_best:  76.25%, tr:  97.75%, tr_best:  98.47%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.423090/  3.459766, val:  70.00%, val_best:  76.25%, tr:  97.96%, tr_best:  98.47%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.410044/  3.456672, val:  75.00%, val_best:  76.25%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.424424/  3.382696, val:  76.25%, val_best:  76.25%, tr:  98.26%, tr_best:  99.08%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.387365/  3.607038, val:  73.33%, val_best:  76.25%, tr:  98.67%, tr_best:  99.08%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.385075/  3.475766, val:  71.67%, val_best:  76.25%, tr:  98.98%, tr_best:  99.08%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.405095/  3.530694, val:  72.92%, val_best:  76.25%, tr:  98.06%, tr_best:  99.08%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.367720/  3.499526, val:  76.67%, val_best:  76.67%, tr:  98.98%, tr_best:  99.08%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.333443/  3.656271, val:  72.92%, val_best:  76.67%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.363451/  3.602831, val:  72.50%, val_best:  76.67%, tr:  99.28%, tr_best:  99.80%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.351144/  3.749030, val:  75.83%, val_best:  76.67%, tr:  99.28%, tr_best:  99.80%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.319310/  3.630162, val:  75.00%, val_best:  76.67%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.306673/  3.766888, val:  74.17%, val_best:  76.67%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.298280/  3.836807, val:  75.00%, val_best:  76.67%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.339206/  3.780203, val:  74.17%, val_best:  76.67%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.322796/  3.957639, val:  72.50%, val_best:  76.67%, tr:  98.67%, tr_best:  99.80%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.300295/  3.738404, val:  75.42%, val_best:  76.67%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.293297/  3.778081, val:  74.17%, val_best:  76.67%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.320658/  4.064106, val:  71.67%, val_best:  76.67%, tr:  99.39%, tr_best:  99.90%\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.313096/  3.817747, val:  75.83%, val_best:  76.67%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.266944/  3.931817, val:  75.00%, val_best:  76.67%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.246038/  3.991951, val:  75.83%, val_best:  76.67%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.263001/  3.886031, val:  76.67%, val_best:  76.67%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.227813/  4.056342, val:  73.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.251148/  3.905186, val:  76.25%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.264913/  3.993002, val:  74.17%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.213677/  4.120529, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.222647/  4.118718, val:  71.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.254604/  4.150789, val:  74.17%, val_best:  76.67%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.224856/  4.102045, val:  73.75%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.232074/  4.142416, val:  74.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.213546/  4.170270, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.224375/  4.226464, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.250244/  4.266604, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.214628/  4.268776, val:  73.75%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.194533/  4.203757, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.201324/  4.304579, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.195277/  4.299779, val:  76.25%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.174649/  4.307024, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.171304/  4.277703, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.165572/  4.391407, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.166751/  4.394962, val:  73.33%, val_best:  77.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.190883/  4.409142, val:  73.75%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.201746/  4.317882, val:  72.08%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.184955/  4.280646, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.159051/  4.467218, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.148634/  4.404595, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.159317/  4.452233, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.151167/  4.594977, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.149765/  4.466763, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.183542/  4.522270, val:  75.00%, val_best:  77.08%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.133457/  4.546087, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.116191/  4.582138, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.112696/  4.530765, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.142802/  4.592979, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.144866/  4.525250, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.163883/  4.761844, val:  74.58%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.132568/  4.678976, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.115968/  4.593724, val:  73.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.102171/  4.767828, val:  72.50%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.117103/  4.701392, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.108685/  4.804997, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.105604/  4.665723, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.100448/  4.695888, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.087482/  4.660661, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.101928/  4.725427, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.087752/  4.843670, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.104502/  4.792672, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.093379/  4.859521, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.089763/  4.861876, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.086202/  4.902261, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.081345/  4.949176, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.077129/  4.965028, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.086818/  4.975963, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.091341/  5.019595, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.084520/  5.043521, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.069632/  4.945512, val:  73.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.074484/  5.063656, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.075334/  5.151036, val:  72.50%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.078309/  5.033460, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.087608/  5.063075, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.073012/  5.104528, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.067394/  5.056461, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.071244/  5.144519, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.066933/  5.194748, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.057277/  5.216924, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.054335/  5.172829, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.052068/  5.171784, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.062226/  5.240370, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.051408/  5.138371, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.051221/  5.205216, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.065437/  5.181580, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.043843/  5.301374, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.047928/  5.226217, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.058644/  5.216268, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.054265/  5.301985, val:  73.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.060230/  5.208148, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.048992/  5.293731, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.053581/  5.344506, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.048811/  5.260880, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.046192/  5.509307, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.056326/  5.510076, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.054694/  5.557904, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.043749/  5.489730, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.051858/  5.538574, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.043047/  5.508559, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.037710/  5.338463, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.042677/  5.379521, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.038421/  5.422268, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.037688/  5.377484, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.047886/  5.510011, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.046354/  5.559125, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.039077/  5.526316, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.036640/  5.491010, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.030706/  5.532222, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.043721/  5.562779, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.051790/  5.617961, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.040719/  5.487231, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.037648/  5.556863, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299ce9b2bcab41d198f5994a79af0f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▆▅▆▇▆▇█▅▇▇███████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▆▇▆▇▇▇▇▇▇█▇▇█████████▇██████████████</td></tr><tr><td>tr_acc</td><td>▁▄▅▆▆▆▆▆▇▇▇▇▇███████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▆▇▆▇▇▇▇▇▇█▇▇█████████▇██████████████</td></tr><tr><td>val_loss</td><td>▂▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▆▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03765</td></tr><tr><td>val_acc_best</td><td>0.775</td></tr><tr><td>val_acc_now</td><td>0.775</td></tr><tr><td>val_loss</td><td>5.55686</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lively-sweep-54</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/u53uqjvr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/u53uqjvr</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_100556-u53uqjvr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d985own0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: ['M', 'M', 200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_101751-d985own0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/d985own0' target=\"_blank\">solar-sweep-55</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vt95hgaf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/d985own0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/d985own0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '2', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.132011/  1.724066, val:  40.83%, val_best:  40.83%, tr:  20.12%, tr_best:  20.12%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.453344/  1.437065, val:  57.50%, val_best:  57.50%, tr:  52.50%, tr_best:  52.50%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.263618/  1.431005, val:  51.67%, val_best:  57.50%, tr:  58.63%, tr_best:  58.63%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.045733/  1.424782, val:  55.83%, val_best:  57.50%, tr:  67.62%, tr_best:  67.62%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.972304/  1.369971, val:  62.92%, val_best:  62.92%, tr:  68.23%, tr_best:  68.23%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.900654/  1.299045, val:  63.75%, val_best:  63.75%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.853506/  1.284251, val:  63.75%, val_best:  63.75%, tr:  73.65%, tr_best:  73.65%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.773319/  1.416888, val:  62.92%, val_best:  63.75%, tr:  74.87%, tr_best:  74.87%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.681410/  1.414723, val:  64.58%, val_best:  64.58%, tr:  78.14%, tr_best:  78.14%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.604341/  1.434830, val:  60.83%, val_best:  64.58%, tr:  84.17%, tr_best:  84.17%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.550017/  1.575189, val:  68.75%, val_best:  68.75%, tr:  86.01%, tr_best:  86.01%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.513389/  1.621109, val:  68.75%, val_best:  68.75%, tr:  87.74%, tr_best:  87.74%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.454104/  1.488412, val:  74.17%, val_best:  74.17%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.405840/  1.698292, val:  70.00%, val_best:  74.17%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.369543/  1.865428, val:  71.67%, val_best:  74.17%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.339660/  1.869333, val:  68.75%, val_best:  74.17%, tr:  95.30%, tr_best:  95.30%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.265432/  1.894535, val:  75.00%, val_best:  75.00%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.229017/  1.891768, val:  76.25%, val_best:  76.25%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.205313/  1.972072, val:  73.75%, val_best:  76.25%, tr:  98.37%, tr_best:  99.39%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.173595/  2.079091, val:  72.08%, val_best:  76.25%, tr:  99.28%, tr_best:  99.39%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.174163/  2.042644, val:  74.58%, val_best:  76.25%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.149319/  2.255099, val:  68.33%, val_best:  76.25%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.135339/  2.172790, val:  76.67%, val_best:  76.67%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.124066/  2.303713, val:  77.50%, val_best:  77.50%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.068159/  2.376092, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.064930/  2.387013, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.062073/  2.461392, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.055704/  2.482505, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.047931/  2.600555, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.034535/  2.748169, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.026747/  2.748312, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.027438/  2.831641, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.025425/  2.889716, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.020050/  2.921292, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.018745/  2.961311, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.017511/  3.034353, val:  75.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.014889/  3.095428, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.013408/  3.141236, val:  75.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.014893/  3.181517, val:  75.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.010867/  3.156884, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.010037/  3.203361, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.010330/  3.346898, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.011117/  3.225783, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.012791/  3.357798, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.009842/  3.359125, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.006012/  3.337466, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.005034/  3.338826, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.004206/  3.395376, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.007137/  3.408823, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.007096/  3.406228, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.005946/  3.490511, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.007534/  3.494438, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.031652/  3.394029, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.023040/  3.337932, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.022924/  3.451631, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.020057/  3.413743, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.017500/  3.335761, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.015096/  3.477533, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.028871/  3.473892, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.014249/  3.467496, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.009657/  3.450961, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.005961/  3.441555, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.008419/  3.456061, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.004933/  3.490146, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.004287/  3.436746, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.002787/  3.484231, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.002856/  3.524774, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.001654/  3.523094, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.002214/  3.552973, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.003080/  3.561566, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.003398/  3.571581, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.001187/  3.569374, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.001496/  3.607011, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.001175/  3.608011, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.000823/  3.596187, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.000802/  3.561375, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.000673/  3.615164, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.000547/  3.612562, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.000449/  3.613990, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.000473/  3.615121, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.000442/  3.611151, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.000402/  3.644323, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.000368/  3.661006, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.000370/  3.667337, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.000347/  3.664386, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.000330/  3.663854, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.000354/  3.680904, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.000351/  3.679352, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.000307/  3.688473, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.000285/  3.692194, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.000339/  3.669948, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000361/  3.714416, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000351/  3.706931, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000411/  3.714215, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000319/  3.716525, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000280/  3.726772, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000291/  3.726740, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000294/  3.744715, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000270/  3.746999, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000255/  3.739763, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.000263/  3.735418, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.000242/  3.750202, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.000343/  3.766618, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.000298/  3.776205, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.000248/  3.760761, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.000257/  3.762070, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.000225/  3.777989, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.000239/  3.782462, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.000234/  3.789718, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.000262/  3.795808, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.000256/  3.792017, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.000226/  3.795080, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.000249/  3.793610, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.000229/  3.795083, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.000234/  3.799551, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.000214/  3.797204, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.000207/  3.796836, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.000199/  3.787941, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.000205/  3.811892, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.000201/  3.811991, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.000198/  3.817695, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.000200/  3.813070, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.000174/  3.809046, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.000192/  3.808157, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.000200/  3.812695, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.000180/  3.819057, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.000191/  3.839337, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.000180/  3.828576, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.000167/  3.830561, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.000158/  3.820472, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.000153/  3.827995, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.000234/  3.833810, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.000172/  3.839367, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.000164/  3.842093, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.000153/  3.840944, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.000156/  3.837182, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.000176/  3.836251, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.000157/  3.835648, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.000143/  3.832721, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.000148/  3.840818, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.000142/  3.847064, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.000141/  3.844771, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.000141/  3.839831, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.000138/  3.847081, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.000146/  3.853538, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.000152/  3.857609, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.000172/  3.869369, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [16]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [128]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.25, 0.5, 0.75, 1.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [1.0,2.0,3.0,4.0,5.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0, 0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [['M','M',200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"net_save/save_now_net_weights_{unique_name}.pth\"]},\n",
    "        \"learning_rate\": {\"values\": [0.001,0.01,0.1,0.0001]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [5]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [100_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True, False]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [True]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [False]},\n",
    "        \"denoise_on\": {\"values\": [True, False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [0]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [True]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"2\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  unique_name_hyper,\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "                        ) \n",
    "    # sigmoid와 BN이 있어야 잘된다.\n",
    "    # average pooling\n",
    "    # 이 낫다. \n",
    "    \n",
    "    # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# sweep_id = '6pj3lh8j'\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
