{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8990/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        smallest_now_T = 99999\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                if epoch == 0 and now_T < smallest_now_T:\n",
    "                    smallest_now_T = now_T\n",
    "                    print(f'smallest_now_T updated: {smallest_now_T}')\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                if now_T < now_time_steps:\n",
    "                    # Î∂ÄÏ°±Ìïú timestep Í∞úÏàò\n",
    "                    diff = now_time_steps - now_T\n",
    "\n",
    "                    # ÎßàÏßÄÎßâ timestep Î≥µÏÇ¨ (shape: [B, 1, C, H, W])\n",
    "                    last_frame = inputs[:, -1:, :, :, :]\n",
    "\n",
    "                    # diffÎßåÌÅº repeatÌïòÏó¨ Ìå®Îî© Íµ¨ÏÑ±\n",
    "                    pad_frames = last_frame.repeat(1, diff, 1, 1, 1)\n",
    "\n",
    "                    # ÏõêÎ≥∏ + Ìå®Îî© Í≤∞Ìï©\n",
    "                    inputs = torch.cat([inputs, pad_frames], dim=1)\n",
    "                else:\n",
    "                    # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                    start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                    # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                    inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            smallest_now_T_val = 99999\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            if epoch == 0 and now_T < smallest_now_T_val:\n",
    "                                smallest_now_T_val = now_T\n",
    "                                print(f'smallest_now_T_val updated: {smallest_now_T_val}')\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "\n",
    "                            if now_T < now_time_steps:\n",
    "                                # Î∂ÄÏ°±Ìïú timestep Í∞úÏàò\n",
    "                                diff = now_time_steps - now_T\n",
    "\n",
    "                                # ÎßàÏßÄÎßâ timestep Î≥µÏÇ¨ (shape: [B, 1, C, H, W])\n",
    "                                last_frame = inputs_val[:, -1:, :, :, :]\n",
    "\n",
    "                                # diffÎßåÌÅº repeatÌïòÏó¨ Ìå®Îî© Íµ¨ÏÑ±\n",
    "                                pad_frames = last_frame.repeat(1, diff, 1, 1, 1)\n",
    "\n",
    "                                # ÏõêÎ≥∏ + Ìå®Îî© Í≤∞Ìï©\n",
    "                                inputs_val = torch.cat([inputs_val, pad_frames], dim=1)\n",
    "                            else:\n",
    "                                pass\n",
    "                            \n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "                \n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.8\n",
    "                elif epoch > 150:\n",
    "                    assert val_acc_best > 0.88\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251114_174530-vnntdfnr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vnntdfnr' target=\"_blank\">sunny-meadow-18542</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vnntdfnr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vnntdfnr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251114_174527_642', 'my_seed': 42, 'TIME': 5, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000.0, 'lif_layer_sg_width': 3.0, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_main.pth', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 12000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 67f09733060e9328908e01cda0ab3532\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.25, v_reset=10000.0, sg_width=3.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.25, v_reset=10000.0, sg_width=3.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=5, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=5, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "smallest_now_T updated: 592\n",
      "fc layer 1 self.abs_max_out: 211.0\n",
      "lif layer 1 self.abs_max_v: 211.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 185.0\n",
      "lif layer 2 self.abs_max_v: 185.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 45.0\n",
      "fc layer 1 self.abs_max_out: 221.0\n",
      "lif layer 1 self.abs_max_v: 256.5\n",
      "fc layer 2 self.abs_max_out: 286.0\n",
      "lif layer 2 self.abs_max_v: 371.5\n",
      "fc layer 3 self.abs_max_out: 123.0\n",
      "fc layer 1 self.abs_max_out: 256.0\n",
      "lif layer 1 self.abs_max_v: 297.5\n",
      "fc layer 2 self.abs_max_out: 369.0\n",
      "lif layer 2 self.abs_max_v: 392.0\n",
      "smallest_now_T updated: 534\n",
      "fc layer 1 self.abs_max_out: 334.0\n",
      "lif layer 1 self.abs_max_v: 334.0\n",
      "fc layer 1 self.abs_max_out: 411.0\n",
      "lif layer 1 self.abs_max_v: 456.5\n",
      "fc layer 2 self.abs_max_out: 419.0\n",
      "lif layer 2 self.abs_max_v: 518.0\n",
      "lif layer 1 self.abs_max_v: 499.0\n",
      "lif layer 2 self.abs_max_v: 649.0\n",
      "fc layer 3 self.abs_max_out: 128.0\n",
      "fc layer 1 self.abs_max_out: 441.0\n",
      "fc layer 2 self.abs_max_out: 476.0\n",
      "fc layer 1 self.abs_max_out: 507.0\n",
      "lif layer 1 self.abs_max_v: 568.0\n",
      "smallest_now_T updated: 407\n",
      "fc layer 1 self.abs_max_out: 566.0\n",
      "fc layer 1 self.abs_max_out: 585.0\n",
      "lif layer 1 self.abs_max_v: 688.5\n",
      "fc layer 3 self.abs_max_out: 158.0\n",
      "lif layer 1 self.abs_max_v: 741.5\n",
      "lif layer 2 self.abs_max_v: 662.0\n",
      "fc layer 3 self.abs_max_out: 160.0\n",
      "lif layer 1 self.abs_max_v: 781.0\n",
      "lif layer 2 self.abs_max_v: 671.5\n",
      "lif layer 2 self.abs_max_v: 801.0\n",
      "fc layer 3 self.abs_max_out: 173.0\n",
      "fc layer 2 self.abs_max_out: 601.0\n",
      "lif layer 2 self.abs_max_v: 802.5\n",
      "fc layer 3 self.abs_max_out: 236.0\n",
      "lif layer 2 self.abs_max_v: 916.5\n",
      "lif layer 2 self.abs_max_v: 968.5\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "fc layer 1 self.abs_max_out: 895.0\n",
      "lif layer 1 self.abs_max_v: 895.0\n",
      "fc layer 2 self.abs_max_out: 609.0\n",
      "smallest_now_T updated: 345\n",
      "fc layer 1 self.abs_max_out: 954.0\n",
      "lif layer 1 self.abs_max_v: 954.0\n",
      "lif layer 1 self.abs_max_v: 974.5\n",
      "lif layer 1 self.abs_max_v: 1052.5\n",
      "lif layer 1 self.abs_max_v: 1161.5\n",
      "fc layer 3 self.abs_max_out: 261.0\n",
      "lif layer 1 self.abs_max_v: 1348.5\n",
      "lif layer 1 self.abs_max_v: 1531.5\n",
      "fc layer 2 self.abs_max_out: 667.0\n",
      "fc layer 1 self.abs_max_out: 1036.0\n",
      "fc layer 1 self.abs_max_out: 1053.0\n",
      "fc layer 2 self.abs_max_out: 696.0\n",
      "fc layer 2 self.abs_max_out: 725.0\n",
      "smallest_now_T updated: 317\n",
      "lif layer 1 self.abs_max_v: 1617.5\n",
      "lif layer 2 self.abs_max_v: 1025.5\n",
      "lif layer 2 self.abs_max_v: 1047.0\n",
      "fc layer 2 self.abs_max_out: 770.0\n",
      "lif layer 2 self.abs_max_v: 1082.5\n",
      "fc layer 1 self.abs_max_out: 1069.0\n",
      "lif layer 2 self.abs_max_v: 1115.0\n",
      "lif layer 2 self.abs_max_v: 1207.5\n",
      "fc layer 2 self.abs_max_out: 773.0\n",
      "lif layer 2 self.abs_max_v: 1341.0\n",
      "smallest_now_T updated: 286\n",
      "fc layer 2 self.abs_max_out: 790.0\n",
      "fc layer 2 self.abs_max_out: 858.0\n",
      "fc layer 1 self.abs_max_out: 1378.0\n",
      "fc layer 1 self.abs_max_out: 1514.0\n",
      "fc layer 1 self.abs_max_out: 1543.0\n",
      "fc layer 2 self.abs_max_out: 897.0\n",
      "fc layer 2 self.abs_max_out: 911.0\n",
      "lif layer 1 self.abs_max_v: 1720.5\n",
      "lif layer 1 self.abs_max_v: 1860.0\n",
      "lif layer 1 self.abs_max_v: 1967.0\n",
      "fc layer 3 self.abs_max_out: 272.0\n",
      "smallest_now_T updated: 247\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "smallest_now_T updated: 192\n",
      "fc layer 3 self.abs_max_out: 305.0\n",
      "fc layer 2 self.abs_max_out: 1001.0\n",
      "fc layer 3 self.abs_max_out: 361.0\n",
      "fc layer 1 self.abs_max_out: 1832.0\n",
      "fc layer 1 self.abs_max_out: 1863.0\n",
      "lif layer 1 self.abs_max_v: 1997.5\n",
      "fc layer 2 self.abs_max_out: 1074.0\n",
      "lif layer 2 self.abs_max_v: 1345.5\n",
      "lif layer 2 self.abs_max_v: 1375.5\n",
      "lif layer 2 self.abs_max_v: 1475.5\n",
      "lif layer 1 self.abs_max_v: 2019.0\n",
      "lif layer 1 self.abs_max_v: 2054.5\n",
      "lif layer 2 self.abs_max_v: 1478.0\n",
      "lif layer 1 self.abs_max_v: 2059.5\n",
      "lif layer 1 self.abs_max_v: 2127.0\n",
      "lif layer 2 self.abs_max_v: 1489.0\n",
      "lif layer 1 self.abs_max_v: 2314.5\n",
      "lif layer 2 self.abs_max_v: 1516.5\n",
      "fc layer 3 self.abs_max_out: 388.0\n",
      "lif layer 2 self.abs_max_v: 1533.0\n",
      "lif layer 2 self.abs_max_v: 1576.5\n",
      "lif layer 2 self.abs_max_v: 1608.5\n",
      "lif layer 2 self.abs_max_v: 1641.5\n",
      "lif layer 2 self.abs_max_v: 1669.0\n",
      "fc layer 2 self.abs_max_out: 1134.0\n",
      "fc layer 2 self.abs_max_out: 1140.0\n",
      "fc layer 2 self.abs_max_out: 1166.0\n",
      "fc layer 2 self.abs_max_out: 1191.0\n",
      "lif layer 1 self.abs_max_v: 2529.0\n",
      "lif layer 1 self.abs_max_v: 2639.0\n",
      "lif layer 1 self.abs_max_v: 2711.5\n",
      "fc layer 2 self.abs_max_out: 1208.0\n",
      "smallest_now_T_val updated: 552\n",
      "smallest_now_T_val updated: 456\n",
      "smallest_now_T_val updated: 448\n",
      "smallest_now_T_val updated: 440\n",
      "smallest_now_T_val updated: 368\n",
      "smallest_now_T_val updated: 137\n",
      "lif layer 1 self.abs_max_v: 2842.0\n",
      "lif layer 2 self.abs_max_v: 1701.0\n",
      "lif layer 1 self.abs_max_v: 3025.0\n",
      "lif layer 1 self.abs_max_v: 3160.5\n",
      "lif layer 2 self.abs_max_v: 1711.0\n",
      "lif layer 2 self.abs_max_v: 1769.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.882288/  2.008694, val:  35.83%, val_best:  35.83%, tr:  79.06%, tr_best:  79.06%, epoch time: 56.84 seconds, 0.95 minutes\n",
      "layer   1  Sparsity: 87.9581%\n",
      "layer   2  Sparsity: 71.6736%\n",
      "layer   3  Sparsity: 72.9532%\n",
      "total_backward_count 4895 real_backward_count 1755  35.853%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 1894.0\n",
      "fc layer 3 self.abs_max_out: 399.0\n",
      "fc layer 2 self.abs_max_out: 1213.0\n",
      "fc layer 2 self.abs_max_out: 1281.0\n",
      "fc layer 3 self.abs_max_out: 475.0\n",
      "lif layer 2 self.abs_max_v: 1813.0\n",
      "lif layer 2 self.abs_max_v: 1833.0\n",
      "lif layer 2 self.abs_max_v: 1841.5\n",
      "lif layer 2 self.abs_max_v: 1885.5\n",
      "lif layer 2 self.abs_max_v: 1937.0\n",
      "fc layer 3 self.abs_max_out: 482.0\n",
      "fc layer 3 self.abs_max_out: 498.0\n",
      "lif layer 2 self.abs_max_v: 1943.0\n",
      "lif layer 2 self.abs_max_v: 1983.0\n",
      "lif layer 2 self.abs_max_v: 2015.0\n",
      "lif layer 1 self.abs_max_v: 3277.0\n",
      "fc layer 1 self.abs_max_out: 1927.0\n",
      "lif layer 1 self.abs_max_v: 3422.5\n",
      "lif layer 1 self.abs_max_v: 3536.5\n",
      "fc layer 1 self.abs_max_out: 2062.0\n",
      "fc layer 1 self.abs_max_out: 2262.0\n",
      "lif layer 1 self.abs_max_v: 3900.0\n",
      "lif layer 1 self.abs_max_v: 4141.5\n",
      "lif layer 2 self.abs_max_v: 2093.5\n",
      "fc layer 2 self.abs_max_out: 1289.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.752136/  1.956461, val:  35.00%, val_best:  35.83%, tr:  88.15%, tr_best:  88.15%, epoch time: 53.00 seconds, 0.88 minutes\n",
      "layer   1  Sparsity: 87.9085%\n",
      "layer   2  Sparsity: 72.7568%\n",
      "layer   3  Sparsity: 69.7801%\n",
      "total_backward_count 9790 real_backward_count 3126  31.931%\n",
      "fc layer 2 self.abs_max_out: 1328.0\n",
      "fc layer 2 self.abs_max_out: 1345.0\n",
      "fc layer 2 self.abs_max_out: 1357.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.752449/  1.917640, val:  39.17%, val_best:  39.17%, tr:  88.05%, tr_best:  88.15%, epoch time: 54.28 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9757%\n",
      "layer   2  Sparsity: 74.1800%\n",
      "layer   3  Sparsity: 70.8215%\n",
      "total_backward_count 14685 real_backward_count 4405  29.997%\n",
      "fc layer 2 self.abs_max_out: 1360.0\n",
      "lif layer 2 self.abs_max_v: 2260.5\n",
      "fc layer 2 self.abs_max_out: 1447.0\n",
      "fc layer 1 self.abs_max_out: 2516.0\n",
      "lif layer 1 self.abs_max_v: 4271.0\n",
      "lif layer 1 self.abs_max_v: 4606.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.735042/  1.915791, val:  42.92%, val_best:  42.92%, tr:  89.89%, tr_best:  89.89%, epoch time: 53.57 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9203%\n",
      "layer   2  Sparsity: 73.8130%\n",
      "layer   3  Sparsity: 70.3774%\n",
      "total_backward_count 19580 real_backward_count 5590  28.550%\n",
      "lif layer 2 self.abs_max_v: 2347.0\n",
      "fc layer 1 self.abs_max_out: 2777.0\n",
      "lif layer 1 self.abs_max_v: 4788.0\n",
      "lif layer 1 self.abs_max_v: 5156.0\n",
      "fc layer 2 self.abs_max_out: 1464.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.728489/  1.941739, val:  44.58%, val_best:  44.58%, tr:  88.36%, tr_best:  89.89%, epoch time: 54.61 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9186%\n",
      "layer   2  Sparsity: 74.2253%\n",
      "layer   3  Sparsity: 70.0854%\n",
      "total_backward_count 24475 real_backward_count 6745  27.559%\n",
      "fc layer 2 self.abs_max_out: 1495.0\n",
      "fc layer 2 self.abs_max_out: 1542.0\n",
      "fc layer 1 self.abs_max_out: 2948.0\n",
      "lif layer 1 self.abs_max_v: 5206.5\n",
      "lif layer 1 self.abs_max_v: 5509.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.732881/  1.925257, val:  38.75%, val_best:  44.58%, tr:  90.40%, tr_best:  90.40%, epoch time: 54.30 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.8628%\n",
      "layer   2  Sparsity: 73.2771%\n",
      "layer   3  Sparsity: 70.2808%\n",
      "total_backward_count 29370 real_backward_count 7876  26.816%\n",
      "fc layer 2 self.abs_max_out: 1621.0\n",
      "fc layer 2 self.abs_max_out: 1629.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.727600/  1.924752, val:  51.25%, val_best:  51.25%, tr:  89.99%, tr_best:  90.40%, epoch time: 55.11 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 88.0034%\n",
      "layer   2  Sparsity: 74.4449%\n",
      "layer   3  Sparsity: 70.7485%\n",
      "total_backward_count 34265 real_backward_count 9064  26.453%\n",
      "fc layer 2 self.abs_max_out: 1632.0\n",
      "fc layer 2 self.abs_max_out: 1675.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.702960/  1.916524, val:  42.08%, val_best:  51.25%, tr:  90.81%, tr_best:  90.81%, epoch time: 54.54 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9666%\n",
      "layer   2  Sparsity: 74.5260%\n",
      "layer   3  Sparsity: 70.1673%\n",
      "total_backward_count 39160 real_backward_count 10141  25.896%\n",
      "fc layer 2 self.abs_max_out: 1679.0\n",
      "fc layer 1 self.abs_max_out: 3029.0\n",
      "fc layer 1 self.abs_max_out: 3182.0\n",
      "lif layer 1 self.abs_max_v: 5787.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.716106/  1.891908, val:  50.83%, val_best:  51.25%, tr:  91.62%, tr_best:  91.62%, epoch time: 52.98 seconds, 0.88 minutes\n",
      "layer   1  Sparsity: 87.9650%\n",
      "layer   2  Sparsity: 74.6971%\n",
      "layer   3  Sparsity: 69.5059%\n",
      "total_backward_count 44055 real_backward_count 11268  25.577%\n",
      "fc layer 2 self.abs_max_out: 1689.0\n",
      "fc layer 3 self.abs_max_out: 500.0\n",
      "fc layer 3 self.abs_max_out: 504.0\n",
      "fc layer 2 self.abs_max_out: 1820.0\n",
      "fc layer 1 self.abs_max_out: 3212.0\n",
      "lif layer 1 self.abs_max_v: 5900.5\n",
      "fc layer 1 self.abs_max_out: 3341.0\n",
      "fc layer 1 self.abs_max_out: 3386.0\n",
      "lif layer 1 self.abs_max_v: 6298.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.697546/  1.946556, val:  43.33%, val_best:  51.25%, tr:  91.62%, tr_best:  91.62%, epoch time: 55.16 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.9296%\n",
      "layer   2  Sparsity: 74.6459%\n",
      "layer   3  Sparsity: 69.5011%\n",
      "total_backward_count 48950 real_backward_count 12359  25.248%\n",
      "fc layer 2 self.abs_max_out: 1906.0\n",
      "fc layer 1 self.abs_max_out: 3403.0\n",
      "fc layer 1 self.abs_max_out: 3489.0\n",
      "lif layer 1 self.abs_max_v: 6448.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.683588/  1.899297, val:  39.58%, val_best:  51.25%, tr:  92.85%, tr_best:  92.85%, epoch time: 53.47 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9336%\n",
      "layer   2  Sparsity: 74.9144%\n",
      "layer   3  Sparsity: 69.3249%\n",
      "total_backward_count 53845 real_backward_count 13384  24.857%\n",
      "lif layer 2 self.abs_max_v: 2355.5\n",
      "lif layer 2 self.abs_max_v: 2477.0\n",
      "lif layer 2 self.abs_max_v: 2611.5\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.704340/  1.905870, val:  37.08%, val_best:  51.25%, tr:  92.03%, tr_best:  92.85%, epoch time: 54.10 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9596%\n",
      "layer   2  Sparsity: 75.5544%\n",
      "layer   3  Sparsity: 69.9280%\n",
      "total_backward_count 58740 real_backward_count 14436  24.576%\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.689311/  1.892953, val:  47.50%, val_best:  51.25%, tr:  92.24%, tr_best:  92.85%, epoch time: 53.77 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.8906%\n",
      "layer   2  Sparsity: 74.0963%\n",
      "layer   3  Sparsity: 69.5227%\n",
      "total_backward_count 63635 real_backward_count 15472  24.314%\n",
      "fc layer 3 self.abs_max_out: 526.0\n",
      "lif layer 2 self.abs_max_v: 2642.5\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.689577/  1.944739, val:  37.08%, val_best:  51.25%, tr:  91.83%, tr_best:  92.85%, epoch time: 54.79 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9368%\n",
      "layer   2  Sparsity: 74.1427%\n",
      "layer   3  Sparsity: 69.0435%\n",
      "total_backward_count 68530 real_backward_count 16527  24.116%\n",
      "lif layer 2 self.abs_max_v: 2714.0\n",
      "lif layer 2 self.abs_max_v: 2864.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.683566/  1.928734, val:  33.33%, val_best:  51.25%, tr:  92.54%, tr_best:  92.85%, epoch time: 54.38 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9640%\n",
      "layer   2  Sparsity: 74.3290%\n",
      "layer   3  Sparsity: 69.7767%\n",
      "total_backward_count 73425 real_backward_count 17513  23.852%\n",
      "fc layer 1 self.abs_max_out: 3504.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.704788/  1.963174, val:  37.50%, val_best:  51.25%, tr:  91.22%, tr_best:  92.85%, epoch time: 53.77 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9520%\n",
      "layer   2  Sparsity: 74.2037%\n",
      "layer   3  Sparsity: 69.4552%\n",
      "total_backward_count 78320 real_backward_count 18571  23.712%\n",
      "fc layer 1 self.abs_max_out: 3514.0\n",
      "fc layer 1 self.abs_max_out: 3625.0\n",
      "lif layer 1 self.abs_max_v: 6681.5\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.702193/  1.944810, val:  38.75%, val_best:  51.25%, tr:  92.13%, tr_best:  92.85%, epoch time: 54.53 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9318%\n",
      "layer   2  Sparsity: 74.3880%\n",
      "layer   3  Sparsity: 69.2297%\n",
      "total_backward_count 83215 real_backward_count 19565  23.511%\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.677925/  1.856765, val:  57.08%, val_best:  57.08%, tr:  94.08%, tr_best:  94.08%, epoch time: 55.21 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.9219%\n",
      "layer   2  Sparsity: 74.6651%\n",
      "layer   3  Sparsity: 69.2349%\n",
      "total_backward_count 88110 real_backward_count 20529  23.299%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.662502/  1.874360, val:  48.33%, val_best:  57.08%, tr:  93.16%, tr_best:  94.08%, epoch time: 56.99 seconds, 0.95 minutes\n",
      "layer   1  Sparsity: 87.9421%\n",
      "layer   2  Sparsity: 75.3952%\n",
      "layer   3  Sparsity: 69.7322%\n",
      "total_backward_count 93005 real_backward_count 21556  23.177%\n",
      "fc layer 1 self.abs_max_out: 3697.0\n",
      "lif layer 1 self.abs_max_v: 6746.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.646642/  1.861054, val:  47.08%, val_best:  57.08%, tr:  94.59%, tr_best:  94.59%, epoch time: 55.64 seconds, 0.93 minutes\n",
      "layer   1  Sparsity: 87.8576%\n",
      "layer   2  Sparsity: 75.1288%\n",
      "layer   3  Sparsity: 69.3648%\n",
      "total_backward_count 97900 real_backward_count 22467  22.949%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.638624/  1.884169, val:  44.17%, val_best:  57.08%, tr:  93.26%, tr_best:  94.59%, epoch time: 54.90 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9626%\n",
      "layer   2  Sparsity: 75.3030%\n",
      "layer   3  Sparsity: 68.9977%\n",
      "total_backward_count 102795 real_backward_count 23444  22.807%\n",
      "fc layer 2 self.abs_max_out: 1942.0\n",
      "fc layer 3 self.abs_max_out: 554.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.645903/  1.880068, val:  43.75%, val_best:  57.08%, tr:  92.95%, tr_best:  94.59%, epoch time: 53.95 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9332%\n",
      "layer   2  Sparsity: 74.6374%\n",
      "layer   3  Sparsity: 69.2766%\n",
      "total_backward_count 107690 real_backward_count 24443  22.698%\n",
      "lif layer 2 self.abs_max_v: 3007.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.638979/  1.817131, val:  54.17%, val_best:  57.08%, tr:  93.56%, tr_best:  94.59%, epoch time: 54.57 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9683%\n",
      "layer   2  Sparsity: 74.4712%\n",
      "layer   3  Sparsity: 68.9068%\n",
      "total_backward_count 112585 real_backward_count 25440  22.596%\n",
      "fc layer 1 self.abs_max_out: 3975.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.620971/  1.832525, val:  52.50%, val_best:  57.08%, tr:  93.67%, tr_best:  94.59%, epoch time: 53.88 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9387%\n",
      "layer   2  Sparsity: 75.0067%\n",
      "layer   3  Sparsity: 68.8034%\n",
      "total_backward_count 117480 real_backward_count 26388  22.462%\n",
      "lif layer 1 self.abs_max_v: 6951.5\n",
      "lif layer 2 self.abs_max_v: 3054.0\n",
      "lif layer 2 self.abs_max_v: 3060.5\n",
      "lif layer 2 self.abs_max_v: 3194.0\n",
      "lif layer 2 self.abs_max_v: 3194.5\n",
      "fc layer 2 self.abs_max_out: 2015.0\n",
      "lif layer 2 self.abs_max_v: 3394.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.613218/  1.825061, val:  51.25%, val_best:  57.08%, tr:  93.46%, tr_best:  94.59%, epoch time: 53.84 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9034%\n",
      "layer   2  Sparsity: 75.1272%\n",
      "layer   3  Sparsity: 69.9199%\n",
      "total_backward_count 122375 real_backward_count 27330  22.333%\n",
      "lif layer 2 self.abs_max_v: 3553.5\n",
      "lif layer 2 self.abs_max_v: 3742.0\n",
      "fc layer 2 self.abs_max_out: 2166.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.636181/  1.868003, val:  52.92%, val_best:  57.08%, tr:  92.34%, tr_best:  94.59%, epoch time: 53.79 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9485%\n",
      "layer   2  Sparsity: 74.3847%\n",
      "layer   3  Sparsity: 70.1774%\n",
      "total_backward_count 127270 real_backward_count 28303  22.239%\n",
      "lif layer 2 self.abs_max_v: 3794.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.642068/  1.827356, val:  54.58%, val_best:  57.08%, tr:  93.26%, tr_best:  94.59%, epoch time: 54.97 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.9026%\n",
      "layer   2  Sparsity: 73.9549%\n",
      "layer   3  Sparsity: 70.4021%\n",
      "total_backward_count 132165 real_backward_count 29302  22.171%\n",
      "fc layer 1 self.abs_max_out: 4039.0\n",
      "lif layer 1 self.abs_max_v: 7257.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.620285/  1.838845, val:  49.58%, val_best:  57.08%, tr:  92.95%, tr_best:  94.59%, epoch time: 54.26 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9310%\n",
      "layer   2  Sparsity: 73.4005%\n",
      "layer   3  Sparsity: 69.2880%\n",
      "total_backward_count 137060 real_backward_count 30238  22.062%\n",
      "fc layer 3 self.abs_max_out: 560.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.618518/  1.857083, val:  47.92%, val_best:  57.08%, tr:  94.69%, tr_best:  94.69%, epoch time: 53.10 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9152%\n",
      "layer   2  Sparsity: 73.0576%\n",
      "layer   3  Sparsity: 68.6813%\n",
      "total_backward_count 141955 real_backward_count 31133  21.932%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.607482/  1.844629, val:  45.00%, val_best:  57.08%, tr:  93.97%, tr_best:  94.69%, epoch time: 54.14 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 88.0167%\n",
      "layer   2  Sparsity: 73.3575%\n",
      "layer   3  Sparsity: 69.5676%\n",
      "total_backward_count 146850 real_backward_count 32025  21.808%\n",
      "fc layer 3 self.abs_max_out: 574.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.605702/  1.836094, val:  50.83%, val_best:  57.08%, tr:  93.26%, tr_best:  94.69%, epoch time: 52.97 seconds, 0.88 minutes\n",
      "layer   1  Sparsity: 87.9286%\n",
      "layer   2  Sparsity: 72.9103%\n",
      "layer   3  Sparsity: 69.2878%\n",
      "total_backward_count 151745 real_backward_count 33014  21.756%\n",
      "fc layer 3 self.abs_max_out: 582.0\n",
      "fc layer 3 self.abs_max_out: 621.0\n",
      "fc layer 1 self.abs_max_out: 4173.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.608819/  1.779217, val:  59.17%, val_best:  59.17%, tr:  94.28%, tr_best:  94.69%, epoch time: 55.58 seconds, 0.93 minutes\n",
      "layer   1  Sparsity: 87.9855%\n",
      "layer   2  Sparsity: 72.7376%\n",
      "layer   3  Sparsity: 69.3552%\n",
      "total_backward_count 156640 real_backward_count 33954  21.676%\n",
      "lif layer 1 self.abs_max_v: 7446.5\n",
      "fc layer 2 self.abs_max_out: 2200.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.604224/  1.874002, val:  43.75%, val_best:  59.17%, tr:  93.16%, tr_best:  94.69%, epoch time: 52.62 seconds, 0.88 minutes\n",
      "layer   1  Sparsity: 87.8710%\n",
      "layer   2  Sparsity: 73.4162%\n",
      "layer   3  Sparsity: 68.9379%\n",
      "total_backward_count 161535 real_backward_count 34881  21.593%\n",
      "fc layer 1 self.abs_max_out: 4277.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.601434/  1.803696, val:  52.50%, val_best:  59.17%, tr:  94.99%, tr_best:  94.99%, epoch time: 54.27 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9209%\n",
      "layer   2  Sparsity: 74.1787%\n",
      "layer   3  Sparsity: 67.9546%\n",
      "total_backward_count 166430 real_backward_count 35786  21.502%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.559859/  1.803046, val:  46.67%, val_best:  59.17%, tr:  93.77%, tr_best:  94.99%, epoch time: 53.49 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.8517%\n",
      "layer   2  Sparsity: 74.6917%\n",
      "layer   3  Sparsity: 68.2139%\n",
      "total_backward_count 171325 real_backward_count 36678  21.408%\n",
      "fc layer 3 self.abs_max_out: 633.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.572224/  1.803791, val:  46.25%, val_best:  59.17%, tr:  93.77%, tr_best:  94.99%, epoch time: 54.60 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9368%\n",
      "layer   2  Sparsity: 74.2797%\n",
      "layer   3  Sparsity: 68.3256%\n",
      "total_backward_count 176220 real_backward_count 37585  21.328%\n",
      "fc layer 1 self.abs_max_out: 4376.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.591944/  1.802445, val:  51.67%, val_best:  59.17%, tr:  94.18%, tr_best:  94.99%, epoch time: 53.92 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9833%\n",
      "layer   2  Sparsity: 73.6979%\n",
      "layer   3  Sparsity: 69.1731%\n",
      "total_backward_count 181115 real_backward_count 38474  21.243%\n",
      "fc layer 1 self.abs_max_out: 4695.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.572135/  1.807086, val:  50.42%, val_best:  59.17%, tr:  95.61%, tr_best:  95.61%, epoch time: 54.09 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9156%\n",
      "layer   2  Sparsity: 72.9153%\n",
      "layer   3  Sparsity: 69.2231%\n",
      "total_backward_count 186010 real_backward_count 39327  21.142%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.587349/  1.826937, val:  49.17%, val_best:  59.17%, tr:  93.36%, tr_best:  95.61%, epoch time: 53.73 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9533%\n",
      "layer   2  Sparsity: 72.6913%\n",
      "layer   3  Sparsity: 68.8243%\n",
      "total_backward_count 190905 real_backward_count 40230  21.073%\n",
      "lif layer 1 self.abs_max_v: 7936.0\n",
      "lif layer 1 self.abs_max_v: 8024.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.570806/  1.832102, val:  45.83%, val_best:  59.17%, tr:  94.79%, tr_best:  95.61%, epoch time: 54.63 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9817%\n",
      "layer   2  Sparsity: 74.1075%\n",
      "layer   3  Sparsity: 68.7801%\n",
      "total_backward_count 195800 real_backward_count 41095  20.988%\n",
      "lif layer 1 self.abs_max_v: 8238.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.574451/  1.775317, val:  53.33%, val_best:  59.17%, tr:  93.67%, tr_best:  95.61%, epoch time: 54.44 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9116%\n",
      "layer   2  Sparsity: 74.3537%\n",
      "layer   3  Sparsity: 68.8599%\n",
      "total_backward_count 200695 real_backward_count 41955  20.905%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.565866/  1.801833, val:  53.33%, val_best:  59.17%, tr:  94.69%, tr_best:  95.61%, epoch time: 53.70 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9551%\n",
      "layer   2  Sparsity: 74.2674%\n",
      "layer   3  Sparsity: 69.1645%\n",
      "total_backward_count 205590 real_backward_count 42805  20.821%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.569549/  1.785246, val:  60.00%, val_best:  60.00%, tr:  93.36%, tr_best:  95.61%, epoch time: 54.42 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.8997%\n",
      "layer   2  Sparsity: 73.3177%\n",
      "layer   3  Sparsity: 69.2521%\n",
      "total_backward_count 210485 real_backward_count 43677  20.751%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.576582/  1.786403, val:  60.42%, val_best:  60.42%, tr:  94.89%, tr_best:  95.61%, epoch time: 55.00 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.9281%\n",
      "layer   2  Sparsity: 73.1302%\n",
      "layer   3  Sparsity: 69.2226%\n",
      "total_backward_count 215380 real_backward_count 44569  20.693%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.575824/  1.787820, val:  55.83%, val_best:  60.42%, tr:  94.08%, tr_best:  95.61%, epoch time: 53.86 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9572%\n",
      "layer   2  Sparsity: 73.4773%\n",
      "layer   3  Sparsity: 69.5560%\n",
      "total_backward_count 220275 real_backward_count 45458  20.637%\n",
      "fc layer 2 self.abs_max_out: 2211.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.559618/  1.791707, val:  49.17%, val_best:  60.42%, tr:  93.46%, tr_best:  95.61%, epoch time: 53.21 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9019%\n",
      "layer   2  Sparsity: 73.0726%\n",
      "layer   3  Sparsity: 69.1699%\n",
      "total_backward_count 225170 real_backward_count 46350  20.584%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.544229/  1.786103, val:  57.50%, val_best:  60.42%, tr:  95.10%, tr_best:  95.61%, epoch time: 53.37 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.8906%\n",
      "layer   2  Sparsity: 72.1800%\n",
      "layer   3  Sparsity: 69.1602%\n",
      "total_backward_count 230065 real_backward_count 47222  20.526%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.553479/  1.816777, val:  45.00%, val_best:  60.42%, tr:  94.18%, tr_best:  95.61%, epoch time: 54.77 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9767%\n",
      "layer   2  Sparsity: 72.4625%\n",
      "layer   3  Sparsity: 69.4171%\n",
      "total_backward_count 234960 real_backward_count 48069  20.458%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.562588/  1.787563, val:  52.50%, val_best:  60.42%, tr:  95.30%, tr_best:  95.61%, epoch time: 53.45 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 88.0144%\n",
      "layer   2  Sparsity: 72.6944%\n",
      "layer   3  Sparsity: 69.0681%\n",
      "total_backward_count 239855 real_backward_count 48895  20.385%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.558981/  1.773498, val:  52.08%, val_best:  60.42%, tr:  94.18%, tr_best:  95.61%, epoch time: 55.10 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.9238%\n",
      "layer   2  Sparsity: 72.5414%\n",
      "layer   3  Sparsity: 68.5945%\n",
      "total_backward_count 244750 real_backward_count 49784  20.341%\n",
      "lif layer 2 self.abs_max_v: 3810.0\n",
      "fc layer 1 self.abs_max_out: 4853.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.551794/  1.778964, val:  50.42%, val_best:  60.42%, tr:  94.69%, tr_best:  95.61%, epoch time: 53.29 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9059%\n",
      "layer   2  Sparsity: 72.2942%\n",
      "layer   3  Sparsity: 68.2617%\n",
      "total_backward_count 249645 real_backward_count 50656  20.291%\n",
      "fc layer 2 self.abs_max_out: 2232.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.554666/  1.777518, val:  57.92%, val_best:  60.42%, tr:  95.30%, tr_best:  95.61%, epoch time: 55.06 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.8901%\n",
      "layer   2  Sparsity: 72.2424%\n",
      "layer   3  Sparsity: 68.6967%\n",
      "total_backward_count 254540 real_backward_count 51465  20.219%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.554141/  1.768448, val:  54.17%, val_best:  60.42%, tr:  93.56%, tr_best:  95.61%, epoch time: 53.29 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9386%\n",
      "layer   2  Sparsity: 72.0974%\n",
      "layer   3  Sparsity: 69.0051%\n",
      "total_backward_count 259435 real_backward_count 52370  20.186%\n",
      "fc layer 2 self.abs_max_out: 2272.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.535409/  1.753623, val:  52.92%, val_best:  60.42%, tr:  93.97%, tr_best:  95.61%, epoch time: 54.64 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9222%\n",
      "layer   2  Sparsity: 72.6818%\n",
      "layer   3  Sparsity: 68.9781%\n",
      "total_backward_count 264330 real_backward_count 53240  20.141%\n",
      "fc layer 1 self.abs_max_out: 5150.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.554909/  1.767505, val:  59.58%, val_best:  60.42%, tr:  94.48%, tr_best:  95.61%, epoch time: 54.49 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9210%\n",
      "layer   2  Sparsity: 73.0709%\n",
      "layer   3  Sparsity: 68.7806%\n",
      "total_backward_count 269225 real_backward_count 54083  20.088%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.529179/  1.775990, val:  47.08%, val_best:  60.42%, tr:  94.08%, tr_best:  95.61%, epoch time: 53.45 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9620%\n",
      "layer   2  Sparsity: 72.9444%\n",
      "layer   3  Sparsity: 68.9014%\n",
      "total_backward_count 274120 real_backward_count 54939  20.042%\n",
      "fc layer 2 self.abs_max_out: 2308.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.500462/  1.725889, val:  52.92%, val_best:  60.42%, tr:  95.40%, tr_best:  95.61%, epoch time: 54.42 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9193%\n",
      "layer   2  Sparsity: 72.0215%\n",
      "layer   3  Sparsity: 69.0177%\n",
      "total_backward_count 279015 real_backward_count 55728  19.973%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.508910/  1.747735, val:  51.67%, val_best:  60.42%, tr:  95.61%, tr_best:  95.61%, epoch time: 53.48 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 88.0307%\n",
      "layer   2  Sparsity: 72.0434%\n",
      "layer   3  Sparsity: 69.0431%\n",
      "total_backward_count 283910 real_backward_count 56516  19.906%\n",
      "lif layer 2 self.abs_max_v: 3972.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.518646/  1.778463, val:  50.00%, val_best:  60.42%, tr:  95.30%, tr_best:  95.61%, epoch time: 55.78 seconds, 0.93 minutes\n",
      "layer   1  Sparsity: 87.9263%\n",
      "layer   2  Sparsity: 71.6013%\n",
      "layer   3  Sparsity: 69.2597%\n",
      "total_backward_count 288805 real_backward_count 57326  19.849%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.510045/  1.785628, val:  46.67%, val_best:  60.42%, tr:  95.20%, tr_best:  95.61%, epoch time: 53.68 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9537%\n",
      "layer   2  Sparsity: 71.5865%\n",
      "layer   3  Sparsity: 68.9880%\n",
      "total_backward_count 293700 real_backward_count 58109  19.785%\n",
      "lif layer 2 self.abs_max_v: 4122.0\n",
      "lif layer 1 self.abs_max_v: 8820.5\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.511931/  1.765760, val:  53.33%, val_best:  60.42%, tr:  93.67%, tr_best:  95.61%, epoch time: 55.46 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.9363%\n",
      "layer   2  Sparsity: 71.8670%\n",
      "layer   3  Sparsity: 68.7773%\n",
      "total_backward_count 298595 real_backward_count 58931  19.736%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.506656/  1.747770, val:  57.92%, val_best:  60.42%, tr:  95.30%, tr_best:  95.61%, epoch time: 54.12 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.8619%\n",
      "layer   2  Sparsity: 71.8641%\n",
      "layer   3  Sparsity: 68.7140%\n",
      "total_backward_count 303490 real_backward_count 59755  19.689%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.495368/  1.776336, val:  47.08%, val_best:  60.42%, tr:  94.99%, tr_best:  95.61%, epoch time: 55.82 seconds, 0.93 minutes\n",
      "layer   1  Sparsity: 87.9377%\n",
      "layer   2  Sparsity: 71.2809%\n",
      "layer   3  Sparsity: 67.8962%\n",
      "total_backward_count 308385 real_backward_count 60568  19.640%\n",
      "lif layer 1 self.abs_max_v: 8840.5\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.510539/  1.782879, val:  44.17%, val_best:  60.42%, tr:  95.81%, tr_best:  95.81%, epoch time: 55.19 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.9734%\n",
      "layer   2  Sparsity: 71.7351%\n",
      "layer   3  Sparsity: 68.3732%\n",
      "total_backward_count 313280 real_backward_count 61362  19.587%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.524220/  1.750642, val:  52.08%, val_best:  60.42%, tr:  95.20%, tr_best:  95.81%, epoch time: 54.92 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.9343%\n",
      "layer   2  Sparsity: 71.5273%\n",
      "layer   3  Sparsity: 68.7480%\n",
      "total_backward_count 318175 real_backward_count 62175  19.541%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.495750/  1.725868, val:  60.00%, val_best:  60.42%, tr:  96.02%, tr_best:  96.02%, epoch time: 53.51 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9728%\n",
      "layer   2  Sparsity: 71.9197%\n",
      "layer   3  Sparsity: 68.3208%\n",
      "total_backward_count 323070 real_backward_count 62939  19.482%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.491764/  1.740302, val:  54.17%, val_best:  60.42%, tr:  94.48%, tr_best:  96.02%, epoch time: 53.79 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9281%\n",
      "layer   2  Sparsity: 71.6164%\n",
      "layer   3  Sparsity: 68.3512%\n",
      "total_backward_count 327965 real_backward_count 63747  19.437%\n",
      "fc layer 2 self.abs_max_out: 2309.0\n",
      "lif layer 2 self.abs_max_v: 4152.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.512618/  1.759222, val:  58.75%, val_best:  60.42%, tr:  94.99%, tr_best:  96.02%, epoch time: 54.45 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9098%\n",
      "layer   2  Sparsity: 71.3674%\n",
      "layer   3  Sparsity: 68.3897%\n",
      "total_backward_count 332860 real_backward_count 64555  19.394%\n",
      "fc layer 2 self.abs_max_out: 2435.0\n",
      "lif layer 2 self.abs_max_v: 4186.0\n",
      "fc layer 2 self.abs_max_out: 2472.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.510930/  1.748213, val:  55.83%, val_best:  60.42%, tr:  95.91%, tr_best:  96.02%, epoch time: 53.78 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9043%\n",
      "layer   2  Sparsity: 71.9268%\n",
      "layer   3  Sparsity: 69.0419%\n",
      "total_backward_count 337755 real_backward_count 65343  19.346%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.511304/  1.739437, val:  56.67%, val_best:  60.42%, tr:  95.51%, tr_best:  96.02%, epoch time: 53.85 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.8865%\n",
      "layer   2  Sparsity: 71.3481%\n",
      "layer   3  Sparsity: 68.8803%\n",
      "total_backward_count 342650 real_backward_count 66138  19.302%\n",
      "fc layer 1 self.abs_max_out: 5219.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.503253/  1.730545, val:  59.58%, val_best:  60.42%, tr:  95.81%, tr_best:  96.02%, epoch time: 55.35 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.9599%\n",
      "layer   2  Sparsity: 71.7720%\n",
      "layer   3  Sparsity: 68.4501%\n",
      "total_backward_count 347545 real_backward_count 66948  19.263%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.495302/  1.772377, val:  52.50%, val_best:  60.42%, tr:  95.91%, tr_best:  96.02%, epoch time: 55.11 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.8802%\n",
      "layer   2  Sparsity: 71.3224%\n",
      "layer   3  Sparsity: 67.9728%\n",
      "total_backward_count 352440 real_backward_count 67721  19.215%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.472423/  1.722312, val:  56.25%, val_best:  60.42%, tr:  94.79%, tr_best:  96.02%, epoch time: 55.55 seconds, 0.93 minutes\n",
      "layer   1  Sparsity: 87.8902%\n",
      "layer   2  Sparsity: 71.5297%\n",
      "layer   3  Sparsity: 68.0432%\n",
      "total_backward_count 357335 real_backward_count 68577  19.191%\n",
      "fc layer 1 self.abs_max_out: 5287.0\n",
      "lif layer 1 self.abs_max_v: 8971.0\n",
      "fc layer 1 self.abs_max_out: 5315.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.482234/  1.755924, val:  50.00%, val_best:  60.42%, tr:  94.99%, tr_best:  96.02%, epoch time: 53.30 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 88.0145%\n",
      "layer   2  Sparsity: 71.3807%\n",
      "layer   3  Sparsity: 67.7380%\n",
      "total_backward_count 362230 real_backward_count 69317  19.136%\n",
      "fc layer 1 self.abs_max_out: 5579.0\n",
      "lif layer 1 self.abs_max_v: 9443.0\n",
      "fc layer 1 self.abs_max_out: 5608.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.476435/  1.748521, val:  49.17%, val_best:  60.42%, tr:  95.81%, tr_best:  96.02%, epoch time: 54.64 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9415%\n",
      "layer   2  Sparsity: 71.6313%\n",
      "layer   3  Sparsity: 68.3559%\n",
      "total_backward_count 367125 real_backward_count 70143  19.106%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.440543/  1.702482, val:  60.00%, val_best:  60.42%, tr:  95.61%, tr_best:  96.02%, epoch time: 53.98 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9445%\n",
      "layer   2  Sparsity: 71.6385%\n",
      "layer   3  Sparsity: 67.9001%\n",
      "total_backward_count 372020 real_backward_count 70874  19.051%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.451854/  1.740449, val:  50.83%, val_best:  60.42%, tr:  95.51%, tr_best:  96.02%, epoch time: 55.80 seconds, 0.93 minutes\n",
      "layer   1  Sparsity: 87.8953%\n",
      "layer   2  Sparsity: 71.3792%\n",
      "layer   3  Sparsity: 67.7845%\n",
      "total_backward_count 376915 real_backward_count 71659  19.012%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.468817/  1.793207, val:  40.42%, val_best:  60.42%, tr:  94.99%, tr_best:  96.02%, epoch time: 53.60 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9543%\n",
      "layer   2  Sparsity: 71.2162%\n",
      "layer   3  Sparsity: 68.1876%\n",
      "total_backward_count 381810 real_backward_count 72451  18.976%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.484297/  1.697555, val:  61.67%, val_best:  61.67%, tr:  96.42%, tr_best:  96.42%, epoch time: 54.35 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9803%\n",
      "layer   2  Sparsity: 71.5042%\n",
      "layer   3  Sparsity: 68.6368%\n",
      "total_backward_count 386705 real_backward_count 73210  18.932%\n",
      "lif layer 2 self.abs_max_v: 4291.0\n",
      "fc layer 2 self.abs_max_out: 2658.0\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.474671/  1.709806, val:  57.08%, val_best:  61.67%, tr:  94.89%, tr_best:  96.42%, epoch time: 53.37 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9604%\n",
      "layer   2  Sparsity: 71.2617%\n",
      "layer   3  Sparsity: 67.9954%\n",
      "total_backward_count 391600 real_backward_count 73992  18.895%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.487421/  1.733886, val:  59.58%, val_best:  61.67%, tr:  94.89%, tr_best:  96.42%, epoch time: 54.60 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9409%\n",
      "layer   2  Sparsity: 71.4727%\n",
      "layer   3  Sparsity: 68.1739%\n",
      "total_backward_count 396495 real_backward_count 74819  18.870%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.472453/  1.742681, val:  49.58%, val_best:  61.67%, tr:  96.02%, tr_best:  96.42%, epoch time: 53.90 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9943%\n",
      "layer   2  Sparsity: 71.6073%\n",
      "layer   3  Sparsity: 68.5545%\n",
      "total_backward_count 401390 real_backward_count 75583  18.830%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.467438/  1.729500, val:  54.17%, val_best:  61.67%, tr:  96.32%, tr_best:  96.42%, epoch time: 53.96 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9319%\n",
      "layer   2  Sparsity: 71.3491%\n",
      "layer   3  Sparsity: 68.7836%\n",
      "total_backward_count 406285 real_backward_count 76315  18.784%\n",
      "fc layer 3 self.abs_max_out: 643.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.463205/  1.742154, val:  51.67%, val_best:  61.67%, tr:  94.79%, tr_best:  96.42%, epoch time: 53.14 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.8921%\n",
      "layer   2  Sparsity: 71.2200%\n",
      "layer   3  Sparsity: 68.8838%\n",
      "total_backward_count 411180 real_backward_count 77103  18.752%\n",
      "fc layer 1 self.abs_max_out: 5711.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.479705/  1.711346, val:  63.75%, val_best:  63.75%, tr:  94.99%, tr_best:  96.42%, epoch time: 56.96 seconds, 0.95 minutes\n",
      "layer   1  Sparsity: 87.9146%\n",
      "layer   2  Sparsity: 71.5455%\n",
      "layer   3  Sparsity: 67.9308%\n",
      "total_backward_count 416075 real_backward_count 77920  18.727%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.470932/  1.771850, val:  47.08%, val_best:  63.75%, tr:  95.91%, tr_best:  96.42%, epoch time: 54.20 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 88.0049%\n",
      "layer   2  Sparsity: 72.5497%\n",
      "layer   3  Sparsity: 68.5942%\n",
      "total_backward_count 420970 real_backward_count 78671  18.688%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.473486/  1.732913, val:  52.50%, val_best:  63.75%, tr:  95.30%, tr_best:  96.42%, epoch time: 54.22 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 88.0316%\n",
      "layer   2  Sparsity: 71.8477%\n",
      "layer   3  Sparsity: 69.1398%\n",
      "total_backward_count 425865 real_backward_count 79442  18.654%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.484866/  1.706978, val:  57.92%, val_best:  63.75%, tr:  95.40%, tr_best:  96.42%, epoch time: 54.31 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.7924%\n",
      "layer   2  Sparsity: 71.1217%\n",
      "layer   3  Sparsity: 68.1916%\n",
      "total_backward_count 430760 real_backward_count 80254  18.631%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.460168/  1.705439, val:  55.42%, val_best:  63.75%, tr:  96.02%, tr_best:  96.42%, epoch time: 53.99 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 88.0244%\n",
      "layer   2  Sparsity: 71.0999%\n",
      "layer   3  Sparsity: 68.3546%\n",
      "total_backward_count 435655 real_backward_count 81009  18.595%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.476301/  1.715314, val:  60.00%, val_best:  63.75%, tr:  94.99%, tr_best:  96.42%, epoch time: 53.97 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 88.0618%\n",
      "layer   2  Sparsity: 71.0832%\n",
      "layer   3  Sparsity: 68.2710%\n",
      "total_backward_count 440550 real_backward_count 81808  18.570%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.464116/  1.709569, val:  59.17%, val_best:  63.75%, tr:  96.02%, tr_best:  96.42%, epoch time: 54.81 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9529%\n",
      "layer   2  Sparsity: 71.3326%\n",
      "layer   3  Sparsity: 68.7647%\n",
      "total_backward_count 445445 real_backward_count 82592  18.541%\n",
      "lif layer 2 self.abs_max_v: 4333.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.446992/  1.666375, val:  60.00%, val_best:  63.75%, tr:  96.02%, tr_best:  96.42%, epoch time: 53.72 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.8732%\n",
      "layer   2  Sparsity: 71.5492%\n",
      "layer   3  Sparsity: 68.9838%\n",
      "total_backward_count 450340 real_backward_count 83346  18.507%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.439245/  1.668705, val:  60.00%, val_best:  63.75%, tr:  94.89%, tr_best:  96.42%, epoch time: 55.04 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.9598%\n",
      "layer   2  Sparsity: 71.8890%\n",
      "layer   3  Sparsity: 68.9678%\n",
      "total_backward_count 455235 real_backward_count 84133  18.481%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.442198/  1.688950, val:  65.42%, val_best:  65.42%, tr:  95.20%, tr_best:  96.42%, epoch time: 53.48 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9335%\n",
      "layer   2  Sparsity: 71.5809%\n",
      "layer   3  Sparsity: 68.8117%\n",
      "total_backward_count 460130 real_backward_count 84896  18.450%\n",
      "lif layer 1 self.abs_max_v: 9984.0\n",
      "lif layer 2 self.abs_max_v: 4392.5\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.486553/  1.731709, val:  57.92%, val_best:  65.42%, tr:  95.30%, tr_best:  96.42%, epoch time: 54.84 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 88.0510%\n",
      "layer   2  Sparsity: 71.3929%\n",
      "layer   3  Sparsity: 68.6778%\n",
      "total_backward_count 465025 real_backward_count 85690  18.427%\n",
      "lif layer 2 self.abs_max_v: 4673.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.460179/  1.711875, val:  62.08%, val_best:  65.42%, tr:  95.20%, tr_best:  96.42%, epoch time: 52.69 seconds, 0.88 minutes\n",
      "layer   1  Sparsity: 87.9051%\n",
      "layer   2  Sparsity: 71.3736%\n",
      "layer   3  Sparsity: 68.2957%\n",
      "total_backward_count 469920 real_backward_count 86456  18.398%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.441163/  1.674867, val:  57.08%, val_best:  65.42%, tr:  96.53%, tr_best:  96.53%, epoch time: 54.86 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9703%\n",
      "layer   2  Sparsity: 71.6726%\n",
      "layer   3  Sparsity: 68.5971%\n",
      "total_backward_count 474815 real_backward_count 87203  18.366%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.423389/  1.677979, val:  53.33%, val_best:  65.42%, tr:  95.30%, tr_best:  96.53%, epoch time: 54.28 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9600%\n",
      "layer   2  Sparsity: 71.7244%\n",
      "layer   3  Sparsity: 69.3076%\n",
      "total_backward_count 479710 real_backward_count 87957  18.335%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.439575/  1.771465, val:  43.33%, val_best:  65.42%, tr:  94.48%, tr_best:  96.53%, epoch time: 54.29 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 88.0317%\n",
      "layer   2  Sparsity: 70.8676%\n",
      "layer   3  Sparsity: 68.5347%\n",
      "total_backward_count 484605 real_backward_count 88732  18.310%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.438323/  1.687584, val:  54.58%, val_best:  65.42%, tr:  96.02%, tr_best:  96.53%, epoch time: 53.87 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9584%\n",
      "layer   2  Sparsity: 70.6660%\n",
      "layer   3  Sparsity: 68.4490%\n",
      "total_backward_count 489500 real_backward_count 89492  18.282%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.419907/  1.680947, val:  58.33%, val_best:  65.42%, tr:  96.12%, tr_best:  96.53%, epoch time: 53.57 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9244%\n",
      "layer   2  Sparsity: 71.1672%\n",
      "layer   3  Sparsity: 68.8926%\n",
      "total_backward_count 494395 real_backward_count 90212  18.247%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.451887/  1.679859, val:  58.33%, val_best:  65.42%, tr:  95.40%, tr_best:  96.53%, epoch time: 54.26 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.8765%\n",
      "layer   2  Sparsity: 70.6561%\n",
      "layer   3  Sparsity: 69.1719%\n",
      "total_backward_count 499290 real_backward_count 90959  18.218%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.453793/  1.709191, val:  57.50%, val_best:  65.42%, tr:  96.53%, tr_best:  96.53%, epoch time: 53.74 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9356%\n",
      "layer   2  Sparsity: 70.8400%\n",
      "layer   3  Sparsity: 69.2821%\n",
      "total_backward_count 504185 real_backward_count 91694  18.187%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.473666/  1.713880, val:  56.25%, val_best:  65.42%, tr:  96.02%, tr_best:  96.53%, epoch time: 54.49 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9008%\n",
      "layer   2  Sparsity: 71.4950%\n",
      "layer   3  Sparsity: 69.6267%\n",
      "total_backward_count 509080 real_backward_count 92444  18.159%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.476993/  1.741338, val:  54.17%, val_best:  65.42%, tr:  96.42%, tr_best:  96.53%, epoch time: 53.61 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.8987%\n",
      "layer   2  Sparsity: 70.6412%\n",
      "layer   3  Sparsity: 69.1517%\n",
      "total_backward_count 513975 real_backward_count 93231  18.139%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.481579/  1.719415, val:  67.50%, val_best:  67.50%, tr:  96.02%, tr_best:  96.53%, epoch time: 55.41 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 88.0135%\n",
      "layer   2  Sparsity: 71.3466%\n",
      "layer   3  Sparsity: 69.3250%\n",
      "total_backward_count 518870 real_backward_count 94002  18.117%\n",
      "fc layer 1 self.abs_max_out: 5722.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.466120/  1.701510, val:  50.83%, val_best:  67.50%, tr:  96.32%, tr_best:  96.53%, epoch time: 52.86 seconds, 0.88 minutes\n",
      "layer   1  Sparsity: 88.0319%\n",
      "layer   2  Sparsity: 70.8644%\n",
      "layer   3  Sparsity: 68.9421%\n",
      "total_backward_count 523765 real_backward_count 94723  18.085%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.442976/  1.707311, val:  50.00%, val_best:  67.50%, tr:  95.10%, tr_best:  96.53%, epoch time: 54.82 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9980%\n",
      "layer   2  Sparsity: 71.0195%\n",
      "layer   3  Sparsity: 68.6111%\n",
      "total_backward_count 528660 real_backward_count 95485  18.062%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.439871/  1.689720, val:  53.33%, val_best:  67.50%, tr:  95.20%, tr_best:  96.53%, epoch time: 53.39 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9356%\n",
      "layer   2  Sparsity: 70.7031%\n",
      "layer   3  Sparsity: 68.2093%\n",
      "total_backward_count 533555 real_backward_count 96223  18.034%\n",
      "fc layer 2 self.abs_max_out: 2736.0\n",
      "fc layer 1 self.abs_max_out: 5777.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.432571/  1.700121, val:  53.75%, val_best:  67.50%, tr:  96.22%, tr_best:  96.53%, epoch time: 52.80 seconds, 0.88 minutes\n",
      "layer   1  Sparsity: 87.9845%\n",
      "layer   2  Sparsity: 70.3020%\n",
      "layer   3  Sparsity: 68.1299%\n",
      "total_backward_count 538450 real_backward_count 96957  18.007%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.424466/  1.632173, val:  62.08%, val_best:  67.50%, tr:  96.42%, tr_best:  96.53%, epoch time: 53.32 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 88.0023%\n",
      "layer   2  Sparsity: 70.0407%\n",
      "layer   3  Sparsity: 68.3404%\n",
      "total_backward_count 543345 real_backward_count 97722  17.985%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.413640/  1.676396, val:  57.92%, val_best:  67.50%, tr:  96.32%, tr_best:  96.53%, epoch time: 53.59 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9489%\n",
      "layer   2  Sparsity: 70.4175%\n",
      "layer   3  Sparsity: 68.9134%\n",
      "total_backward_count 548240 real_backward_count 98416  17.951%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.426897/  1.654334, val:  64.17%, val_best:  67.50%, tr:  97.14%, tr_best:  97.14%, epoch time: 54.18 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9121%\n",
      "layer   2  Sparsity: 70.1712%\n",
      "layer   3  Sparsity: 68.8043%\n",
      "total_backward_count 553135 real_backward_count 99171  17.929%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.418119/  1.665317, val:  52.50%, val_best:  67.50%, tr:  95.81%, tr_best:  97.14%, epoch time: 52.14 seconds, 0.87 minutes\n",
      "layer   1  Sparsity: 87.9276%\n",
      "layer   2  Sparsity: 69.8516%\n",
      "layer   3  Sparsity: 68.0614%\n",
      "total_backward_count 558030 real_backward_count 99907  17.904%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.411932/  1.685951, val:  47.92%, val_best:  67.50%, tr:  95.61%, tr_best:  97.14%, epoch time: 54.09 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9000%\n",
      "layer   2  Sparsity: 70.0401%\n",
      "layer   3  Sparsity: 68.3042%\n",
      "total_backward_count 562925 real_backward_count 100652  17.880%\n",
      "fc layer 1 self.abs_max_out: 6005.0\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.407238/  1.703751, val:  57.50%, val_best:  67.50%, tr:  96.22%, tr_best:  97.14%, epoch time: 53.77 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9855%\n",
      "layer   2  Sparsity: 70.1355%\n",
      "layer   3  Sparsity: 68.7306%\n",
      "total_backward_count 567820 real_backward_count 101367  17.852%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.417738/  1.679335, val:  56.25%, val_best:  67.50%, tr:  95.51%, tr_best:  97.14%, epoch time: 53.99 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9008%\n",
      "layer   2  Sparsity: 70.7454%\n",
      "layer   3  Sparsity: 68.7918%\n",
      "total_backward_count 572715 real_backward_count 102109  17.829%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.421007/  1.633586, val:  62.08%, val_best:  67.50%, tr:  96.22%, tr_best:  97.14%, epoch time: 54.12 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9229%\n",
      "layer   2  Sparsity: 70.3576%\n",
      "layer   3  Sparsity: 68.6376%\n",
      "total_backward_count 577610 real_backward_count 102830  17.803%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.424694/  1.704035, val:  56.25%, val_best:  67.50%, tr:  95.30%, tr_best:  97.14%, epoch time: 53.61 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9719%\n",
      "layer   2  Sparsity: 70.6573%\n",
      "layer   3  Sparsity: 68.7509%\n",
      "total_backward_count 582505 real_backward_count 103532  17.774%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.430672/  1.671470, val:  59.58%, val_best:  67.50%, tr:  97.34%, tr_best:  97.34%, epoch time: 53.09 seconds, 0.88 minutes\n",
      "layer   1  Sparsity: 87.9128%\n",
      "layer   2  Sparsity: 71.1538%\n",
      "layer   3  Sparsity: 69.3524%\n",
      "total_backward_count 587400 real_backward_count 104213  17.741%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.435646/  1.702359, val:  59.58%, val_best:  67.50%, tr:  95.81%, tr_best:  97.34%, epoch time: 53.84 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9448%\n",
      "layer   2  Sparsity: 70.4985%\n",
      "layer   3  Sparsity: 68.7053%\n",
      "total_backward_count 592295 real_backward_count 104929  17.716%\n",
      "fc layer 3 self.abs_max_out: 666.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.415238/  1.702303, val:  55.00%, val_best:  67.50%, tr:  96.53%, tr_best:  97.34%, epoch time: 54.80 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9705%\n",
      "layer   2  Sparsity: 70.2502%\n",
      "layer   3  Sparsity: 68.1929%\n",
      "total_backward_count 597190 real_backward_count 105646  17.691%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.424859/  1.749160, val:  42.92%, val_best:  67.50%, tr:  97.04%, tr_best:  97.34%, epoch time: 53.84 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9030%\n",
      "layer   2  Sparsity: 70.5386%\n",
      "layer   3  Sparsity: 68.2118%\n",
      "total_backward_count 602085 real_backward_count 106351  17.664%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.429140/  1.687468, val:  57.50%, val_best:  67.50%, tr:  96.32%, tr_best:  97.34%, epoch time: 53.68 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9165%\n",
      "layer   2  Sparsity: 70.2448%\n",
      "layer   3  Sparsity: 67.9701%\n",
      "total_backward_count 606980 real_backward_count 107056  17.637%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.403870/  1.649077, val:  60.42%, val_best:  67.50%, tr:  96.12%, tr_best:  97.34%, epoch time: 52.47 seconds, 0.87 minutes\n",
      "layer   1  Sparsity: 87.8456%\n",
      "layer   2  Sparsity: 70.1239%\n",
      "layer   3  Sparsity: 68.4123%\n",
      "total_backward_count 611875 real_backward_count 107759  17.611%\n",
      "lif layer 1 self.abs_max_v: 10357.5\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.417527/  1.682730, val:  64.58%, val_best:  67.50%, tr:  95.30%, tr_best:  97.34%, epoch time: 53.34 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9206%\n",
      "layer   2  Sparsity: 70.5093%\n",
      "layer   3  Sparsity: 68.6751%\n",
      "total_backward_count 616770 real_backward_count 108507  17.593%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.436022/  1.681486, val:  61.25%, val_best:  67.50%, tr:  95.40%, tr_best:  97.34%, epoch time: 53.51 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9410%\n",
      "layer   2  Sparsity: 70.0925%\n",
      "layer   3  Sparsity: 68.8272%\n",
      "total_backward_count 621665 real_backward_count 109275  17.578%\n",
      "fc layer 1 self.abs_max_out: 6074.0\n",
      "lif layer 1 self.abs_max_v: 10532.0\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.437515/  1.713002, val:  55.42%, val_best:  67.50%, tr:  97.14%, tr_best:  97.34%, epoch time: 54.39 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9364%\n",
      "layer   2  Sparsity: 69.8491%\n",
      "layer   3  Sparsity: 68.8953%\n",
      "total_backward_count 626560 real_backward_count 109978  17.553%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.435565/  1.707056, val:  58.33%, val_best:  67.50%, tr:  95.91%, tr_best:  97.34%, epoch time: 53.54 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9258%\n",
      "layer   2  Sparsity: 69.9299%\n",
      "layer   3  Sparsity: 69.0129%\n",
      "total_backward_count 631455 real_backward_count 110707  17.532%\n",
      "fc layer 1 self.abs_max_out: 6319.0\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.405212/  1.679728, val:  60.83%, val_best:  67.50%, tr:  97.55%, tr_best:  97.55%, epoch time: 55.72 seconds, 0.93 minutes\n",
      "layer   1  Sparsity: 87.9747%\n",
      "layer   2  Sparsity: 70.5139%\n",
      "layer   3  Sparsity: 69.4954%\n",
      "total_backward_count 636350 real_backward_count 111384  17.504%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.411111/  1.681108, val:  50.83%, val_best:  67.50%, tr:  96.42%, tr_best:  97.55%, epoch time: 53.75 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9428%\n",
      "layer   2  Sparsity: 70.3440%\n",
      "layer   3  Sparsity: 69.3244%\n",
      "total_backward_count 641245 real_backward_count 112091  17.480%\n",
      "fc layer 3 self.abs_max_out: 689.0\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.380179/  1.650303, val:  58.75%, val_best:  67.50%, tr:  96.83%, tr_best:  97.55%, epoch time: 53.79 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9647%\n",
      "layer   2  Sparsity: 70.5269%\n",
      "layer   3  Sparsity: 69.3036%\n",
      "total_backward_count 646140 real_backward_count 112774  17.453%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.391212/  1.649623, val:  59.58%, val_best:  67.50%, tr:  96.12%, tr_best:  97.55%, epoch time: 54.96 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.9622%\n",
      "layer   2  Sparsity: 70.1941%\n",
      "layer   3  Sparsity: 68.8346%\n",
      "total_backward_count 651035 real_backward_count 113475  17.430%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.390415/  1.665822, val:  57.92%, val_best:  67.50%, tr:  96.94%, tr_best:  97.55%, epoch time: 53.66 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.8220%\n",
      "layer   2  Sparsity: 70.0857%\n",
      "layer   3  Sparsity: 68.9177%\n",
      "total_backward_count 655930 real_backward_count 114168  17.406%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.397898/  1.676423, val:  53.33%, val_best:  67.50%, tr:  95.71%, tr_best:  97.55%, epoch time: 53.96 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9585%\n",
      "layer   2  Sparsity: 69.7180%\n",
      "layer   3  Sparsity: 68.9256%\n",
      "total_backward_count 660825 real_backward_count 114892  17.386%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.389101/  1.637691, val:  63.75%, val_best:  67.50%, tr:  96.83%, tr_best:  97.55%, epoch time: 54.02 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9774%\n",
      "layer   2  Sparsity: 70.2633%\n",
      "layer   3  Sparsity: 69.2244%\n",
      "total_backward_count 665720 real_backward_count 115587  17.363%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.357062/  1.623491, val:  57.50%, val_best:  67.50%, tr:  96.83%, tr_best:  97.55%, epoch time: 53.77 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9731%\n",
      "layer   2  Sparsity: 70.5905%\n",
      "layer   3  Sparsity: 69.8838%\n",
      "total_backward_count 670615 real_backward_count 116282  17.340%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.347152/  1.623330, val:  54.58%, val_best:  67.50%, tr:  96.94%, tr_best:  97.55%, epoch time: 53.58 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.8990%\n",
      "layer   2  Sparsity: 70.3788%\n",
      "layer   3  Sparsity: 69.4342%\n",
      "total_backward_count 675510 real_backward_count 116967  17.315%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.380370/  1.636037, val:  64.17%, val_best:  67.50%, tr:  96.94%, tr_best:  97.55%, epoch time: 54.83 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9680%\n",
      "layer   2  Sparsity: 70.7392%\n",
      "layer   3  Sparsity: 69.8059%\n",
      "total_backward_count 680405 real_backward_count 117689  17.297%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.388272/  1.674449, val:  61.67%, val_best:  67.50%, tr:  97.45%, tr_best:  97.55%, epoch time: 53.70 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.8975%\n",
      "layer   2  Sparsity: 70.3943%\n",
      "layer   3  Sparsity: 69.5507%\n",
      "total_backward_count 685300 real_backward_count 118365  17.272%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.393021/  1.655978, val:  60.00%, val_best:  67.50%, tr:  96.02%, tr_best:  97.55%, epoch time: 54.00 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9753%\n",
      "layer   2  Sparsity: 70.6491%\n",
      "layer   3  Sparsity: 69.3364%\n",
      "total_backward_count 690195 real_backward_count 119063  17.251%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.387668/  1.647968, val:  57.92%, val_best:  67.50%, tr:  96.32%, tr_best:  97.55%, epoch time: 53.60 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9263%\n",
      "layer   2  Sparsity: 70.7340%\n",
      "layer   3  Sparsity: 69.8096%\n",
      "total_backward_count 695090 real_backward_count 119765  17.230%\n",
      "fc layer 1 self.abs_max_out: 6484.0\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.375849/  1.666358, val:  53.75%, val_best:  67.50%, tr:  96.02%, tr_best:  97.55%, epoch time: 54.99 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.9411%\n",
      "layer   2  Sparsity: 70.0839%\n",
      "layer   3  Sparsity: 69.7479%\n",
      "total_backward_count 699985 real_backward_count 120447  17.207%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.392441/  1.665889, val:  59.17%, val_best:  67.50%, tr:  96.02%, tr_best:  97.55%, epoch time: 53.72 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9100%\n",
      "layer   2  Sparsity: 70.0658%\n",
      "layer   3  Sparsity: 69.0119%\n",
      "total_backward_count 704880 real_backward_count 121155  17.188%\n",
      "lif layer 2 self.abs_max_v: 4734.5\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.382436/  1.629052, val:  57.92%, val_best:  67.50%, tr:  96.22%, tr_best:  97.55%, epoch time: 54.56 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9388%\n",
      "layer   2  Sparsity: 70.1222%\n",
      "layer   3  Sparsity: 68.8010%\n",
      "total_backward_count 709775 real_backward_count 121842  17.166%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.379294/  1.621547, val:  61.25%, val_best:  67.50%, tr:  97.24%, tr_best:  97.55%, epoch time: 53.85 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9666%\n",
      "layer   2  Sparsity: 69.8746%\n",
      "layer   3  Sparsity: 69.1847%\n",
      "total_backward_count 714670 real_backward_count 122487  17.139%\n",
      "fc layer 2 self.abs_max_out: 2777.0\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.384613/  1.650601, val:  59.17%, val_best:  67.50%, tr:  96.12%, tr_best:  97.55%, epoch time: 53.37 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9949%\n",
      "layer   2  Sparsity: 70.1299%\n",
      "layer   3  Sparsity: 69.5096%\n",
      "total_backward_count 719565 real_backward_count 123192  17.120%\n",
      "fc layer 2 self.abs_max_out: 2810.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.379739/  1.674313, val:  56.25%, val_best:  67.50%, tr:  97.45%, tr_best:  97.55%, epoch time: 54.88 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9517%\n",
      "layer   2  Sparsity: 70.1491%\n",
      "layer   3  Sparsity: 69.6664%\n",
      "total_backward_count 724460 real_backward_count 123821  17.091%\n",
      "lif layer 2 self.abs_max_v: 4892.5\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.381707/  1.685507, val:  53.33%, val_best:  67.50%, tr:  97.04%, tr_best:  97.55%, epoch time: 54.42 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.8912%\n",
      "layer   2  Sparsity: 70.4132%\n",
      "layer   3  Sparsity: 69.3540%\n",
      "total_backward_count 729355 real_backward_count 124511  17.071%\n",
      "fc layer 2 self.abs_max_out: 2859.0\n",
      "lif layer 2 self.abs_max_v: 5086.0\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.387696/  1.608286, val:  61.67%, val_best:  67.50%, tr:  96.22%, tr_best:  97.55%, epoch time: 55.14 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.9711%\n",
      "layer   2  Sparsity: 70.1824%\n",
      "layer   3  Sparsity: 69.3539%\n",
      "total_backward_count 734250 real_backward_count 125276  17.062%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.355824/  1.636415, val:  58.33%, val_best:  67.50%, tr:  96.53%, tr_best:  97.55%, epoch time: 53.90 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.8930%\n",
      "layer   2  Sparsity: 70.0125%\n",
      "layer   3  Sparsity: 69.4502%\n",
      "total_backward_count 739145 real_backward_count 125940  17.039%\n",
      "fc layer 2 self.abs_max_out: 2870.0\n",
      "lif layer 2 self.abs_max_v: 5110.5\n",
      "lif layer 2 self.abs_max_v: 5158.5\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.390979/  1.699139, val:  51.25%, val_best:  67.50%, tr:  96.42%, tr_best:  97.55%, epoch time: 55.00 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.9668%\n",
      "layer   2  Sparsity: 69.5816%\n",
      "layer   3  Sparsity: 69.6206%\n",
      "total_backward_count 744040 real_backward_count 126631  17.019%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.390687/  1.651183, val:  57.92%, val_best:  67.50%, tr:  96.94%, tr_best:  97.55%, epoch time: 53.35 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9484%\n",
      "layer   2  Sparsity: 69.7447%\n",
      "layer   3  Sparsity: 68.7615%\n",
      "total_backward_count 748935 real_backward_count 127296  16.997%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.395866/  1.645626, val:  63.33%, val_best:  67.50%, tr:  97.04%, tr_best:  97.55%, epoch time: 55.00 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 88.0494%\n",
      "layer   2  Sparsity: 69.7267%\n",
      "layer   3  Sparsity: 69.2606%\n",
      "total_backward_count 753830 real_backward_count 127958  16.974%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.381391/  1.632166, val:  62.08%, val_best:  67.50%, tr:  97.85%, tr_best:  97.85%, epoch time: 53.58 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9939%\n",
      "layer   2  Sparsity: 70.1497%\n",
      "layer   3  Sparsity: 69.3057%\n",
      "total_backward_count 758725 real_backward_count 128677  16.960%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.360651/  1.636941, val:  56.25%, val_best:  67.50%, tr:  96.22%, tr_best:  97.85%, epoch time: 54.71 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9275%\n",
      "layer   2  Sparsity: 69.8564%\n",
      "layer   3  Sparsity: 68.9852%\n",
      "total_backward_count 763620 real_backward_count 129331  16.937%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.367124/  1.630551, val:  62.92%, val_best:  67.50%, tr:  96.02%, tr_best:  97.85%, epoch time: 53.74 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9355%\n",
      "layer   2  Sparsity: 69.9472%\n",
      "layer   3  Sparsity: 69.1519%\n",
      "total_backward_count 768515 real_backward_count 130012  16.917%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.376055/  1.642499, val:  60.00%, val_best:  67.50%, tr:  96.83%, tr_best:  97.85%, epoch time: 54.55 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9915%\n",
      "layer   2  Sparsity: 69.8896%\n",
      "layer   3  Sparsity: 68.8912%\n",
      "total_backward_count 773410 real_backward_count 130653  16.893%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.399195/  1.678673, val:  56.25%, val_best:  67.50%, tr:  96.63%, tr_best:  97.85%, epoch time: 54.03 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9182%\n",
      "layer   2  Sparsity: 69.4080%\n",
      "layer   3  Sparsity: 68.5566%\n",
      "total_backward_count 778305 real_backward_count 131374  16.880%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.385007/  1.645059, val:  63.75%, val_best:  67.50%, tr:  96.12%, tr_best:  97.85%, epoch time: 54.43 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.8669%\n",
      "layer   2  Sparsity: 69.6561%\n",
      "layer   3  Sparsity: 68.5590%\n",
      "total_backward_count 783200 real_backward_count 132048  16.860%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.353426/  1.599286, val:  62.50%, val_best:  67.50%, tr:  96.73%, tr_best:  97.85%, epoch time: 54.12 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9596%\n",
      "layer   2  Sparsity: 69.4200%\n",
      "layer   3  Sparsity: 68.9164%\n",
      "total_backward_count 788095 real_backward_count 132699  16.838%\n",
      "lif layer 1 self.abs_max_v: 10613.0\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.361208/  1.623406, val:  63.33%, val_best:  67.50%, tr:  96.73%, tr_best:  97.85%, epoch time: 53.56 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9545%\n",
      "layer   2  Sparsity: 69.2852%\n",
      "layer   3  Sparsity: 68.8756%\n",
      "total_backward_count 792990 real_backward_count 133408  16.823%\n",
      "fc layer 1 self.abs_max_out: 6932.0\n",
      "lif layer 1 self.abs_max_v: 10816.0\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.364148/  1.653648, val:  54.58%, val_best:  67.50%, tr:  96.63%, tr_best:  97.85%, epoch time: 54.28 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9359%\n",
      "layer   2  Sparsity: 69.4984%\n",
      "layer   3  Sparsity: 69.2563%\n",
      "total_backward_count 797885 real_backward_count 134076  16.804%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.373696/  1.652734, val:  55.00%, val_best:  67.50%, tr:  96.63%, tr_best:  97.85%, epoch time: 53.65 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9343%\n",
      "layer   2  Sparsity: 69.4690%\n",
      "layer   3  Sparsity: 69.0057%\n",
      "total_backward_count 802780 real_backward_count 134733  16.783%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.383387/  1.650537, val:  62.50%, val_best:  67.50%, tr:  96.73%, tr_best:  97.85%, epoch time: 55.10 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.8875%\n",
      "layer   2  Sparsity: 69.6432%\n",
      "layer   3  Sparsity: 68.8751%\n",
      "total_backward_count 807675 real_backward_count 135397  16.764%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.369386/  1.617916, val:  60.42%, val_best:  67.50%, tr:  96.02%, tr_best:  97.85%, epoch time: 53.67 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9456%\n",
      "layer   2  Sparsity: 69.6111%\n",
      "layer   3  Sparsity: 68.8793%\n",
      "total_backward_count 812570 real_backward_count 136077  16.746%\n",
      "fc layer 2 self.abs_max_out: 2880.0\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.368534/  1.637393, val:  62.50%, val_best:  67.50%, tr:  97.34%, tr_best:  97.85%, epoch time: 54.48 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 88.0488%\n",
      "layer   2  Sparsity: 69.1615%\n",
      "layer   3  Sparsity: 68.6470%\n",
      "total_backward_count 817465 real_backward_count 136736  16.727%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.370375/  1.637693, val:  57.92%, val_best:  67.50%, tr:  96.73%, tr_best:  97.85%, epoch time: 54.69 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.8785%\n",
      "layer   2  Sparsity: 69.2366%\n",
      "layer   3  Sparsity: 68.8151%\n",
      "total_backward_count 822360 real_backward_count 137389  16.707%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.363012/  1.614221, val:  59.58%, val_best:  67.50%, tr:  96.83%, tr_best:  97.85%, epoch time: 55.00 seconds, 0.92 minutes\n",
      "layer   1  Sparsity: 87.9324%\n",
      "layer   2  Sparsity: 69.2567%\n",
      "layer   3  Sparsity: 68.4237%\n",
      "total_backward_count 827255 real_backward_count 138059  16.689%\n",
      "lif layer 2 self.abs_max_v: 5159.5\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.359113/  1.608917, val:  63.75%, val_best:  67.50%, tr:  96.83%, tr_best:  97.85%, epoch time: 53.85 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.9238%\n",
      "layer   2  Sparsity: 69.9874%\n",
      "layer   3  Sparsity: 68.7016%\n",
      "total_backward_count 832150 real_backward_count 138736  16.672%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.345180/  1.633207, val:  67.50%, val_best:  67.50%, tr:  96.63%, tr_best:  97.85%, epoch time: 54.74 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9959%\n",
      "layer   2  Sparsity: 69.7242%\n",
      "layer   3  Sparsity: 68.6865%\n",
      "total_backward_count 837045 real_backward_count 139380  16.651%\n",
      "fc layer 2 self.abs_max_out: 2889.0\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.347281/  1.653264, val:  55.42%, val_best:  67.50%, tr:  97.75%, tr_best:  97.85%, epoch time: 54.75 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 87.9647%\n",
      "layer   2  Sparsity: 69.5371%\n",
      "layer   3  Sparsity: 69.3812%\n",
      "total_backward_count 841940 real_backward_count 140004  16.629%\n",
      "fc layer 2 self.abs_max_out: 3038.0\n",
      "lif layer 2 self.abs_max_v: 5496.5\n",
      "fc layer 2 self.abs_max_out: 3153.0\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.367702/  1.677987, val:  52.50%, val_best:  67.50%, tr:  97.34%, tr_best:  97.85%, epoch time: 55.53 seconds, 0.93 minutes\n",
      "layer   1  Sparsity: 87.9620%\n",
      "layer   2  Sparsity: 69.3934%\n",
      "layer   3  Sparsity: 70.2397%\n",
      "total_backward_count 846835 real_backward_count 140692  16.614%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.379590/  1.644488, val:  62.50%, val_best:  67.50%, tr:  96.42%, tr_best:  97.85%, epoch time: 56.22 seconds, 0.94 minutes\n",
      "layer   1  Sparsity: 87.9668%\n",
      "layer   2  Sparsity: 69.7119%\n",
      "layer   3  Sparsity: 69.7121%\n",
      "total_backward_count 851730 real_backward_count 141335  16.594%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.376201/  1.648889, val:  63.33%, val_best:  67.50%, tr:  97.04%, tr_best:  97.85%, epoch time: 55.72 seconds, 0.93 minutes\n",
      "layer   1  Sparsity: 87.9030%\n",
      "layer   2  Sparsity: 69.3364%\n",
      "layer   3  Sparsity: 68.2442%\n",
      "total_backward_count 856625 real_backward_count 141978  16.574%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.372205/  1.618371, val:  72.50%, val_best:  72.50%, tr:  97.04%, tr_best:  97.85%, epoch time: 55.98 seconds, 0.93 minutes\n",
      "layer   1  Sparsity: 87.9077%\n",
      "layer   2  Sparsity: 68.9328%\n",
      "layer   3  Sparsity: 68.5002%\n",
      "total_backward_count 861520 real_backward_count 142670  16.560%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.350727/  1.606437, val:  65.83%, val_best:  72.50%, tr:  97.14%, tr_best:  97.85%, epoch time: 54.33 seconds, 0.91 minutes\n",
      "layer   1  Sparsity: 88.0019%\n",
      "layer   2  Sparsity: 68.9541%\n",
      "layer   3  Sparsity: 68.5452%\n",
      "total_backward_count 866415 real_backward_count 143291  16.538%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.352750/  1.615937, val:  68.75%, val_best:  72.50%, tr:  96.73%, tr_best:  97.85%, epoch time: 54.15 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.8740%\n",
      "layer   2  Sparsity: 69.2126%\n",
      "layer   3  Sparsity: 68.1512%\n",
      "total_backward_count 871310 real_backward_count 143898  16.515%\n",
      "lif layer 1 self.abs_max_v: 10852.5\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.336847/  1.659432, val:  61.25%, val_best:  72.50%, tr:  97.34%, tr_best:  97.85%, epoch time: 54.27 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 88.0110%\n",
      "layer   2  Sparsity: 69.0842%\n",
      "layer   3  Sparsity: 68.6218%\n",
      "total_backward_count 876205 real_backward_count 144547  16.497%\n",
      "fc layer 3 self.abs_max_out: 724.0\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.335864/  1.621373, val:  63.33%, val_best:  72.50%, tr:  97.04%, tr_best:  97.85%, epoch time: 53.70 seconds, 0.89 minutes\n",
      "layer   1  Sparsity: 87.9763%\n",
      "layer   2  Sparsity: 69.3051%\n",
      "layer   3  Sparsity: 69.0282%\n",
      "total_backward_count 881100 real_backward_count 145188  16.478%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.336743/  1.631875, val:  58.33%, val_best:  72.50%, tr:  97.14%, tr_best:  97.85%, epoch time: 52.00 seconds, 0.87 minutes\n",
      "layer   1  Sparsity: 87.8448%\n",
      "layer   2  Sparsity: 69.2450%\n",
      "layer   3  Sparsity: 68.8934%\n",
      "total_backward_count 885995 real_backward_count 145814  16.458%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.326095/  1.615819, val:  58.33%, val_best:  72.50%, tr:  97.55%, tr_best:  97.85%, epoch time: 52.83 seconds, 0.88 minutes\n",
      "layer   1  Sparsity: 87.9008%\n",
      "layer   2  Sparsity: 69.2043%\n",
      "layer   3  Sparsity: 68.5165%\n",
      "total_backward_count 890890 real_backward_count 146453  16.439%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.343281/  1.615285, val:  61.67%, val_best:  72.50%, tr:  97.85%, tr_best:  97.85%, epoch time: 51.82 seconds, 0.86 minutes\n",
      "layer   1  Sparsity: 87.8899%\n",
      "layer   2  Sparsity: 69.2591%\n",
      "layer   3  Sparsity: 68.6536%\n",
      "total_backward_count 895785 real_backward_count 147075  16.419%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.339439/  1.659963, val:  46.25%, val_best:  72.50%, tr:  96.42%, tr_best:  97.85%, epoch time: 53.90 seconds, 0.90 minutes\n",
      "layer   1  Sparsity: 87.8910%\n",
      "layer   2  Sparsity: 69.1892%\n",
      "layer   3  Sparsity: 68.8984%\n",
      "total_backward_count 900680 real_backward_count 147708  16.400%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.339296/  1.610767, val:  62.50%, val_best:  72.50%, tr:  97.45%, tr_best:  97.85%, epoch time: 50.14 seconds, 0.84 minutes\n",
      "layer   1  Sparsity: 87.8574%\n",
      "layer   2  Sparsity: 69.2724%\n",
      "layer   3  Sparsity: 68.3069%\n",
      "total_backward_count 905575 real_backward_count 148318  16.378%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.339112/  1.621635, val:  62.08%, val_best:  72.50%, tr:  97.55%, tr_best:  97.85%, epoch time: 50.22 seconds, 0.84 minutes\n",
      "layer   1  Sparsity: 87.9283%\n",
      "layer   2  Sparsity: 69.1732%\n",
      "layer   3  Sparsity: 69.1029%\n",
      "total_backward_count 910470 real_backward_count 148946  16.359%\n",
      "lif layer 1 self.abs_max_v: 10921.5\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.344902/  1.619527, val:  64.58%, val_best:  72.50%, tr:  97.04%, tr_best:  97.85%, epoch time: 49.41 seconds, 0.82 minutes\n",
      "layer   1  Sparsity: 87.9702%\n",
      "layer   2  Sparsity: 69.0751%\n",
      "layer   3  Sparsity: 68.8479%\n",
      "total_backward_count 915365 real_backward_count 149609  16.344%\n",
      "lif layer 1 self.abs_max_v: 11335.5\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.376914/  1.639542, val:  62.50%, val_best:  72.50%, tr:  96.83%, tr_best:  97.85%, epoch time: 50.02 seconds, 0.83 minutes\n",
      "layer   1  Sparsity: 88.0054%\n",
      "layer   2  Sparsity: 69.1726%\n",
      "layer   3  Sparsity: 69.6555%\n",
      "total_backward_count 920260 real_backward_count 150299  16.332%\n",
      "lif layer 1 self.abs_max_v: 11737.0\n",
      "fc layer 1 self.abs_max_out: 7077.0\n",
      "lif layer 1 self.abs_max_v: 12325.5\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.374957/  1.631465, val:  64.17%, val_best:  72.50%, tr:  97.65%, tr_best:  97.85%, epoch time: 49.45 seconds, 0.82 minutes\n",
      "layer   1  Sparsity: 87.9196%\n",
      "layer   2  Sparsity: 69.1725%\n",
      "layer   3  Sparsity: 70.0655%\n",
      "total_backward_count 925155 real_backward_count 150945  16.316%\n",
      "fc layer 1 self.abs_max_out: 7088.0\n",
      "lif layer 1 self.abs_max_v: 12327.0\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.354658/  1.637938, val:  69.58%, val_best:  72.50%, tr:  96.94%, tr_best:  97.85%, epoch time: 48.68 seconds, 0.81 minutes\n",
      "layer   1  Sparsity: 87.9124%\n",
      "layer   2  Sparsity: 69.2221%\n",
      "layer   3  Sparsity: 70.0586%\n",
      "total_backward_count 930050 real_backward_count 151537  16.293%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.357336/  1.641298, val:  59.58%, val_best:  72.50%, tr:  97.14%, tr_best:  97.85%, epoch time: 48.70 seconds, 0.81 minutes\n",
      "layer   1  Sparsity: 87.9435%\n",
      "layer   2  Sparsity: 69.4756%\n",
      "layer   3  Sparsity: 69.7879%\n",
      "total_backward_count 934945 real_backward_count 152131  16.272%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.367529/  1.678948, val:  50.00%, val_best:  72.50%, tr:  97.65%, tr_best:  97.85%, epoch time: 47.96 seconds, 0.80 minutes\n",
      "layer   1  Sparsity: 87.9654%\n",
      "layer   2  Sparsity: 69.1838%\n",
      "layer   3  Sparsity: 69.7474%\n",
      "total_backward_count 939840 real_backward_count 152735  16.251%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.371282/  1.678443, val:  55.00%, val_best:  72.50%, tr:  97.85%, tr_best:  97.85%, epoch time: 48.11 seconds, 0.80 minutes\n",
      "layer   1  Sparsity: 87.9373%\n",
      "layer   2  Sparsity: 69.1034%\n",
      "layer   3  Sparsity: 69.6721%\n",
      "total_backward_count 944735 real_backward_count 153347  16.232%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.371680/  1.669682, val:  51.25%, val_best:  72.50%, tr:  97.85%, tr_best:  97.85%, epoch time: 48.75 seconds, 0.81 minutes\n",
      "layer   1  Sparsity: 87.8542%\n",
      "layer   2  Sparsity: 69.0442%\n",
      "layer   3  Sparsity: 69.9130%\n",
      "total_backward_count 949630 real_backward_count 153956  16.212%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.363188/  1.654204, val:  50.83%, val_best:  72.50%, tr:  98.16%, tr_best:  98.16%, epoch time: 48.25 seconds, 0.80 minutes\n",
      "layer   1  Sparsity: 87.8880%\n",
      "layer   2  Sparsity: 68.9140%\n",
      "layer   3  Sparsity: 69.2014%\n",
      "total_backward_count 954525 real_backward_count 154577  16.194%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.339315/  1.636370, val:  58.75%, val_best:  72.50%, tr:  97.34%, tr_best:  98.16%, epoch time: 48.11 seconds, 0.80 minutes\n",
      "layer   1  Sparsity: 87.9456%\n",
      "layer   2  Sparsity: 69.2770%\n",
      "layer   3  Sparsity: 69.0857%\n",
      "total_backward_count 959420 real_backward_count 155191  16.176%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.324383/  1.592974, val:  61.25%, val_best:  72.50%, tr:  97.14%, tr_best:  98.16%, epoch time: 48.47 seconds, 0.81 minutes\n",
      "layer   1  Sparsity: 87.8992%\n",
      "layer   2  Sparsity: 69.2149%\n",
      "layer   3  Sparsity: 69.3400%\n",
      "total_backward_count 964315 real_backward_count 155806  16.157%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.329743/  1.608851, val:  62.08%, val_best:  72.50%, tr:  97.24%, tr_best:  98.16%, epoch time: 48.51 seconds, 0.81 minutes\n",
      "layer   1  Sparsity: 87.8500%\n",
      "layer   2  Sparsity: 69.1256%\n",
      "layer   3  Sparsity: 69.3065%\n",
      "total_backward_count 969210 real_backward_count 156433  16.140%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.345839/  1.633027, val:  66.25%, val_best:  72.50%, tr:  97.65%, tr_best:  98.16%, epoch time: 49.19 seconds, 0.82 minutes\n",
      "layer   1  Sparsity: 87.8990%\n",
      "layer   2  Sparsity: 69.1677%\n",
      "layer   3  Sparsity: 68.9678%\n",
      "total_backward_count 974105 real_backward_count 157048  16.122%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.364872/  1.619343, val:  61.67%, val_best:  72.50%, tr:  97.45%, tr_best:  98.16%, epoch time: 48.68 seconds, 0.81 minutes\n",
      "layer   1  Sparsity: 87.9593%\n",
      "layer   2  Sparsity: 68.7914%\n",
      "layer   3  Sparsity: 69.6336%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57769f0dfbc2439a933bab9e8954c40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÉ‚ñá‚ñÑ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÉ‚ñá‚ñÑ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.97446</td></tr><tr><td>tr_epoch_loss</td><td>1.36487</td></tr><tr><td>val_acc_best</td><td>0.725</td></tr><tr><td>val_acc_now</td><td>0.61667</td></tr><tr><td>val_loss</td><td>1.61934</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-meadow-18542</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vnntdfnr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vnntdfnr</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251114_174530-vnntdfnr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### my_snn control board (Gesture) ########################\n",
    "decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "unique_name = 'main'\n",
    "run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "my_snn_system(  devices = \"1\",\n",
    "                single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "                unique_name = run_name,\n",
    "                my_seed = 42,\n",
    "                TIME = 5, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "                BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "                # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "                which_data = 'DVS_GESTURE_TONIC',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "                lif_layer_sg_width = 3.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "\n",
    "                synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "                synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "                # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                cfg = [200, 200], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                # cfg = ['M', 'M', 64], \n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',512],\n",
    "                # cfg = ['M',200],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = ['M',200,200],\n",
    "                # cfg = ['M','M',1024,512,256,128,64],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [],        \n",
    "                \n",
    "                net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "                # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                epoch_num = 200,\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "                BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                dvs_clipping = 5, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                dvs_duration = 12_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "                # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "                # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "                DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "                trace_on = False,   # True # False\n",
    "                OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "                exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                extra_train_dataset = -1, \n",
    "\n",
    "                num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "                chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                pin_memory = True, # True # False \n",
    "\n",
    "                UDA_on = False,  # DECREPATED # uda\n",
    "                alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                bias = False, # True # False \n",
    "\n",
    "                last_lif = False, # True # False \n",
    "\n",
    "                temporal_filter = 5, \n",
    "                initial_pooling = 1,\n",
    "\n",
    "                temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "                quantize_bit_list=[8,8,8],\n",
    "                scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "# 1w -11~-9\n",
    "# 1b -11~ -7\n",
    "# 2w -10~-8\n",
    "# 2b -10~-8\n",
    "# 3w -10\n",
    "# 3b -10\n",
    "                ) \n",
    "\n",
    "# num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# num_workers = batch_size / num_GPU\n",
    "# num_workers = batch_size / num_CPU\n",
    "\n",
    "# sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# average pooling  \n",
    "# Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# # Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# # wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'grid', # 'random', 'bayes', 'grid'\n",
    "#     'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         # \"devices\": {\"values\": [\"1\"]},\n",
    "#         \"single_step\": {\"values\": [True]},\n",
    "#         # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "#         # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "#         \"my_seed\": {\"values\": [42]},\n",
    "#         \"TIME\": {\"values\": [5, 10, 15]},\n",
    "#         \"BATCH\": {\"values\": [1]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "#         \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "#         \"data_path\": {\"values\": ['/data2']},\n",
    "#         \"rate_coding\": {\"values\": [False]},\n",
    "#         \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "#         \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_threshold\": {\"values\": [0.25]},\n",
    "#         \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "#         # \"lif_layer_sg_width\": {\"values\": [4.0]},\n",
    "#         \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "#         \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "#         \"synapse_conv_stride\": {\"values\": [1]},\n",
    "#         \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "#         \"synapse_trace_const1\": {\"values\": [1]},\n",
    "#         \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "#         \"pre_trained\": {\"values\": [False]},\n",
    "#         \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "#         \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "#         \"net_print\": {\"values\": [True]},\n",
    "\n",
    "#         \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "#         \"learning_rate\": {\"values\": [1/512]}, \n",
    "#         \"epoch_num\": {\"values\": [200]}, \n",
    "#         \"tdBN_on\": {\"values\": [False]},\n",
    "#         \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "#         \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"optimizer_what\": {\"values\": ['SGD']},\n",
    "#         \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "#         \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"dvs_clipping\": {\"values\": [5, 10, 15, 20, 25, 30]}, \n",
    "\n",
    "#         \"dvs_duration\": {\"values\": [12_000, 25_000, 50_000, 75_000, 100_000]}, \n",
    "\n",
    "#         \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "#         \"trace_on\": {\"values\": [False]},\n",
    "#         \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "#         \"merge_polarities\": {\"values\": [True]},\n",
    "#         \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "#         \"num_workers\": {\"values\": [2]},\n",
    "#         \"chaching_on\": {\"values\": [True]},\n",
    "#         \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "#         \"UDA_on\": {\"values\": [False]},\n",
    "#         \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "#         \"bias\": {\"values\": [False]},\n",
    "\n",
    "#         \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "#         \"temporal_filter\": {\"values\": [5]},\n",
    "#         \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "#         \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "#         \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "#         \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "#         \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "#         \"scale_exp_1w\": {\"values\": [-9]},\n",
    "#         # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "#         # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "#         # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "#         # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "#         # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "#         # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "#     my_snn_system(  \n",
    "#         devices  =  \"1\",\n",
    "#         single_step  =  wandb.config.single_step,\n",
    "#         unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "#         my_seed  =  wandb.config.my_seed,\n",
    "#         TIME  =  wandb.config.TIME,\n",
    "#         BATCH  =  wandb.config.BATCH,\n",
    "#         IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "#         which_data  =  wandb.config.which_data,\n",
    "#         data_path  =  wandb.config.data_path,\n",
    "#         rate_coding  =  wandb.config.rate_coding,\n",
    "#         lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "#         lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "#         lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "#         lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "#         lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "#         synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "#         synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "#         synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "#         synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "#         synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "#         pre_trained  =  wandb.config.pre_trained,\n",
    "#         convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "#         cfg  =  wandb.config.cfg,\n",
    "#         net_print  =  wandb.config.net_print,\n",
    "#         pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "#         learning_rate  =  wandb.config.learning_rate,\n",
    "#         epoch_num  =  wandb.config.epoch_num,\n",
    "#         tdBN_on  =  wandb.config.tdBN_on,\n",
    "#         BN_on  =  wandb.config.BN_on,\n",
    "#         surrogate  =  wandb.config.surrogate,\n",
    "#         BPTT_on  =  wandb.config.BPTT_on,\n",
    "#         optimizer_what  =  wandb.config.optimizer_what,\n",
    "#         scheduler_name  =  wandb.config.scheduler_name,\n",
    "#         ddp_on  =  wandb.config.ddp_on,\n",
    "#         dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "#         dvs_duration  =  wandb.config.dvs_duration,\n",
    "#         DFA_on  =  wandb.config.DFA_on,\n",
    "#         trace_on  =  wandb.config.trace_on,\n",
    "#         OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "#         exclude_class  =  wandb.config.exclude_class,\n",
    "#         merge_polarities  =  wandb.config.merge_polarities,\n",
    "#         denoise_on  =  wandb.config.denoise_on,\n",
    "#         extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "#         num_workers  =  wandb.config.num_workers,\n",
    "#         chaching_on  =  wandb.config.chaching_on,\n",
    "#         pin_memory  =  wandb.config.pin_memory,\n",
    "#         UDA_on  =  wandb.config.UDA_on,\n",
    "#         alpha_uda  =  wandb.config.alpha_uda,\n",
    "#         bias  =  wandb.config.bias,\n",
    "#         last_lif  =  wandb.config.last_lif,\n",
    "#         temporal_filter  =  wandb.config.temporal_filter,\n",
    "#         initial_pooling  =  wandb.config.initial_pooling,\n",
    "#         temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "#         quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "#         scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "#                         ) \n",
    "#     # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "#     # average pooling\n",
    "#     # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "#     # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# sweep_id = 'celzwsdu'\n",
    "# # sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
