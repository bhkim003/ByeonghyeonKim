{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    " \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                  FC_RESERVOIR=False):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        if self.FC_RESERVOIR == True:\n",
    "            self.reservoir = nn.Linear(in_features=self.in_channel*self.in_spike_size*self.in_spike_size, out_features=self.in_channel*self.in_spike_size*self.in_spike_size, bias=True)\n",
    "        else:\n",
    "            self.reservoir = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                            kernel_size=self.receptive_size, \n",
    "                                            stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.reservoir.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.reservoir.bias, 0)\n",
    "\n",
    "        # membrane potential 초기화\n",
    "        self.v = torch.full((self.in_channel, self.in_spike_size, self.in_spike_size), fill_value=self.v_init, requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # depthwise conv reservoir: pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            # fc conv reservoir: pre_spike[t] [batch_size, in_channel*in_spike_size*in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "                \n",
    "            recurrent_current = self.reservoir(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                 no_reservoir = False, FC_RESERVOIR=False):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.no_reservoir = no_reservoir\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight,\n",
    "                                       FC_RESERVOIR=FC_RESERVOIR)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.TIME_STEP == x.size(1), f\"Time step mismatch: {x.size(1)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        if (self.FC_RESERVOIR == True):\n",
    "            x = x.reshape(x.size(0), x.size(1), -1)\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            with torch.no_grad():\n",
    "                x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        \n",
    "        ## 11번째 클래스 배제 ########################################################################\n",
    "        exclude_class = 10\n",
    "        if dvs_duration > 0:\n",
    "            train_file_name = f'modules/dvs_gesture_class_index/train_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            test_file_name = f'modules/dvs_gesture_class_index/test_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            if (os.path.isfile(train_file_name) and os.path.isfile(test_file_name)):\n",
    "                print('\\ndvsgestrue 10 class indices exist. we want to exclude the 11th class\\n')\n",
    "                with open(train_file_name, 'rb') as f:\n",
    "                    train_indices = pickle.load(f)\n",
    "                with open(test_file_name, 'rb') as f:\n",
    "                    test_indices = pickle.load(f)\n",
    "            else:\n",
    "                print('\\ndvsgestrue 10 class indices doesn\\'t exist. we want to exclude the 11th class\\n')\n",
    "                train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "                test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "                with open(train_file_name, 'wb') as f:\n",
    "                    pickle.dump(train_indices, f)\n",
    "                with open(test_file_name, 'wb') as f:\n",
    "                    pickle.dump(test_indices, f)\n",
    "        else:\n",
    "            train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "            test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "        ################################################################################################\n",
    "\n",
    "        # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "        # mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "    # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "            # 처리 로직 작성\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # if rate_coding == True:\n",
    "        #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        # else:\n",
    "        #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        \n",
    "\n",
    "        ###########################################################################################################################        \n",
    "        if (which_data == 'n_tidigits'):\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif (which_data == 'heidelberg'):\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "        # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "        # print(labels)\n",
    "            \n",
    "        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "        ####################################################################################################################### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # if rate_coding == True:\n",
    "            #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            # else:\n",
    "            #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, \n",
    "                            no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: gqw22z3b\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/gqw22z3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zl0ai597 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.9686514349716002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.05870199172056384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 8.646612096823821\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: CIFAR10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_191103-zl0ai597</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/zl0ai597' target=\"_blank\">helpful-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/gqw22z3b' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/gqw22z3b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/gqw22z3b' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/gqw22z3b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/zl0ai597' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/zl0ai597</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "RESERVOIR_NET(\n",
      "  (reservoir): RESERVOIR(\n",
      "    (reservoir): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n"
     ]
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '5', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.batch_size\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    FC_RESERVOIR  =  wandb.config.FC_RESERVOIR\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'CIFAR10' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': f'{which_data_hyper} fc_reservoir',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"min\": 0.00001, \"max\": 0.1},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64, 128, 256]},\n",
    "        \"time_step\": {\"values\": [4,5,6,7,8]},\n",
    "        \"decay\": {\"min\": 0.25, \"max\": 1.0},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [20]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [32]},\n",
    "        \"dvs_duration\": {\"values\": [1000000]},\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [True, False]},\n",
    "        \"FC_RESERVOIR\": {\"values\": [True]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
