{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                  FC_RESERVOIR=False):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        if self.FC_RESERVOIR == True:\n",
    "            self.reservoir = nn.Linear(in_features=self.in_channel*self.in_spike_size*self.in_spike_size, out_features=self.in_channel*self.in_spike_size*self.in_spike_size, bias=True)\n",
    "        else:\n",
    "            self.reservoir = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                            kernel_size=self.receptive_size, \n",
    "                                            stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.reservoir.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.reservoir.bias, 0)\n",
    "\n",
    "        # membrane potential 초기화\n",
    "        self.v = torch.full((self.in_channel, self.in_spike_size, self.in_spike_size), fill_value=self.v_init, requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # depthwise conv reservoir: pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            # fc conv reservoir: pre_spike[t] [batch_size, in_channel*in_spike_size*in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "                \n",
    "            recurrent_current = self.reservoir(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                 no_reservoir = False, FC_RESERVOIR=False):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.no_reservoir = no_reservoir\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight,\n",
    "                                       FC_RESERVOIR=FC_RESERVOIR)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.TIME_STEP == x.size(1), f\"Time step mismatch: {x.size(1)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        if (self.FC_RESERVOIR == True):\n",
    "            x = x.reshape(x.size(0), x.size(1), -1)\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            with torch.no_grad():\n",
    "                x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        \n",
    "        ## 11번째 클래스 배제 ########################################################################\n",
    "        exclude_class = 10\n",
    "        if dvs_duration > 0:\n",
    "            train_file_name = f'modules/dvs_gesture_class_index/train_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            test_file_name = f'modules/dvs_gesture_class_index/test_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            if (os.path.isfile(train_file_name) and os.path.isfile(test_file_name)):\n",
    "                print('\\ndvsgestrue 10 class indices exist. we want to exclude the 11th class\\n')\n",
    "                with open(train_file_name, 'rb') as f:\n",
    "                    train_indices = pickle.load(f)\n",
    "                with open(test_file_name, 'rb') as f:\n",
    "                    test_indices = pickle.load(f)\n",
    "            else:\n",
    "                print('\\ndvsgestrue 10 class indices doesn\\'t exist. we want to exclude the 11th class\\n')\n",
    "                train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "                test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "                with open(train_file_name, 'wb') as f:\n",
    "                    pickle.dump(train_indices, f)\n",
    "                with open(test_file_name, 'wb') as f:\n",
    "                    pickle.dump(test_indices, f)\n",
    "        else:\n",
    "            train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "            test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "        ################################################################################################\n",
    "\n",
    "        # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "        # mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "    # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "            # 처리 로직 작성\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # if rate_coding == True:\n",
    "        #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        # else:\n",
    "        #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        \n",
    "\n",
    "        ###########################################################################################################################        \n",
    "        if (which_data == 'n_tidigits'):\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif (which_data == 'heidelberg'):\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "        # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "        # print(labels)\n",
    "            \n",
    "        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "        ####################################################################################################################### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # if rate_coding == True:\n",
    "            #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            # else:\n",
    "            #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, \n",
    "                            no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: d5ebsqp9\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/d5ebsqp9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xf1gh4k9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 2.238710091146486\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: CIFAR10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_220745-xf1gh4k9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/xf1gh4k9' target=\"_blank\">distinctive-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/d5ebsqp9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/d5ebsqp9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/d5ebsqp9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/d5ebsqp9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/xf1gh4k9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/xf1gh4k9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "RESERVOIR_NET(\n",
      "  (reservoir): RESERVOIR(\n",
      "    (reservoir): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "Train Accuracy: 22.56%\n",
      "Test loss: 2.062, Val Accuracy: 29.11%\n",
      "Epoch 2\n",
      "Train Accuracy: 30.44%\n",
      "Test loss: 1.977, Val Accuracy: 32.46%\n",
      "Epoch 3\n",
      "Train Accuracy: 32.68%\n",
      "Test loss: 1.936, Val Accuracy: 33.74%\n",
      "Epoch 4\n",
      "Train Accuracy: 33.90%\n",
      "Test loss: 1.903, Val Accuracy: 35.11%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3738e8b9dd4f22a75467d456989701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='21.686 MB of 21.686 MB uploaded (21.184 MB deduped)\\r'), FloatProgress(value=1.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 97.4%"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>iter_accuracy</td><td>▂▁▂▄▅▅▆▅▆▆▅▆▆▇▅▆▇▆▇▇▇▇▆▇▆█▇▇▇███▆▇██▇▆██</td></tr><tr><td>tr_accuracy</td><td>▁▆▇█</td></tr><tr><td>val_accuracy</td><td>▁▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>iter_accuracy</td><td>37.20238</td></tr><tr><td>tr_accuracy</td><td>33.9</td></tr><tr><td>val_accuracy</td><td>35.11</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">distinctive-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/xf1gh4k9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/xf1gh4k9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a><br/>Synced 7 W&B file(s), 0 media file(s), 36 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240726_220745-xf1gh4k9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: enpk69iw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 1.9658853241952303\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: CIFAR10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_221129-enpk69iw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/enpk69iw' target=\"_blank\">volcanic-sweep-2</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/d5ebsqp9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/d5ebsqp9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/d5ebsqp9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/d5ebsqp9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/enpk69iw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/enpk69iw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "RESERVOIR_NET(\n",
      "  (classifier): Linear(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "Train Accuracy: 27.09%\n",
      "Test loss: 1.894, Val Accuracy: 33.41%\n",
      "Epoch 2\n",
      "Train Accuracy: 31.38%\n",
      "Test loss: 1.872, Val Accuracy: 34.21%\n",
      "Epoch 3\n",
      "Train Accuracy: 32.58%\n",
      "Test loss: 1.871, Val Accuracy: 34.68%\n",
      "Epoch 4\n",
      "Train Accuracy: 32.81%\n",
      "Test loss: 1.847, Val Accuracy: 35.49%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a689103527e404a9c2aa8e571c22b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.501 MB of 0.501 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>iter_accuracy</td><td>▁▄▅▆▆▅▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▆▆███▇▆▆▆▇█▇▇▇▆████</td></tr><tr><td>tr_accuracy</td><td>▁▆██</td></tr><tr><td>val_accuracy</td><td>▁▄▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>iter_accuracy</td><td>35.71429</td></tr><tr><td>tr_accuracy</td><td>32.806</td></tr><tr><td>val_accuracy</td><td>35.49</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">volcanic-sweep-2</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/enpk69iw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/enpk69iw</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240726_221129-enpk69iw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hl8yhre7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 3.773358707554443\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: CIFAR10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '5', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.batch_size\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    FC_RESERVOIR  =  wandb.config.FC_RESERVOIR\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'CIFAR10' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': f'{which_data_hyper} fc_reservoir',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"values\": [0.001]},\n",
    "        \"batch_size\": {\"values\": [512]},\n",
    "        \"time_step\": {\"values\": [4,5,6,7,8]},\n",
    "        \"decay\": {\"values\": [0.7]},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [4]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [32]},\n",
    "        \"dvs_duration\": {\"values\": [1000000]},\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [True, False]},\n",
    "        \"FC_RESERVOIR\": {\"values\": [True]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
