{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40788/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 27lqvb5e\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rciyrys8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 24043\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_201956-rciyrys8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rciyrys8' target=\"_blank\">charmed-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rciyrys8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rciyrys8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251117_202005_331', 'my_seed': 24043, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2.5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 270.0\n",
      "lif layer 1 self.abs_max_v: 270.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 241.0\n",
      "lif layer 2 self.abs_max_v: 241.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 53.0\n",
      "fc layer 1 self.abs_max_out: 304.0\n",
      "lif layer 1 self.abs_max_v: 399.0\n",
      "lif layer 2 self.abs_max_v: 244.0\n",
      "fc layer 3 self.abs_max_out: 89.0\n",
      "fc layer 2 self.abs_max_out: 272.0\n",
      "lif layer 2 self.abs_max_v: 319.0\n",
      "fc layer 3 self.abs_max_out: 97.0\n",
      "lif layer 1 self.abs_max_v: 432.0\n",
      "lif layer 2 self.abs_max_v: 362.0\n",
      "fc layer 3 self.abs_max_out: 110.0\n",
      "lif layer 1 self.abs_max_v: 439.0\n",
      "fc layer 2 self.abs_max_out: 304.0\n",
      "lif layer 1 self.abs_max_v: 473.5\n",
      "lif layer 2 self.abs_max_v: 401.5\n",
      "fc layer 1 self.abs_max_out: 305.0\n",
      "fc layer 1 self.abs_max_out: 348.0\n",
      "fc layer 2 self.abs_max_out: 313.0\n",
      "lif layer 2 self.abs_max_v: 481.0\n",
      "fc layer 1 self.abs_max_out: 385.0\n",
      "lif layer 2 self.abs_max_v: 491.5\n",
      "fc layer 3 self.abs_max_out: 115.0\n",
      "fc layer 1 self.abs_max_out: 404.0\n",
      "fc layer 2 self.abs_max_out: 320.0\n",
      "fc layer 3 self.abs_max_out: 128.0\n",
      "fc layer 1 self.abs_max_out: 566.0\n",
      "lif layer 1 self.abs_max_v: 569.0\n",
      "fc layer 2 self.abs_max_out: 336.0\n",
      "lif layer 2 self.abs_max_v: 531.0\n",
      "lif layer 2 self.abs_max_v: 547.0\n",
      "fc layer 1 self.abs_max_out: 675.0\n",
      "lif layer 1 self.abs_max_v: 675.0\n",
      "fc layer 2 self.abs_max_out: 414.0\n",
      "lif layer 2 self.abs_max_v: 571.5\n",
      "lif layer 2 self.abs_max_v: 579.0\n",
      "fc layer 2 self.abs_max_out: 428.0\n",
      "lif layer 2 self.abs_max_v: 608.5\n",
      "fc layer 2 self.abs_max_out: 433.0\n",
      "fc layer 2 self.abs_max_out: 467.0\n",
      "fc layer 3 self.abs_max_out: 139.0\n",
      "lif layer 2 self.abs_max_v: 623.0\n",
      "lif layer 2 self.abs_max_v: 708.0\n",
      "fc layer 1 self.abs_max_out: 692.0\n",
      "lif layer 1 self.abs_max_v: 692.0\n",
      "lif layer 2 self.abs_max_v: 758.0\n",
      "lif layer 2 self.abs_max_v: 771.5\n",
      "lif layer 1 self.abs_max_v: 740.0\n",
      "fc layer 3 self.abs_max_out: 244.0\n",
      "fc layer 2 self.abs_max_out: 470.0\n",
      "fc layer 2 self.abs_max_out: 482.0\n",
      "lif layer 2 self.abs_max_v: 844.0\n",
      "fc layer 2 self.abs_max_out: 490.0\n",
      "fc layer 2 self.abs_max_out: 524.0\n",
      "fc layer 2 self.abs_max_out: 578.0\n",
      "lif layer 1 self.abs_max_v: 744.0\n",
      "lif layer 2 self.abs_max_v: 849.5\n",
      "lif layer 1 self.abs_max_v: 773.5\n",
      "lif layer 2 self.abs_max_v: 851.0\n",
      "fc layer 1 self.abs_max_out: 699.0\n",
      "lif layer 2 self.abs_max_v: 879.0\n",
      "lif layer 2 self.abs_max_v: 913.5\n",
      "fc layer 2 self.abs_max_out: 585.0\n",
      "lif layer 2 self.abs_max_v: 924.5\n",
      "fc layer 2 self.abs_max_out: 658.0\n",
      "fc layer 1 self.abs_max_out: 834.0\n",
      "lif layer 1 self.abs_max_v: 834.0\n",
      "lif layer 1 self.abs_max_v: 866.5\n",
      "fc layer 3 self.abs_max_out: 247.0\n",
      "lif layer 1 self.abs_max_v: 929.5\n",
      "lif layer 2 self.abs_max_v: 927.5\n",
      "fc layer 3 self.abs_max_out: 256.0\n",
      "lif layer 2 self.abs_max_v: 958.0\n",
      "fc layer 1 self.abs_max_out: 836.0\n",
      "lif layer 1 self.abs_max_v: 978.0\n",
      "fc layer 1 self.abs_max_out: 891.0\n",
      "fc layer 1 self.abs_max_out: 937.0\n",
      "fc layer 1 self.abs_max_out: 1024.0\n",
      "lif layer 1 self.abs_max_v: 1024.0\n",
      "lif layer 1 self.abs_max_v: 1199.5\n",
      "lif layer 2 self.abs_max_v: 1008.0\n",
      "lif layer 2 self.abs_max_v: 1075.5\n",
      "lif layer 2 self.abs_max_v: 1099.0\n",
      "fc layer 2 self.abs_max_out: 672.0\n",
      "fc layer 2 self.abs_max_out: 713.0\n",
      "fc layer 2 self.abs_max_out: 733.0\n",
      "fc layer 2 self.abs_max_out: 772.0\n",
      "lif layer 2 self.abs_max_v: 1124.0\n",
      "fc layer 3 self.abs_max_out: 283.0\n",
      "lif layer 1 self.abs_max_v: 1243.5\n",
      "lif layer 2 self.abs_max_v: 1184.0\n",
      "lif layer 2 self.abs_max_v: 1189.0\n",
      "lif layer 2 self.abs_max_v: 1245.5\n",
      "lif layer 1 self.abs_max_v: 1327.0\n",
      "lif layer 1 self.abs_max_v: 1464.5\n",
      "lif layer 1 self.abs_max_v: 1581.5\n",
      "fc layer 2 self.abs_max_out: 830.0\n",
      "fc layer 2 self.abs_max_out: 879.0\n",
      "lif layer 2 self.abs_max_v: 1268.5\n",
      "lif layer 2 self.abs_max_v: 1298.0\n",
      "lif layer 2 self.abs_max_v: 1367.0\n",
      "lif layer 2 self.abs_max_v: 1375.5\n",
      "lif layer 2 self.abs_max_v: 1431.0\n",
      "lif layer 2 self.abs_max_v: 1482.5\n",
      "fc layer 1 self.abs_max_out: 1100.0\n",
      "fc layer 3 self.abs_max_out: 284.0\n",
      "fc layer 3 self.abs_max_out: 288.0\n",
      "lif layer 1 self.abs_max_v: 1658.0\n",
      "lif layer 1 self.abs_max_v: 1816.0\n",
      "fc layer 3 self.abs_max_out: 291.0\n",
      "fc layer 2 self.abs_max_out: 915.0\n",
      "fc layer 1 self.abs_max_out: 1116.0\n",
      "fc layer 1 self.abs_max_out: 1152.0\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "fc layer 1 self.abs_max_out: 1155.0\n",
      "fc layer 2 self.abs_max_out: 926.0\n",
      "fc layer 2 self.abs_max_out: 932.0\n",
      "fc layer 2 self.abs_max_out: 995.0\n",
      "fc layer 3 self.abs_max_out: 327.0\n",
      "fc layer 1 self.abs_max_out: 1189.0\n",
      "fc layer 3 self.abs_max_out: 335.0\n",
      "fc layer 3 self.abs_max_out: 405.0\n",
      "lif layer 1 self.abs_max_v: 1975.0\n",
      "lif layer 1 self.abs_max_v: 2013.5\n",
      "fc layer 1 self.abs_max_out: 1255.0\n",
      "fc layer 1 self.abs_max_out: 1276.0\n",
      "fc layer 1 self.abs_max_out: 1358.0\n",
      "fc layer 1 self.abs_max_out: 1442.0\n",
      "fc layer 2 self.abs_max_out: 999.0\n",
      "lif layer 1 self.abs_max_v: 2120.0\n",
      "fc layer 2 self.abs_max_out: 1013.0\n",
      "lif layer 2 self.abs_max_v: 1625.5\n",
      "lif layer 2 self.abs_max_v: 1626.0\n",
      "fc layer 2 self.abs_max_out: 1027.0\n",
      "lif layer 2 self.abs_max_v: 1635.0\n",
      "lif layer 2 self.abs_max_v: 1701.5\n",
      "lif layer 2 self.abs_max_v: 1703.0\n",
      "fc layer 2 self.abs_max_out: 1056.0\n",
      "lif layer 2 self.abs_max_v: 1904.5\n",
      "fc layer 1 self.abs_max_out: 1473.0\n",
      "fc layer 2 self.abs_max_out: 1089.0\n",
      "lif layer 1 self.abs_max_v: 2150.0\n",
      "fc layer 2 self.abs_max_out: 1169.0\n",
      "fc layer 1 self.abs_max_out: 1651.0\n",
      "fc layer 1 self.abs_max_out: 1659.0\n",
      "lif layer 1 self.abs_max_v: 2392.5\n",
      "fc layer 1 self.abs_max_out: 1833.0\n",
      "lif layer 1 self.abs_max_v: 2522.5\n",
      "fc layer 1 self.abs_max_out: 1956.0\n",
      "lif layer 1 self.abs_max_v: 2526.5\n",
      "lif layer 2 self.abs_max_v: 1928.5\n",
      "fc layer 2 self.abs_max_out: 1188.0\n",
      "fc layer 2 self.abs_max_out: 1193.0\n",
      "fc layer 2 self.abs_max_out: 1208.0\n",
      "lif layer 2 self.abs_max_v: 1930.5\n",
      "fc layer 2 self.abs_max_out: 1209.0\n",
      "fc layer 2 self.abs_max_out: 1216.0\n",
      "fc layer 2 self.abs_max_out: 1247.0\n",
      "fc layer 2 self.abs_max_out: 1282.0\n",
      "fc layer 3 self.abs_max_out: 427.0\n",
      "fc layer 2 self.abs_max_out: 1299.0\n",
      "lif layer 2 self.abs_max_v: 1989.0\n",
      "lif layer 2 self.abs_max_v: 2017.0\n",
      "lif layer 1 self.abs_max_v: 2781.0\n",
      "lif layer 1 self.abs_max_v: 2972.5\n",
      "lif layer 1 self.abs_max_v: 2974.5\n",
      "lif layer 1 self.abs_max_v: 3010.5\n",
      "lif layer 1 self.abs_max_v: 3093.5\n",
      "lif layer 1 self.abs_max_v: 3171.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.749556/  1.937608, val:  32.08%, val_best:  32.08%, tr:  98.06%, tr_best:  98.06%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   2  Sparsity: 72.3973%\n",
      "layer   3  Sparsity: 69.9171%\n",
      "total_backward_count 9790 real_backward_count 2093  21.379%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1358.0\n",
      "fc layer 3 self.abs_max_out: 442.0\n",
      "fc layer 3 self.abs_max_out: 452.0\n",
      "fc layer 3 self.abs_max_out: 464.0\n",
      "fc layer 3 self.abs_max_out: 476.0\n",
      "lif layer 2 self.abs_max_v: 2088.5\n",
      "fc layer 2 self.abs_max_out: 1372.0\n",
      "lif layer 2 self.abs_max_v: 2099.5\n",
      "fc layer 2 self.abs_max_out: 1426.0\n",
      "fc layer 2 self.abs_max_out: 1427.0\n",
      "fc layer 2 self.abs_max_out: 1504.0\n",
      "lif layer 2 self.abs_max_v: 2117.5\n",
      "lif layer 2 self.abs_max_v: 2134.0\n",
      "lif layer 2 self.abs_max_v: 2161.0\n",
      "lif layer 1 self.abs_max_v: 3203.5\n",
      "lif layer 1 self.abs_max_v: 3257.0\n",
      "lif layer 1 self.abs_max_v: 3310.5\n",
      "lif layer 2 self.abs_max_v: 2212.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.648312/  1.937904, val:  37.08%, val_best:  37.08%, tr:  99.08%, tr_best:  99.08%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   2  Sparsity: 74.9448%\n",
      "layer   3  Sparsity: 68.2736%\n",
      "total_backward_count 19580 real_backward_count 3715  18.973%\n",
      "fc layer 2 self.abs_max_out: 1512.0\n",
      "fc layer 2 self.abs_max_out: 1530.0\n",
      "lif layer 2 self.abs_max_v: 2301.0\n",
      "lif layer 2 self.abs_max_v: 2424.0\n",
      "fc layer 1 self.abs_max_out: 2040.0\n",
      "fc layer 2 self.abs_max_out: 1591.0\n",
      "lif layer 1 self.abs_max_v: 3341.5\n",
      "lif layer 1 self.abs_max_v: 3343.0\n",
      "lif layer 1 self.abs_max_v: 3353.5\n",
      "lif layer 2 self.abs_max_v: 2429.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.613863/  1.888507, val:  41.25%, val_best:  41.25%, tr:  98.88%, tr_best:  99.08%, epoch time: 74.10 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0754%\n",
      "layer   2  Sparsity: 74.6191%\n",
      "layer   3  Sparsity: 67.5834%\n",
      "total_backward_count 29370 real_backward_count 5183  17.647%\n",
      "fc layer 2 self.abs_max_out: 1615.0\n",
      "fc layer 2 self.abs_max_out: 1725.0\n",
      "fc layer 3 self.abs_max_out: 486.0\n",
      "lif layer 1 self.abs_max_v: 3456.0\n",
      "fc layer 1 self.abs_max_out: 2147.0\n",
      "lif layer 1 self.abs_max_v: 3532.5\n",
      "fc layer 3 self.abs_max_out: 488.0\n",
      "fc layer 3 self.abs_max_out: 516.0\n",
      "fc layer 3 self.abs_max_out: 524.0\n",
      "fc layer 3 self.abs_max_out: 526.0\n",
      "fc layer 3 self.abs_max_out: 532.0\n",
      "fc layer 3 self.abs_max_out: 536.0\n",
      "lif layer 1 self.abs_max_v: 3560.5\n",
      "lif layer 2 self.abs_max_v: 2442.0\n",
      "lif layer 2 self.abs_max_v: 2483.0\n",
      "lif layer 2 self.abs_max_v: 2572.5\n",
      "fc layer 1 self.abs_max_out: 2253.0\n",
      "fc layer 2 self.abs_max_out: 1731.0\n",
      "lif layer 1 self.abs_max_v: 3796.5\n",
      "lif layer 1 self.abs_max_v: 3836.0\n",
      "fc layer 1 self.abs_max_out: 2258.0\n",
      "lif layer 2 self.abs_max_v: 2608.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.621993/  1.905134, val:  41.25%, val_best:  41.25%, tr:  99.18%, tr_best:  99.18%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0227%\n",
      "layer   2  Sparsity: 74.5585%\n",
      "layer   3  Sparsity: 65.7414%\n",
      "total_backward_count 39160 real_backward_count 6599  16.851%\n",
      "fc layer 1 self.abs_max_out: 2294.0\n",
      "fc layer 1 self.abs_max_out: 2371.0\n",
      "fc layer 1 self.abs_max_out: 2472.0\n",
      "lif layer 1 self.abs_max_v: 4107.0\n",
      "lif layer 1 self.abs_max_v: 4236.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.602784/  1.861723, val:  41.67%, val_best:  41.67%, tr:  99.28%, tr_best:  99.28%, epoch time: 73.48 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0967%\n",
      "layer   2  Sparsity: 75.4751%\n",
      "layer   3  Sparsity: 64.8297%\n",
      "total_backward_count 48950 real_backward_count 7947  16.235%\n",
      "fc layer 2 self.abs_max_out: 1754.0\n",
      "fc layer 2 self.abs_max_out: 1757.0\n",
      "fc layer 2 self.abs_max_out: 1760.0\n",
      "fc layer 3 self.abs_max_out: 543.0\n",
      "fc layer 3 self.abs_max_out: 566.0\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "fc layer 3 self.abs_max_out: 596.0\n",
      "fc layer 1 self.abs_max_out: 2662.0\n",
      "fc layer 1 self.abs_max_out: 2769.0\n",
      "lif layer 1 self.abs_max_v: 4582.5\n",
      "lif layer 1 self.abs_max_v: 4708.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.559109/  1.859181, val:  42.08%, val_best:  42.08%, tr:  99.49%, tr_best:  99.49%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0934%\n",
      "layer   2  Sparsity: 74.7587%\n",
      "layer   3  Sparsity: 65.1604%\n",
      "total_backward_count 58740 real_backward_count 9272  15.785%\n",
      "fc layer 2 self.abs_max_out: 1771.0\n",
      "fc layer 2 self.abs_max_out: 1787.0\n",
      "fc layer 2 self.abs_max_out: 1796.0\n",
      "fc layer 1 self.abs_max_out: 2819.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.558629/  1.810784, val:  52.92%, val_best:  52.92%, tr:  99.69%, tr_best:  99.69%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0739%\n",
      "layer   2  Sparsity: 74.9519%\n",
      "layer   3  Sparsity: 65.3874%\n",
      "total_backward_count 68530 real_backward_count 10631  15.513%\n",
      "fc layer 1 self.abs_max_out: 2823.0\n",
      "lif layer 1 self.abs_max_v: 5063.5\n",
      "fc layer 1 self.abs_max_out: 2883.0\n",
      "fc layer 1 self.abs_max_out: 3088.0\n",
      "lif layer 1 self.abs_max_v: 5113.5\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.556340/  1.795634, val:  60.42%, val_best:  60.42%, tr:  99.28%, tr_best:  99.69%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 75.2946%\n",
      "layer   3  Sparsity: 66.3116%\n",
      "total_backward_count 78320 real_backward_count 11918  15.217%\n",
      "fc layer 1 self.abs_max_out: 3231.0\n",
      "lif layer 1 self.abs_max_v: 5273.0\n",
      "lif layer 1 self.abs_max_v: 5346.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.553785/  1.789938, val:  60.00%, val_best:  60.42%, tr:  99.59%, tr_best:  99.69%, epoch time: 74.02 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   2  Sparsity: 75.4157%\n",
      "layer   3  Sparsity: 66.4462%\n",
      "total_backward_count 88110 real_backward_count 13199  14.980%\n",
      "fc layer 3 self.abs_max_out: 616.0\n",
      "fc layer 2 self.abs_max_out: 1799.0\n",
      "fc layer 2 self.abs_max_out: 1818.0\n",
      "fc layer 2 self.abs_max_out: 1871.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.506670/  1.771757, val:  53.33%, val_best:  60.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 74.27 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0903%\n",
      "layer   2  Sparsity: 75.5996%\n",
      "layer   3  Sparsity: 65.8104%\n",
      "total_backward_count 97900 real_backward_count 14347  14.655%\n",
      "fc layer 3 self.abs_max_out: 643.0\n",
      "fc layer 1 self.abs_max_out: 3370.0\n",
      "lif layer 1 self.abs_max_v: 5554.5\n",
      "lif layer 1 self.abs_max_v: 5658.5\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.520489/  1.791260, val:  49.17%, val_best:  60.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   2  Sparsity: 75.5760%\n",
      "layer   3  Sparsity: 66.3615%\n",
      "total_backward_count 107690 real_backward_count 15571  14.459%\n",
      "lif layer 1 self.abs_max_v: 5849.5\n",
      "fc layer 1 self.abs_max_out: 3383.0\n",
      "fc layer 1 self.abs_max_out: 3582.0\n",
      "lif layer 1 self.abs_max_v: 5867.5\n",
      "lif layer 1 self.abs_max_v: 5899.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.511192/  1.839843, val:  46.25%, val_best:  60.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0962%\n",
      "layer   2  Sparsity: 75.0615%\n",
      "layer   3  Sparsity: 66.6091%\n",
      "total_backward_count 117480 real_backward_count 16814  14.312%\n",
      "fc layer 1 self.abs_max_out: 3773.0\n",
      "lif layer 1 self.abs_max_v: 6193.5\n",
      "lif layer 1 self.abs_max_v: 6236.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.525586/  1.771374, val:  56.25%, val_best:  60.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 73.94 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0993%\n",
      "layer   2  Sparsity: 74.5094%\n",
      "layer   3  Sparsity: 67.0163%\n",
      "total_backward_count 127270 real_backward_count 17977  14.125%\n",
      "fc layer 2 self.abs_max_out: 1887.0\n",
      "fc layer 2 self.abs_max_out: 1968.0\n",
      "fc layer 2 self.abs_max_out: 1972.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.499551/  1.762272, val:  54.58%, val_best:  60.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.92 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0607%\n",
      "layer   2  Sparsity: 74.4582%\n",
      "layer   3  Sparsity: 67.3583%\n",
      "total_backward_count 137060 real_backward_count 19154  13.975%\n",
      "lif layer 2 self.abs_max_v: 2774.5\n",
      "lif layer 2 self.abs_max_v: 2939.5\n",
      "lif layer 1 self.abs_max_v: 6273.5\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.474833/  1.737497, val:  59.17%, val_best:  60.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1260%\n",
      "layer   2  Sparsity: 74.1156%\n",
      "layer   3  Sparsity: 67.1752%\n",
      "total_backward_count 146850 real_backward_count 20370  13.871%\n",
      "fc layer 3 self.abs_max_out: 653.0\n",
      "fc layer 3 self.abs_max_out: 655.0\n",
      "fc layer 3 self.abs_max_out: 679.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.465305/  1.729017, val:  63.75%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 73.9317%\n",
      "layer   3  Sparsity: 66.9543%\n",
      "total_backward_count 156640 real_backward_count 21537  13.749%\n",
      "fc layer 2 self.abs_max_out: 2004.0\n",
      "fc layer 2 self.abs_max_out: 2028.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.472858/  1.752695, val:  53.75%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0929%\n",
      "layer   2  Sparsity: 73.6977%\n",
      "layer   3  Sparsity: 67.0458%\n",
      "total_backward_count 166430 real_backward_count 22675  13.624%\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.467521/  1.725372, val:  52.08%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.47 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0441%\n",
      "layer   2  Sparsity: 73.2127%\n",
      "layer   3  Sparsity: 66.5826%\n",
      "total_backward_count 176220 real_backward_count 23805  13.509%\n",
      "fc layer 2 self.abs_max_out: 2065.0\n",
      "fc layer 2 self.abs_max_out: 2095.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.430623/  1.728913, val:  59.58%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.64 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0493%\n",
      "layer   2  Sparsity: 73.1708%\n",
      "layer   3  Sparsity: 66.9063%\n",
      "total_backward_count 186010 real_backward_count 24875  13.373%\n",
      "fc layer 2 self.abs_max_out: 2110.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.419191/  1.690526, val:  56.67%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0559%\n",
      "layer   2  Sparsity: 73.1478%\n",
      "layer   3  Sparsity: 67.0500%\n",
      "total_backward_count 195800 real_backward_count 25982  13.270%\n",
      "fc layer 1 self.abs_max_out: 3832.0\n",
      "lif layer 1 self.abs_max_v: 6298.0\n",
      "lif layer 1 self.abs_max_v: 6421.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.438879/  1.677092, val:  47.50%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.19 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 73.0242%\n",
      "layer   3  Sparsity: 67.4551%\n",
      "total_backward_count 205590 real_backward_count 27103  13.183%\n",
      "fc layer 2 self.abs_max_out: 2153.0\n",
      "fc layer 2 self.abs_max_out: 2170.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.430674/  1.709254, val:  55.00%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 72.8713%\n",
      "layer   3  Sparsity: 68.2655%\n",
      "total_backward_count 215380 real_backward_count 28174  13.081%\n",
      "fc layer 2 self.abs_max_out: 2188.0\n",
      "lif layer 2 self.abs_max_v: 2952.5\n",
      "fc layer 2 self.abs_max_out: 2213.0\n",
      "lif layer 1 self.abs_max_v: 6429.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.429249/  1.701418, val:  58.75%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.93 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1046%\n",
      "layer   2  Sparsity: 72.5984%\n",
      "layer   3  Sparsity: 68.4398%\n",
      "total_backward_count 225170 real_backward_count 29227  12.980%\n",
      "fc layer 2 self.abs_max_out: 2241.0\n",
      "fc layer 1 self.abs_max_out: 3955.0\n",
      "lif layer 1 self.abs_max_v: 6509.0\n",
      "lif layer 1 self.abs_max_v: 6782.5\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.417478/  1.663334, val:  64.17%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1019%\n",
      "layer   2  Sparsity: 71.8342%\n",
      "layer   3  Sparsity: 68.3289%\n",
      "total_backward_count 234960 real_backward_count 30293  12.893%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.408889/  1.732687, val:  42.50%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   2  Sparsity: 72.1363%\n",
      "layer   3  Sparsity: 68.4800%\n",
      "total_backward_count 244750 real_backward_count 31313  12.794%\n",
      "lif layer 2 self.abs_max_v: 3120.5\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.398344/  1.653157, val:  59.17%, val_best:  64.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.92 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 72.9128%\n",
      "layer   3  Sparsity: 69.0600%\n",
      "total_backward_count 254540 real_backward_count 32370  12.717%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.388707/  1.633646, val:  64.17%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0655%\n",
      "layer   2  Sparsity: 72.4797%\n",
      "layer   3  Sparsity: 68.8776%\n",
      "total_backward_count 264330 real_backward_count 33374  12.626%\n",
      "lif layer 2 self.abs_max_v: 3231.5\n",
      "fc layer 1 self.abs_max_out: 4055.0\n",
      "lif layer 1 self.abs_max_v: 7033.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.424263/  1.665722, val:  71.67%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.83 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   2  Sparsity: 72.4268%\n",
      "layer   3  Sparsity: 68.8082%\n",
      "total_backward_count 274120 real_backward_count 34354  12.532%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.392965/  1.635182, val:  72.92%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   2  Sparsity: 72.1800%\n",
      "layer   3  Sparsity: 69.0445%\n",
      "total_backward_count 283910 real_backward_count 35314  12.438%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.395802/  1.632793, val:  67.92%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   2  Sparsity: 71.5520%\n",
      "layer   3  Sparsity: 68.4122%\n",
      "total_backward_count 293700 real_backward_count 36313  12.364%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.370777/  1.652490, val:  54.17%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.05 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0293%\n",
      "layer   2  Sparsity: 71.4722%\n",
      "layer   3  Sparsity: 68.9767%\n",
      "total_backward_count 303490 real_backward_count 37245  12.272%\n",
      "lif layer 2 self.abs_max_v: 3300.5\n",
      "lif layer 2 self.abs_max_v: 3324.5\n",
      "fc layer 2 self.abs_max_out: 2282.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.379655/  1.606787, val:  69.58%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0879%\n",
      "layer   2  Sparsity: 70.9653%\n",
      "layer   3  Sparsity: 68.8343%\n",
      "total_backward_count 313280 real_backward_count 38209  12.196%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.371371/  1.627445, val:  62.50%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.37 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   2  Sparsity: 71.4460%\n",
      "layer   3  Sparsity: 69.1991%\n",
      "total_backward_count 323070 real_backward_count 39142  12.116%\n",
      "lif layer 2 self.abs_max_v: 3496.0\n",
      "lif layer 2 self.abs_max_v: 3554.5\n",
      "fc layer 1 self.abs_max_out: 4142.0\n",
      "lif layer 1 self.abs_max_v: 7072.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.358205/  1.619812, val:  70.00%, val_best:  72.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0238%\n",
      "layer   2  Sparsity: 71.1055%\n",
      "layer   3  Sparsity: 68.7646%\n",
      "total_backward_count 332860 real_backward_count 40024  12.024%\n",
      "fc layer 2 self.abs_max_out: 2300.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.351903/  1.560294, val:  73.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.80 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   2  Sparsity: 71.1712%\n",
      "layer   3  Sparsity: 68.6644%\n",
      "total_backward_count 342650 real_backward_count 40925  11.944%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.338603/  1.570140, val:  72.50%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1059%\n",
      "layer   2  Sparsity: 71.5226%\n",
      "layer   3  Sparsity: 69.6317%\n",
      "total_backward_count 352440 real_backward_count 41805  11.862%\n",
      "fc layer 1 self.abs_max_out: 4164.0\n",
      "lif layer 1 self.abs_max_v: 7107.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.330537/  1.595723, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1007%\n",
      "layer   2  Sparsity: 71.5439%\n",
      "layer   3  Sparsity: 69.6834%\n",
      "total_backward_count 362230 real_backward_count 42642  11.772%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.324152/  1.537424, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0617%\n",
      "layer   2  Sparsity: 71.1342%\n",
      "layer   3  Sparsity: 68.5194%\n",
      "total_backward_count 372020 real_backward_count 43537  11.703%\n",
      "fc layer 2 self.abs_max_out: 2313.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.312644/  1.559678, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0611%\n",
      "layer   2  Sparsity: 71.1044%\n",
      "layer   3  Sparsity: 69.0426%\n",
      "total_backward_count 381810 real_backward_count 44420  11.634%\n",
      "fc layer 2 self.abs_max_out: 2328.0\n",
      "fc layer 2 self.abs_max_out: 2338.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.318250/  1.534989, val:  73.75%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0557%\n",
      "layer   2  Sparsity: 71.0488%\n",
      "layer   3  Sparsity: 68.3578%\n",
      "total_backward_count 391600 real_backward_count 45227  11.549%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.301769/  1.568819, val:  72.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 70.7915%\n",
      "layer   3  Sparsity: 68.9863%\n",
      "total_backward_count 401390 real_backward_count 46079  11.480%\n",
      "fc layer 1 self.abs_max_out: 4190.0\n",
      "lif layer 1 self.abs_max_v: 7227.5\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.310848/  1.589413, val:  71.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.50 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0625%\n",
      "layer   2  Sparsity: 70.7766%\n",
      "layer   3  Sparsity: 68.6555%\n",
      "total_backward_count 411180 real_backward_count 46924  11.412%\n",
      "lif layer 2 self.abs_max_v: 3601.0\n",
      "fc layer 3 self.abs_max_out: 709.0\n",
      "fc layer 1 self.abs_max_out: 4333.0\n",
      "lif layer 1 self.abs_max_v: 7475.5\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.305167/  1.571158, val:  67.92%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.39 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0571%\n",
      "layer   2  Sparsity: 70.5744%\n",
      "layer   3  Sparsity: 69.2617%\n",
      "total_backward_count 420970 real_backward_count 47765  11.346%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.328870/  1.554122, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   2  Sparsity: 70.5852%\n",
      "layer   3  Sparsity: 69.1820%\n",
      "total_backward_count 430760 real_backward_count 48566  11.274%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.305402/  1.554007, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.53 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0399%\n",
      "layer   2  Sparsity: 70.3351%\n",
      "layer   3  Sparsity: 68.5991%\n",
      "total_backward_count 440550 real_backward_count 49306  11.192%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.304207/  1.563892, val:  71.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 70.4356%\n",
      "layer   3  Sparsity: 69.0065%\n",
      "total_backward_count 450340 real_backward_count 50086  11.122%\n",
      "fc layer 1 self.abs_max_out: 4400.0\n",
      "lif layer 1 self.abs_max_v: 7516.5\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.288078/  1.547237, val:  63.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0605%\n",
      "layer   2  Sparsity: 69.7486%\n",
      "layer   3  Sparsity: 67.7962%\n",
      "total_backward_count 460130 real_backward_count 50835  11.048%\n",
      "fc layer 3 self.abs_max_out: 723.0\n",
      "fc layer 3 self.abs_max_out: 733.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.289639/  1.559862, val:  69.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0868%\n",
      "layer   2  Sparsity: 70.2885%\n",
      "layer   3  Sparsity: 68.2892%\n",
      "total_backward_count 469920 real_backward_count 51625  10.986%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.303128/  1.505061, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 70.3040%\n",
      "layer   3  Sparsity: 68.6222%\n",
      "total_backward_count 479710 real_backward_count 52403  10.924%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.284331/  1.512406, val:  83.75%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   2  Sparsity: 70.2655%\n",
      "layer   3  Sparsity: 67.8747%\n",
      "total_backward_count 489500 real_backward_count 53142  10.856%\n",
      "lif layer 1 self.abs_max_v: 7581.0\n",
      "fc layer 1 self.abs_max_out: 4453.0\n",
      "lif layer 1 self.abs_max_v: 7730.5\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.286281/  1.555188, val:  73.75%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0469%\n",
      "layer   2  Sparsity: 70.3222%\n",
      "layer   3  Sparsity: 68.3925%\n",
      "total_backward_count 499290 real_backward_count 53881  10.792%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.275304/  1.522697, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.94 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0904%\n",
      "layer   2  Sparsity: 70.1146%\n",
      "layer   3  Sparsity: 67.8953%\n",
      "total_backward_count 509080 real_backward_count 54639  10.733%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.280653/  1.527028, val:  64.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   2  Sparsity: 70.1322%\n",
      "layer   3  Sparsity: 67.7457%\n",
      "total_backward_count 518870 real_backward_count 55346  10.667%\n",
      "fc layer 1 self.abs_max_out: 4488.0\n",
      "fc layer 1 self.abs_max_out: 4499.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.271591/  1.511314, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   2  Sparsity: 69.9352%\n",
      "layer   3  Sparsity: 68.5104%\n",
      "total_backward_count 528660 real_backward_count 56116  10.615%\n",
      "fc layer 2 self.abs_max_out: 2376.0\n",
      "fc layer 1 self.abs_max_out: 4606.0\n",
      "lif layer 1 self.abs_max_v: 7970.5\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.264504/  1.563397, val:  63.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0500%\n",
      "layer   2  Sparsity: 69.5789%\n",
      "layer   3  Sparsity: 68.1334%\n",
      "total_backward_count 538450 real_backward_count 56814  10.551%\n",
      "fc layer 3 self.abs_max_out: 767.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.243038/  1.477766, val:  79.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   2  Sparsity: 69.5677%\n",
      "layer   3  Sparsity: 68.1364%\n",
      "total_backward_count 548240 real_backward_count 57520  10.492%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.239433/  1.471275, val:  75.42%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.72 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0828%\n",
      "layer   2  Sparsity: 69.8109%\n",
      "layer   3  Sparsity: 68.0428%\n",
      "total_backward_count 558030 real_backward_count 58236  10.436%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.222466/  1.448712, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0769%\n",
      "layer   2  Sparsity: 70.0612%\n",
      "layer   3  Sparsity: 68.6450%\n",
      "total_backward_count 567820 real_backward_count 58911  10.375%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.211100/  1.468231, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.72 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   2  Sparsity: 69.9308%\n",
      "layer   3  Sparsity: 69.1196%\n",
      "total_backward_count 577610 real_backward_count 59581  10.315%\n",
      "lif layer 1 self.abs_max_v: 7973.5\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.203824/  1.435471, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.63 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0519%\n",
      "layer   2  Sparsity: 70.1799%\n",
      "layer   3  Sparsity: 68.8244%\n",
      "total_backward_count 587400 real_backward_count 60190  10.247%\n",
      "lif layer 2 self.abs_max_v: 3602.0\n",
      "lif layer 1 self.abs_max_v: 8046.5\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.202502/  1.479942, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.14 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0915%\n",
      "layer   2  Sparsity: 69.7780%\n",
      "layer   3  Sparsity: 68.3230%\n",
      "total_backward_count 597190 real_backward_count 60822  10.185%\n",
      "lif layer 2 self.abs_max_v: 3608.0\n",
      "lif layer 2 self.abs_max_v: 3681.0\n",
      "fc layer 1 self.abs_max_out: 4697.0\n",
      "lif layer 1 self.abs_max_v: 8118.5\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.210008/  1.467999, val:  74.17%, val_best:  85.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.30 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0903%\n",
      "layer   2  Sparsity: 69.3538%\n",
      "layer   3  Sparsity: 68.2986%\n",
      "total_backward_count 606980 real_backward_count 61486  10.130%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.212735/  1.472475, val:  72.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   2  Sparsity: 69.2663%\n",
      "layer   3  Sparsity: 68.9507%\n",
      "total_backward_count 616770 real_backward_count 62139  10.075%\n",
      "fc layer 2 self.abs_max_out: 2395.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.205532/  1.472076, val:  63.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0769%\n",
      "layer   2  Sparsity: 69.3487%\n",
      "layer   3  Sparsity: 68.7905%\n",
      "total_backward_count 626560 real_backward_count 62750  10.015%\n",
      "fc layer 2 self.abs_max_out: 2407.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.196501/  1.426882, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   2  Sparsity: 69.1236%\n",
      "layer   3  Sparsity: 68.9414%\n",
      "total_backward_count 636350 real_backward_count 63395   9.962%\n",
      "fc layer 1 self.abs_max_out: 4726.0\n",
      "fc layer 1 self.abs_max_out: 4830.0\n",
      "lif layer 1 self.abs_max_v: 8457.5\n",
      "lif layer 1 self.abs_max_v: 8566.5\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.193276/  1.436304, val:  75.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.01 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 69.2383%\n",
      "layer   3  Sparsity: 68.4360%\n",
      "total_backward_count 646140 real_backward_count 64031   9.910%\n",
      "fc layer 2 self.abs_max_out: 2451.0\n",
      "fc layer 2 self.abs_max_out: 2466.0\n",
      "fc layer 2 self.abs_max_out: 2482.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.194671/  1.427971, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0966%\n",
      "layer   2  Sparsity: 69.1945%\n",
      "layer   3  Sparsity: 68.5984%\n",
      "total_backward_count 655930 real_backward_count 64658   9.857%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.196757/  1.443051, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   2  Sparsity: 69.2698%\n",
      "layer   3  Sparsity: 69.5352%\n",
      "total_backward_count 665720 real_backward_count 65285   9.807%\n",
      "fc layer 2 self.abs_max_out: 2490.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.200341/  1.446297, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0657%\n",
      "layer   2  Sparsity: 69.1435%\n",
      "layer   3  Sparsity: 69.9092%\n",
      "total_backward_count 675510 real_backward_count 65869   9.751%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.187652/  1.465168, val:  76.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.48 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1013%\n",
      "layer   2  Sparsity: 69.1111%\n",
      "layer   3  Sparsity: 69.3406%\n",
      "total_backward_count 685300 real_backward_count 66445   9.696%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.201609/  1.446890, val:  73.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0539%\n",
      "layer   2  Sparsity: 69.0817%\n",
      "layer   3  Sparsity: 69.3598%\n",
      "total_backward_count 695090 real_backward_count 67056   9.647%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.201527/  1.454706, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0579%\n",
      "layer   2  Sparsity: 68.8616%\n",
      "layer   3  Sparsity: 69.2499%\n",
      "total_backward_count 704880 real_backward_count 67664   9.599%\n",
      "lif layer 2 self.abs_max_v: 3738.5\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.201979/  1.441528, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1150%\n",
      "layer   2  Sparsity: 68.6233%\n",
      "layer   3  Sparsity: 69.0939%\n",
      "total_backward_count 714670 real_backward_count 68206   9.544%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.191174/  1.428358, val:  83.75%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   2  Sparsity: 68.6870%\n",
      "layer   3  Sparsity: 68.7751%\n",
      "total_backward_count 724460 real_backward_count 68777   9.494%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.171011/  1.401427, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   2  Sparsity: 68.7222%\n",
      "layer   3  Sparsity: 68.3756%\n",
      "total_backward_count 734250 real_backward_count 69335   9.443%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.164128/  1.390404, val:  87.08%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   2  Sparsity: 68.8503%\n",
      "layer   3  Sparsity: 68.3003%\n",
      "total_backward_count 744040 real_backward_count 69895   9.394%\n",
      "lif layer 2 self.abs_max_v: 3815.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.155157/  1.386375, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1316%\n",
      "layer   2  Sparsity: 69.3805%\n",
      "layer   3  Sparsity: 68.4371%\n",
      "total_backward_count 753830 real_backward_count 70409   9.340%\n",
      "lif layer 2 self.abs_max_v: 3833.5\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.153409/  1.381889, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.47 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   2  Sparsity: 69.4153%\n",
      "layer   3  Sparsity: 68.8168%\n",
      "total_backward_count 763620 real_backward_count 70956   9.292%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.148781/  1.362474, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.09 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   2  Sparsity: 68.8962%\n",
      "layer   3  Sparsity: 68.3549%\n",
      "total_backward_count 773410 real_backward_count 71486   9.243%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.142819/  1.389471, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   2  Sparsity: 69.0005%\n",
      "layer   3  Sparsity: 68.3280%\n",
      "total_backward_count 783200 real_backward_count 71992   9.192%\n",
      "lif layer 2 self.abs_max_v: 3869.5\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.142158/  1.393070, val:  76.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.51 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   2  Sparsity: 68.9072%\n",
      "layer   3  Sparsity: 68.0354%\n",
      "total_backward_count 792990 real_backward_count 72526   9.146%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.141738/  1.374919, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 68.9999%\n",
      "layer   3  Sparsity: 68.1374%\n",
      "total_backward_count 802780 real_backward_count 73075   9.103%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.129427/  1.406581, val:  70.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0963%\n",
      "layer   2  Sparsity: 69.0919%\n",
      "layer   3  Sparsity: 68.6659%\n",
      "total_backward_count 812570 real_backward_count 73651   9.064%\n",
      "fc layer 2 self.abs_max_out: 2491.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.124920/  1.432748, val:  76.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0569%\n",
      "layer   2  Sparsity: 68.9845%\n",
      "layer   3  Sparsity: 69.0340%\n",
      "total_backward_count 822360 real_backward_count 74169   9.019%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.146462/  1.404234, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.38 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0538%\n",
      "layer   2  Sparsity: 68.8576%\n",
      "layer   3  Sparsity: 69.2020%\n",
      "total_backward_count 832150 real_backward_count 74687   8.975%\n",
      "fc layer 2 self.abs_max_out: 2513.0\n",
      "fc layer 2 self.abs_max_out: 2518.0\n",
      "lif layer 1 self.abs_max_v: 8569.5\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.132263/  1.351752, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0991%\n",
      "layer   2  Sparsity: 68.8418%\n",
      "layer   3  Sparsity: 68.5858%\n",
      "total_backward_count 841940 real_backward_count 75201   8.932%\n",
      "fc layer 2 self.abs_max_out: 2560.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.122356/  1.349209, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.91 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   2  Sparsity: 69.1861%\n",
      "layer   3  Sparsity: 68.6715%\n",
      "total_backward_count 851730 real_backward_count 75710   8.889%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.113391/  1.361713, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0820%\n",
      "layer   2  Sparsity: 68.7022%\n",
      "layer   3  Sparsity: 68.8327%\n",
      "total_backward_count 861520 real_backward_count 76210   8.846%\n",
      "fc layer 1 self.abs_max_out: 4854.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.113937/  1.366378, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.24 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   2  Sparsity: 68.5080%\n",
      "layer   3  Sparsity: 68.8107%\n",
      "total_backward_count 871310 real_backward_count 76723   8.805%\n",
      "fc layer 1 self.abs_max_out: 4905.0\n",
      "lif layer 1 self.abs_max_v: 8632.5\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.102150/  1.366852, val:  79.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1008%\n",
      "layer   2  Sparsity: 68.4157%\n",
      "layer   3  Sparsity: 68.7588%\n",
      "total_backward_count 881100 real_backward_count 77239   8.766%\n",
      "fc layer 1 self.abs_max_out: 4981.0\n",
      "fc layer 1 self.abs_max_out: 4996.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.101580/  1.356071, val:  77.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   2  Sparsity: 68.2566%\n",
      "layer   3  Sparsity: 68.4432%\n",
      "total_backward_count 890890 real_backward_count 77746   8.727%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.087927/  1.342636, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.80 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   2  Sparsity: 68.2676%\n",
      "layer   3  Sparsity: 69.3428%\n",
      "total_backward_count 900680 real_backward_count 78230   8.686%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.096189/  1.370007, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.84 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   2  Sparsity: 68.3811%\n",
      "layer   3  Sparsity: 69.2908%\n",
      "total_backward_count 910470 real_backward_count 78700   8.644%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.113211/  1.366282, val:  80.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1071%\n",
      "layer   2  Sparsity: 68.4598%\n",
      "layer   3  Sparsity: 69.1908%\n",
      "total_backward_count 920260 real_backward_count 79182   8.604%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.117712/  1.367856, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   2  Sparsity: 68.5084%\n",
      "layer   3  Sparsity: 68.6322%\n",
      "total_backward_count 930050 real_backward_count 79692   8.569%\n",
      "fc layer 1 self.abs_max_out: 5008.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.102818/  1.374746, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   2  Sparsity: 68.5749%\n",
      "layer   3  Sparsity: 68.6346%\n",
      "total_backward_count 939840 real_backward_count 80176   8.531%\n",
      "lif layer 1 self.abs_max_v: 8650.5\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.075181/  1.343552, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 68.6611%\n",
      "layer   3  Sparsity: 68.7128%\n",
      "total_backward_count 949630 real_backward_count 80574   8.485%\n",
      "fc layer 1 self.abs_max_out: 5025.0\n",
      "fc layer 1 self.abs_max_out: 5087.0\n",
      "fc layer 3 self.abs_max_out: 786.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.099929/  1.343687, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0865%\n",
      "layer   2  Sparsity: 68.5792%\n",
      "layer   3  Sparsity: 68.6573%\n",
      "total_backward_count 959420 real_backward_count 81059   8.449%\n",
      "fc layer 1 self.abs_max_out: 5107.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.074252/  1.348161, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0464%\n",
      "layer   2  Sparsity: 68.5997%\n",
      "layer   3  Sparsity: 69.3476%\n",
      "total_backward_count 969210 real_backward_count 81477   8.407%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.095168/  1.355059, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.00 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1041%\n",
      "layer   2  Sparsity: 68.7408%\n",
      "layer   3  Sparsity: 69.5417%\n",
      "total_backward_count 979000 real_backward_count 81873   8.363%\n",
      "fc layer 1 self.abs_max_out: 5108.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.084020/  1.361179, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0276%\n",
      "layer   2  Sparsity: 68.6064%\n",
      "layer   3  Sparsity: 69.0619%\n",
      "total_backward_count 988790 real_backward_count 82333   8.327%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.072191/  1.323121, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.57 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0800%\n",
      "layer   2  Sparsity: 68.5726%\n",
      "layer   3  Sparsity: 69.4547%\n",
      "total_backward_count 998580 real_backward_count 82723   8.284%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.080890/  1.364638, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0687%\n",
      "layer   2  Sparsity: 68.5518%\n",
      "layer   3  Sparsity: 69.6864%\n",
      "total_backward_count 1008370 real_backward_count 83135   8.244%\n",
      "lif layer 1 self.abs_max_v: 8909.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.076758/  1.328012, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 68.4543%\n",
      "layer   3  Sparsity: 69.7388%\n",
      "total_backward_count 1018160 real_backward_count 83583   8.209%\n",
      "fc layer 3 self.abs_max_out: 788.0\n",
      "fc layer 3 self.abs_max_out: 849.0\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.078984/  1.313663, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 68.6384%\n",
      "layer   3  Sparsity: 69.2797%\n",
      "total_backward_count 1027950 real_backward_count 84047   8.176%\n",
      "lif layer 2 self.abs_max_v: 3999.0\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.064271/  1.320714, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.83 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0779%\n",
      "layer   2  Sparsity: 68.5868%\n",
      "layer   3  Sparsity: 69.3203%\n",
      "total_backward_count 1037740 real_backward_count 84482   8.141%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.065588/  1.322934, val:  85.00%, val_best:  90.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0811%\n",
      "layer   2  Sparsity: 68.5624%\n",
      "layer   3  Sparsity: 69.1067%\n",
      "total_backward_count 1047530 real_backward_count 84919   8.107%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.073705/  1.375207, val:  81.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.53 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   2  Sparsity: 68.6554%\n",
      "layer   3  Sparsity: 68.4317%\n",
      "total_backward_count 1057320 real_backward_count 85351   8.072%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.069796/  1.313586, val:  88.33%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   2  Sparsity: 68.6120%\n",
      "layer   3  Sparsity: 68.0568%\n",
      "total_backward_count 1067110 real_backward_count 85811   8.041%\n",
      "fc layer 1 self.abs_max_out: 5128.0\n",
      "lif layer 1 self.abs_max_v: 8922.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.061920/  1.315997, val:  81.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 68.5231%\n",
      "layer   3  Sparsity: 67.6242%\n",
      "total_backward_count 1076900 real_backward_count 86202   8.005%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.037547/  1.302029, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.00 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0723%\n",
      "layer   2  Sparsity: 68.3252%\n",
      "layer   3  Sparsity: 67.6790%\n",
      "total_backward_count 1086690 real_backward_count 86592   7.968%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.062349/  1.298733, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.19 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0732%\n",
      "layer   2  Sparsity: 68.4171%\n",
      "layer   3  Sparsity: 67.9815%\n",
      "total_backward_count 1096480 real_backward_count 87029   7.937%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.045163/  1.289308, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.56 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0485%\n",
      "layer   2  Sparsity: 68.7368%\n",
      "layer   3  Sparsity: 68.0039%\n",
      "total_backward_count 1106270 real_backward_count 87405   7.901%\n",
      "fc layer 3 self.abs_max_out: 853.0\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.032723/  1.284251, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.88 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   2  Sparsity: 68.4811%\n",
      "layer   3  Sparsity: 68.3979%\n",
      "total_backward_count 1116060 real_backward_count 87815   7.868%\n",
      "fc layer 3 self.abs_max_out: 861.0\n",
      "fc layer 1 self.abs_max_out: 5233.0\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.031457/  1.285895, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.08 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   2  Sparsity: 68.1671%\n",
      "layer   3  Sparsity: 68.1166%\n",
      "total_backward_count 1125850 real_backward_count 88191   7.833%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.026419/  1.289018, val:  82.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.80 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0998%\n",
      "layer   2  Sparsity: 68.2464%\n",
      "layer   3  Sparsity: 68.2052%\n",
      "total_backward_count 1135640 real_backward_count 88569   7.799%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.014258/  1.300646, val:  82.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.59 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.1107%\n",
      "layer   2  Sparsity: 68.2931%\n",
      "layer   3  Sparsity: 68.7520%\n",
      "total_backward_count 1145430 real_backward_count 88947   7.765%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.026303/  1.269748, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0607%\n",
      "layer   2  Sparsity: 68.2212%\n",
      "layer   3  Sparsity: 69.0760%\n",
      "total_backward_count 1155220 real_backward_count 89314   7.731%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.010122/  1.284012, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0588%\n",
      "layer   2  Sparsity: 68.5104%\n",
      "layer   3  Sparsity: 68.6899%\n",
      "total_backward_count 1165010 real_backward_count 89663   7.696%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.000619/  1.266817, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0752%\n",
      "layer   2  Sparsity: 68.3202%\n",
      "layer   3  Sparsity: 67.9138%\n",
      "total_backward_count 1174800 real_backward_count 90015   7.662%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.013602/  1.272112, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0318%\n",
      "layer   2  Sparsity: 68.2238%\n",
      "layer   3  Sparsity: 68.0008%\n",
      "total_backward_count 1184590 real_backward_count 90407   7.632%\n",
      "fc layer 1 self.abs_max_out: 5309.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.006941/  1.245763, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0774%\n",
      "layer   2  Sparsity: 68.3163%\n",
      "layer   3  Sparsity: 68.1145%\n",
      "total_backward_count 1194380 real_backward_count 90731   7.596%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.002463/  1.277234, val:  82.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0835%\n",
      "layer   2  Sparsity: 68.1216%\n",
      "layer   3  Sparsity: 68.6009%\n",
      "total_backward_count 1204170 real_backward_count 91108   7.566%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.007209/  1.291215, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.53 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0821%\n",
      "layer   2  Sparsity: 68.1201%\n",
      "layer   3  Sparsity: 68.1917%\n",
      "total_backward_count 1213960 real_backward_count 91484   7.536%\n",
      "fc layer 1 self.abs_max_out: 5364.0\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  0.998813/  1.246222, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0682%\n",
      "layer   2  Sparsity: 68.4099%\n",
      "layer   3  Sparsity: 68.0948%\n",
      "total_backward_count 1223750 real_backward_count 91843   7.505%\n",
      "fc layer 2 self.abs_max_out: 2638.0\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  0.988857/  1.251184, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0551%\n",
      "layer   2  Sparsity: 68.2396%\n",
      "layer   3  Sparsity: 68.3653%\n",
      "total_backward_count 1233540 real_backward_count 92196   7.474%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  0.962676/  1.216421, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   2  Sparsity: 68.1981%\n",
      "layer   3  Sparsity: 68.3094%\n",
      "total_backward_count 1243330 real_backward_count 92547   7.443%\n",
      "fc layer 2 self.abs_max_out: 2655.0\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  0.951799/  1.253336, val:  82.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.93 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   2  Sparsity: 68.4600%\n",
      "layer   3  Sparsity: 68.1179%\n",
      "total_backward_count 1253120 real_backward_count 92911   7.414%\n",
      "fc layer 1 self.abs_max_out: 5388.0\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  0.967406/  1.237287, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   2  Sparsity: 68.4951%\n",
      "layer   3  Sparsity: 68.1205%\n",
      "total_backward_count 1262910 real_backward_count 93308   7.388%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  0.975913/  1.245183, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0570%\n",
      "layer   2  Sparsity: 68.7048%\n",
      "layer   3  Sparsity: 67.7996%\n",
      "total_backward_count 1272700 real_backward_count 93662   7.359%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  0.970507/  1.235906, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1064%\n",
      "layer   2  Sparsity: 68.6853%\n",
      "layer   3  Sparsity: 68.3976%\n",
      "total_backward_count 1282490 real_backward_count 94001   7.330%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  0.973110/  1.246809, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.29 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1047%\n",
      "layer   2  Sparsity: 68.2679%\n",
      "layer   3  Sparsity: 68.2245%\n",
      "total_backward_count 1292280 real_backward_count 94338   7.300%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  0.976592/  1.233554, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1083%\n",
      "layer   2  Sparsity: 68.0625%\n",
      "layer   3  Sparsity: 68.2954%\n",
      "total_backward_count 1302070 real_backward_count 94698   7.273%\n",
      "fc layer 1 self.abs_max_out: 5428.0\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  0.954858/  1.228011, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.00 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   2  Sparsity: 68.0651%\n",
      "layer   3  Sparsity: 68.1689%\n",
      "total_backward_count 1311860 real_backward_count 95030   7.244%\n",
      "fc layer 1 self.abs_max_out: 5439.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  0.948036/  1.219118, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.89 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   2  Sparsity: 67.9130%\n",
      "layer   3  Sparsity: 67.5579%\n",
      "total_backward_count 1321650 real_backward_count 95363   7.215%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  0.944728/  1.259929, val:  79.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.17 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0659%\n",
      "layer   2  Sparsity: 68.0336%\n",
      "layer   3  Sparsity: 67.6665%\n",
      "total_backward_count 1331440 real_backward_count 95706   7.188%\n",
      "fc layer 3 self.abs_max_out: 866.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  0.925265/  1.183939, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   2  Sparsity: 68.1792%\n",
      "layer   3  Sparsity: 67.8541%\n",
      "total_backward_count 1341230 real_backward_count 96034   7.160%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  0.917590/  1.201085, val:  84.17%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0571%\n",
      "layer   2  Sparsity: 68.1114%\n",
      "layer   3  Sparsity: 68.0663%\n",
      "total_backward_count 1351020 real_backward_count 96351   7.132%\n",
      "fc layer 3 self.abs_max_out: 878.0\n",
      "fc layer 1 self.abs_max_out: 5462.0\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  0.919667/  1.217936, val:  79.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   2  Sparsity: 67.9882%\n",
      "layer   3  Sparsity: 68.5958%\n",
      "total_backward_count 1360810 real_backward_count 96652   7.103%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  0.929863/  1.209988, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.10 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   2  Sparsity: 68.0739%\n",
      "layer   3  Sparsity: 68.5508%\n",
      "total_backward_count 1370600 real_backward_count 96977   7.076%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  0.941472/  1.218296, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0689%\n",
      "layer   2  Sparsity: 67.8266%\n",
      "layer   3  Sparsity: 68.5461%\n",
      "total_backward_count 1380390 real_backward_count 97314   7.050%\n",
      "fc layer 1 self.abs_max_out: 5547.0\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  0.942343/  1.210965, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0591%\n",
      "layer   2  Sparsity: 67.8458%\n",
      "layer   3  Sparsity: 68.5749%\n",
      "total_backward_count 1390180 real_backward_count 97652   7.024%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  0.935144/  1.197651, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   2  Sparsity: 67.9336%\n",
      "layer   3  Sparsity: 68.5167%\n",
      "total_backward_count 1399970 real_backward_count 97992   7.000%\n",
      "fc layer 3 self.abs_max_out: 887.0\n",
      "fc layer 2 self.abs_max_out: 2685.0\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  0.931772/  1.171436, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.91 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   2  Sparsity: 67.9676%\n",
      "layer   3  Sparsity: 68.1523%\n",
      "total_backward_count 1409760 real_backward_count 98305   6.973%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  0.932331/  1.200985, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.67 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 67.9699%\n",
      "layer   3  Sparsity: 67.9685%\n",
      "total_backward_count 1419550 real_backward_count 98628   6.948%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  0.930169/  1.211073, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.97 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1273%\n",
      "layer   2  Sparsity: 68.2939%\n",
      "layer   3  Sparsity: 68.1376%\n",
      "total_backward_count 1429340 real_backward_count 98955   6.923%\n",
      "fc layer 1 self.abs_max_out: 5554.0\n",
      "lif layer 1 self.abs_max_v: 9068.5\n",
      "fc layer 3 self.abs_max_out: 895.0\n",
      "fc layer 3 self.abs_max_out: 906.0\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  0.918070/  1.215528, val:  84.17%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.27 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0566%\n",
      "layer   2  Sparsity: 68.3589%\n",
      "layer   3  Sparsity: 68.2783%\n",
      "total_backward_count 1439130 real_backward_count 99258   6.897%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  0.928202/  1.221831, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.52 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   2  Sparsity: 68.1338%\n",
      "layer   3  Sparsity: 68.0793%\n",
      "total_backward_count 1448920 real_backward_count 99558   6.871%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  0.924826/  1.191028, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1146%\n",
      "layer   2  Sparsity: 68.0980%\n",
      "layer   3  Sparsity: 67.8403%\n",
      "total_backward_count 1458710 real_backward_count 99855   6.845%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  0.916414/  1.229794, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.18 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0524%\n",
      "layer   2  Sparsity: 67.9547%\n",
      "layer   3  Sparsity: 68.2211%\n",
      "total_backward_count 1468500 real_backward_count 100150   6.820%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  0.938461/  1.207404, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0811%\n",
      "layer   2  Sparsity: 67.6938%\n",
      "layer   3  Sparsity: 68.2773%\n",
      "total_backward_count 1478290 real_backward_count 100440   6.794%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  0.940180/  1.209758, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   2  Sparsity: 67.6025%\n",
      "layer   3  Sparsity: 67.6155%\n",
      "total_backward_count 1488080 real_backward_count 100726   6.769%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  0.934931/  1.223378, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.12 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0939%\n",
      "layer   2  Sparsity: 67.7972%\n",
      "layer   3  Sparsity: 67.8267%\n",
      "total_backward_count 1497870 real_backward_count 101079   6.748%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  0.919818/  1.196798, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.37 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1228%\n",
      "layer   2  Sparsity: 67.8508%\n",
      "layer   3  Sparsity: 68.3726%\n",
      "total_backward_count 1507660 real_backward_count 101403   6.726%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  0.923230/  1.173407, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.52 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   2  Sparsity: 67.8254%\n",
      "layer   3  Sparsity: 68.4678%\n",
      "total_backward_count 1517450 real_backward_count 101694   6.702%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  0.907480/  1.181840, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.95 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1094%\n",
      "layer   2  Sparsity: 67.8484%\n",
      "layer   3  Sparsity: 68.3192%\n",
      "total_backward_count 1527240 real_backward_count 101950   6.675%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  0.913475/  1.176977, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.09 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   2  Sparsity: 67.6771%\n",
      "layer   3  Sparsity: 68.1448%\n",
      "total_backward_count 1537030 real_backward_count 102264   6.653%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  0.889336/  1.202898, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.39 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1205%\n",
      "layer   2  Sparsity: 67.7305%\n",
      "layer   3  Sparsity: 68.2338%\n",
      "total_backward_count 1546820 real_backward_count 102550   6.630%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  0.909078/  1.241770, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1492%\n",
      "layer   2  Sparsity: 67.8762%\n",
      "layer   3  Sparsity: 68.7906%\n",
      "total_backward_count 1556610 real_backward_count 102808   6.605%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  0.912269/  1.168867, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.40 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0555%\n",
      "layer   2  Sparsity: 67.6212%\n",
      "layer   3  Sparsity: 68.6041%\n",
      "total_backward_count 1566400 real_backward_count 103089   6.581%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  0.901264/  1.157143, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0992%\n",
      "layer   2  Sparsity: 67.5148%\n",
      "layer   3  Sparsity: 68.4018%\n",
      "total_backward_count 1576190 real_backward_count 103374   6.558%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  0.895573/  1.186762, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.71 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   2  Sparsity: 67.6743%\n",
      "layer   3  Sparsity: 69.0909%\n",
      "total_backward_count 1585980 real_backward_count 103621   6.534%\n",
      "fc layer 2 self.abs_max_out: 2703.0\n",
      "lif layer 1 self.abs_max_v: 9113.0\n",
      "lif layer 1 self.abs_max_v: 9293.5\n",
      "fc layer 1 self.abs_max_out: 5601.0\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  0.894927/  1.197028, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.25 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0769%\n",
      "layer   2  Sparsity: 68.1933%\n",
      "layer   3  Sparsity: 68.9384%\n",
      "total_backward_count 1595770 real_backward_count 103869   6.509%\n",
      "lif layer 1 self.abs_max_v: 9442.5\n",
      "lif layer 1 self.abs_max_v: 9491.0\n",
      "lif layer 1 self.abs_max_v: 9613.5\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  0.900837/  1.190515, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   2  Sparsity: 68.0385%\n",
      "layer   3  Sparsity: 69.0826%\n",
      "total_backward_count 1605560 real_backward_count 104123   6.485%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  0.900803/  1.175244, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.21 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   2  Sparsity: 67.9358%\n",
      "layer   3  Sparsity: 68.1331%\n",
      "total_backward_count 1615350 real_backward_count 104420   6.464%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  0.891883/  1.160226, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0877%\n",
      "layer   2  Sparsity: 68.1543%\n",
      "layer   3  Sparsity: 67.9723%\n",
      "total_backward_count 1625140 real_backward_count 104718   6.444%\n",
      "fc layer 1 self.abs_max_out: 5655.0\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  0.879632/  1.162477, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0659%\n",
      "layer   2  Sparsity: 68.1588%\n",
      "layer   3  Sparsity: 67.8616%\n",
      "total_backward_count 1634930 real_backward_count 104997   6.422%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  0.873657/  1.158552, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.68 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   2  Sparsity: 67.9158%\n",
      "layer   3  Sparsity: 67.8932%\n",
      "total_backward_count 1644720 real_backward_count 105245   6.399%\n",
      "fc layer 2 self.abs_max_out: 2741.0\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  0.885968/  1.183798, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0718%\n",
      "layer   2  Sparsity: 67.6687%\n",
      "layer   3  Sparsity: 67.5683%\n",
      "total_backward_count 1654510 real_backward_count 105524   6.378%\n",
      "fc layer 2 self.abs_max_out: 2785.0\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  0.889410/  1.179226, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0390%\n",
      "layer   2  Sparsity: 67.5581%\n",
      "layer   3  Sparsity: 67.3422%\n",
      "total_backward_count 1664300 real_backward_count 105796   6.357%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  0.883832/  1.160794, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.56 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   2  Sparsity: 67.6373%\n",
      "layer   3  Sparsity: 68.0637%\n",
      "total_backward_count 1674090 real_backward_count 106067   6.336%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  0.878417/  1.178113, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.72 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0694%\n",
      "layer   2  Sparsity: 67.4458%\n",
      "layer   3  Sparsity: 68.1970%\n",
      "total_backward_count 1683880 real_backward_count 106330   6.315%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  0.868779/  1.176325, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.47 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0962%\n",
      "layer   2  Sparsity: 67.4916%\n",
      "layer   3  Sparsity: 67.6761%\n",
      "total_backward_count 1693670 real_backward_count 106623   6.295%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  0.863209/  1.154548, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.69 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0956%\n",
      "layer   2  Sparsity: 67.5739%\n",
      "layer   3  Sparsity: 68.2556%\n",
      "total_backward_count 1703460 real_backward_count 106873   6.274%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  0.860138/  1.126774, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1118%\n",
      "layer   2  Sparsity: 67.3589%\n",
      "layer   3  Sparsity: 68.2193%\n",
      "total_backward_count 1713250 real_backward_count 107108   6.252%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  0.850252/  1.130902, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.11 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0314%\n",
      "layer   2  Sparsity: 67.3286%\n",
      "layer   3  Sparsity: 68.1642%\n",
      "total_backward_count 1723040 real_backward_count 107354   6.230%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  0.855547/  1.154279, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0408%\n",
      "layer   2  Sparsity: 67.5135%\n",
      "layer   3  Sparsity: 68.4547%\n",
      "total_backward_count 1732830 real_backward_count 107612   6.210%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  0.856656/  1.179234, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.68 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0449%\n",
      "layer   2  Sparsity: 67.1416%\n",
      "layer   3  Sparsity: 68.0953%\n",
      "total_backward_count 1742620 real_backward_count 107884   6.191%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  0.857558/  1.137674, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0968%\n",
      "layer   2  Sparsity: 67.5429%\n",
      "layer   3  Sparsity: 68.2953%\n",
      "total_backward_count 1752410 real_backward_count 108124   6.170%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  0.844169/  1.126370, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1048%\n",
      "layer   2  Sparsity: 67.4635%\n",
      "layer   3  Sparsity: 68.0967%\n",
      "total_backward_count 1762200 real_backward_count 108367   6.150%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  0.848519/  1.137585, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0498%\n",
      "layer   2  Sparsity: 67.2644%\n",
      "layer   3  Sparsity: 68.0952%\n",
      "total_backward_count 1771990 real_backward_count 108612   6.129%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  0.839744/  1.152868, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0379%\n",
      "layer   2  Sparsity: 67.4349%\n",
      "layer   3  Sparsity: 68.0548%\n",
      "total_backward_count 1781780 real_backward_count 108856   6.109%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  0.824167/  1.129012, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.38 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1005%\n",
      "layer   2  Sparsity: 67.4005%\n",
      "layer   3  Sparsity: 68.5617%\n",
      "total_backward_count 1791570 real_backward_count 109068   6.088%\n",
      "fc layer 1 self.abs_max_out: 5702.0\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  0.835102/  1.172903, val:  82.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0939%\n",
      "layer   2  Sparsity: 67.4466%\n",
      "layer   3  Sparsity: 68.7878%\n",
      "total_backward_count 1801360 real_backward_count 109330   6.069%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  0.834369/  1.124554, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.29 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1034%\n",
      "layer   2  Sparsity: 67.5736%\n",
      "layer   3  Sparsity: 68.2984%\n",
      "total_backward_count 1811150 real_backward_count 109581   6.050%\n",
      "fc layer 2 self.abs_max_out: 2794.0\n",
      "fc layer 1 self.abs_max_out: 5755.0\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  0.829324/  1.104058, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0588%\n",
      "layer   2  Sparsity: 67.4701%\n",
      "layer   3  Sparsity: 68.2677%\n",
      "total_backward_count 1820940 real_backward_count 109821   6.031%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  0.827749/  1.128141, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.48 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   2  Sparsity: 67.6176%\n",
      "layer   3  Sparsity: 68.1998%\n",
      "total_backward_count 1830730 real_backward_count 110079   6.013%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  0.834440/  1.136671, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.16 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0863%\n",
      "layer   2  Sparsity: 67.6166%\n",
      "layer   3  Sparsity: 67.8654%\n",
      "total_backward_count 1840520 real_backward_count 110298   5.993%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  0.825092/  1.127222, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.38 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   2  Sparsity: 67.6957%\n",
      "layer   3  Sparsity: 67.9313%\n",
      "total_backward_count 1850310 real_backward_count 110508   5.972%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  0.833583/  1.118639, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.30 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   2  Sparsity: 67.7732%\n",
      "layer   3  Sparsity: 67.7128%\n",
      "total_backward_count 1860100 real_backward_count 110723   5.953%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  0.827791/  1.120979, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   2  Sparsity: 67.7236%\n",
      "layer   3  Sparsity: 67.9085%\n",
      "total_backward_count 1869890 real_backward_count 110941   5.933%\n",
      "fc layer 3 self.abs_max_out: 924.0\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  0.821142/  1.132815, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   2  Sparsity: 67.5119%\n",
      "layer   3  Sparsity: 68.4108%\n",
      "total_backward_count 1879680 real_backward_count 111166   5.914%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  0.818059/  1.137149, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 67.5294%\n",
      "layer   3  Sparsity: 68.0780%\n",
      "total_backward_count 1889470 real_backward_count 111402   5.896%\n",
      "lif layer 2 self.abs_max_v: 4032.0\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  0.809581/  1.106603, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.82 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   2  Sparsity: 67.3726%\n",
      "layer   3  Sparsity: 67.7150%\n",
      "total_backward_count 1899260 real_backward_count 111644   5.878%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  0.807044/  1.103122, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.08 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0927%\n",
      "layer   2  Sparsity: 67.4071%\n",
      "layer   3  Sparsity: 67.9440%\n",
      "total_backward_count 1909050 real_backward_count 111851   5.859%\n",
      "fc layer 2 self.abs_max_out: 2816.0\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  0.806035/  1.093828, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   2  Sparsity: 67.5653%\n",
      "layer   3  Sparsity: 67.4831%\n",
      "total_backward_count 1918840 real_backward_count 112077   5.841%\n",
      "fc layer 3 self.abs_max_out: 931.0\n",
      "fc layer 3 self.abs_max_out: 932.0\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  0.800741/  1.100974, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1082%\n",
      "layer   2  Sparsity: 67.2546%\n",
      "layer   3  Sparsity: 67.6557%\n",
      "total_backward_count 1928630 real_backward_count 112279   5.822%\n",
      "fc layer 3 self.abs_max_out: 933.0\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  0.807908/  1.082955, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.71 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0513%\n",
      "layer   2  Sparsity: 67.3572%\n",
      "layer   3  Sparsity: 67.5091%\n",
      "total_backward_count 1938420 real_backward_count 112508   5.804%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  0.794941/  1.086905, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0588%\n",
      "layer   2  Sparsity: 67.3521%\n",
      "layer   3  Sparsity: 66.8613%\n",
      "total_backward_count 1948210 real_backward_count 112713   5.785%\n",
      "fc layer 3 self.abs_max_out: 964.0\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  0.786878/  1.104030, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.91 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   2  Sparsity: 67.1924%\n",
      "layer   3  Sparsity: 67.2084%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5446388a9254145afb1d26173c2a4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.78688</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>1.10403</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">charmed-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rciyrys8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rciyrys8</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_201956-rciyrys8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8s99kqf2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 25100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_002354-8s99kqf2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8s99kqf2' target=\"_blank\">dulcet-sweep-2</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8s99kqf2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8s99kqf2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251118_002403_272', 'my_seed': 25100, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 252.0\n",
      "lif layer 1 self.abs_max_v: 252.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 145.0\n",
      "lif layer 2 self.abs_max_v: 145.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 16.0\n",
      "fc layer 1 self.abs_max_out: 254.0\n",
      "lif layer 1 self.abs_max_v: 290.5\n",
      "fc layer 2 self.abs_max_out: 282.0\n",
      "lif layer 2 self.abs_max_v: 293.0\n",
      "fc layer 3 self.abs_max_out: 139.0\n",
      "lif layer 1 self.abs_max_v: 340.5\n",
      "lif layer 2 self.abs_max_v: 363.0\n",
      "fc layer 2 self.abs_max_out: 375.0\n",
      "lif layer 2 self.abs_max_v: 442.5\n",
      "lif layer 2 self.abs_max_v: 447.5\n",
      "fc layer 3 self.abs_max_out: 150.0\n",
      "fc layer 1 self.abs_max_out: 264.0\n",
      "fc layer 1 self.abs_max_out: 340.0\n",
      "lif layer 1 self.abs_max_v: 350.5\n",
      "lif layer 2 self.abs_max_v: 492.0\n",
      "fc layer 1 self.abs_max_out: 408.0\n",
      "lif layer 1 self.abs_max_v: 411.5\n",
      "lif layer 1 self.abs_max_v: 422.0\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "lif layer 1 self.abs_max_v: 466.0\n",
      "fc layer 2 self.abs_max_out: 446.0\n",
      "lif layer 1 self.abs_max_v: 489.5\n",
      "lif layer 1 self.abs_max_v: 533.0\n",
      "fc layer 1 self.abs_max_out: 463.0\n",
      "lif layer 1 self.abs_max_v: 595.0\n",
      "fc layer 2 self.abs_max_out: 461.0\n",
      "fc layer 2 self.abs_max_out: 465.0\n",
      "lif layer 2 self.abs_max_v: 681.5\n",
      "fc layer 1 self.abs_max_out: 468.0\n",
      "fc layer 2 self.abs_max_out: 539.0\n",
      "fc layer 1 self.abs_max_out: 492.0\n",
      "lif layer 2 self.abs_max_v: 692.5\n",
      "lif layer 2 self.abs_max_v: 836.5\n",
      "fc layer 3 self.abs_max_out: 198.0\n",
      "fc layer 1 self.abs_max_out: 504.0\n",
      "fc layer 3 self.abs_max_out: 220.0\n",
      "fc layer 1 self.abs_max_out: 516.0\n",
      "lif layer 1 self.abs_max_v: 620.5\n",
      "fc layer 1 self.abs_max_out: 719.0\n",
      "lif layer 1 self.abs_max_v: 724.0\n",
      "fc layer 3 self.abs_max_out: 227.0\n",
      "fc layer 1 self.abs_max_out: 779.0\n",
      "lif layer 1 self.abs_max_v: 779.0\n",
      "lif layer 2 self.abs_max_v: 855.5\n",
      "fc layer 3 self.abs_max_out: 267.0\n",
      "fc layer 2 self.abs_max_out: 548.0\n",
      "lif layer 2 self.abs_max_v: 876.0\n",
      "lif layer 2 self.abs_max_v: 926.0\n",
      "lif layer 2 self.abs_max_v: 941.5\n",
      "fc layer 2 self.abs_max_out: 594.0\n",
      "fc layer 1 self.abs_max_out: 856.0\n",
      "lif layer 1 self.abs_max_v: 856.0\n",
      "fc layer 2 self.abs_max_out: 596.0\n",
      "lif layer 1 self.abs_max_v: 907.0\n",
      "lif layer 2 self.abs_max_v: 943.0\n",
      "fc layer 1 self.abs_max_out: 928.0\n",
      "lif layer 1 self.abs_max_v: 928.0\n",
      "lif layer 1 self.abs_max_v: 941.5\n",
      "fc layer 3 self.abs_max_out: 290.0\n",
      "fc layer 2 self.abs_max_out: 625.0\n",
      "lif layer 2 self.abs_max_v: 1050.0\n",
      "fc layer 1 self.abs_max_out: 937.0\n",
      "lif layer 2 self.abs_max_v: 1083.0\n",
      "fc layer 2 self.abs_max_out: 731.0\n",
      "lif layer 1 self.abs_max_v: 955.5\n",
      "lif layer 1 self.abs_max_v: 977.5\n",
      "lif layer 2 self.abs_max_v: 1121.0\n",
      "fc layer 2 self.abs_max_out: 779.0\n",
      "lif layer 2 self.abs_max_v: 1156.0\n",
      "lif layer 2 self.abs_max_v: 1187.5\n",
      "lif layer 1 self.abs_max_v: 1028.0\n",
      "fc layer 1 self.abs_max_out: 1063.0\n",
      "lif layer 1 self.abs_max_v: 1063.0\n",
      "fc layer 1 self.abs_max_out: 1137.0\n",
      "lif layer 1 self.abs_max_v: 1137.0\n",
      "lif layer 1 self.abs_max_v: 1143.5\n",
      "lif layer 1 self.abs_max_v: 1199.0\n",
      "fc layer 3 self.abs_max_out: 320.0\n",
      "fc layer 2 self.abs_max_out: 782.0\n",
      "fc layer 2 self.abs_max_out: 857.0\n",
      "lif layer 1 self.abs_max_v: 1201.5\n",
      "fc layer 1 self.abs_max_out: 1186.0\n",
      "fc layer 1 self.abs_max_out: 1256.0\n",
      "lif layer 1 self.abs_max_v: 1256.0\n",
      "fc layer 1 self.abs_max_out: 1288.0\n",
      "lif layer 1 self.abs_max_v: 1305.0\n",
      "lif layer 1 self.abs_max_v: 1315.0\n",
      "lif layer 1 self.abs_max_v: 1326.0\n",
      "lif layer 2 self.abs_max_v: 1196.0\n",
      "lif layer 2 self.abs_max_v: 1241.0\n",
      "fc layer 2 self.abs_max_out: 929.0\n",
      "lif layer 1 self.abs_max_v: 1339.5\n",
      "lif layer 2 self.abs_max_v: 1286.5\n",
      "lif layer 2 self.abs_max_v: 1317.5\n",
      "lif layer 2 self.abs_max_v: 1346.0\n",
      "fc layer 1 self.abs_max_out: 1324.0\n",
      "fc layer 2 self.abs_max_out: 1054.0\n",
      "fc layer 3 self.abs_max_out: 329.0\n",
      "fc layer 1 self.abs_max_out: 1346.0\n",
      "lif layer 1 self.abs_max_v: 1346.0\n",
      "lif layer 1 self.abs_max_v: 1472.0\n",
      "lif layer 1 self.abs_max_v: 1581.0\n",
      "fc layer 1 self.abs_max_out: 1349.0\n",
      "fc layer 1 self.abs_max_out: 1425.0\n",
      "fc layer 3 self.abs_max_out: 331.0\n",
      "lif layer 1 self.abs_max_v: 1657.5\n",
      "lif layer 1 self.abs_max_v: 1770.5\n",
      "lif layer 1 self.abs_max_v: 1857.5\n",
      "lif layer 1 self.abs_max_v: 1872.5\n",
      "fc layer 3 self.abs_max_out: 344.0\n",
      "lif layer 1 self.abs_max_v: 1934.0\n",
      "fc layer 2 self.abs_max_out: 1056.0\n",
      "lif layer 1 self.abs_max_v: 2069.0\n",
      "lif layer 1 self.abs_max_v: 2161.0\n",
      "lif layer 1 self.abs_max_v: 2219.5\n",
      "lif layer 1 self.abs_max_v: 2281.5\n",
      "lif layer 1 self.abs_max_v: 2340.0\n",
      "fc layer 2 self.abs_max_out: 1115.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.789464/  1.906298, val:  48.33%, val_best:  48.33%, tr:  97.14%, tr_best:  97.14%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0415%\n",
      "layer   2  Sparsity: 76.6719%\n",
      "layer   3  Sparsity: 74.2268%\n",
      "total_backward_count 9790 real_backward_count 2191  22.380%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 353.0\n",
      "fc layer 3 self.abs_max_out: 368.0\n",
      "fc layer 1 self.abs_max_out: 1454.0\n",
      "fc layer 1 self.abs_max_out: 1543.0\n",
      "fc layer 1 self.abs_max_out: 1589.0\n",
      "fc layer 1 self.abs_max_out: 1603.0\n",
      "lif layer 2 self.abs_max_v: 1348.5\n",
      "fc layer 3 self.abs_max_out: 371.0\n",
      "fc layer 1 self.abs_max_out: 1637.0\n",
      "fc layer 1 self.abs_max_out: 1680.0\n",
      "fc layer 1 self.abs_max_out: 1822.0\n",
      "lif layer 1 self.abs_max_v: 2353.0\n",
      "fc layer 3 self.abs_max_out: 372.0\n",
      "lif layer 1 self.abs_max_v: 2449.0\n",
      "lif layer 1 self.abs_max_v: 2852.5\n",
      "lif layer 1 self.abs_max_v: 2989.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.689479/  1.925451, val:  53.33%, val_best:  53.33%, tr:  99.49%, tr_best:  99.49%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0528%\n",
      "layer   2  Sparsity: 78.0799%\n",
      "layer   3  Sparsity: 72.3959%\n",
      "total_backward_count 19580 real_backward_count 3750  19.152%\n",
      "fc layer 3 self.abs_max_out: 391.0\n",
      "lif layer 2 self.abs_max_v: 1361.5\n",
      "lif layer 2 self.abs_max_v: 1366.0\n",
      "lif layer 2 self.abs_max_v: 1394.0\n",
      "lif layer 2 self.abs_max_v: 1408.5\n",
      "lif layer 2 self.abs_max_v: 1443.0\n",
      "lif layer 2 self.abs_max_v: 1462.5\n",
      "lif layer 2 self.abs_max_v: 1472.5\n",
      "lif layer 2 self.abs_max_v: 1487.5\n",
      "fc layer 3 self.abs_max_out: 392.0\n",
      "fc layer 3 self.abs_max_out: 430.0\n",
      "fc layer 1 self.abs_max_out: 1841.0\n",
      "fc layer 1 self.abs_max_out: 1882.0\n",
      "lif layer 2 self.abs_max_v: 1638.0\n",
      "fc layer 2 self.abs_max_out: 1126.0\n",
      "lif layer 2 self.abs_max_v: 1651.0\n",
      "lif layer 2 self.abs_max_v: 1722.5\n",
      "lif layer 2 self.abs_max_v: 1781.5\n",
      "lif layer 2 self.abs_max_v: 1829.0\n",
      "fc layer 1 self.abs_max_out: 1989.0\n",
      "lif layer 1 self.abs_max_v: 3351.5\n",
      "fc layer 3 self.abs_max_out: 436.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.655671/  1.896432, val:  45.42%, val_best:  53.33%, tr:  99.28%, tr_best:  99.49%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0754%\n",
      "layer   2  Sparsity: 77.7001%\n",
      "layer   3  Sparsity: 71.0704%\n",
      "total_backward_count 29370 real_backward_count 5241  17.845%\n",
      "fc layer 2 self.abs_max_out: 1150.0\n",
      "fc layer 2 self.abs_max_out: 1173.0\n",
      "fc layer 2 self.abs_max_out: 1182.0\n",
      "fc layer 2 self.abs_max_out: 1186.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.648409/  1.897467, val:  41.67%, val_best:  53.33%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.24 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 78.2849%\n",
      "layer   3  Sparsity: 70.5205%\n",
      "total_backward_count 39160 real_backward_count 6591  16.831%\n",
      "fc layer 2 self.abs_max_out: 1212.0\n",
      "fc layer 2 self.abs_max_out: 1228.0\n",
      "fc layer 3 self.abs_max_out: 442.0\n",
      "fc layer 1 self.abs_max_out: 2034.0\n",
      "fc layer 1 self.abs_max_out: 2060.0\n",
      "fc layer 1 self.abs_max_out: 2081.0\n",
      "lif layer 1 self.abs_max_v: 3628.0\n",
      "fc layer 2 self.abs_max_out: 1259.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.615167/  1.884486, val:  41.67%, val_best:  53.33%, tr:  99.59%, tr_best:  99.80%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0933%\n",
      "layer   2  Sparsity: 78.5833%\n",
      "layer   3  Sparsity: 70.1175%\n",
      "total_backward_count 48950 real_backward_count 7921  16.182%\n",
      "fc layer 2 self.abs_max_out: 1276.0\n",
      "fc layer 3 self.abs_max_out: 456.0\n",
      "fc layer 3 self.abs_max_out: 468.0\n",
      "fc layer 1 self.abs_max_out: 2088.0\n",
      "fc layer 1 self.abs_max_out: 2196.0\n",
      "lif layer 1 self.abs_max_v: 3638.0\n",
      "lif layer 1 self.abs_max_v: 3739.5\n",
      "fc layer 2 self.abs_max_out: 1282.0\n",
      "fc layer 2 self.abs_max_out: 1310.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.601376/  1.829821, val:  51.67%, val_best:  53.33%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0892%\n",
      "layer   2  Sparsity: 77.5099%\n",
      "layer   3  Sparsity: 70.6493%\n",
      "total_backward_count 58740 real_backward_count 9203  15.667%\n",
      "fc layer 3 self.abs_max_out: 493.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.578318/  1.809520, val:  52.50%, val_best:  53.33%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0599%\n",
      "layer   2  Sparsity: 77.6268%\n",
      "layer   3  Sparsity: 70.6777%\n",
      "total_backward_count 68530 real_backward_count 10488  15.304%\n",
      "fc layer 2 self.abs_max_out: 1321.0\n",
      "fc layer 2 self.abs_max_out: 1327.0\n",
      "fc layer 2 self.abs_max_out: 1374.0\n",
      "fc layer 2 self.abs_max_out: 1465.0\n",
      "lif layer 1 self.abs_max_v: 3749.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.580549/  1.799382, val:  59.58%, val_best:  59.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0438%\n",
      "layer   2  Sparsity: 77.7396%\n",
      "layer   3  Sparsity: 71.3228%\n",
      "total_backward_count 78320 real_backward_count 11732  14.980%\n",
      "fc layer 1 self.abs_max_out: 2275.0\n",
      "lif layer 1 self.abs_max_v: 3752.5\n",
      "fc layer 1 self.abs_max_out: 2296.0\n",
      "fc layer 1 self.abs_max_out: 2498.0\n",
      "lif layer 1 self.abs_max_v: 4087.0\n",
      "lif layer 1 self.abs_max_v: 4304.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.564281/  1.796040, val:  44.17%, val_best:  59.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.37 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0736%\n",
      "layer   2  Sparsity: 77.0320%\n",
      "layer   3  Sparsity: 71.3393%\n",
      "total_backward_count 88110 real_backward_count 13015  14.771%\n",
      "fc layer 3 self.abs_max_out: 548.0\n",
      "fc layer 1 self.abs_max_out: 2526.0\n",
      "lif layer 1 self.abs_max_v: 4395.5\n",
      "lif layer 2 self.abs_max_v: 1875.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.539346/  1.815656, val:  47.08%, val_best:  59.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 72.49 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0877%\n",
      "layer   2  Sparsity: 76.9261%\n",
      "layer   3  Sparsity: 70.6074%\n",
      "total_backward_count 97900 real_backward_count 14250  14.556%\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.549694/  1.763638, val:  58.33%, val_best:  59.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.89 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 76.8817%\n",
      "layer   3  Sparsity: 71.0523%\n",
      "total_backward_count 107690 real_backward_count 15486  14.380%\n",
      "fc layer 1 self.abs_max_out: 2649.0\n",
      "lif layer 1 self.abs_max_v: 4439.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.529669/  1.778827, val:  50.42%, val_best:  59.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.64 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1118%\n",
      "layer   2  Sparsity: 76.3518%\n",
      "layer   3  Sparsity: 71.5449%\n",
      "total_backward_count 117480 real_backward_count 16651  14.173%\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.545954/  1.798558, val:  47.50%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0667%\n",
      "layer   2  Sparsity: 76.7155%\n",
      "layer   3  Sparsity: 72.1001%\n",
      "total_backward_count 127270 real_backward_count 17826  14.006%\n",
      "lif layer 2 self.abs_max_v: 1901.5\n",
      "lif layer 2 self.abs_max_v: 1908.5\n",
      "lif layer 2 self.abs_max_v: 1917.5\n",
      "lif layer 2 self.abs_max_v: 1920.5\n",
      "lif layer 2 self.abs_max_v: 2046.5\n",
      "lif layer 2 self.abs_max_v: 2092.5\n",
      "lif layer 1 self.abs_max_v: 4440.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.524469/  1.745705, val:  62.50%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0659%\n",
      "layer   2  Sparsity: 76.1323%\n",
      "layer   3  Sparsity: 72.5569%\n",
      "total_backward_count 137060 real_backward_count 19018  13.876%\n",
      "lif layer 1 self.abs_max_v: 4614.5\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.513310/  1.744788, val:  50.83%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.72 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1160%\n",
      "layer   2  Sparsity: 76.3295%\n",
      "layer   3  Sparsity: 72.5048%\n",
      "total_backward_count 146850 real_backward_count 20162  13.730%\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.511742/  1.758452, val:  46.67%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0833%\n",
      "layer   2  Sparsity: 75.6160%\n",
      "layer   3  Sparsity: 72.1641%\n",
      "total_backward_count 156640 real_backward_count 21310  13.604%\n",
      "fc layer 1 self.abs_max_out: 2668.0\n",
      "fc layer 1 self.abs_max_out: 2916.0\n",
      "lif layer 1 self.abs_max_v: 4678.5\n",
      "lif layer 1 self.abs_max_v: 5078.5\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.521860/  1.752205, val:  48.33%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0533%\n",
      "layer   2  Sparsity: 76.0073%\n",
      "layer   3  Sparsity: 72.6387%\n",
      "total_backward_count 166430 real_backward_count 22412  13.466%\n",
      "fc layer 2 self.abs_max_out: 1519.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.498439/  1.752724, val:  47.92%, val_best:  62.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0975%\n",
      "layer   2  Sparsity: 75.9248%\n",
      "layer   3  Sparsity: 72.3696%\n",
      "total_backward_count 176220 real_backward_count 23556  13.367%\n",
      "fc layer 2 self.abs_max_out: 1526.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.494093/  1.732298, val:  55.00%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.03 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0638%\n",
      "layer   2  Sparsity: 75.6139%\n",
      "layer   3  Sparsity: 71.8086%\n",
      "total_backward_count 186010 real_backward_count 24663  13.259%\n",
      "fc layer 1 self.abs_max_out: 2939.0\n",
      "lif layer 1 self.abs_max_v: 5214.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.474908/  1.723200, val:  53.33%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.06 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0986%\n",
      "layer   2  Sparsity: 75.7255%\n",
      "layer   3  Sparsity: 71.5985%\n",
      "total_backward_count 195800 real_backward_count 25793  13.173%\n",
      "fc layer 2 self.abs_max_out: 1545.0\n",
      "fc layer 2 self.abs_max_out: 1622.0\n",
      "fc layer 2 self.abs_max_out: 1651.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.476666/  1.710436, val:  58.33%, val_best:  62.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0450%\n",
      "layer   2  Sparsity: 75.8479%\n",
      "layer   3  Sparsity: 71.7244%\n",
      "total_backward_count 205590 real_backward_count 26854  13.062%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.441779/  1.702671, val:  54.58%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.20 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0620%\n",
      "layer   2  Sparsity: 75.9867%\n",
      "layer   3  Sparsity: 71.8583%\n",
      "total_backward_count 215380 real_backward_count 27889  12.949%\n",
      "fc layer 3 self.abs_max_out: 561.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.443942/  1.663900, val:  57.92%, val_best:  62.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.33 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0913%\n",
      "layer   2  Sparsity: 75.8807%\n",
      "layer   3  Sparsity: 72.4544%\n",
      "total_backward_count 225170 real_backward_count 28965  12.864%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.437058/  1.688236, val:  61.25%, val_best:  62.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0789%\n",
      "layer   2  Sparsity: 75.6120%\n",
      "layer   3  Sparsity: 72.7507%\n",
      "total_backward_count 234960 real_backward_count 29994  12.766%\n",
      "fc layer 2 self.abs_max_out: 1677.0\n",
      "fc layer 2 self.abs_max_out: 1698.0\n",
      "fc layer 2 self.abs_max_out: 1736.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.483874/  1.699199, val:  60.83%, val_best:  62.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1321%\n",
      "layer   2  Sparsity: 75.1590%\n",
      "layer   3  Sparsity: 72.5169%\n",
      "total_backward_count 244750 real_backward_count 31029  12.678%\n",
      "lif layer 2 self.abs_max_v: 2131.0\n",
      "fc layer 1 self.abs_max_out: 3070.0\n",
      "lif layer 1 self.abs_max_v: 5272.5\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.480108/  1.702959, val:  62.50%, val_best:  62.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0636%\n",
      "layer   2  Sparsity: 75.7221%\n",
      "layer   3  Sparsity: 72.7642%\n",
      "total_backward_count 254540 real_backward_count 32066  12.598%\n",
      "lif layer 1 self.abs_max_v: 5368.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.447951/  1.635091, val:  69.17%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.82 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   2  Sparsity: 76.0463%\n",
      "layer   3  Sparsity: 72.4016%\n",
      "total_backward_count 264330 real_backward_count 33048  12.503%\n",
      "fc layer 1 self.abs_max_out: 3081.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.430332/  1.652605, val:  70.83%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.06 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   2  Sparsity: 75.8815%\n",
      "layer   3  Sparsity: 71.9660%\n",
      "total_backward_count 274120 real_backward_count 34041  12.418%\n",
      "fc layer 3 self.abs_max_out: 576.0\n",
      "lif layer 1 self.abs_max_v: 5415.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.412751/  1.617637, val:  69.58%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.86 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 75.8141%\n",
      "layer   3  Sparsity: 71.7126%\n",
      "total_backward_count 283910 real_backward_count 35064  12.350%\n",
      "fc layer 1 self.abs_max_out: 3178.0\n",
      "lif layer 1 self.abs_max_v: 5654.0\n",
      "fc layer 3 self.abs_max_out: 577.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.398095/  1.651218, val:  70.42%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   2  Sparsity: 75.6999%\n",
      "layer   3  Sparsity: 71.6117%\n",
      "total_backward_count 293700 real_backward_count 35999  12.257%\n",
      "lif layer 2 self.abs_max_v: 2186.5\n",
      "fc layer 3 self.abs_max_out: 646.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.384411/  1.622762, val:  61.67%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.51 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1055%\n",
      "layer   2  Sparsity: 75.6984%\n",
      "layer   3  Sparsity: 71.5608%\n",
      "total_backward_count 303490 real_backward_count 36964  12.180%\n",
      "fc layer 1 self.abs_max_out: 3303.0\n",
      "lif layer 1 self.abs_max_v: 5836.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.395096/  1.624200, val:  73.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.28 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1032%\n",
      "layer   2  Sparsity: 75.6269%\n",
      "layer   3  Sparsity: 71.2231%\n",
      "total_backward_count 313280 real_backward_count 37870  12.088%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.391171/  1.657172, val:  62.50%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0938%\n",
      "layer   2  Sparsity: 75.6874%\n",
      "layer   3  Sparsity: 71.1265%\n",
      "total_backward_count 323070 real_backward_count 38782  12.004%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.390148/  1.672871, val:  63.33%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0543%\n",
      "layer   2  Sparsity: 75.2504%\n",
      "layer   3  Sparsity: 71.8252%\n",
      "total_backward_count 332860 real_backward_count 39688  11.923%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.390108/  1.599338, val:  71.25%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1340%\n",
      "layer   2  Sparsity: 75.3852%\n",
      "layer   3  Sparsity: 71.5747%\n",
      "total_backward_count 342650 real_backward_count 40587  11.845%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.365621/  1.609825, val:  61.67%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.62 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 75.2764%\n",
      "layer   3  Sparsity: 71.7255%\n",
      "total_backward_count 352440 real_backward_count 41477  11.769%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.376402/  1.603901, val:  69.58%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0961%\n",
      "layer   2  Sparsity: 74.8245%\n",
      "layer   3  Sparsity: 70.7743%\n",
      "total_backward_count 362230 real_backward_count 42333  11.687%\n",
      "fc layer 2 self.abs_max_out: 1798.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.358001/  1.586679, val:  73.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0427%\n",
      "layer   2  Sparsity: 74.7738%\n",
      "layer   3  Sparsity: 71.0408%\n",
      "total_backward_count 372020 real_backward_count 43242  11.624%\n",
      "fc layer 3 self.abs_max_out: 648.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.368284/  1.618785, val:  51.25%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   2  Sparsity: 75.3320%\n",
      "layer   3  Sparsity: 71.4199%\n",
      "total_backward_count 381810 real_backward_count 44091  11.548%\n",
      "lif layer 2 self.abs_max_v: 2312.0\n",
      "lif layer 2 self.abs_max_v: 2371.5\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.357278/  1.602188, val:  72.08%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0915%\n",
      "layer   2  Sparsity: 75.0059%\n",
      "layer   3  Sparsity: 71.2324%\n",
      "total_backward_count 391600 real_backward_count 44958  11.481%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.337456/  1.594194, val:  67.92%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.63 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   2  Sparsity: 75.2313%\n",
      "layer   3  Sparsity: 71.1291%\n",
      "total_backward_count 401390 real_backward_count 45824  11.416%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.336910/  1.599061, val:  74.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0393%\n",
      "layer   2  Sparsity: 75.2334%\n",
      "layer   3  Sparsity: 70.9724%\n",
      "total_backward_count 411180 real_backward_count 46728  11.364%\n",
      "lif layer 1 self.abs_max_v: 5985.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.343624/  1.563648, val:  62.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.24 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 75.3454%\n",
      "layer   3  Sparsity: 71.3945%\n",
      "total_backward_count 420970 real_backward_count 47571  11.300%\n",
      "fc layer 1 self.abs_max_out: 3327.0\n",
      "lif layer 1 self.abs_max_v: 6093.5\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.314846/  1.562231, val:  69.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 75.2085%\n",
      "layer   3  Sparsity: 71.2394%\n",
      "total_backward_count 430760 real_backward_count 48416  11.240%\n",
      "fc layer 1 self.abs_max_out: 3367.0\n",
      "lif layer 1 self.abs_max_v: 6195.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.324522/  1.545766, val:  72.92%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   2  Sparsity: 75.1423%\n",
      "layer   3  Sparsity: 70.7944%\n",
      "total_backward_count 440550 real_backward_count 49239  11.177%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.309084/  1.519244, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1116%\n",
      "layer   2  Sparsity: 75.1184%\n",
      "layer   3  Sparsity: 71.3777%\n",
      "total_backward_count 450340 real_backward_count 50065  11.117%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.299131/  1.538734, val:  74.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   2  Sparsity: 75.0100%\n",
      "layer   3  Sparsity: 71.5122%\n",
      "total_backward_count 460130 real_backward_count 50884  11.059%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.289038/  1.522706, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0620%\n",
      "layer   2  Sparsity: 74.9552%\n",
      "layer   3  Sparsity: 71.6008%\n",
      "total_backward_count 469920 real_backward_count 51656  10.993%\n",
      "fc layer 2 self.abs_max_out: 1808.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.289191/  1.532758, val:  77.50%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   2  Sparsity: 74.3633%\n",
      "layer   3  Sparsity: 71.6827%\n",
      "total_backward_count 479710 real_backward_count 52439  10.931%\n",
      "fc layer 1 self.abs_max_out: 3372.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.281270/  1.506052, val:  76.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.63 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1027%\n",
      "layer   2  Sparsity: 74.6566%\n",
      "layer   3  Sparsity: 71.6973%\n",
      "total_backward_count 489500 real_backward_count 53176  10.863%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.268723/  1.523189, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0685%\n",
      "layer   2  Sparsity: 74.6014%\n",
      "layer   3  Sparsity: 71.2925%\n",
      "total_backward_count 499290 real_backward_count 53941  10.804%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.272974/  1.536861, val:  70.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.94 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1027%\n",
      "layer   2  Sparsity: 74.7310%\n",
      "layer   3  Sparsity: 71.1174%\n",
      "total_backward_count 509080 real_backward_count 54659  10.737%\n",
      "lif layer 2 self.abs_max_v: 2437.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.281969/  1.514677, val:  73.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0464%\n",
      "layer   2  Sparsity: 74.9616%\n",
      "layer   3  Sparsity: 71.7114%\n",
      "total_backward_count 518870 real_backward_count 55430  10.683%\n",
      "lif layer 2 self.abs_max_v: 2585.5\n",
      "fc layer 1 self.abs_max_out: 3436.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.284371/  1.507718, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   2  Sparsity: 74.8016%\n",
      "layer   3  Sparsity: 71.7726%\n",
      "total_backward_count 528660 real_backward_count 56167  10.624%\n",
      "fc layer 1 self.abs_max_out: 3481.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.272200/  1.515047, val:  72.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.03 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0942%\n",
      "layer   2  Sparsity: 74.8491%\n",
      "layer   3  Sparsity: 72.2169%\n",
      "total_backward_count 538450 real_backward_count 56885  10.565%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.277693/  1.491966, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.37 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0384%\n",
      "layer   2  Sparsity: 74.6594%\n",
      "layer   3  Sparsity: 72.6265%\n",
      "total_backward_count 548240 real_backward_count 57603  10.507%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.283189/  1.492785, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.28 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0555%\n",
      "layer   2  Sparsity: 74.1144%\n",
      "layer   3  Sparsity: 72.2016%\n",
      "total_backward_count 558030 real_backward_count 58265  10.441%\n",
      "lif layer 1 self.abs_max_v: 6355.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.255140/  1.497886, val:  75.83%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.98 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0547%\n",
      "layer   2  Sparsity: 74.6128%\n",
      "layer   3  Sparsity: 71.6945%\n",
      "total_backward_count 567820 real_backward_count 58944  10.381%\n",
      "fc layer 1 self.abs_max_out: 3491.0\n",
      "fc layer 1 self.abs_max_out: 3538.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.272414/  1.509974, val:  75.83%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.62 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0734%\n",
      "layer   2  Sparsity: 74.4657%\n",
      "layer   3  Sparsity: 71.7909%\n",
      "total_backward_count 577610 real_backward_count 59642  10.326%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.253013/  1.484434, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0596%\n",
      "layer   2  Sparsity: 74.4701%\n",
      "layer   3  Sparsity: 71.5388%\n",
      "total_backward_count 587400 real_backward_count 60295  10.265%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.247086/  1.476257, val:  77.08%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0976%\n",
      "layer   2  Sparsity: 74.2260%\n",
      "layer   3  Sparsity: 71.5910%\n",
      "total_backward_count 597190 real_backward_count 60933  10.203%\n",
      "fc layer 2 self.abs_max_out: 1820.0\n",
      "fc layer 2 self.abs_max_out: 1828.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.233799/  1.491723, val:  67.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0768%\n",
      "layer   2  Sparsity: 73.8471%\n",
      "layer   3  Sparsity: 72.0867%\n",
      "total_backward_count 606980 real_backward_count 61595  10.148%\n",
      "fc layer 2 self.abs_max_out: 1844.0\n",
      "fc layer 2 self.abs_max_out: 1898.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.256718/  1.522494, val:  72.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   2  Sparsity: 74.0251%\n",
      "layer   3  Sparsity: 72.0238%\n",
      "total_backward_count 616770 real_backward_count 62258  10.094%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.270491/  1.482829, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.34 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   2  Sparsity: 74.0998%\n",
      "layer   3  Sparsity: 72.3944%\n",
      "total_backward_count 626560 real_backward_count 62879  10.036%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.270415/  1.493032, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   2  Sparsity: 74.1596%\n",
      "layer   3  Sparsity: 72.5315%\n",
      "total_backward_count 636350 real_backward_count 63503   9.979%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.276753/  1.496745, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   2  Sparsity: 74.4952%\n",
      "layer   3  Sparsity: 72.4500%\n",
      "total_backward_count 646140 real_backward_count 64119   9.923%\n",
      "fc layer 2 self.abs_max_out: 1941.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.272746/  1.498757, val:  69.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0548%\n",
      "layer   2  Sparsity: 74.4587%\n",
      "layer   3  Sparsity: 72.5608%\n",
      "total_backward_count 655930 real_backward_count 64773   9.875%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.268752/  1.491162, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   2  Sparsity: 74.1632%\n",
      "layer   3  Sparsity: 72.4027%\n",
      "total_backward_count 665720 real_backward_count 65407   9.825%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.277674/  1.488968, val:  77.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1135%\n",
      "layer   2  Sparsity: 74.0866%\n",
      "layer   3  Sparsity: 72.3745%\n",
      "total_backward_count 675510 real_backward_count 66021   9.774%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.261045/  1.461711, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0310%\n",
      "layer   2  Sparsity: 73.6616%\n",
      "layer   3  Sparsity: 71.8172%\n",
      "total_backward_count 685300 real_backward_count 66642   9.725%\n",
      "fc layer 1 self.abs_max_out: 3728.0\n",
      "fc layer 3 self.abs_max_out: 649.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.233081/  1.462366, val:  75.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0587%\n",
      "layer   2  Sparsity: 73.6742%\n",
      "layer   3  Sparsity: 71.4342%\n",
      "total_backward_count 695090 real_backward_count 67258   9.676%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.236494/  1.440498, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.88 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0466%\n",
      "layer   2  Sparsity: 73.6883%\n",
      "layer   3  Sparsity: 71.2540%\n",
      "total_backward_count 704880 real_backward_count 67896   9.632%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.227153/  1.437074, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.12 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   2  Sparsity: 73.8905%\n",
      "layer   3  Sparsity: 71.5413%\n",
      "total_backward_count 714670 real_backward_count 68522   9.588%\n",
      "fc layer 1 self.abs_max_out: 3782.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.241398/  1.442777, val:  76.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.30 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0348%\n",
      "layer   2  Sparsity: 74.1674%\n",
      "layer   3  Sparsity: 71.3986%\n",
      "total_backward_count 724460 real_backward_count 69170   9.548%\n",
      "fc layer 3 self.abs_max_out: 650.0\n",
      "fc layer 3 self.abs_max_out: 652.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.211813/  1.437772, val:  72.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.18 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0521%\n",
      "layer   2  Sparsity: 73.9535%\n",
      "layer   3  Sparsity: 71.4045%\n",
      "total_backward_count 734250 real_backward_count 69748   9.499%\n",
      "fc layer 2 self.abs_max_out: 1958.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.200996/  1.445971, val:  70.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.83 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0999%\n",
      "layer   2  Sparsity: 73.8032%\n",
      "layer   3  Sparsity: 71.6803%\n",
      "total_backward_count 744040 real_backward_count 70308   9.449%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.197582/  1.438842, val:  75.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.78 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0590%\n",
      "layer   2  Sparsity: 73.7107%\n",
      "layer   3  Sparsity: 71.9423%\n",
      "total_backward_count 753830 real_backward_count 70859   9.400%\n",
      "fc layer 3 self.abs_max_out: 654.0\n",
      "lif layer 1 self.abs_max_v: 6462.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.193557/  1.428283, val:  80.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   2  Sparsity: 73.5643%\n",
      "layer   3  Sparsity: 71.9338%\n",
      "total_backward_count 763620 real_backward_count 71441   9.356%\n",
      "fc layer 3 self.abs_max_out: 684.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.189663/  1.455101, val:  72.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   2  Sparsity: 73.8478%\n",
      "layer   3  Sparsity: 71.7804%\n",
      "total_backward_count 773410 real_backward_count 71999   9.309%\n",
      "fc layer 1 self.abs_max_out: 3840.0\n",
      "fc layer 2 self.abs_max_out: 1959.0\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.197230/  1.434324, val:  79.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.19 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0544%\n",
      "layer   2  Sparsity: 73.7907%\n",
      "layer   3  Sparsity: 71.6109%\n",
      "total_backward_count 783200 real_backward_count 72575   9.266%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.189716/  1.418123, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0488%\n",
      "layer   2  Sparsity: 73.6916%\n",
      "layer   3  Sparsity: 72.2332%\n",
      "total_backward_count 792990 real_backward_count 73170   9.227%\n",
      "fc layer 2 self.abs_max_out: 1990.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.180278/  1.420227, val:  72.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0709%\n",
      "layer   2  Sparsity: 73.9351%\n",
      "layer   3  Sparsity: 72.2597%\n",
      "total_backward_count 802780 real_backward_count 73749   9.187%\n",
      "fc layer 1 self.abs_max_out: 3862.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.188519/  1.423636, val:  81.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.38 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   2  Sparsity: 74.1582%\n",
      "layer   3  Sparsity: 72.7587%\n",
      "total_backward_count 812570 real_backward_count 74352   9.150%\n",
      "fc layer 2 self.abs_max_out: 2008.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.186202/  1.403314, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.90 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   2  Sparsity: 74.1800%\n",
      "layer   3  Sparsity: 72.5582%\n",
      "total_backward_count 822360 real_backward_count 74878   9.105%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.186966/  1.412934, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   2  Sparsity: 73.7384%\n",
      "layer   3  Sparsity: 72.5269%\n",
      "total_backward_count 832150 real_backward_count 75437   9.065%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.179881/  1.441915, val:  78.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.78 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0454%\n",
      "layer   2  Sparsity: 73.6674%\n",
      "layer   3  Sparsity: 72.2409%\n",
      "total_backward_count 841940 real_backward_count 75972   9.023%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.184760/  1.411572, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0975%\n",
      "layer   2  Sparsity: 73.7589%\n",
      "layer   3  Sparsity: 71.9055%\n",
      "total_backward_count 851730 real_backward_count 76514   8.983%\n",
      "lif layer 2 self.abs_max_v: 2624.5\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.195758/  1.428755, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0638%\n",
      "layer   2  Sparsity: 73.6206%\n",
      "layer   3  Sparsity: 72.1231%\n",
      "total_backward_count 861520 real_backward_count 77034   8.942%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.178717/  1.409369, val:  81.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0477%\n",
      "layer   2  Sparsity: 73.4172%\n",
      "layer   3  Sparsity: 72.0419%\n",
      "total_backward_count 871310 real_backward_count 77524   8.897%\n",
      "lif layer 1 self.abs_max_v: 6537.5\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.168567/  1.428766, val:  76.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   2  Sparsity: 73.3322%\n",
      "layer   3  Sparsity: 72.3356%\n",
      "total_backward_count 881100 real_backward_count 78012   8.854%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.165494/  1.370546, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0193%\n",
      "layer   2  Sparsity: 73.5077%\n",
      "layer   3  Sparsity: 72.2637%\n",
      "total_backward_count 890890 real_backward_count 78485   8.810%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.152675/  1.405742, val:  79.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.83 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0520%\n",
      "layer   2  Sparsity: 73.7640%\n",
      "layer   3  Sparsity: 72.3818%\n",
      "total_backward_count 900680 real_backward_count 78983   8.769%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.176630/  1.400686, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   2  Sparsity: 73.5902%\n",
      "layer   3  Sparsity: 72.1223%\n",
      "total_backward_count 910470 real_backward_count 79507   8.733%\n",
      "fc layer 1 self.abs_max_out: 3949.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.175515/  1.402690, val:  85.83%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   2  Sparsity: 73.4499%\n",
      "layer   3  Sparsity: 72.1204%\n",
      "total_backward_count 920260 real_backward_count 80041   8.698%\n",
      "lif layer 2 self.abs_max_v: 2628.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.173083/  1.436970, val:  77.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.74 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0596%\n",
      "layer   2  Sparsity: 73.4221%\n",
      "layer   3  Sparsity: 72.3844%\n",
      "total_backward_count 930050 real_backward_count 80488   8.654%\n",
      "fc layer 3 self.abs_max_out: 685.0\n",
      "fc layer 3 self.abs_max_out: 694.0\n",
      "fc layer 1 self.abs_max_out: 4008.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.185714/  1.406735, val:  80.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   2  Sparsity: 73.3476%\n",
      "layer   3  Sparsity: 72.6869%\n",
      "total_backward_count 939840 real_backward_count 81013   8.620%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.175677/  1.415225, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   2  Sparsity: 73.5152%\n",
      "layer   3  Sparsity: 72.9854%\n",
      "total_backward_count 949630 real_backward_count 81465   8.579%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.180508/  1.401328, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1061%\n",
      "layer   2  Sparsity: 73.7975%\n",
      "layer   3  Sparsity: 72.8065%\n",
      "total_backward_count 959420 real_backward_count 81969   8.544%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.156089/  1.388504, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   2  Sparsity: 73.8342%\n",
      "layer   3  Sparsity: 73.1188%\n",
      "total_backward_count 969210 real_backward_count 82422   8.504%\n",
      "fc layer 2 self.abs_max_out: 2086.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.151155/  1.403110, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.53 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0934%\n",
      "layer   2  Sparsity: 73.7452%\n",
      "layer   3  Sparsity: 72.8476%\n",
      "total_backward_count 979000 real_backward_count 82886   8.466%\n",
      "fc layer 1 self.abs_max_out: 4044.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.163353/  1.388092, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   2  Sparsity: 73.9081%\n",
      "layer   3  Sparsity: 72.5080%\n",
      "total_backward_count 988790 real_backward_count 83356   8.430%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.158088/  1.411390, val:  78.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0719%\n",
      "layer   2  Sparsity: 73.7222%\n",
      "layer   3  Sparsity: 72.9863%\n",
      "total_backward_count 998580 real_backward_count 83827   8.395%\n",
      "lif layer 2 self.abs_max_v: 2641.0\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.164348/  1.398445, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   2  Sparsity: 73.7053%\n",
      "layer   3  Sparsity: 73.0781%\n",
      "total_backward_count 1008370 real_backward_count 84269   8.357%\n",
      "lif layer 1 self.abs_max_v: 6543.5\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.159496/  1.413816, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0968%\n",
      "layer   2  Sparsity: 73.6548%\n",
      "layer   3  Sparsity: 72.9278%\n",
      "total_backward_count 1018160 real_backward_count 84753   8.324%\n",
      "lif layer 2 self.abs_max_v: 2649.0\n",
      "lif layer 2 self.abs_max_v: 2703.0\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.170142/  1.381094, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.52 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 73.5128%\n",
      "layer   3  Sparsity: 72.8876%\n",
      "total_backward_count 1027950 real_backward_count 85215   8.290%\n",
      "fc layer 3 self.abs_max_out: 699.0\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.153056/  1.392833, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1202%\n",
      "layer   2  Sparsity: 73.6298%\n",
      "layer   3  Sparsity: 72.8314%\n",
      "total_backward_count 1037740 real_backward_count 85668   8.255%\n",
      "lif layer 1 self.abs_max_v: 6817.5\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.145855/  1.371523, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0953%\n",
      "layer   2  Sparsity: 73.8125%\n",
      "layer   3  Sparsity: 72.7918%\n",
      "total_backward_count 1047530 real_backward_count 86094   8.219%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.148779/  1.398483, val:  78.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.28 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   2  Sparsity: 73.4484%\n",
      "layer   3  Sparsity: 72.7476%\n",
      "total_backward_count 1057320 real_backward_count 86550   8.186%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.161276/  1.403205, val:  80.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.38 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0925%\n",
      "layer   2  Sparsity: 73.2572%\n",
      "layer   3  Sparsity: 72.6965%\n",
      "total_backward_count 1067110 real_backward_count 87005   8.153%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.154058/  1.391237, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.36 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0855%\n",
      "layer   2  Sparsity: 73.5320%\n",
      "layer   3  Sparsity: 72.8833%\n",
      "total_backward_count 1076900 real_backward_count 87406   8.116%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.163080/  1.393064, val:  85.83%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0503%\n",
      "layer   2  Sparsity: 73.7272%\n",
      "layer   3  Sparsity: 73.1099%\n",
      "total_backward_count 1086690 real_backward_count 87820   8.081%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.161277/  1.371454, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.46 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0925%\n",
      "layer   2  Sparsity: 73.6681%\n",
      "layer   3  Sparsity: 72.7679%\n",
      "total_backward_count 1096480 real_backward_count 88264   8.050%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.126856/  1.387222, val:  77.50%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.38 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   2  Sparsity: 73.6656%\n",
      "layer   3  Sparsity: 72.1954%\n",
      "total_backward_count 1106270 real_backward_count 88717   8.019%\n",
      "fc layer 1 self.abs_max_out: 4113.0\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.129763/  1.378390, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.08 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0626%\n",
      "layer   2  Sparsity: 73.8555%\n",
      "layer   3  Sparsity: 72.2209%\n",
      "total_backward_count 1116060 real_backward_count 89121   7.985%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.112387/  1.346535, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.42 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   2  Sparsity: 73.8056%\n",
      "layer   3  Sparsity: 72.6482%\n",
      "total_backward_count 1125850 real_backward_count 89544   7.953%\n",
      "lif layer 2 self.abs_max_v: 2705.5\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.114121/  1.361504, val:  76.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0974%\n",
      "layer   2  Sparsity: 73.9170%\n",
      "layer   3  Sparsity: 72.5114%\n",
      "total_backward_count 1135640 real_backward_count 89920   7.918%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.124394/  1.374534, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.61 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0903%\n",
      "layer   2  Sparsity: 73.7379%\n",
      "layer   3  Sparsity: 72.5877%\n",
      "total_backward_count 1145430 real_backward_count 90275   7.881%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.129922/  1.390746, val:  81.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   2  Sparsity: 73.6109%\n",
      "layer   3  Sparsity: 72.2095%\n",
      "total_backward_count 1155220 real_backward_count 90661   7.848%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.119705/  1.351755, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.03 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 73.8195%\n",
      "layer   3  Sparsity: 73.1858%\n",
      "total_backward_count 1165010 real_backward_count 90989   7.810%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.125420/  1.372152, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0888%\n",
      "layer   2  Sparsity: 73.6787%\n",
      "layer   3  Sparsity: 73.1448%\n",
      "total_backward_count 1174800 real_backward_count 91407   7.781%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.126185/  1.348281, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 73.7891%\n",
      "layer   3  Sparsity: 72.7446%\n",
      "total_backward_count 1184590 real_backward_count 91843   7.753%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.124934/  1.343869, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.93 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   2  Sparsity: 73.6782%\n",
      "layer   3  Sparsity: 72.5317%\n",
      "total_backward_count 1194380 real_backward_count 92210   7.720%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.118083/  1.396407, val:  77.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   2  Sparsity: 73.6149%\n",
      "layer   3  Sparsity: 72.2923%\n",
      "total_backward_count 1204170 real_backward_count 92610   7.691%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.113221/  1.375677, val:  80.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   2  Sparsity: 73.7089%\n",
      "layer   3  Sparsity: 72.5897%\n",
      "total_backward_count 1213960 real_backward_count 92965   7.658%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.114087/  1.369761, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0410%\n",
      "layer   2  Sparsity: 73.8176%\n",
      "layer   3  Sparsity: 72.6947%\n",
      "total_backward_count 1223750 real_backward_count 93360   7.629%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.106904/  1.374324, val:  80.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.40 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0581%\n",
      "layer   2  Sparsity: 73.9211%\n",
      "layer   3  Sparsity: 72.4813%\n",
      "total_backward_count 1233540 real_backward_count 93755   7.600%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.113513/  1.347399, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.28 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0715%\n",
      "layer   2  Sparsity: 73.6261%\n",
      "layer   3  Sparsity: 72.7126%\n",
      "total_backward_count 1243330 real_backward_count 94146   7.572%\n",
      "fc layer 3 self.abs_max_out: 709.0\n",
      "fc layer 3 self.abs_max_out: 715.0\n",
      "fc layer 3 self.abs_max_out: 741.0\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.101893/  1.334760, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0538%\n",
      "layer   2  Sparsity: 73.4865%\n",
      "layer   3  Sparsity: 72.4924%\n",
      "total_backward_count 1253120 real_backward_count 94527   7.543%\n",
      "fc layer 1 self.abs_max_out: 4200.0\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.104884/  1.351738, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.08 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0888%\n",
      "layer   2  Sparsity: 73.6689%\n",
      "layer   3  Sparsity: 72.5334%\n",
      "total_backward_count 1262910 real_backward_count 94891   7.514%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.100055/  1.336491, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.57 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0556%\n",
      "layer   2  Sparsity: 73.6915%\n",
      "layer   3  Sparsity: 72.3117%\n",
      "total_backward_count 1272700 real_backward_count 95301   7.488%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.090195/  1.318716, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0517%\n",
      "layer   2  Sparsity: 73.7608%\n",
      "layer   3  Sparsity: 72.4870%\n",
      "total_backward_count 1282490 real_backward_count 95677   7.460%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.073153/  1.319622, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.29 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   2  Sparsity: 73.6472%\n",
      "layer   3  Sparsity: 72.3262%\n",
      "total_backward_count 1292280 real_backward_count 96026   7.431%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.079152/  1.334657, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.56 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   2  Sparsity: 73.5276%\n",
      "layer   3  Sparsity: 72.4373%\n",
      "total_backward_count 1302070 real_backward_count 96349   7.400%\n",
      "fc layer 1 self.abs_max_out: 4248.0\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.086164/  1.327288, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0868%\n",
      "layer   2  Sparsity: 73.3937%\n",
      "layer   3  Sparsity: 72.2234%\n",
      "total_backward_count 1311860 real_backward_count 96692   7.371%\n",
      "fc layer 1 self.abs_max_out: 4321.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.079922/  1.339088, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 73.5865%\n",
      "layer   3  Sparsity: 72.5762%\n",
      "total_backward_count 1321650 real_backward_count 97082   7.346%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.075404/  1.301503, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0959%\n",
      "layer   2  Sparsity: 73.4302%\n",
      "layer   3  Sparsity: 72.5619%\n",
      "total_backward_count 1331440 real_backward_count 97428   7.317%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.072409/  1.295035, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.89 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0514%\n",
      "layer   2  Sparsity: 73.4710%\n",
      "layer   3  Sparsity: 72.5664%\n",
      "total_backward_count 1341230 real_backward_count 97773   7.290%\n",
      "fc layer 3 self.abs_max_out: 744.0\n",
      "fc layer 2 self.abs_max_out: 2090.0\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.070272/  1.323590, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0739%\n",
      "layer   2  Sparsity: 73.5507%\n",
      "layer   3  Sparsity: 72.2969%\n",
      "total_backward_count 1351020 real_backward_count 98151   7.265%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.076559/  1.315568, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0818%\n",
      "layer   2  Sparsity: 73.6262%\n",
      "layer   3  Sparsity: 72.3554%\n",
      "total_backward_count 1360810 real_backward_count 98506   7.239%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.060070/  1.327546, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.68 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0729%\n",
      "layer   2  Sparsity: 73.6744%\n",
      "layer   3  Sparsity: 72.5520%\n",
      "total_backward_count 1370600 real_backward_count 98865   7.213%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.062727/  1.332249, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.97 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0835%\n",
      "layer   2  Sparsity: 73.8037%\n",
      "layer   3  Sparsity: 72.9858%\n",
      "total_backward_count 1380390 real_backward_count 99188   7.186%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.064080/  1.288878, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.78 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0381%\n",
      "layer   2  Sparsity: 73.5979%\n",
      "layer   3  Sparsity: 72.9596%\n",
      "total_backward_count 1390180 real_backward_count 99462   7.155%\n",
      "fc layer 3 self.abs_max_out: 750.0\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.066675/  1.315740, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.25 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1090%\n",
      "layer   2  Sparsity: 73.6869%\n",
      "layer   3  Sparsity: 73.2615%\n",
      "total_backward_count 1399970 real_backward_count 99809   7.129%\n",
      "fc layer 3 self.abs_max_out: 768.0\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.064382/  1.299901, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0962%\n",
      "layer   2  Sparsity: 73.7783%\n",
      "layer   3  Sparsity: 72.9577%\n",
      "total_backward_count 1409760 real_backward_count 100127   7.102%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.074484/  1.326182, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.29 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0518%\n",
      "layer   2  Sparsity: 73.6265%\n",
      "layer   3  Sparsity: 73.1144%\n",
      "total_backward_count 1419550 real_backward_count 100492   7.079%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.081148/  1.322466, val:  85.00%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 73.7942%\n",
      "layer   3  Sparsity: 73.1995%\n",
      "total_backward_count 1429340 real_backward_count 100818   7.053%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.081072/  1.324473, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0636%\n",
      "layer   2  Sparsity: 73.9030%\n",
      "layer   3  Sparsity: 73.6765%\n",
      "total_backward_count 1439130 real_backward_count 101125   7.027%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.082699/  1.314213, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0349%\n",
      "layer   2  Sparsity: 73.8349%\n",
      "layer   3  Sparsity: 73.5174%\n",
      "total_backward_count 1448920 real_backward_count 101429   7.000%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.070909/  1.305897, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 73.7197%\n",
      "layer   3  Sparsity: 73.4076%\n",
      "total_backward_count 1458710 real_backward_count 101749   6.975%\n",
      "fc layer 3 self.abs_max_out: 775.0\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.059287/  1.292261, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   2  Sparsity: 73.9110%\n",
      "layer   3  Sparsity: 73.5056%\n",
      "total_backward_count 1468500 real_backward_count 102094   6.952%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.073441/  1.324118, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.88 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0783%\n",
      "layer   2  Sparsity: 73.8195%\n",
      "layer   3  Sparsity: 73.4123%\n",
      "total_backward_count 1478290 real_backward_count 102431   6.929%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.065516/  1.320721, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.04 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0503%\n",
      "layer   2  Sparsity: 73.7080%\n",
      "layer   3  Sparsity: 73.1820%\n",
      "total_backward_count 1488080 real_backward_count 102734   6.904%\n",
      "lif layer 2 self.abs_max_v: 2724.0\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.075867/  1.328199, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.34 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   2  Sparsity: 73.2891%\n",
      "layer   3  Sparsity: 72.8082%\n",
      "total_backward_count 1497870 real_backward_count 103029   6.878%\n",
      "lif layer 2 self.abs_max_v: 2727.0\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.060551/  1.308996, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   2  Sparsity: 73.5140%\n",
      "layer   3  Sparsity: 72.7962%\n",
      "total_backward_count 1507660 real_backward_count 103355   6.855%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.043549/  1.321675, val:  80.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1078%\n",
      "layer   2  Sparsity: 73.7492%\n",
      "layer   3  Sparsity: 72.7943%\n",
      "total_backward_count 1517450 real_backward_count 103655   6.831%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.043630/  1.291606, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0892%\n",
      "layer   2  Sparsity: 73.7546%\n",
      "layer   3  Sparsity: 72.1506%\n",
      "total_backward_count 1527240 real_backward_count 103970   6.808%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.048361/  1.287397, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.71 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0344%\n",
      "layer   2  Sparsity: 73.8148%\n",
      "layer   3  Sparsity: 72.5447%\n",
      "total_backward_count 1537030 real_backward_count 104325   6.787%\n",
      "fc layer 2 self.abs_max_out: 2114.0\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.042814/  1.295679, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0498%\n",
      "layer   2  Sparsity: 73.7578%\n",
      "layer   3  Sparsity: 73.0743%\n",
      "total_backward_count 1546820 real_backward_count 104627   6.764%\n",
      "fc layer 3 self.abs_max_out: 800.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.035872/  1.283998, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0483%\n",
      "layer   2  Sparsity: 73.6998%\n",
      "layer   3  Sparsity: 73.0784%\n",
      "total_backward_count 1556610 real_backward_count 104896   6.739%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.040567/  1.269358, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.13 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0936%\n",
      "layer   2  Sparsity: 73.6982%\n",
      "layer   3  Sparsity: 72.8356%\n",
      "total_backward_count 1566400 real_backward_count 105185   6.715%\n",
      "fc layer 3 self.abs_max_out: 834.0\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.035450/  1.279987, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   2  Sparsity: 73.6504%\n",
      "layer   3  Sparsity: 72.7440%\n",
      "total_backward_count 1576190 real_backward_count 105487   6.693%\n",
      "fc layer 2 self.abs_max_out: 2170.0\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.030939/  1.291374, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0715%\n",
      "layer   2  Sparsity: 73.9156%\n",
      "layer   3  Sparsity: 72.4770%\n",
      "total_backward_count 1585980 real_backward_count 105761   6.668%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.032948/  1.261950, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0789%\n",
      "layer   2  Sparsity: 73.5199%\n",
      "layer   3  Sparsity: 72.3126%\n",
      "total_backward_count 1595770 real_backward_count 106049   6.646%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.023819/  1.255175, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0729%\n",
      "layer   2  Sparsity: 73.5882%\n",
      "layer   3  Sparsity: 72.3540%\n",
      "total_backward_count 1605560 real_backward_count 106295   6.620%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.005405/  1.287944, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0648%\n",
      "layer   2  Sparsity: 73.8635%\n",
      "layer   3  Sparsity: 72.5292%\n",
      "total_backward_count 1615350 real_backward_count 106550   6.596%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.010565/  1.256493, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.61 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   2  Sparsity: 73.9707%\n",
      "layer   3  Sparsity: 72.6579%\n",
      "total_backward_count 1625140 real_backward_count 106817   6.573%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.026170/  1.298957, val:  78.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0316%\n",
      "layer   2  Sparsity: 73.8082%\n",
      "layer   3  Sparsity: 72.5767%\n",
      "total_backward_count 1634930 real_backward_count 107092   6.550%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.021336/  1.263273, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0995%\n",
      "layer   2  Sparsity: 73.8778%\n",
      "layer   3  Sparsity: 72.7275%\n",
      "total_backward_count 1644720 real_backward_count 107350   6.527%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.022796/  1.277460, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.22 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 73.5889%\n",
      "layer   3  Sparsity: 72.9911%\n",
      "total_backward_count 1654510 real_backward_count 107629   6.505%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.031193/  1.273518, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.87 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0897%\n",
      "layer   2  Sparsity: 73.6516%\n",
      "layer   3  Sparsity: 72.8583%\n",
      "total_backward_count 1664300 real_backward_count 107912   6.484%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.025651/  1.287791, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.96 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1197%\n",
      "layer   2  Sparsity: 73.9328%\n",
      "layer   3  Sparsity: 72.7733%\n",
      "total_backward_count 1674090 real_backward_count 108200   6.463%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.037315/  1.291208, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0365%\n",
      "layer   2  Sparsity: 73.6760%\n",
      "layer   3  Sparsity: 73.0689%\n",
      "total_backward_count 1683880 real_backward_count 108475   6.442%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.042631/  1.291713, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1231%\n",
      "layer   2  Sparsity: 73.4785%\n",
      "layer   3  Sparsity: 72.8016%\n",
      "total_backward_count 1693670 real_backward_count 108734   6.420%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.030461/  1.279746, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1047%\n",
      "layer   2  Sparsity: 73.4780%\n",
      "layer   3  Sparsity: 72.7615%\n",
      "total_backward_count 1703460 real_backward_count 109004   6.399%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.026398/  1.260353, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   2  Sparsity: 73.5142%\n",
      "layer   3  Sparsity: 72.7599%\n",
      "total_backward_count 1713250 real_backward_count 109274   6.378%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.022331/  1.285258, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   2  Sparsity: 73.6519%\n",
      "layer   3  Sparsity: 72.7576%\n",
      "total_backward_count 1723040 real_backward_count 109528   6.357%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.025945/  1.276260, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.86 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0554%\n",
      "layer   2  Sparsity: 73.7637%\n",
      "layer   3  Sparsity: 72.6050%\n",
      "total_backward_count 1732830 real_backward_count 109777   6.335%\n",
      "fc layer 1 self.abs_max_out: 4364.0\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.022419/  1.275951, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.85 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0833%\n",
      "layer   2  Sparsity: 73.8463%\n",
      "layer   3  Sparsity: 72.2098%\n",
      "total_backward_count 1742620 real_backward_count 110018   6.313%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.026995/  1.265090, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0983%\n",
      "layer   2  Sparsity: 73.8313%\n",
      "layer   3  Sparsity: 71.8888%\n",
      "total_backward_count 1752410 real_backward_count 110321   6.295%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.016678/  1.255649, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.71 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0571%\n",
      "layer   2  Sparsity: 73.8068%\n",
      "layer   3  Sparsity: 72.0468%\n",
      "total_backward_count 1762200 real_backward_count 110583   6.275%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.002649/  1.282008, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.00 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0895%\n",
      "layer   2  Sparsity: 73.6834%\n",
      "layer   3  Sparsity: 72.4863%\n",
      "total_backward_count 1771990 real_backward_count 110869   6.257%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.014412/  1.274312, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   2  Sparsity: 73.5658%\n",
      "layer   3  Sparsity: 72.8401%\n",
      "total_backward_count 1781780 real_backward_count 111107   6.236%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.011403/  1.284056, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0500%\n",
      "layer   2  Sparsity: 73.5859%\n",
      "layer   3  Sparsity: 72.6144%\n",
      "total_backward_count 1791570 real_backward_count 111358   6.216%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.017752/  1.273712, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0563%\n",
      "layer   2  Sparsity: 73.7078%\n",
      "layer   3  Sparsity: 72.5335%\n",
      "total_backward_count 1801360 real_backward_count 111629   6.197%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.011336/  1.260476, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   2  Sparsity: 73.8744%\n",
      "layer   3  Sparsity: 72.3961%\n",
      "total_backward_count 1811150 real_backward_count 111874   6.177%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  0.998571/  1.282843, val:  77.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0358%\n",
      "layer   2  Sparsity: 73.9222%\n",
      "layer   3  Sparsity: 72.6576%\n",
      "total_backward_count 1820940 real_backward_count 112094   6.156%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  0.993474/  1.255216, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.08 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 73.8408%\n",
      "layer   3  Sparsity: 72.8996%\n",
      "total_backward_count 1830730 real_backward_count 112301   6.134%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  0.990065/  1.258488, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1249%\n",
      "layer   2  Sparsity: 73.7219%\n",
      "layer   3  Sparsity: 73.0079%\n",
      "total_backward_count 1840520 real_backward_count 112529   6.114%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.006541/  1.280529, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   2  Sparsity: 73.7488%\n",
      "layer   3  Sparsity: 72.9589%\n",
      "total_backward_count 1850310 real_backward_count 112750   6.094%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.001481/  1.256390, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1132%\n",
      "layer   2  Sparsity: 73.7391%\n",
      "layer   3  Sparsity: 73.1334%\n",
      "total_backward_count 1860100 real_backward_count 112999   6.075%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.003114/  1.254720, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   2  Sparsity: 73.7448%\n",
      "layer   3  Sparsity: 73.0642%\n",
      "total_backward_count 1869890 real_backward_count 113229   6.055%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.012371/  1.257954, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.00 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   2  Sparsity: 73.8947%\n",
      "layer   3  Sparsity: 73.0558%\n",
      "total_backward_count 1879680 real_backward_count 113472   6.037%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.009651/  1.264467, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0934%\n",
      "layer   2  Sparsity: 73.8548%\n",
      "layer   3  Sparsity: 73.2055%\n",
      "total_backward_count 1889470 real_backward_count 113712   6.018%\n",
      "fc layer 1 self.abs_max_out: 4426.0\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.016754/  1.268661, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0404%\n",
      "layer   2  Sparsity: 73.5870%\n",
      "layer   3  Sparsity: 73.4146%\n",
      "total_backward_count 1899260 real_backward_count 113948   6.000%\n",
      "fc layer 3 self.abs_max_out: 845.0\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.007226/  1.273416, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0611%\n",
      "layer   2  Sparsity: 73.5592%\n",
      "layer   3  Sparsity: 73.2179%\n",
      "total_backward_count 1909050 real_backward_count 114177   5.981%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.018758/  1.275236, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0920%\n",
      "layer   2  Sparsity: 73.5833%\n",
      "layer   3  Sparsity: 72.9845%\n",
      "total_backward_count 1918840 real_backward_count 114419   5.963%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.001408/  1.255403, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1108%\n",
      "layer   2  Sparsity: 73.5652%\n",
      "layer   3  Sparsity: 72.6568%\n",
      "total_backward_count 1928630 real_backward_count 114627   5.943%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  0.991051/  1.239603, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.31 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   2  Sparsity: 73.7333%\n",
      "layer   3  Sparsity: 72.6715%\n",
      "total_backward_count 1938420 real_backward_count 114829   5.924%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  0.975574/  1.244573, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.84 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0718%\n",
      "layer   2  Sparsity: 73.7897%\n",
      "layer   3  Sparsity: 72.8252%\n",
      "total_backward_count 1948210 real_backward_count 115046   5.905%\n",
      "lif layer 2 self.abs_max_v: 2762.0\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  0.974740/  1.239666, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.42 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0832%\n",
      "layer   2  Sparsity: 73.8258%\n",
      "layer   3  Sparsity: 73.0536%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561041c2e10e485eaad7e21c2bc3d367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñà‚ñá‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.97474</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.85833</td></tr><tr><td>val_loss</td><td>1.23967</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dulcet-sweep-2</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8s99kqf2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8s99kqf2</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_002354-8s99kqf2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kwggzj7j with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 31287\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_042711-kwggzj7j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kwggzj7j' target=\"_blank\">snowy-sweep-3</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kwggzj7j' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kwggzj7j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251118_042721_003', 'my_seed': 31287, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5.5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 215.0\n",
      "lif layer 1 self.abs_max_v: 215.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 133.0\n",
      "lif layer 2 self.abs_max_v: 133.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 262.0\n",
      "lif layer 1 self.abs_max_v: 298.0\n",
      "fc layer 2 self.abs_max_out: 203.0\n",
      "lif layer 2 self.abs_max_v: 229.5\n",
      "fc layer 3 self.abs_max_out: 42.0\n",
      "fc layer 2 self.abs_max_out: 270.0\n",
      "lif layer 2 self.abs_max_v: 301.5\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "fc layer 2 self.abs_max_out: 292.0\n",
      "lif layer 2 self.abs_max_v: 381.5\n",
      "lif layer 1 self.abs_max_v: 330.5\n",
      "fc layer 1 self.abs_max_out: 311.0\n",
      "lif layer 1 self.abs_max_v: 339.5\n",
      "fc layer 2 self.abs_max_out: 302.0\n",
      "lif layer 1 self.abs_max_v: 361.0\n",
      "lif layer 2 self.abs_max_v: 445.5\n",
      "fc layer 3 self.abs_max_out: 140.0\n",
      "fc layer 1 self.abs_max_out: 332.0\n",
      "fc layer 2 self.abs_max_out: 330.0\n",
      "lif layer 2 self.abs_max_v: 553.0\n",
      "fc layer 3 self.abs_max_out: 158.0\n",
      "fc layer 1 self.abs_max_out: 460.0\n",
      "lif layer 1 self.abs_max_v: 460.0\n",
      "fc layer 2 self.abs_max_out: 414.0\n",
      "fc layer 1 self.abs_max_out: 583.0\n",
      "lif layer 1 self.abs_max_v: 583.0\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "lif layer 1 self.abs_max_v: 784.5\n",
      "fc layer 2 self.abs_max_out: 415.0\n",
      "lif layer 2 self.abs_max_v: 564.0\n",
      "fc layer 1 self.abs_max_out: 586.0\n",
      "fc layer 3 self.abs_max_out: 169.0\n",
      "fc layer 1 self.abs_max_out: 665.0\n",
      "fc layer 2 self.abs_max_out: 423.0\n",
      "fc layer 2 self.abs_max_out: 454.0\n",
      "fc layer 2 self.abs_max_out: 486.0\n",
      "fc layer 1 self.abs_max_out: 674.0\n",
      "fc layer 2 self.abs_max_out: 607.0\n",
      "lif layer 2 self.abs_max_v: 712.5\n",
      "fc layer 3 self.abs_max_out: 181.0\n",
      "lif layer 2 self.abs_max_v: 718.0\n",
      "fc layer 3 self.abs_max_out: 191.0\n",
      "lif layer 2 self.abs_max_v: 749.5\n",
      "lif layer 2 self.abs_max_v: 785.5\n",
      "fc layer 3 self.abs_max_out: 202.0\n",
      "fc layer 3 self.abs_max_out: 213.0\n",
      "fc layer 1 self.abs_max_out: 752.0\n",
      "fc layer 1 self.abs_max_out: 782.0\n",
      "fc layer 1 self.abs_max_out: 821.0\n",
      "lif layer 1 self.abs_max_v: 821.0\n",
      "lif layer 2 self.abs_max_v: 816.5\n",
      "fc layer 1 self.abs_max_out: 872.0\n",
      "lif layer 1 self.abs_max_v: 923.0\n",
      "fc layer 2 self.abs_max_out: 608.0\n",
      "lif layer 1 self.abs_max_v: 948.5\n",
      "fc layer 3 self.abs_max_out: 215.0\n",
      "lif layer 2 self.abs_max_v: 833.0\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "fc layer 3 self.abs_max_out: 248.0\n",
      "fc layer 2 self.abs_max_out: 611.0\n",
      "fc layer 1 self.abs_max_out: 886.0\n",
      "fc layer 1 self.abs_max_out: 955.0\n",
      "lif layer 1 self.abs_max_v: 955.0\n",
      "fc layer 2 self.abs_max_out: 646.0\n",
      "fc layer 2 self.abs_max_out: 701.0\n",
      "lif layer 2 self.abs_max_v: 843.0\n",
      "lif layer 2 self.abs_max_v: 867.5\n",
      "lif layer 2 self.abs_max_v: 885.5\n",
      "lif layer 1 self.abs_max_v: 966.0\n",
      "fc layer 3 self.abs_max_out: 257.0\n",
      "fc layer 3 self.abs_max_out: 291.0\n",
      "lif layer 2 self.abs_max_v: 934.0\n",
      "lif layer 2 self.abs_max_v: 981.0\n",
      "lif layer 2 self.abs_max_v: 991.5\n",
      "lif layer 1 self.abs_max_v: 994.0\n",
      "lif layer 2 self.abs_max_v: 994.0\n",
      "lif layer 2 self.abs_max_v: 1030.0\n",
      "lif layer 2 self.abs_max_v: 1080.0\n",
      "fc layer 2 self.abs_max_out: 826.0\n",
      "lif layer 2 self.abs_max_v: 1115.0\n",
      "fc layer 3 self.abs_max_out: 294.0\n",
      "fc layer 2 self.abs_max_out: 862.0\n",
      "lif layer 1 self.abs_max_v: 1113.0\n",
      "fc layer 2 self.abs_max_out: 889.0\n",
      "fc layer 2 self.abs_max_out: 894.0\n",
      "fc layer 2 self.abs_max_out: 911.0\n",
      "lif layer 2 self.abs_max_v: 1201.5\n",
      "lif layer 2 self.abs_max_v: 1221.0\n",
      "fc layer 3 self.abs_max_out: 296.0\n",
      "fc layer 1 self.abs_max_out: 1009.0\n",
      "fc layer 2 self.abs_max_out: 939.0\n",
      "fc layer 1 self.abs_max_out: 1086.0\n",
      "fc layer 1 self.abs_max_out: 1148.0\n",
      "lif layer 1 self.abs_max_v: 1148.0\n",
      "fc layer 2 self.abs_max_out: 990.0\n",
      "fc layer 2 self.abs_max_out: 1021.0\n",
      "lif layer 1 self.abs_max_v: 1155.5\n",
      "fc layer 1 self.abs_max_out: 1164.0\n",
      "lif layer 1 self.abs_max_v: 1244.5\n",
      "fc layer 2 self.abs_max_out: 1033.0\n",
      "lif layer 1 self.abs_max_v: 1265.0\n",
      "lif layer 1 self.abs_max_v: 1363.0\n",
      "lif layer 1 self.abs_max_v: 1485.5\n",
      "lif layer 1 self.abs_max_v: 1735.0\n",
      "fc layer 3 self.abs_max_out: 319.0\n",
      "fc layer 3 self.abs_max_out: 332.0\n",
      "lif layer 2 self.abs_max_v: 1244.0\n",
      "fc layer 1 self.abs_max_out: 1167.0\n",
      "fc layer 3 self.abs_max_out: 334.0\n",
      "fc layer 3 self.abs_max_out: 357.0\n",
      "fc layer 1 self.abs_max_out: 1180.0\n",
      "lif layer 1 self.abs_max_v: 1766.5\n",
      "lif layer 2 self.abs_max_v: 1249.0\n",
      "lif layer 2 self.abs_max_v: 1260.0\n",
      "fc layer 1 self.abs_max_out: 1204.0\n",
      "lif layer 2 self.abs_max_v: 1262.5\n",
      "fc layer 1 self.abs_max_out: 1220.0\n",
      "fc layer 1 self.abs_max_out: 1276.0\n",
      "fc layer 1 self.abs_max_out: 1310.0\n",
      "fc layer 1 self.abs_max_out: 1314.0\n",
      "fc layer 1 self.abs_max_out: 1367.0\n",
      "lif layer 2 self.abs_max_v: 1293.0\n",
      "lif layer 2 self.abs_max_v: 1301.0\n",
      "fc layer 1 self.abs_max_out: 1417.0\n",
      "fc layer 1 self.abs_max_out: 1441.0\n",
      "fc layer 2 self.abs_max_out: 1052.0\n",
      "fc layer 1 self.abs_max_out: 1442.0\n",
      "lif layer 1 self.abs_max_v: 1810.0\n",
      "lif layer 1 self.abs_max_v: 1868.0\n",
      "lif layer 1 self.abs_max_v: 2039.0\n",
      "lif layer 1 self.abs_max_v: 2115.5\n",
      "lif layer 1 self.abs_max_v: 2442.5\n",
      "lif layer 1 self.abs_max_v: 2568.5\n",
      "lif layer 2 self.abs_max_v: 1301.5\n",
      "lif layer 2 self.abs_max_v: 1303.5\n",
      "lif layer 2 self.abs_max_v: 1305.5\n",
      "lif layer 2 self.abs_max_v: 1385.0\n",
      "fc layer 1 self.abs_max_out: 1514.0\n",
      "fc layer 1 self.abs_max_out: 1522.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.771998/  1.979311, val:  33.33%, val_best:  33.33%, tr:  97.04%, tr_best:  97.04%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 77.1258%\n",
      "layer   3  Sparsity: 72.4762%\n",
      "total_backward_count 9790 real_backward_count 2194  22.411%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 385.0\n",
      "fc layer 1 self.abs_max_out: 1551.0\n",
      "fc layer 1 self.abs_max_out: 1584.0\n",
      "fc layer 1 self.abs_max_out: 1621.0\n",
      "fc layer 2 self.abs_max_out: 1054.0\n",
      "fc layer 1 self.abs_max_out: 1624.0\n",
      "fc layer 2 self.abs_max_out: 1107.0\n",
      "fc layer 3 self.abs_max_out: 388.0\n",
      "fc layer 3 self.abs_max_out: 401.0\n",
      "fc layer 1 self.abs_max_out: 1663.0\n",
      "lif layer 1 self.abs_max_v: 2729.5\n",
      "lif layer 1 self.abs_max_v: 2781.0\n",
      "fc layer 1 self.abs_max_out: 1699.0\n",
      "fc layer 1 self.abs_max_out: 1717.0\n",
      "fc layer 1 self.abs_max_out: 1788.0\n",
      "fc layer 1 self.abs_max_out: 1880.0\n",
      "fc layer 2 self.abs_max_out: 1127.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.670887/  1.939242, val:  40.00%, val_best:  40.00%, tr:  98.98%, tr_best:  98.98%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   2  Sparsity: 77.9489%\n",
      "layer   3  Sparsity: 70.1233%\n",
      "total_backward_count 19580 real_backward_count 3755  19.178%\n",
      "lif layer 2 self.abs_max_v: 1393.0\n",
      "fc layer 2 self.abs_max_out: 1141.0\n",
      "lif layer 2 self.abs_max_v: 1433.0\n",
      "lif layer 2 self.abs_max_v: 1465.5\n",
      "lif layer 2 self.abs_max_v: 1487.0\n",
      "lif layer 1 self.abs_max_v: 3010.5\n",
      "lif layer 1 self.abs_max_v: 3068.0\n",
      "fc layer 1 self.abs_max_out: 1993.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.641018/  1.886161, val:  38.75%, val_best:  40.00%, tr:  99.28%, tr_best:  99.28%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0470%\n",
      "layer   2  Sparsity: 77.5920%\n",
      "layer   3  Sparsity: 69.4554%\n",
      "total_backward_count 29370 real_backward_count 5239  17.838%\n",
      "lif layer 2 self.abs_max_v: 1496.0\n",
      "lif layer 2 self.abs_max_v: 1501.0\n",
      "lif layer 2 self.abs_max_v: 1604.5\n",
      "fc layer 2 self.abs_max_out: 1190.0\n",
      "fc layer 3 self.abs_max_out: 405.0\n",
      "fc layer 2 self.abs_max_out: 1197.0\n",
      "fc layer 3 self.abs_max_out: 420.0\n",
      "fc layer 1 self.abs_max_out: 2089.0\n",
      "lif layer 1 self.abs_max_v: 3142.5\n",
      "lif layer 1 self.abs_max_v: 3203.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.603437/  1.859254, val:  51.25%, val_best:  51.25%, tr:  99.28%, tr_best:  99.28%, epoch time: 72.01 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   2  Sparsity: 78.4383%\n",
      "layer   3  Sparsity: 69.1381%\n",
      "total_backward_count 39160 real_backward_count 6695  17.097%\n",
      "fc layer 2 self.abs_max_out: 1234.0\n",
      "fc layer 2 self.abs_max_out: 1271.0\n",
      "fc layer 3 self.abs_max_out: 439.0\n",
      "lif layer 1 self.abs_max_v: 3236.0\n",
      "lif layer 1 self.abs_max_v: 3437.5\n",
      "lif layer 1 self.abs_max_v: 3527.0\n",
      "fc layer 1 self.abs_max_out: 2110.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.584204/  1.863311, val:  41.25%, val_best:  51.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   2  Sparsity: 77.4534%\n",
      "layer   3  Sparsity: 68.5143%\n",
      "total_backward_count 48950 real_backward_count 8048  16.441%\n",
      "fc layer 3 self.abs_max_out: 450.0\n",
      "fc layer 1 self.abs_max_out: 2125.0\n",
      "fc layer 1 self.abs_max_out: 2186.0\n",
      "fc layer 3 self.abs_max_out: 462.0\n",
      "fc layer 1 self.abs_max_out: 2340.0\n",
      "lif layer 1 self.abs_max_v: 3754.5\n",
      "lif layer 1 self.abs_max_v: 3915.5\n",
      "lif layer 2 self.abs_max_v: 1694.0\n",
      "lif layer 2 self.abs_max_v: 1735.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.569926/  1.844982, val:  56.25%, val_best:  56.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.11 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0369%\n",
      "layer   2  Sparsity: 76.7362%\n",
      "layer   3  Sparsity: 67.9319%\n",
      "total_backward_count 58740 real_backward_count 9397  15.998%\n",
      "fc layer 3 self.abs_max_out: 468.0\n",
      "fc layer 2 self.abs_max_out: 1289.0\n",
      "lif layer 2 self.abs_max_v: 1751.5\n",
      "lif layer 2 self.abs_max_v: 1880.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.569110/  1.819962, val:  42.08%, val_best:  56.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.36 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1028%\n",
      "layer   2  Sparsity: 76.1586%\n",
      "layer   3  Sparsity: 67.9326%\n",
      "total_backward_count 68530 real_backward_count 10729  15.656%\n",
      "fc layer 1 self.abs_max_out: 2357.0\n",
      "lif layer 1 self.abs_max_v: 3943.5\n",
      "fc layer 1 self.abs_max_out: 2376.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.554126/  1.796451, val:  49.58%, val_best:  56.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 71.86 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 76.6475%\n",
      "layer   3  Sparsity: 68.5560%\n",
      "total_backward_count 78320 real_backward_count 11999  15.320%\n",
      "fc layer 2 self.abs_max_out: 1297.0\n",
      "fc layer 3 self.abs_max_out: 470.0\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "fc layer 1 self.abs_max_out: 2489.0\n",
      "fc layer 1 self.abs_max_out: 2563.0\n",
      "lif layer 1 self.abs_max_v: 4243.5\n",
      "lif layer 1 self.abs_max_v: 4408.0\n",
      "fc layer 2 self.abs_max_out: 1314.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.527085/  1.736302, val:  58.75%, val_best:  58.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0440%\n",
      "layer   2  Sparsity: 75.8652%\n",
      "layer   3  Sparsity: 68.9727%\n",
      "total_backward_count 88110 real_backward_count 13303  15.098%\n",
      "fc layer 3 self.abs_max_out: 479.0\n",
      "lif layer 2 self.abs_max_v: 1889.5\n",
      "fc layer 3 self.abs_max_out: 481.0\n",
      "lif layer 2 self.abs_max_v: 1926.0\n",
      "fc layer 3 self.abs_max_out: 504.0\n",
      "fc layer 1 self.abs_max_out: 2592.0\n",
      "fc layer 1 self.abs_max_out: 2698.0\n",
      "fc layer 3 self.abs_max_out: 510.0\n",
      "fc layer 2 self.abs_max_out: 1353.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.495358/  1.773709, val:  47.08%, val_best:  58.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 72.49 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1258%\n",
      "layer   2  Sparsity: 75.9463%\n",
      "layer   3  Sparsity: 69.3096%\n",
      "total_backward_count 97900 real_backward_count 14554  14.866%\n",
      "fc layer 3 self.abs_max_out: 519.0\n",
      "lif layer 1 self.abs_max_v: 4532.5\n",
      "fc layer 2 self.abs_max_out: 1387.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.488255/  1.737693, val:  50.83%, val_best:  58.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0880%\n",
      "layer   2  Sparsity: 75.6542%\n",
      "layer   3  Sparsity: 68.9548%\n",
      "total_backward_count 107690 real_backward_count 15806  14.677%\n",
      "lif layer 2 self.abs_max_v: 1967.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.493596/  1.756970, val:  46.25%, val_best:  58.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   2  Sparsity: 75.8861%\n",
      "layer   3  Sparsity: 70.1057%\n",
      "total_backward_count 117480 real_backward_count 17037  14.502%\n",
      "fc layer 1 self.abs_max_out: 2788.0\n",
      "lif layer 1 self.abs_max_v: 4577.0\n",
      "lif layer 1 self.abs_max_v: 4754.5\n",
      "fc layer 2 self.abs_max_out: 1408.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.477363/  1.720870, val:  67.08%, val_best:  67.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.13 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0526%\n",
      "layer   2  Sparsity: 75.7930%\n",
      "layer   3  Sparsity: 69.9516%\n",
      "total_backward_count 127270 real_backward_count 18252  14.341%\n",
      "lif layer 2 self.abs_max_v: 1995.0\n",
      "fc layer 1 self.abs_max_out: 2817.0\n",
      "lif layer 1 self.abs_max_v: 4806.0\n",
      "lif layer 2 self.abs_max_v: 2036.5\n",
      "fc layer 2 self.abs_max_out: 1472.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.488814/  1.713457, val:  59.17%, val_best:  67.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 73.14 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   2  Sparsity: 75.9277%\n",
      "layer   3  Sparsity: 70.9287%\n",
      "total_backward_count 137060 real_backward_count 19483  14.215%\n",
      "lif layer 2 self.abs_max_v: 2042.5\n",
      "lif layer 2 self.abs_max_v: 2063.5\n",
      "lif layer 2 self.abs_max_v: 2183.5\n",
      "fc layer 1 self.abs_max_out: 3007.0\n",
      "lif layer 1 self.abs_max_v: 4923.5\n",
      "lif layer 1 self.abs_max_v: 5103.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.475310/  1.683777, val:  58.75%, val_best:  67.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0858%\n",
      "layer   2  Sparsity: 75.6179%\n",
      "layer   3  Sparsity: 70.5164%\n",
      "total_backward_count 146850 real_backward_count 20654  14.065%\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.469002/  1.697731, val:  65.00%, val_best:  67.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   2  Sparsity: 74.9515%\n",
      "layer   3  Sparsity: 70.3798%\n",
      "total_backward_count 156640 real_backward_count 21831  13.937%\n",
      "lif layer 2 self.abs_max_v: 2223.5\n",
      "lif layer 2 self.abs_max_v: 2234.0\n",
      "fc layer 3 self.abs_max_out: 550.0\n",
      "fc layer 3 self.abs_max_out: 554.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.467367/  1.673468, val:  62.08%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.72 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   2  Sparsity: 74.5373%\n",
      "layer   3  Sparsity: 69.7892%\n",
      "total_backward_count 166430 real_backward_count 23012  13.827%\n",
      "lif layer 2 self.abs_max_v: 2272.5\n",
      "lif layer 2 self.abs_max_v: 2278.5\n",
      "fc layer 3 self.abs_max_out: 596.0\n",
      "fc layer 3 self.abs_max_out: 604.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.439852/  1.709615, val:  44.17%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0942%\n",
      "layer   2  Sparsity: 75.0144%\n",
      "layer   3  Sparsity: 70.4699%\n",
      "total_backward_count 176220 real_backward_count 24154  13.707%\n",
      "lif layer 2 self.abs_max_v: 2317.5\n",
      "lif layer 2 self.abs_max_v: 2352.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.444064/  1.686211, val:  57.92%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   2  Sparsity: 75.3529%\n",
      "layer   3  Sparsity: 70.0192%\n",
      "total_backward_count 186010 real_backward_count 25296  13.599%\n",
      "fc layer 2 self.abs_max_out: 1476.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.437554/  1.669834, val:  64.58%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 74.9957%\n",
      "layer   3  Sparsity: 69.7507%\n",
      "total_backward_count 195800 real_backward_count 26443  13.505%\n",
      "lif layer 2 self.abs_max_v: 2423.0\n",
      "lif layer 2 self.abs_max_v: 2454.5\n",
      "lif layer 2 self.abs_max_v: 2499.5\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.432402/  1.683419, val:  63.75%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0959%\n",
      "layer   2  Sparsity: 74.7803%\n",
      "layer   3  Sparsity: 69.9005%\n",
      "total_backward_count 205590 real_backward_count 27545  13.398%\n",
      "fc layer 1 self.abs_max_out: 3061.0\n",
      "lif layer 1 self.abs_max_v: 5230.5\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.433343/  1.678786, val:  56.25%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.71 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0653%\n",
      "layer   2  Sparsity: 74.6085%\n",
      "layer   3  Sparsity: 71.2587%\n",
      "total_backward_count 215380 real_backward_count 28640  13.297%\n",
      "fc layer 1 self.abs_max_out: 3133.0\n",
      "lif layer 1 self.abs_max_v: 5333.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.434780/  1.663783, val:  55.00%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.52 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0550%\n",
      "layer   2  Sparsity: 74.8846%\n",
      "layer   3  Sparsity: 71.5521%\n",
      "total_backward_count 225170 real_backward_count 29701  13.190%\n",
      "fc layer 1 self.abs_max_out: 3225.0\n",
      "lif layer 1 self.abs_max_v: 5482.5\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.430569/  1.702317, val:  57.08%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   2  Sparsity: 74.5951%\n",
      "layer   3  Sparsity: 70.8910%\n",
      "total_backward_count 234960 real_backward_count 30749  13.087%\n",
      "lif layer 1 self.abs_max_v: 5538.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.409079/  1.648571, val:  70.42%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.66 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0758%\n",
      "layer   2  Sparsity: 74.4837%\n",
      "layer   3  Sparsity: 70.8886%\n",
      "total_backward_count 244750 real_backward_count 31800  12.993%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.395248/  1.612177, val:  82.08%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.12 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 74.6459%\n",
      "layer   3  Sparsity: 70.8549%\n",
      "total_backward_count 254540 real_backward_count 32817  12.893%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.386011/  1.631248, val:  60.83%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.46 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0969%\n",
      "layer   2  Sparsity: 74.9741%\n",
      "layer   3  Sparsity: 70.3413%\n",
      "total_backward_count 264330 real_backward_count 33818  12.794%\n",
      "fc layer 2 self.abs_max_out: 1507.0\n",
      "fc layer 3 self.abs_max_out: 608.0\n",
      "fc layer 1 self.abs_max_out: 3238.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.381070/  1.617767, val:  68.33%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.03 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0576%\n",
      "layer   2  Sparsity: 75.0119%\n",
      "layer   3  Sparsity: 70.8833%\n",
      "total_backward_count 274120 real_backward_count 34859  12.717%\n",
      "fc layer 2 self.abs_max_out: 1560.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.382433/  1.631946, val:  59.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.09 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0956%\n",
      "layer   2  Sparsity: 74.7094%\n",
      "layer   3  Sparsity: 71.3589%\n",
      "total_backward_count 283910 real_backward_count 35803  12.611%\n",
      "lif layer 2 self.abs_max_v: 2570.5\n",
      "lif layer 2 self.abs_max_v: 2659.5\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.375111/  1.619348, val:  73.33%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   2  Sparsity: 74.4491%\n",
      "layer   3  Sparsity: 71.8241%\n",
      "total_backward_count 293700 real_backward_count 36748  12.512%\n",
      "fc layer 1 self.abs_max_out: 3362.0\n",
      "lif layer 1 self.abs_max_v: 5726.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.368199/  1.604998, val:  58.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0966%\n",
      "layer   2  Sparsity: 74.7655%\n",
      "layer   3  Sparsity: 71.2413%\n",
      "total_backward_count 303490 real_backward_count 37724  12.430%\n",
      "fc layer 3 self.abs_max_out: 627.0\n",
      "fc layer 3 self.abs_max_out: 632.0\n",
      "fc layer 1 self.abs_max_out: 3386.0\n",
      "lif layer 1 self.abs_max_v: 5727.5\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.353214/  1.606346, val:  62.08%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1270%\n",
      "layer   2  Sparsity: 74.9541%\n",
      "layer   3  Sparsity: 71.4960%\n",
      "total_backward_count 313280 real_backward_count 38684  12.348%\n",
      "fc layer 2 self.abs_max_out: 1607.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.359511/  1.559020, val:  81.25%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.78 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1035%\n",
      "layer   2  Sparsity: 74.0210%\n",
      "layer   3  Sparsity: 71.3632%\n",
      "total_backward_count 323070 real_backward_count 39707  12.291%\n",
      "fc layer 2 self.abs_max_out: 1641.0\n",
      "fc layer 1 self.abs_max_out: 3423.0\n",
      "lif layer 1 self.abs_max_v: 5770.0\n",
      "fc layer 3 self.abs_max_out: 669.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.349465/  1.616294, val:  64.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.36 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1076%\n",
      "layer   2  Sparsity: 74.4747%\n",
      "layer   3  Sparsity: 71.8248%\n",
      "total_backward_count 332860 real_backward_count 40605  12.199%\n",
      "fc layer 1 self.abs_max_out: 3575.0\n",
      "lif layer 1 self.abs_max_v: 6056.5\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.365803/  1.596462, val:  75.42%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.12 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   2  Sparsity: 74.7389%\n",
      "layer   3  Sparsity: 71.5984%\n",
      "total_backward_count 342650 real_backward_count 41536  12.122%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.349367/  1.577129, val:  76.25%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.98 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0733%\n",
      "layer   2  Sparsity: 74.6334%\n",
      "layer   3  Sparsity: 71.8281%\n",
      "total_backward_count 352440 real_backward_count 42448  12.044%\n",
      "lif layer 1 self.abs_max_v: 6093.5\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.358974/  1.628599, val:  68.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.76 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0806%\n",
      "layer   2  Sparsity: 74.0560%\n",
      "layer   3  Sparsity: 71.8347%\n",
      "total_backward_count 362230 real_backward_count 43345  11.966%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.362188/  1.562507, val:  75.83%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0396%\n",
      "layer   2  Sparsity: 74.1870%\n",
      "layer   3  Sparsity: 71.4817%\n",
      "total_backward_count 372020 real_backward_count 44200  11.881%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.349184/  1.559509, val:  77.50%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   2  Sparsity: 73.8190%\n",
      "layer   3  Sparsity: 71.7678%\n",
      "total_backward_count 381810 real_backward_count 45074  11.805%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.339511/  1.598835, val:  64.58%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.76 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   2  Sparsity: 74.1185%\n",
      "layer   3  Sparsity: 71.8658%\n",
      "total_backward_count 391600 real_backward_count 45961  11.737%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.352531/  1.556430, val:  76.67%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.83 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1126%\n",
      "layer   2  Sparsity: 74.4061%\n",
      "layer   3  Sparsity: 72.0348%\n",
      "total_backward_count 401390 real_backward_count 46790  11.657%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.326707/  1.549511, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   2  Sparsity: 74.0126%\n",
      "layer   3  Sparsity: 71.1791%\n",
      "total_backward_count 411180 real_backward_count 47658  11.591%\n",
      "fc layer 2 self.abs_max_out: 1676.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.323282/  1.553283, val:  75.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.27 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   2  Sparsity: 73.8579%\n",
      "layer   3  Sparsity: 71.4237%\n",
      "total_backward_count 420970 real_backward_count 48531  11.528%\n",
      "fc layer 1 self.abs_max_out: 3584.0\n",
      "lif layer 1 self.abs_max_v: 6176.5\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.306950/  1.532810, val:  77.92%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.27 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   2  Sparsity: 74.3503%\n",
      "layer   3  Sparsity: 71.5796%\n",
      "total_backward_count 430760 real_backward_count 49380  11.463%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.316593/  1.559091, val:  63.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.31 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   2  Sparsity: 73.9301%\n",
      "layer   3  Sparsity: 71.2441%\n",
      "total_backward_count 440550 real_backward_count 50251  11.406%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.298963/  1.546686, val:  59.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   2  Sparsity: 74.2358%\n",
      "layer   3  Sparsity: 71.3360%\n",
      "total_backward_count 450340 real_backward_count 51075  11.341%\n",
      "fc layer 1 self.abs_max_out: 3678.0\n",
      "lif layer 1 self.abs_max_v: 6316.5\n",
      "fc layer 2 self.abs_max_out: 1748.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.303723/  1.571965, val:  71.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.98 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   2  Sparsity: 73.9560%\n",
      "layer   3  Sparsity: 72.0649%\n",
      "total_backward_count 460130 real_backward_count 51831  11.264%\n",
      "fc layer 1 self.abs_max_out: 3731.0\n",
      "lif layer 1 self.abs_max_v: 6407.5\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.296218/  1.522509, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.09 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1108%\n",
      "layer   2  Sparsity: 74.0016%\n",
      "layer   3  Sparsity: 72.2531%\n",
      "total_backward_count 469920 real_backward_count 52622  11.198%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.294745/  1.532006, val:  73.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.57 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0262%\n",
      "layer   2  Sparsity: 74.1695%\n",
      "layer   3  Sparsity: 72.7192%\n",
      "total_backward_count 479710 real_backward_count 53404  11.133%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.322784/  1.545800, val:  73.75%, val_best:  83.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.57 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1137%\n",
      "layer   2  Sparsity: 73.9993%\n",
      "layer   3  Sparsity: 72.6036%\n",
      "total_backward_count 489500 real_backward_count 54197  11.072%\n",
      "lif layer 1 self.abs_max_v: 6436.5\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.307358/  1.529218, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1132%\n",
      "layer   2  Sparsity: 73.6397%\n",
      "layer   3  Sparsity: 72.8189%\n",
      "total_backward_count 499290 real_backward_count 54988  11.013%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.290040/  1.509089, val:  70.83%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.79 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1251%\n",
      "layer   2  Sparsity: 73.4687%\n",
      "layer   3  Sparsity: 72.9314%\n",
      "total_backward_count 509080 real_backward_count 55693  10.940%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.296668/  1.487575, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.20 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0508%\n",
      "layer   2  Sparsity: 73.4135%\n",
      "layer   3  Sparsity: 72.3506%\n",
      "total_backward_count 518870 real_backward_count 56512  10.891%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.272165/  1.494510, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.44 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.1186%\n",
      "layer   2  Sparsity: 73.4055%\n",
      "layer   3  Sparsity: 71.9131%\n",
      "total_backward_count 528660 real_backward_count 57267  10.832%\n",
      "fc layer 1 self.abs_max_out: 3732.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.278661/  1.511136, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.10 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 73.2196%\n",
      "layer   3  Sparsity: 72.7254%\n",
      "total_backward_count 538450 real_backward_count 57989  10.770%\n",
      "fc layer 1 self.abs_max_out: 3772.0\n",
      "lif layer 1 self.abs_max_v: 6508.5\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.283263/  1.510408, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.61 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 73.1860%\n",
      "layer   3  Sparsity: 72.5959%\n",
      "total_backward_count 548240 real_backward_count 58788  10.723%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.265878/  1.474447, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   2  Sparsity: 73.4588%\n",
      "layer   3  Sparsity: 72.3859%\n",
      "total_backward_count 558030 real_backward_count 59503  10.663%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.263232/  1.459666, val:  78.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1044%\n",
      "layer   2  Sparsity: 73.5736%\n",
      "layer   3  Sparsity: 71.9493%\n",
      "total_backward_count 567820 real_backward_count 60200  10.602%\n",
      "fc layer 2 self.abs_max_out: 1904.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.265225/  1.452105, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0366%\n",
      "layer   2  Sparsity: 73.7487%\n",
      "layer   3  Sparsity: 72.3836%\n",
      "total_backward_count 577610 real_backward_count 60956  10.553%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.218173/  1.459453, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0710%\n",
      "layer   2  Sparsity: 73.7124%\n",
      "layer   3  Sparsity: 72.5518%\n",
      "total_backward_count 587400 real_backward_count 61675  10.500%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.241503/  1.456229, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   2  Sparsity: 73.6255%\n",
      "layer   3  Sparsity: 72.6497%\n",
      "total_backward_count 597190 real_backward_count 62364  10.443%\n",
      "lif layer 1 self.abs_max_v: 6530.5\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.255131/  1.481879, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.40 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   2  Sparsity: 73.4284%\n",
      "layer   3  Sparsity: 72.5669%\n",
      "total_backward_count 606980 real_backward_count 63038  10.386%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.258602/  1.491459, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0515%\n",
      "layer   2  Sparsity: 73.6840%\n",
      "layer   3  Sparsity: 72.4237%\n",
      "total_backward_count 616770 real_backward_count 63695  10.327%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.263088/  1.483716, val:  80.83%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0971%\n",
      "layer   2  Sparsity: 73.4580%\n",
      "layer   3  Sparsity: 72.4054%\n",
      "total_backward_count 626560 real_backward_count 64405  10.279%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.264216/  1.481308, val:  77.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.62 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0907%\n",
      "layer   2  Sparsity: 73.1632%\n",
      "layer   3  Sparsity: 73.1026%\n",
      "total_backward_count 636350 real_backward_count 65129  10.235%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.262035/  1.479909, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.09 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0256%\n",
      "layer   2  Sparsity: 73.2315%\n",
      "layer   3  Sparsity: 73.2850%\n",
      "total_backward_count 646140 real_backward_count 65824  10.187%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.272137/  1.483346, val:  80.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.24 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0596%\n",
      "layer   2  Sparsity: 73.3134%\n",
      "layer   3  Sparsity: 72.9936%\n",
      "total_backward_count 655930 real_backward_count 66512  10.140%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.269889/  1.470334, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 73.4066%\n",
      "layer   3  Sparsity: 72.5255%\n",
      "total_backward_count 665720 real_backward_count 67142  10.086%\n",
      "lif layer 2 self.abs_max_v: 2674.5\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.253091/  1.452211, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.30 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0607%\n",
      "layer   2  Sparsity: 73.2139%\n",
      "layer   3  Sparsity: 72.7344%\n",
      "total_backward_count 675510 real_backward_count 67772  10.033%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.249504/  1.480190, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0840%\n",
      "layer   2  Sparsity: 73.3405%\n",
      "layer   3  Sparsity: 72.9607%\n",
      "total_backward_count 685300 real_backward_count 68362   9.975%\n",
      "lif layer 2 self.abs_max_v: 2721.5\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.244419/  1.468970, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.14 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 73.0147%\n",
      "layer   3  Sparsity: 72.6587%\n",
      "total_backward_count 695090 real_backward_count 68993   9.926%\n",
      "lif layer 2 self.abs_max_v: 2778.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.238154/  1.486825, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.96 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1281%\n",
      "layer   2  Sparsity: 73.1635%\n",
      "layer   3  Sparsity: 72.2895%\n",
      "total_backward_count 704880 real_backward_count 69675   9.885%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.256600/  1.485993, val:  83.75%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.98 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0975%\n",
      "layer   2  Sparsity: 73.3883%\n",
      "layer   3  Sparsity: 72.6402%\n",
      "total_backward_count 714670 real_backward_count 70308   9.838%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.244032/  1.464144, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   2  Sparsity: 73.1776%\n",
      "layer   3  Sparsity: 72.8116%\n",
      "total_backward_count 724460 real_backward_count 70946   9.793%\n",
      "lif layer 2 self.abs_max_v: 2957.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.253829/  1.483363, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.93 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0796%\n",
      "layer   2  Sparsity: 73.1909%\n",
      "layer   3  Sparsity: 72.9458%\n",
      "total_backward_count 734250 real_backward_count 71617   9.754%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.244957/  1.459333, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.82 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0820%\n",
      "layer   2  Sparsity: 73.3696%\n",
      "layer   3  Sparsity: 72.6534%\n",
      "total_backward_count 744040 real_backward_count 72202   9.704%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.243657/  1.451908, val:  76.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.94 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1008%\n",
      "layer   2  Sparsity: 73.3849%\n",
      "layer   3  Sparsity: 73.0082%\n",
      "total_backward_count 753830 real_backward_count 72834   9.662%\n",
      "fc layer 1 self.abs_max_out: 3822.0\n",
      "lif layer 1 self.abs_max_v: 6643.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.224115/  1.480863, val:  72.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.63 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0740%\n",
      "layer   2  Sparsity: 73.0759%\n",
      "layer   3  Sparsity: 72.7239%\n",
      "total_backward_count 763620 real_backward_count 73454   9.619%\n",
      "lif layer 1 self.abs_max_v: 6644.5\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.236553/  1.458518, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.57 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0186%\n",
      "layer   2  Sparsity: 73.2278%\n",
      "layer   3  Sparsity: 73.1279%\n",
      "total_backward_count 773410 real_backward_count 74088   9.579%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.238369/  1.457450, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.56 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   2  Sparsity: 73.5596%\n",
      "layer   3  Sparsity: 73.4123%\n",
      "total_backward_count 783200 real_backward_count 74666   9.533%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.231539/  1.443022, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.88 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0718%\n",
      "layer   2  Sparsity: 73.0378%\n",
      "layer   3  Sparsity: 72.6483%\n",
      "total_backward_count 792990 real_backward_count 75291   9.495%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.233117/  1.463691, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.64 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   2  Sparsity: 72.8872%\n",
      "layer   3  Sparsity: 72.3950%\n",
      "total_backward_count 802780 real_backward_count 75912   9.456%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.225583/  1.451471, val:  77.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.52 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0991%\n",
      "layer   2  Sparsity: 73.0736%\n",
      "layer   3  Sparsity: 72.7674%\n",
      "total_backward_count 812570 real_backward_count 76508   9.416%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.223775/  1.468394, val:  71.25%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.39 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 72.8075%\n",
      "layer   3  Sparsity: 73.1092%\n",
      "total_backward_count 822360 real_backward_count 77075   9.372%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.226493/  1.452806, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.65 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0979%\n",
      "layer   2  Sparsity: 73.1425%\n",
      "layer   3  Sparsity: 72.8742%\n",
      "total_backward_count 832150 real_backward_count 77632   9.329%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.224784/  1.446127, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.81 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0832%\n",
      "layer   2  Sparsity: 73.2069%\n",
      "layer   3  Sparsity: 72.5998%\n",
      "total_backward_count 841940 real_backward_count 78311   9.301%\n",
      "lif layer 2 self.abs_max_v: 3002.0\n",
      "lif layer 2 self.abs_max_v: 3089.0\n",
      "lif layer 2 self.abs_max_v: 3131.5\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.211424/  1.451199, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.71 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0535%\n",
      "layer   2  Sparsity: 73.1653%\n",
      "layer   3  Sparsity: 73.1771%\n",
      "total_backward_count 851730 real_backward_count 78880   9.261%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.205486/  1.451985, val:  72.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1119%\n",
      "layer   2  Sparsity: 73.1463%\n",
      "layer   3  Sparsity: 72.7004%\n",
      "total_backward_count 861520 real_backward_count 79443   9.221%\n",
      "lif layer 2 self.abs_max_v: 3143.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.220948/  1.414447, val:  86.25%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.36 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0921%\n",
      "layer   2  Sparsity: 73.0479%\n",
      "layer   3  Sparsity: 72.3504%\n",
      "total_backward_count 871310 real_backward_count 79984   9.180%\n",
      "lif layer 2 self.abs_max_v: 3179.0\n",
      "lif layer 2 self.abs_max_v: 3205.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.193920/  1.399899, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   2  Sparsity: 72.7904%\n",
      "layer   3  Sparsity: 72.6925%\n",
      "total_backward_count 881100 real_backward_count 80531   9.140%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.186037/  1.419085, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 73.0236%\n",
      "layer   3  Sparsity: 72.7460%\n",
      "total_backward_count 890890 real_backward_count 81083   9.101%\n",
      "lif layer 2 self.abs_max_v: 3226.0\n",
      "fc layer 2 self.abs_max_out: 1978.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.199877/  1.406829, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.29 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   2  Sparsity: 73.1618%\n",
      "layer   3  Sparsity: 73.0948%\n",
      "total_backward_count 900680 real_backward_count 81634   9.064%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.181172/  1.412097, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0471%\n",
      "layer   2  Sparsity: 73.0655%\n",
      "layer   3  Sparsity: 73.0113%\n",
      "total_backward_count 910470 real_backward_count 82167   9.025%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.170676/  1.395360, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.10 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0739%\n",
      "layer   2  Sparsity: 73.1242%\n",
      "layer   3  Sparsity: 72.9340%\n",
      "total_backward_count 920260 real_backward_count 82674   8.984%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.179036/  1.406291, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0586%\n",
      "layer   2  Sparsity: 72.8196%\n",
      "layer   3  Sparsity: 72.9340%\n",
      "total_backward_count 930050 real_backward_count 83224   8.948%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.180995/  1.417771, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.30 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 72.7767%\n",
      "layer   3  Sparsity: 72.7744%\n",
      "total_backward_count 939840 real_backward_count 83771   8.913%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.173146/  1.401279, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 73.1295%\n",
      "layer   3  Sparsity: 72.7683%\n",
      "total_backward_count 949630 real_backward_count 84288   8.876%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.185080/  1.403593, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.71 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   2  Sparsity: 73.2744%\n",
      "layer   3  Sparsity: 73.3929%\n",
      "total_backward_count 959420 real_backward_count 84782   8.837%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.190802/  1.422132, val:  80.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   2  Sparsity: 73.1013%\n",
      "layer   3  Sparsity: 73.0114%\n",
      "total_backward_count 969210 real_backward_count 85326   8.804%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.177497/  1.406111, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.09 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   2  Sparsity: 72.8785%\n",
      "layer   3  Sparsity: 73.2811%\n",
      "total_backward_count 979000 real_backward_count 85819   8.766%\n",
      "lif layer 1 self.abs_max_v: 6713.5\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.172691/  1.401472, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.39 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1208%\n",
      "layer   2  Sparsity: 73.0830%\n",
      "layer   3  Sparsity: 73.3165%\n",
      "total_backward_count 988790 real_backward_count 86295   8.727%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.158680/  1.384850, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.49 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   2  Sparsity: 72.9179%\n",
      "layer   3  Sparsity: 73.1780%\n",
      "total_backward_count 998580 real_backward_count 86779   8.690%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.163641/  1.412822, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.98 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0533%\n",
      "layer   2  Sparsity: 72.9416%\n",
      "layer   3  Sparsity: 73.6999%\n",
      "total_backward_count 1008370 real_backward_count 87272   8.655%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.173527/  1.398870, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.03 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   2  Sparsity: 72.5073%\n",
      "layer   3  Sparsity: 73.4616%\n",
      "total_backward_count 1018160 real_backward_count 87742   8.618%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.148514/  1.417008, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.81 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0381%\n",
      "layer   2  Sparsity: 72.7823%\n",
      "layer   3  Sparsity: 73.0762%\n",
      "total_backward_count 1027950 real_backward_count 88237   8.584%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.154116/  1.397177, val:  82.08%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   2  Sparsity: 72.7026%\n",
      "layer   3  Sparsity: 72.8139%\n",
      "total_backward_count 1037740 real_backward_count 88707   8.548%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.160193/  1.377073, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   2  Sparsity: 72.9201%\n",
      "layer   3  Sparsity: 72.9190%\n",
      "total_backward_count 1047530 real_backward_count 89171   8.513%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.159880/  1.386382, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.31 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1098%\n",
      "layer   2  Sparsity: 73.0654%\n",
      "layer   3  Sparsity: 73.0115%\n",
      "total_backward_count 1057320 real_backward_count 89660   8.480%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.146365/  1.389248, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.17 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0570%\n",
      "layer   2  Sparsity: 72.9919%\n",
      "layer   3  Sparsity: 72.9091%\n",
      "total_backward_count 1067110 real_backward_count 90145   8.448%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.140022/  1.370978, val:  82.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.08 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0788%\n",
      "layer   2  Sparsity: 73.3554%\n",
      "layer   3  Sparsity: 73.1239%\n",
      "total_backward_count 1076900 real_backward_count 90613   8.414%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.139685/  1.387662, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.92 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1073%\n",
      "layer   2  Sparsity: 73.0617%\n",
      "layer   3  Sparsity: 73.2027%\n",
      "total_backward_count 1086690 real_backward_count 91053   8.379%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.141914/  1.365029, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.99 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   2  Sparsity: 72.9737%\n",
      "layer   3  Sparsity: 73.4027%\n",
      "total_backward_count 1096480 real_backward_count 91542   8.349%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.146860/  1.400105, val:  73.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.52 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0334%\n",
      "layer   2  Sparsity: 72.7765%\n",
      "layer   3  Sparsity: 73.7009%\n",
      "total_backward_count 1106270 real_backward_count 92027   8.319%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.157634/  1.391000, val:  78.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1167%\n",
      "layer   2  Sparsity: 73.0615%\n",
      "layer   3  Sparsity: 73.5174%\n",
      "total_backward_count 1116060 real_backward_count 92472   8.286%\n",
      "fc layer 3 self.abs_max_out: 702.0\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.149403/  1.389577, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.08 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   2  Sparsity: 72.8330%\n",
      "layer   3  Sparsity: 73.5712%\n",
      "total_backward_count 1125850 real_backward_count 92955   8.256%\n",
      "lif layer 1 self.abs_max_v: 6722.0\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.154605/  1.376719, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.00 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   2  Sparsity: 72.8537%\n",
      "layer   3  Sparsity: 73.8894%\n",
      "total_backward_count 1135640 real_backward_count 93387   8.223%\n",
      "lif layer 1 self.abs_max_v: 6723.5\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.150999/  1.387133, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   2  Sparsity: 72.9539%\n",
      "layer   3  Sparsity: 74.0776%\n",
      "total_backward_count 1145430 real_backward_count 93844   8.193%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.153638/  1.369633, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.78 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   2  Sparsity: 72.8362%\n",
      "layer   3  Sparsity: 73.8404%\n",
      "total_backward_count 1155220 real_backward_count 94306   8.163%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.130133/  1.365760, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.61 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   2  Sparsity: 72.6025%\n",
      "layer   3  Sparsity: 74.0420%\n",
      "total_backward_count 1165010 real_backward_count 94708   8.129%\n",
      "lif layer 2 self.abs_max_v: 3264.0\n",
      "fc layer 1 self.abs_max_out: 3833.0\n",
      "lif layer 1 self.abs_max_v: 6745.0\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.141649/  1.346749, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0524%\n",
      "layer   2  Sparsity: 72.7249%\n",
      "layer   3  Sparsity: 74.3234%\n",
      "total_backward_count 1174800 real_backward_count 95168   8.101%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.136919/  1.408978, val:  77.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.13 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0570%\n",
      "layer   2  Sparsity: 72.7606%\n",
      "layer   3  Sparsity: 74.1858%\n",
      "total_backward_count 1184590 real_backward_count 95610   8.071%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.144471/  1.357325, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.95 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0455%\n",
      "layer   2  Sparsity: 72.6851%\n",
      "layer   3  Sparsity: 73.6072%\n",
      "total_backward_count 1194380 real_backward_count 96094   8.046%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.141898/  1.367130, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.91 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   2  Sparsity: 72.6865%\n",
      "layer   3  Sparsity: 73.7789%\n",
      "total_backward_count 1204170 real_backward_count 96506   8.014%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.132195/  1.362560, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.05 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   2  Sparsity: 72.8473%\n",
      "layer   3  Sparsity: 74.2087%\n",
      "total_backward_count 1213960 real_backward_count 96924   7.984%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.141467/  1.350902, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.92 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   2  Sparsity: 72.8178%\n",
      "layer   3  Sparsity: 74.6963%\n",
      "total_backward_count 1223750 real_backward_count 97339   7.954%\n",
      "lif layer 2 self.abs_max_v: 3337.5\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.132226/  1.367188, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0868%\n",
      "layer   2  Sparsity: 72.9113%\n",
      "layer   3  Sparsity: 74.5244%\n",
      "total_backward_count 1233540 real_backward_count 97741   7.924%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.131896/  1.346257, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.20 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0064%\n",
      "layer   2  Sparsity: 72.9377%\n",
      "layer   3  Sparsity: 74.4314%\n",
      "total_backward_count 1243330 real_backward_count 98155   7.895%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.132797/  1.354960, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.56 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0903%\n",
      "layer   2  Sparsity: 73.0637%\n",
      "layer   3  Sparsity: 74.6171%\n",
      "total_backward_count 1253120 real_backward_count 98531   7.863%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.109746/  1.360935, val:  80.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.10 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   2  Sparsity: 72.9801%\n",
      "layer   3  Sparsity: 74.4242%\n",
      "total_backward_count 1262910 real_backward_count 98929   7.833%\n",
      "fc layer 1 self.abs_max_out: 3836.0\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.106182/  1.340716, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.89 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   2  Sparsity: 72.6793%\n",
      "layer   3  Sparsity: 73.9875%\n",
      "total_backward_count 1272700 real_backward_count 99344   7.806%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.109533/  1.355739, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.13 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0920%\n",
      "layer   2  Sparsity: 72.6408%\n",
      "layer   3  Sparsity: 74.0975%\n",
      "total_backward_count 1282490 real_backward_count 99780   7.780%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.126514/  1.366447, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.91 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   2  Sparsity: 72.6655%\n",
      "layer   3  Sparsity: 73.9874%\n",
      "total_backward_count 1292280 real_backward_count 100178   7.752%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.121802/  1.372352, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.68 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0677%\n",
      "layer   2  Sparsity: 72.8551%\n",
      "layer   3  Sparsity: 74.3973%\n",
      "total_backward_count 1302070 real_backward_count 100589   7.725%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.118656/  1.363404, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1116%\n",
      "layer   2  Sparsity: 72.7967%\n",
      "layer   3  Sparsity: 74.0394%\n",
      "total_backward_count 1311860 real_backward_count 101018   7.700%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.109798/  1.336758, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.10 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   2  Sparsity: 72.9105%\n",
      "layer   3  Sparsity: 74.0009%\n",
      "total_backward_count 1321650 real_backward_count 101409   7.673%\n",
      "lif layer 2 self.abs_max_v: 3418.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.117154/  1.345487, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.82 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 72.6247%\n",
      "layer   3  Sparsity: 73.8752%\n",
      "total_backward_count 1331440 real_backward_count 101792   7.645%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.119614/  1.373563, val:  80.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.39 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   2  Sparsity: 72.7276%\n",
      "layer   3  Sparsity: 74.4803%\n",
      "total_backward_count 1341230 real_backward_count 102190   7.619%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.132692/  1.365594, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.10 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1206%\n",
      "layer   2  Sparsity: 72.5860%\n",
      "layer   3  Sparsity: 74.0559%\n",
      "total_backward_count 1351020 real_backward_count 102578   7.593%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.123670/  1.361366, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.98 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0745%\n",
      "layer   2  Sparsity: 72.6079%\n",
      "layer   3  Sparsity: 74.1772%\n",
      "total_backward_count 1360810 real_backward_count 102998   7.569%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.120787/  1.330009, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.90 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   2  Sparsity: 72.7420%\n",
      "layer   3  Sparsity: 73.6220%\n",
      "total_backward_count 1370600 real_backward_count 103395   7.544%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.109057/  1.348590, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.06 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0640%\n",
      "layer   2  Sparsity: 72.7632%\n",
      "layer   3  Sparsity: 73.4905%\n",
      "total_backward_count 1380390 real_backward_count 103760   7.517%\n",
      "fc layer 1 self.abs_max_out: 3844.0\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.097112/  1.322099, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.27 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 72.9497%\n",
      "layer   3  Sparsity: 74.0863%\n",
      "total_backward_count 1390180 real_backward_count 104111   7.489%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.097740/  1.349335, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   2  Sparsity: 72.8096%\n",
      "layer   3  Sparsity: 74.1167%\n",
      "total_backward_count 1399970 real_backward_count 104444   7.460%\n",
      "fc layer 3 self.abs_max_out: 713.0\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.107648/  1.347643, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.63 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0986%\n",
      "layer   2  Sparsity: 72.7737%\n",
      "layer   3  Sparsity: 74.2165%\n",
      "total_backward_count 1409760 real_backward_count 104814   7.435%\n",
      "fc layer 3 self.abs_max_out: 716.0\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.104996/  1.314402, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.49 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   2  Sparsity: 72.9049%\n",
      "layer   3  Sparsity: 74.0035%\n",
      "total_backward_count 1419550 real_backward_count 105204   7.411%\n",
      "fc layer 1 self.abs_max_out: 3897.0\n",
      "lif layer 1 self.abs_max_v: 6817.5\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.092155/  1.319210, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.16 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   2  Sparsity: 72.8517%\n",
      "layer   3  Sparsity: 74.1059%\n",
      "total_backward_count 1429340 real_backward_count 105562   7.385%\n",
      "fc layer 3 self.abs_max_out: 722.0\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.090350/  1.344067, val:  80.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.61 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   2  Sparsity: 72.7510%\n",
      "layer   3  Sparsity: 74.0076%\n",
      "total_backward_count 1439130 real_backward_count 105887   7.358%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.096648/  1.354090, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.31 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   2  Sparsity: 72.5681%\n",
      "layer   3  Sparsity: 74.2841%\n",
      "total_backward_count 1448920 real_backward_count 106216   7.331%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.093609/  1.339565, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 72.7346%\n",
      "layer   3  Sparsity: 74.1640%\n",
      "total_backward_count 1458710 real_backward_count 106561   7.305%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.084142/  1.302657, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 72.7007%\n",
      "layer   3  Sparsity: 74.3171%\n",
      "total_backward_count 1468500 real_backward_count 106928   7.281%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.090040/  1.316306, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0702%\n",
      "layer   2  Sparsity: 72.8591%\n",
      "layer   3  Sparsity: 74.3105%\n",
      "total_backward_count 1478290 real_backward_count 107279   7.257%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.103642/  1.357435, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1047%\n",
      "layer   2  Sparsity: 72.7842%\n",
      "layer   3  Sparsity: 74.4560%\n",
      "total_backward_count 1488080 real_backward_count 107692   7.237%\n",
      "fc layer 3 self.abs_max_out: 756.0\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.098158/  1.345669, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1040%\n",
      "layer   2  Sparsity: 72.9256%\n",
      "layer   3  Sparsity: 74.1796%\n",
      "total_backward_count 1497870 real_backward_count 108038   7.213%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.103374/  1.332536, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0905%\n",
      "layer   2  Sparsity: 73.0993%\n",
      "layer   3  Sparsity: 74.6099%\n",
      "total_backward_count 1507660 real_backward_count 108404   7.190%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.097889/  1.331807, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   2  Sparsity: 73.0610%\n",
      "layer   3  Sparsity: 74.3922%\n",
      "total_backward_count 1517450 real_backward_count 108770   7.168%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.094419/  1.354104, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0721%\n",
      "layer   2  Sparsity: 72.9575%\n",
      "layer   3  Sparsity: 74.0570%\n",
      "total_backward_count 1527240 real_backward_count 109114   7.145%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.109677/  1.315917, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   2  Sparsity: 72.8824%\n",
      "layer   3  Sparsity: 73.7879%\n",
      "total_backward_count 1537030 real_backward_count 109461   7.122%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.091207/  1.317432, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.90 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1081%\n",
      "layer   2  Sparsity: 72.9038%\n",
      "layer   3  Sparsity: 73.4724%\n",
      "total_backward_count 1546820 real_backward_count 109828   7.100%\n",
      "fc layer 3 self.abs_max_out: 761.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.093969/  1.325183, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.53 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0569%\n",
      "layer   2  Sparsity: 72.9421%\n",
      "layer   3  Sparsity: 73.9064%\n",
      "total_backward_count 1556610 real_backward_count 110171   7.078%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.087542/  1.325041, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0319%\n",
      "layer   2  Sparsity: 73.1252%\n",
      "layer   3  Sparsity: 74.1793%\n",
      "total_backward_count 1566400 real_backward_count 110536   7.057%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.100819/  1.327453, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   2  Sparsity: 73.2653%\n",
      "layer   3  Sparsity: 74.3133%\n",
      "total_backward_count 1576190 real_backward_count 110847   7.033%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.112955/  1.346429, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0544%\n",
      "layer   2  Sparsity: 73.0905%\n",
      "layer   3  Sparsity: 74.6035%\n",
      "total_backward_count 1585980 real_backward_count 111171   7.010%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.116104/  1.368703, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.05 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0497%\n",
      "layer   2  Sparsity: 73.0774%\n",
      "layer   3  Sparsity: 74.5251%\n",
      "total_backward_count 1595770 real_backward_count 111508   6.988%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.101070/  1.342518, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.98 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0704%\n",
      "layer   2  Sparsity: 73.0282%\n",
      "layer   3  Sparsity: 74.2270%\n",
      "total_backward_count 1605560 real_backward_count 111834   6.965%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.088090/  1.324512, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 73.2118%\n",
      "layer   3  Sparsity: 73.6347%\n",
      "total_backward_count 1615350 real_backward_count 112159   6.943%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.094684/  1.325920, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   2  Sparsity: 73.1121%\n",
      "layer   3  Sparsity: 73.8769%\n",
      "total_backward_count 1625140 real_backward_count 112472   6.921%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.083043/  1.335876, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.09 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0306%\n",
      "layer   2  Sparsity: 73.1209%\n",
      "layer   3  Sparsity: 74.0005%\n",
      "total_backward_count 1634930 real_backward_count 112781   6.898%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.080822/  1.321779, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.58 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0581%\n",
      "layer   2  Sparsity: 73.2983%\n",
      "layer   3  Sparsity: 74.1107%\n",
      "total_backward_count 1644720 real_backward_count 113089   6.876%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.082036/  1.333593, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.49 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   2  Sparsity: 73.2781%\n",
      "layer   3  Sparsity: 74.3362%\n",
      "total_backward_count 1654510 real_backward_count 113437   6.856%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.090644/  1.343019, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.17 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   2  Sparsity: 72.9363%\n",
      "layer   3  Sparsity: 74.5115%\n",
      "total_backward_count 1664300 real_backward_count 113756   6.835%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.076034/  1.313640, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0583%\n",
      "layer   2  Sparsity: 72.8417%\n",
      "layer   3  Sparsity: 74.4654%\n",
      "total_backward_count 1674090 real_backward_count 114058   6.813%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.062077/  1.314099, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   2  Sparsity: 73.0775%\n",
      "layer   3  Sparsity: 74.5371%\n",
      "total_backward_count 1683880 real_backward_count 114372   6.792%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.069299/  1.290293, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.64 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0389%\n",
      "layer   2  Sparsity: 73.2112%\n",
      "layer   3  Sparsity: 74.3419%\n",
      "total_backward_count 1693670 real_backward_count 114655   6.770%\n",
      "fc layer 3 self.abs_max_out: 780.0\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.062039/  1.301224, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0904%\n",
      "layer   2  Sparsity: 73.3244%\n",
      "layer   3  Sparsity: 74.4020%\n",
      "total_backward_count 1703460 real_backward_count 114989   6.750%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.055489/  1.311759, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.42 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0969%\n",
      "layer   2  Sparsity: 73.2436%\n",
      "layer   3  Sparsity: 74.6969%\n",
      "total_backward_count 1713250 real_backward_count 115319   6.731%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.054713/  1.307448, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1164%\n",
      "layer   2  Sparsity: 73.0543%\n",
      "layer   3  Sparsity: 74.7692%\n",
      "total_backward_count 1723040 real_backward_count 115620   6.710%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.061396/  1.313136, val:  82.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.24 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0907%\n",
      "layer   2  Sparsity: 72.9585%\n",
      "layer   3  Sparsity: 74.3083%\n",
      "total_backward_count 1732830 real_backward_count 115889   6.688%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.048642/  1.271535, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.08 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1105%\n",
      "layer   2  Sparsity: 73.3912%\n",
      "layer   3  Sparsity: 74.1302%\n",
      "total_backward_count 1742620 real_backward_count 116184   6.667%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.041252/  1.296341, val:  82.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1054%\n",
      "layer   2  Sparsity: 73.2704%\n",
      "layer   3  Sparsity: 74.0338%\n",
      "total_backward_count 1752410 real_backward_count 116434   6.644%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.035218/  1.293817, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0518%\n",
      "layer   2  Sparsity: 73.1761%\n",
      "layer   3  Sparsity: 74.2485%\n",
      "total_backward_count 1762200 real_backward_count 116703   6.623%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.033928/  1.286135, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.84 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   2  Sparsity: 73.1142%\n",
      "layer   3  Sparsity: 74.5799%\n",
      "total_backward_count 1771990 real_backward_count 116988   6.602%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.038598/  1.290695, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   2  Sparsity: 73.0958%\n",
      "layer   3  Sparsity: 74.5073%\n",
      "total_backward_count 1781780 real_backward_count 117276   6.582%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.046526/  1.290498, val:  87.50%, val_best:  91.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.46 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   2  Sparsity: 73.0854%\n",
      "layer   3  Sparsity: 75.0516%\n",
      "total_backward_count 1791570 real_backward_count 117545   6.561%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.049332/  1.275391, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   2  Sparsity: 73.1493%\n",
      "layer   3  Sparsity: 75.0395%\n",
      "total_backward_count 1801360 real_backward_count 117802   6.540%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.046002/  1.288859, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.82 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   2  Sparsity: 73.1043%\n",
      "layer   3  Sparsity: 75.0425%\n",
      "total_backward_count 1811150 real_backward_count 118056   6.518%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.051812/  1.288831, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0671%\n",
      "layer   2  Sparsity: 73.0180%\n",
      "layer   3  Sparsity: 74.7690%\n",
      "total_backward_count 1820940 real_backward_count 118290   6.496%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.043554/  1.265964, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.46 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0988%\n",
      "layer   2  Sparsity: 73.1837%\n",
      "layer   3  Sparsity: 74.4612%\n",
      "total_backward_count 1830730 real_backward_count 118552   6.476%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.025984/  1.302206, val:  82.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   2  Sparsity: 73.3293%\n",
      "layer   3  Sparsity: 74.8470%\n",
      "total_backward_count 1840520 real_backward_count 118834   6.457%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.026844/  1.281882, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0460%\n",
      "layer   2  Sparsity: 73.2922%\n",
      "layer   3  Sparsity: 74.8030%\n",
      "total_backward_count 1850310 real_backward_count 119092   6.436%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.042554/  1.285003, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0991%\n",
      "layer   2  Sparsity: 73.2576%\n",
      "layer   3  Sparsity: 74.8840%\n",
      "total_backward_count 1860100 real_backward_count 119370   6.417%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.046758/  1.276510, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0524%\n",
      "layer   2  Sparsity: 73.1900%\n",
      "layer   3  Sparsity: 74.9470%\n",
      "total_backward_count 1869890 real_backward_count 119665   6.400%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.048562/  1.309997, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.56 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   2  Sparsity: 73.1402%\n",
      "layer   3  Sparsity: 75.4370%\n",
      "total_backward_count 1879680 real_backward_count 119959   6.382%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.053925/  1.305185, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.31 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   2  Sparsity: 73.4785%\n",
      "layer   3  Sparsity: 75.2338%\n",
      "total_backward_count 1889470 real_backward_count 120221   6.363%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.060931/  1.321076, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.69 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   2  Sparsity: 73.1670%\n",
      "layer   3  Sparsity: 74.8240%\n",
      "total_backward_count 1899260 real_backward_count 120524   6.346%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.059285/  1.290514, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.94 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1096%\n",
      "layer   2  Sparsity: 73.1692%\n",
      "layer   3  Sparsity: 74.9402%\n",
      "total_backward_count 1909050 real_backward_count 120807   6.328%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.058565/  1.312946, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.07 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1035%\n",
      "layer   2  Sparsity: 73.2431%\n",
      "layer   3  Sparsity: 75.3662%\n",
      "total_backward_count 1918840 real_backward_count 121074   6.310%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.053441/  1.303296, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.65 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   2  Sparsity: 73.2875%\n",
      "layer   3  Sparsity: 75.1283%\n",
      "total_backward_count 1928630 real_backward_count 121340   6.292%\n",
      "fc layer 1 self.abs_max_out: 3935.0\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.053731/  1.296394, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.90 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0611%\n",
      "layer   2  Sparsity: 73.2003%\n",
      "layer   3  Sparsity: 74.9310%\n",
      "total_backward_count 1938420 real_backward_count 121595   6.273%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.046288/  1.286142, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.84 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   2  Sparsity: 73.0185%\n",
      "layer   3  Sparsity: 75.1347%\n",
      "total_backward_count 1948210 real_backward_count 121843   6.254%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.051940/  1.280967, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.55 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 73.1603%\n",
      "layer   3  Sparsity: 74.8526%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78321c2ec9054d61bb70bd4e6348525d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.05194</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>1.28097</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snowy-sweep-3</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kwggzj7j' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kwggzj7j</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_042711-kwggzj7j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a17jwskc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 31229\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_082914-a17jwskc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a17jwskc' target=\"_blank\">lively-sweep-4</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a17jwskc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a17jwskc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251118_082924_325', 'my_seed': 31229, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 327.0\n",
      "lif layer 1 self.abs_max_v: 327.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 244.0\n",
      "lif layer 2 self.abs_max_v: 244.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 64.0\n",
      "fc layer 2 self.abs_max_out: 331.0\n",
      "lif layer 2 self.abs_max_v: 448.0\n",
      "fc layer 3 self.abs_max_out: 88.0\n",
      "lif layer 1 self.abs_max_v: 336.5\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 1 self.abs_max_out: 334.0\n",
      "fc layer 2 self.abs_max_out: 346.0\n",
      "fc layer 1 self.abs_max_out: 364.0\n",
      "lif layer 1 self.abs_max_v: 364.0\n",
      "fc layer 1 self.abs_max_out: 423.0\n",
      "lif layer 1 self.abs_max_v: 423.0\n",
      "fc layer 1 self.abs_max_out: 498.0\n",
      "lif layer 1 self.abs_max_v: 498.0\n",
      "fc layer 2 self.abs_max_out: 355.0\n",
      "lif layer 2 self.abs_max_v: 470.0\n",
      "fc layer 3 self.abs_max_out: 142.0\n",
      "fc layer 1 self.abs_max_out: 532.0\n",
      "lif layer 1 self.abs_max_v: 532.0\n",
      "lif layer 2 self.abs_max_v: 486.0\n",
      "fc layer 3 self.abs_max_out: 162.0\n",
      "fc layer 1 self.abs_max_out: 543.0\n",
      "lif layer 1 self.abs_max_v: 543.0\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "fc layer 2 self.abs_max_out: 364.0\n",
      "lif layer 2 self.abs_max_v: 537.5\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "lif layer 1 self.abs_max_v: 574.5\n",
      "fc layer 2 self.abs_max_out: 371.0\n",
      "fc layer 2 self.abs_max_out: 411.0\n",
      "fc layer 2 self.abs_max_out: 469.0\n",
      "lif layer 2 self.abs_max_v: 661.5\n",
      "fc layer 1 self.abs_max_out: 643.0\n",
      "lif layer 1 self.abs_max_v: 643.0\n",
      "fc layer 2 self.abs_max_out: 479.0\n",
      "lif layer 1 self.abs_max_v: 675.0\n",
      "lif layer 1 self.abs_max_v: 734.5\n",
      "lif layer 2 self.abs_max_v: 698.0\n",
      "fc layer 1 self.abs_max_out: 650.0\n",
      "fc layer 3 self.abs_max_out: 250.0\n",
      "fc layer 1 self.abs_max_out: 651.0\n",
      "fc layer 2 self.abs_max_out: 513.0\n",
      "fc layer 1 self.abs_max_out: 757.0\n",
      "lif layer 1 self.abs_max_v: 757.0\n",
      "lif layer 2 self.abs_max_v: 713.0\n",
      "fc layer 2 self.abs_max_out: 586.0\n",
      "fc layer 1 self.abs_max_out: 981.0\n",
      "lif layer 1 self.abs_max_v: 981.0\n",
      "fc layer 2 self.abs_max_out: 634.0\n",
      "lif layer 2 self.abs_max_v: 715.0\n",
      "fc layer 2 self.abs_max_out: 643.0\n",
      "lif layer 2 self.abs_max_v: 771.5\n",
      "fc layer 2 self.abs_max_out: 663.0\n",
      "lif layer 2 self.abs_max_v: 891.0\n",
      "fc layer 3 self.abs_max_out: 286.0\n",
      "lif layer 2 self.abs_max_v: 891.5\n",
      "fc layer 2 self.abs_max_out: 680.0\n",
      "fc layer 2 self.abs_max_out: 766.0\n",
      "lif layer 2 self.abs_max_v: 898.5\n",
      "lif layer 1 self.abs_max_v: 993.5\n",
      "lif layer 1 self.abs_max_v: 1103.5\n",
      "lif layer 2 self.abs_max_v: 953.0\n",
      "lif layer 2 self.abs_max_v: 954.5\n",
      "lif layer 2 self.abs_max_v: 977.5\n",
      "fc layer 1 self.abs_max_out: 1231.0\n",
      "lif layer 1 self.abs_max_v: 1231.0\n",
      "fc layer 2 self.abs_max_out: 769.0\n",
      "fc layer 2 self.abs_max_out: 782.0\n",
      "fc layer 3 self.abs_max_out: 375.0\n",
      "lif layer 2 self.abs_max_v: 981.0\n",
      "lif layer 2 self.abs_max_v: 1032.5\n",
      "lif layer 2 self.abs_max_v: 1052.0\n",
      "lif layer 2 self.abs_max_v: 1155.0\n",
      "fc layer 2 self.abs_max_out: 790.0\n",
      "fc layer 2 self.abs_max_out: 820.0\n",
      "fc layer 2 self.abs_max_out: 864.0\n",
      "lif layer 2 self.abs_max_v: 1166.5\n",
      "fc layer 2 self.abs_max_out: 908.0\n",
      "fc layer 2 self.abs_max_out: 954.0\n",
      "fc layer 2 self.abs_max_out: 1000.0\n",
      "fc layer 2 self.abs_max_out: 1019.0\n",
      "lif layer 2 self.abs_max_v: 1169.0\n",
      "fc layer 2 self.abs_max_out: 1038.0\n",
      "lif layer 2 self.abs_max_v: 1187.0\n",
      "lif layer 1 self.abs_max_v: 1240.5\n",
      "lif layer 1 self.abs_max_v: 1266.5\n",
      "lif layer 2 self.abs_max_v: 1237.0\n",
      "lif layer 1 self.abs_max_v: 1621.0\n",
      "lif layer 1 self.abs_max_v: 1715.5\n",
      "lif layer 2 self.abs_max_v: 1254.5\n",
      "lif layer 2 self.abs_max_v: 1267.0\n",
      "lif layer 2 self.abs_max_v: 1331.5\n",
      "fc layer 2 self.abs_max_out: 1100.0\n",
      "fc layer 2 self.abs_max_out: 1103.0\n",
      "fc layer 2 self.abs_max_out: 1104.0\n",
      "fc layer 2 self.abs_max_out: 1117.0\n",
      "fc layer 2 self.abs_max_out: 1145.0\n",
      "lif layer 2 self.abs_max_v: 1403.5\n",
      "lif layer 2 self.abs_max_v: 1460.0\n",
      "lif layer 2 self.abs_max_v: 1514.0\n",
      "fc layer 3 self.abs_max_out: 394.0\n",
      "fc layer 2 self.abs_max_out: 1159.0\n",
      "fc layer 2 self.abs_max_out: 1368.0\n",
      "fc layer 1 self.abs_max_out: 1233.0\n",
      "fc layer 1 self.abs_max_out: 1286.0\n",
      "fc layer 1 self.abs_max_out: 1332.0\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "lif layer 1 self.abs_max_v: 1778.0\n",
      "fc layer 1 self.abs_max_out: 1397.0\n",
      "lif layer 1 self.abs_max_v: 1871.0\n",
      "fc layer 1 self.abs_max_out: 1466.0\n",
      "lif layer 2 self.abs_max_v: 1551.5\n",
      "lif layer 1 self.abs_max_v: 1901.5\n",
      "lif layer 1 self.abs_max_v: 2009.5\n",
      "fc layer 3 self.abs_max_out: 431.0\n",
      "lif layer 1 self.abs_max_v: 2050.0\n",
      "lif layer 2 self.abs_max_v: 1737.0\n",
      "lif layer 1 self.abs_max_v: 2057.0\n",
      "lif layer 1 self.abs_max_v: 2145.5\n",
      "lif layer 1 self.abs_max_v: 2154.0\n",
      "lif layer 1 self.abs_max_v: 2196.0\n",
      "lif layer 1 self.abs_max_v: 2224.0\n",
      "fc layer 1 self.abs_max_out: 1500.0\n",
      "lif layer 1 self.abs_max_v: 2502.0\n",
      "fc layer 3 self.abs_max_out: 460.0\n",
      "fc layer 1 self.abs_max_out: 1591.0\n",
      "fc layer 1 self.abs_max_out: 1608.0\n",
      "lif layer 1 self.abs_max_v: 2554.5\n",
      "lif layer 1 self.abs_max_v: 2796.0\n",
      "fc layer 1 self.abs_max_out: 1638.0\n",
      "lif layer 1 self.abs_max_v: 2982.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.703114/  1.933804, val:  36.67%, val_best:  36.67%, tr:  97.85%, tr_best:  97.85%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0967%\n",
      "layer   2  Sparsity: 72.1038%\n",
      "layer   3  Sparsity: 67.5945%\n",
      "total_backward_count 9790 real_backward_count 2054  20.981%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 1713.0\n",
      "lif layer 2 self.abs_max_v: 1753.5\n",
      "fc layer 3 self.abs_max_out: 479.0\n",
      "fc layer 3 self.abs_max_out: 498.0\n",
      "fc layer 3 self.abs_max_out: 531.0\n",
      "fc layer 3 self.abs_max_out: 541.0\n",
      "lif layer 2 self.abs_max_v: 1788.0\n",
      "fc layer 1 self.abs_max_out: 1756.0\n",
      "lif layer 1 self.abs_max_v: 3060.0\n",
      "lif layer 1 self.abs_max_v: 3141.0\n",
      "fc layer 1 self.abs_max_out: 1790.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.589868/  1.830787, val:  47.08%, val_best:  47.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.48 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0710%\n",
      "layer   2  Sparsity: 72.3073%\n",
      "layer   3  Sparsity: 64.3240%\n",
      "total_backward_count 19580 real_backward_count 3588  18.325%\n",
      "fc layer 1 self.abs_max_out: 1793.0\n",
      "fc layer 1 self.abs_max_out: 1841.0\n",
      "fc layer 3 self.abs_max_out: 580.0\n",
      "fc layer 1 self.abs_max_out: 1970.0\n",
      "lif layer 1 self.abs_max_v: 3218.0\n",
      "fc layer 1 self.abs_max_out: 2093.0\n",
      "lif layer 1 self.abs_max_v: 3702.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.535908/  1.810001, val:  42.50%, val_best:  47.08%, tr:  99.39%, tr_best:  99.80%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1016%\n",
      "layer   2  Sparsity: 72.1897%\n",
      "layer   3  Sparsity: 63.0653%\n",
      "total_backward_count 29370 real_backward_count 5002  17.031%\n",
      "lif layer 2 self.abs_max_v: 1950.5\n",
      "fc layer 2 self.abs_max_out: 1370.0\n",
      "fc layer 2 self.abs_max_out: 1390.0\n",
      "fc layer 2 self.abs_max_out: 1411.0\n",
      "fc layer 1 self.abs_max_out: 2138.0\n",
      "lif layer 2 self.abs_max_v: 1956.0\n",
      "lif layer 2 self.abs_max_v: 2031.0\n",
      "fc layer 2 self.abs_max_out: 1415.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.514767/  1.818730, val:  50.00%, val_best:  50.00%, tr:  99.49%, tr_best:  99.80%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   2  Sparsity: 72.4916%\n",
      "layer   3  Sparsity: 62.4586%\n",
      "total_backward_count 39160 real_backward_count 6302  16.093%\n",
      "fc layer 2 self.abs_max_out: 1460.0\n",
      "fc layer 1 self.abs_max_out: 2323.0\n",
      "fc layer 3 self.abs_max_out: 592.0\n",
      "fc layer 2 self.abs_max_out: 1468.0\n",
      "lif layer 1 self.abs_max_v: 3823.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.493953/  1.789973, val:  42.08%, val_best:  50.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0605%\n",
      "layer   2  Sparsity: 72.1191%\n",
      "layer   3  Sparsity: 63.0823%\n",
      "total_backward_count 48950 real_backward_count 7588  15.502%\n",
      "fc layer 1 self.abs_max_out: 2327.0\n",
      "fc layer 1 self.abs_max_out: 2448.0\n",
      "lif layer 1 self.abs_max_v: 3991.0\n",
      "lif layer 1 self.abs_max_v: 4183.5\n",
      "lif layer 1 self.abs_max_v: 4186.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.466115/  1.729519, val:  53.75%, val_best:  53.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0992%\n",
      "layer   2  Sparsity: 71.8845%\n",
      "layer   3  Sparsity: 62.0377%\n",
      "total_backward_count 58740 real_backward_count 8851  15.068%\n",
      "fc layer 2 self.abs_max_out: 1483.0\n",
      "lif layer 2 self.abs_max_v: 2204.5\n",
      "fc layer 2 self.abs_max_out: 1502.0\n",
      "lif layer 2 self.abs_max_v: 2327.0\n",
      "lif layer 2 self.abs_max_v: 2340.5\n",
      "lif layer 2 self.abs_max_v: 2370.0\n",
      "fc layer 2 self.abs_max_out: 1596.0\n",
      "fc layer 3 self.abs_max_out: 607.0\n",
      "lif layer 2 self.abs_max_v: 2451.5\n",
      "lif layer 2 self.abs_max_v: 2518.0\n",
      "lif layer 2 self.abs_max_v: 2627.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.430667/  1.706102, val:  52.92%, val_best:  53.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.88 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0415%\n",
      "layer   2  Sparsity: 71.4271%\n",
      "layer   3  Sparsity: 62.2581%\n",
      "total_backward_count 68530 real_backward_count 10090  14.723%\n",
      "lif layer 2 self.abs_max_v: 2648.0\n",
      "fc layer 3 self.abs_max_out: 617.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.428809/  1.687973, val:  52.50%, val_best:  53.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0868%\n",
      "layer   2  Sparsity: 71.1583%\n",
      "layer   3  Sparsity: 62.0884%\n",
      "total_backward_count 78320 real_backward_count 11270  14.390%\n",
      "fc layer 3 self.abs_max_out: 650.0\n",
      "lif layer 2 self.abs_max_v: 2683.5\n",
      "lif layer 2 self.abs_max_v: 2774.5\n",
      "lif layer 2 self.abs_max_v: 2812.5\n",
      "fc layer 3 self.abs_max_out: 666.0\n",
      "fc layer 3 self.abs_max_out: 673.0\n",
      "fc layer 1 self.abs_max_out: 2581.0\n",
      "fc layer 3 self.abs_max_out: 684.0\n",
      "lif layer 1 self.abs_max_v: 4203.0\n",
      "lif layer 1 self.abs_max_v: 4279.0\n",
      "lif layer 1 self.abs_max_v: 4312.5\n",
      "lif layer 1 self.abs_max_v: 4373.5\n",
      "fc layer 2 self.abs_max_out: 1600.0\n",
      "fc layer 2 self.abs_max_out: 1627.0\n",
      "fc layer 2 self.abs_max_out: 1631.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.413123/  1.685245, val:  52.50%, val_best:  53.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.23 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   2  Sparsity: 70.5638%\n",
      "layer   3  Sparsity: 61.7860%\n",
      "total_backward_count 88110 real_backward_count 12412  14.087%\n",
      "fc layer 3 self.abs_max_out: 726.0\n",
      "fc layer 2 self.abs_max_out: 1675.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.408115/  1.694475, val:  49.17%, val_best:  53.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   2  Sparsity: 70.5338%\n",
      "layer   3  Sparsity: 62.0346%\n",
      "total_backward_count 97900 real_backward_count 13603  13.895%\n",
      "fc layer 3 self.abs_max_out: 744.0\n",
      "fc layer 1 self.abs_max_out: 2603.0\n",
      "lif layer 1 self.abs_max_v: 4502.5\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.395446/  1.682664, val:  54.17%, val_best:  54.17%, tr:  99.49%, tr_best:  99.80%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0462%\n",
      "layer   2  Sparsity: 71.0276%\n",
      "layer   3  Sparsity: 62.5041%\n",
      "total_backward_count 107690 real_backward_count 14805  13.748%\n",
      "fc layer 1 self.abs_max_out: 2658.0\n",
      "fc layer 1 self.abs_max_out: 2834.0\n",
      "lif layer 1 self.abs_max_v: 4709.5\n",
      "lif layer 1 self.abs_max_v: 4793.5\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.375792/  1.671839, val:  45.83%, val_best:  54.17%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1115%\n",
      "layer   2  Sparsity: 71.2597%\n",
      "layer   3  Sparsity: 63.1316%\n",
      "total_backward_count 117480 real_backward_count 15947  13.574%\n",
      "lif layer 2 self.abs_max_v: 2838.0\n",
      "lif layer 2 self.abs_max_v: 2876.0\n",
      "lif layer 2 self.abs_max_v: 2904.0\n",
      "lif layer 2 self.abs_max_v: 2939.0\n",
      "fc layer 3 self.abs_max_out: 761.0\n",
      "fc layer 3 self.abs_max_out: 784.0\n",
      "fc layer 1 self.abs_max_out: 2856.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.366239/  1.658416, val:  55.00%, val_best:  55.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0290%\n",
      "layer   2  Sparsity: 70.1036%\n",
      "layer   3  Sparsity: 62.3666%\n",
      "total_backward_count 127270 real_backward_count 17088  13.427%\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.350100/  1.659824, val:  48.33%, val_best:  55.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   2  Sparsity: 70.2000%\n",
      "layer   3  Sparsity: 62.7692%\n",
      "total_backward_count 137060 real_backward_count 18191  13.272%\n",
      "lif layer 1 self.abs_max_v: 4846.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.369848/  1.655243, val:  59.17%, val_best:  59.17%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   2  Sparsity: 70.0253%\n",
      "layer   3  Sparsity: 63.9102%\n",
      "total_backward_count 146850 real_backward_count 19324  13.159%\n",
      "fc layer 2 self.abs_max_out: 1742.0\n",
      "fc layer 1 self.abs_max_out: 2893.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.345651/  1.631217, val:  52.92%, val_best:  59.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   2  Sparsity: 70.0038%\n",
      "layer   3  Sparsity: 63.8687%\n",
      "total_backward_count 156640 real_backward_count 20428  13.041%\n",
      "lif layer 2 self.abs_max_v: 3057.5\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.332228/  1.629249, val:  55.83%, val_best:  59.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.07 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   2  Sparsity: 69.8207%\n",
      "layer   3  Sparsity: 63.9940%\n",
      "total_backward_count 166430 real_backward_count 21582  12.968%\n",
      "lif layer 1 self.abs_max_v: 5087.0\n",
      "lif layer 1 self.abs_max_v: 5209.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.320638/  1.639420, val:  55.42%, val_best:  59.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.07 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   2  Sparsity: 69.4642%\n",
      "layer   3  Sparsity: 64.4575%\n",
      "total_backward_count 176220 real_backward_count 22701  12.882%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.320847/  1.622592, val:  52.50%, val_best:  59.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0737%\n",
      "layer   2  Sparsity: 69.2353%\n",
      "layer   3  Sparsity: 63.8855%\n",
      "total_backward_count 186010 real_backward_count 23745  12.765%\n",
      "fc layer 2 self.abs_max_out: 1746.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.325218/  1.596473, val:  63.75%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1072%\n",
      "layer   2  Sparsity: 68.8884%\n",
      "layer   3  Sparsity: 63.9529%\n",
      "total_backward_count 195800 real_backward_count 24836  12.684%\n",
      "fc layer 1 self.abs_max_out: 2964.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.299626/  1.586259, val:  55.83%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0238%\n",
      "layer   2  Sparsity: 68.8354%\n",
      "layer   3  Sparsity: 63.6280%\n",
      "total_backward_count 205590 real_backward_count 25907  12.601%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.286436/  1.587926, val:  61.25%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   2  Sparsity: 69.3222%\n",
      "layer   3  Sparsity: 64.7378%\n",
      "total_backward_count 215380 real_backward_count 26922  12.500%\n",
      "fc layer 1 self.abs_max_out: 2986.0\n",
      "fc layer 2 self.abs_max_out: 1799.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.296788/  1.597010, val:  55.83%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1038%\n",
      "layer   2  Sparsity: 68.9248%\n",
      "layer   3  Sparsity: 65.7658%\n",
      "total_backward_count 225170 real_backward_count 27942  12.409%\n",
      "fc layer 1 self.abs_max_out: 3015.0\n",
      "fc layer 1 self.abs_max_out: 3025.0\n",
      "lif layer 1 self.abs_max_v: 5317.5\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.295820/  1.571162, val:  55.00%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0728%\n",
      "layer   2  Sparsity: 68.8694%\n",
      "layer   3  Sparsity: 65.3592%\n",
      "total_backward_count 234960 real_backward_count 28972  12.331%\n",
      "fc layer 1 self.abs_max_out: 3045.0\n",
      "fc layer 2 self.abs_max_out: 1811.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.288808/  1.580214, val:  59.58%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1229%\n",
      "layer   2  Sparsity: 69.1467%\n",
      "layer   3  Sparsity: 65.0968%\n",
      "total_backward_count 244750 real_backward_count 29931  12.229%\n",
      "fc layer 1 self.abs_max_out: 3046.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.280308/  1.555359, val:  61.25%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.46 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0451%\n",
      "layer   2  Sparsity: 68.9026%\n",
      "layer   3  Sparsity: 64.8734%\n",
      "total_backward_count 254540 real_backward_count 30924  12.149%\n",
      "fc layer 2 self.abs_max_out: 1817.0\n",
      "fc layer 2 self.abs_max_out: 1882.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.256262/  1.511556, val:  72.92%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   2  Sparsity: 68.8059%\n",
      "layer   3  Sparsity: 64.6234%\n",
      "total_backward_count 264330 real_backward_count 31908  12.071%\n",
      "fc layer 2 self.abs_max_out: 1925.0\n",
      "lif layer 1 self.abs_max_v: 5319.5\n",
      "lif layer 1 self.abs_max_v: 5456.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.234757/  1.540326, val:  55.83%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   2  Sparsity: 68.2988%\n",
      "layer   3  Sparsity: 64.7345%\n",
      "total_backward_count 274120 real_backward_count 32924  12.011%\n",
      "fc layer 1 self.abs_max_out: 3072.0\n",
      "lif layer 2 self.abs_max_v: 3087.0\n",
      "fc layer 1 self.abs_max_out: 3207.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.225813/  1.520008, val:  69.17%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   2  Sparsity: 68.0258%\n",
      "layer   3  Sparsity: 64.9169%\n",
      "total_backward_count 283910 real_backward_count 33867  11.929%\n",
      "lif layer 1 self.abs_max_v: 5604.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.243378/  1.538828, val:  61.67%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.83 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   2  Sparsity: 67.7096%\n",
      "layer   3  Sparsity: 64.8247%\n",
      "total_backward_count 293700 real_backward_count 34804  11.850%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.232988/  1.496941, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   2  Sparsity: 67.3125%\n",
      "layer   3  Sparsity: 65.0399%\n",
      "total_backward_count 303490 real_backward_count 35730  11.773%\n",
      "fc layer 2 self.abs_max_out: 1927.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.236187/  1.502313, val:  65.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 67.9836%\n",
      "layer   3  Sparsity: 65.3511%\n",
      "total_backward_count 313280 real_backward_count 36593  11.681%\n",
      "fc layer 3 self.abs_max_out: 787.0\n",
      "lif layer 2 self.abs_max_v: 3106.0\n",
      "lif layer 2 self.abs_max_v: 3138.5\n",
      "fc layer 2 self.abs_max_out: 1981.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.231002/  1.488020, val:  75.00%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   2  Sparsity: 68.2976%\n",
      "layer   3  Sparsity: 64.8616%\n",
      "total_backward_count 323070 real_backward_count 37512  11.611%\n",
      "fc layer 2 self.abs_max_out: 2001.0\n",
      "fc layer 1 self.abs_max_out: 3256.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.203400/  1.457309, val:  70.42%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.23 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1023%\n",
      "layer   2  Sparsity: 68.6992%\n",
      "layer   3  Sparsity: 64.9994%\n",
      "total_backward_count 332860 real_backward_count 38329  11.515%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.181803/  1.425662, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1006%\n",
      "layer   2  Sparsity: 67.7783%\n",
      "layer   3  Sparsity: 65.2979%\n",
      "total_backward_count 342650 real_backward_count 39183  11.435%\n",
      "fc layer 3 self.abs_max_out: 834.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.174338/  1.471110, val:  70.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0583%\n",
      "layer   2  Sparsity: 67.5541%\n",
      "layer   3  Sparsity: 65.4499%\n",
      "total_backward_count 352440 real_backward_count 40023  11.356%\n",
      "fc layer 1 self.abs_max_out: 3258.0\n",
      "fc layer 1 self.abs_max_out: 3407.0\n",
      "fc layer 2 self.abs_max_out: 2008.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.190822/  1.436329, val:  70.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.06 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.1037%\n",
      "layer   2  Sparsity: 67.1954%\n",
      "layer   3  Sparsity: 65.4386%\n",
      "total_backward_count 362230 real_backward_count 40890  11.288%\n",
      "fc layer 2 self.abs_max_out: 2037.0\n",
      "lif layer 1 self.abs_max_v: 5768.5\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.171416/  1.465537, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   2  Sparsity: 67.3941%\n",
      "layer   3  Sparsity: 65.7929%\n",
      "total_backward_count 372020 real_backward_count 41713  11.213%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.187762/  1.410354, val:  81.25%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0435%\n",
      "layer   2  Sparsity: 67.6806%\n",
      "layer   3  Sparsity: 65.4030%\n",
      "total_backward_count 381810 real_backward_count 42502  11.132%\n",
      "fc layer 1 self.abs_max_out: 3464.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.173622/  1.430024, val:  73.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.71 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   2  Sparsity: 67.2644%\n",
      "layer   3  Sparsity: 65.4886%\n",
      "total_backward_count 391600 real_backward_count 43307  11.059%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.187512/  1.421044, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   2  Sparsity: 67.3652%\n",
      "layer   3  Sparsity: 65.4459%\n",
      "total_backward_count 401390 real_backward_count 44114  10.990%\n",
      "lif layer 1 self.abs_max_v: 5997.5\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.161529/  1.511093, val:  50.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 67.3494%\n",
      "layer   3  Sparsity: 64.5559%\n",
      "total_backward_count 411180 real_backward_count 44900  10.920%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.159622/  1.394744, val:  74.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.19 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   2  Sparsity: 67.2597%\n",
      "layer   3  Sparsity: 65.1627%\n",
      "total_backward_count 420970 real_backward_count 45673  10.849%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.149760/  1.401988, val:  80.83%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   2  Sparsity: 67.3471%\n",
      "layer   3  Sparsity: 65.2753%\n",
      "total_backward_count 430760 real_backward_count 46407  10.773%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.155687/  1.419400, val:  70.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   2  Sparsity: 67.4792%\n",
      "layer   3  Sparsity: 65.6836%\n",
      "total_backward_count 440550 real_backward_count 47171  10.707%\n",
      "fc layer 1 self.abs_max_out: 3468.0\n",
      "lif layer 2 self.abs_max_v: 3298.5\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.155352/  1.415420, val:  77.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.64 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0911%\n",
      "layer   2  Sparsity: 67.1326%\n",
      "layer   3  Sparsity: 65.6080%\n",
      "total_backward_count 450340 real_backward_count 47945  10.646%\n",
      "fc layer 3 self.abs_max_out: 856.0\n",
      "fc layer 1 self.abs_max_out: 3490.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.156475/  1.437209, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   2  Sparsity: 66.6322%\n",
      "layer   3  Sparsity: 65.4132%\n",
      "total_backward_count 460130 real_backward_count 48708  10.586%\n",
      "lif layer 2 self.abs_max_v: 3501.0\n",
      "fc layer 1 self.abs_max_out: 3539.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.150568/  1.421908, val:  79.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   2  Sparsity: 66.4639%\n",
      "layer   3  Sparsity: 65.9128%\n",
      "total_backward_count 469920 real_backward_count 49470  10.527%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.129385/  1.403577, val:  75.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.83 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0624%\n",
      "layer   2  Sparsity: 67.0594%\n",
      "layer   3  Sparsity: 65.4888%\n",
      "total_backward_count 479710 real_backward_count 50191  10.463%\n",
      "fc layer 1 self.abs_max_out: 3629.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.123477/  1.379859, val:  81.67%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0953%\n",
      "layer   2  Sparsity: 67.4729%\n",
      "layer   3  Sparsity: 65.3146%\n",
      "total_backward_count 489500 real_backward_count 50871  10.392%\n",
      "fc layer 1 self.abs_max_out: 3676.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.106772/  1.342917, val:  84.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0375%\n",
      "layer   2  Sparsity: 67.1463%\n",
      "layer   3  Sparsity: 65.6072%\n",
      "total_backward_count 499290 real_backward_count 51557  10.326%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.115141/  1.372496, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0508%\n",
      "layer   2  Sparsity: 66.9014%\n",
      "layer   3  Sparsity: 65.7640%\n",
      "total_backward_count 509080 real_backward_count 52235  10.261%\n",
      "fc layer 1 self.abs_max_out: 3849.0\n",
      "fc layer 2 self.abs_max_out: 2038.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.132118/  1.393157, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 67.0496%\n",
      "layer   3  Sparsity: 66.1199%\n",
      "total_backward_count 518870 real_backward_count 52836  10.183%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.119037/  1.400998, val:  77.92%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0440%\n",
      "layer   2  Sparsity: 67.0321%\n",
      "layer   3  Sparsity: 65.6720%\n",
      "total_backward_count 528660 real_backward_count 53511  10.122%\n",
      "fc layer 2 self.abs_max_out: 2068.0\n",
      "lif layer 2 self.abs_max_v: 3501.5\n",
      "lif layer 1 self.abs_max_v: 6088.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.103962/  1.348715, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.45 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0460%\n",
      "layer   2  Sparsity: 67.0541%\n",
      "layer   3  Sparsity: 65.2796%\n",
      "total_backward_count 538450 real_backward_count 54153  10.057%\n",
      "lif layer 1 self.abs_max_v: 6162.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.097423/  1.362562, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.90 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0592%\n",
      "layer   2  Sparsity: 66.8942%\n",
      "layer   3  Sparsity: 65.5128%\n",
      "total_backward_count 548240 real_backward_count 54796   9.995%\n",
      "fc layer 1 self.abs_max_out: 3919.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.098056/  1.339738, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0968%\n",
      "layer   2  Sparsity: 66.8461%\n",
      "layer   3  Sparsity: 65.3734%\n",
      "total_backward_count 558030 real_backward_count 55470   9.940%\n",
      "fc layer 2 self.abs_max_out: 2111.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.072177/  1.337421, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0865%\n",
      "layer   2  Sparsity: 67.0959%\n",
      "layer   3  Sparsity: 65.2336%\n",
      "total_backward_count 567820 real_backward_count 56088   9.878%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.074037/  1.347803, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   2  Sparsity: 67.0601%\n",
      "layer   3  Sparsity: 65.4089%\n",
      "total_backward_count 577610 real_backward_count 56744   9.824%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.085153/  1.340186, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0977%\n",
      "layer   2  Sparsity: 67.1983%\n",
      "layer   3  Sparsity: 66.0290%\n",
      "total_backward_count 587400 real_backward_count 57314   9.757%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.076713/  1.320603, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1012%\n",
      "layer   2  Sparsity: 66.9774%\n",
      "layer   3  Sparsity: 66.3683%\n",
      "total_backward_count 597190 real_backward_count 57953   9.704%\n",
      "lif layer 2 self.abs_max_v: 3534.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.081473/  1.387545, val:  64.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.08 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0351%\n",
      "layer   2  Sparsity: 66.6363%\n",
      "layer   3  Sparsity: 66.5872%\n",
      "total_backward_count 606980 real_backward_count 58594   9.653%\n",
      "fc layer 1 self.abs_max_out: 3966.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.083635/  1.355690, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0662%\n",
      "layer   2  Sparsity: 66.1577%\n",
      "layer   3  Sparsity: 65.6733%\n",
      "total_backward_count 616770 real_backward_count 59243   9.605%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.098500/  1.333745, val:  85.83%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.50 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0522%\n",
      "layer   2  Sparsity: 66.5896%\n",
      "layer   3  Sparsity: 66.2231%\n",
      "total_backward_count 626560 real_backward_count 59884   9.558%\n",
      "lif layer 2 self.abs_max_v: 3541.0\n",
      "fc layer 2 self.abs_max_out: 2165.0\n",
      "fc layer 3 self.abs_max_out: 875.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.075557/  1.360361, val:  75.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0545%\n",
      "layer   2  Sparsity: 67.0927%\n",
      "layer   3  Sparsity: 66.3821%\n",
      "total_backward_count 636350 real_backward_count 60488   9.505%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.075494/  1.332943, val:  74.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   2  Sparsity: 67.0293%\n",
      "layer   3  Sparsity: 66.1529%\n",
      "total_backward_count 646140 real_backward_count 61067   9.451%\n",
      "fc layer 1 self.abs_max_out: 4007.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.063895/  1.374722, val:  67.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   2  Sparsity: 66.8118%\n",
      "layer   3  Sparsity: 65.8148%\n",
      "total_backward_count 655930 real_backward_count 61695   9.406%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.048481/  1.305051, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0560%\n",
      "layer   2  Sparsity: 66.9356%\n",
      "layer   3  Sparsity: 66.1790%\n",
      "total_backward_count 665720 real_backward_count 62273   9.354%\n",
      "lif layer 1 self.abs_max_v: 6224.5\n",
      "lif layer 1 self.abs_max_v: 6401.5\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.024955/  1.291521, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0386%\n",
      "layer   2  Sparsity: 66.9639%\n",
      "layer   3  Sparsity: 65.5312%\n",
      "total_backward_count 675510 real_backward_count 62825   9.300%\n",
      "fc layer 2 self.abs_max_out: 2204.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.052834/  1.317202, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.89 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 66.8121%\n",
      "layer   3  Sparsity: 65.4850%\n",
      "total_backward_count 685300 real_backward_count 63433   9.256%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.059817/  1.343881, val:  78.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.14 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0745%\n",
      "layer   2  Sparsity: 66.1038%\n",
      "layer   3  Sparsity: 66.0351%\n",
      "total_backward_count 695090 real_backward_count 64041   9.213%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.043659/  1.296985, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.21 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1292%\n",
      "layer   2  Sparsity: 66.3078%\n",
      "layer   3  Sparsity: 66.6591%\n",
      "total_backward_count 704880 real_backward_count 64606   9.166%\n",
      "lif layer 1 self.abs_max_v: 6473.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.053341/  1.304152, val:  85.42%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.88 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0631%\n",
      "layer   2  Sparsity: 66.8858%\n",
      "layer   3  Sparsity: 66.7562%\n",
      "total_backward_count 714670 real_backward_count 65190   9.122%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.065362/  1.292267, val:  86.67%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0806%\n",
      "layer   2  Sparsity: 66.8137%\n",
      "layer   3  Sparsity: 66.2465%\n",
      "total_backward_count 724460 real_backward_count 65753   9.076%\n",
      "fc layer 1 self.abs_max_out: 4019.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.055216/  1.316003, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   2  Sparsity: 66.7004%\n",
      "layer   3  Sparsity: 66.4644%\n",
      "total_backward_count 734250 real_backward_count 66300   9.030%\n",
      "fc layer 1 self.abs_max_out: 4193.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.049932/  1.290388, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.89 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   2  Sparsity: 66.7085%\n",
      "layer   3  Sparsity: 66.7270%\n",
      "total_backward_count 744040 real_backward_count 66786   8.976%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.042384/  1.326697, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   2  Sparsity: 66.6476%\n",
      "layer   3  Sparsity: 66.7226%\n",
      "total_backward_count 753830 real_backward_count 67328   8.931%\n",
      "fc layer 2 self.abs_max_out: 2286.0\n",
      "lif layer 1 self.abs_max_v: 6604.5\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.045458/  1.285041, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   2  Sparsity: 66.7272%\n",
      "layer   3  Sparsity: 66.0705%\n",
      "total_backward_count 763620 real_backward_count 67872   8.888%\n",
      "fc layer 3 self.abs_max_out: 889.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.040187/  1.296718, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0953%\n",
      "layer   2  Sparsity: 66.7201%\n",
      "layer   3  Sparsity: 66.1018%\n",
      "total_backward_count 773410 real_backward_count 68393   8.843%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.044329/  1.312957, val:  87.08%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.85 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1018%\n",
      "layer   2  Sparsity: 66.4393%\n",
      "layer   3  Sparsity: 66.2684%\n",
      "total_backward_count 783200 real_backward_count 68900   8.797%\n",
      "fc layer 1 self.abs_max_out: 4228.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.042330/  1.302420, val:  81.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.62 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0498%\n",
      "layer   2  Sparsity: 66.1830%\n",
      "layer   3  Sparsity: 65.7594%\n",
      "total_backward_count 792990 real_backward_count 69405   8.752%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.042442/  1.323520, val:  77.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0440%\n",
      "layer   2  Sparsity: 66.3651%\n",
      "layer   3  Sparsity: 65.8315%\n",
      "total_backward_count 802780 real_backward_count 69914   8.709%\n",
      "lif layer 1 self.abs_max_v: 6863.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.051008/  1.301478, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.82 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0623%\n",
      "layer   2  Sparsity: 66.5646%\n",
      "layer   3  Sparsity: 66.1904%\n",
      "total_backward_count 812570 real_backward_count 70421   8.666%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.055231/  1.316842, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0516%\n",
      "layer   2  Sparsity: 66.6156%\n",
      "layer   3  Sparsity: 66.5930%\n",
      "total_backward_count 822360 real_backward_count 70942   8.627%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.031165/  1.338070, val:  74.17%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   2  Sparsity: 66.5769%\n",
      "layer   3  Sparsity: 66.7586%\n",
      "total_backward_count 832150 real_backward_count 71455   8.587%\n",
      "lif layer 1 self.abs_max_v: 6904.0\n",
      "fc layer 1 self.abs_max_out: 4296.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.022863/  1.300429, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.21 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0789%\n",
      "layer   2  Sparsity: 66.7436%\n",
      "layer   3  Sparsity: 66.9594%\n",
      "total_backward_count 841940 real_backward_count 71951   8.546%\n",
      "lif layer 2 self.abs_max_v: 3564.5\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.016112/  1.291723, val:  80.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.51 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1160%\n",
      "layer   2  Sparsity: 66.4137%\n",
      "layer   3  Sparsity: 66.8135%\n",
      "total_backward_count 851730 real_backward_count 72422   8.503%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.016363/  1.271836, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0932%\n",
      "layer   2  Sparsity: 66.4033%\n",
      "layer   3  Sparsity: 66.2781%\n",
      "total_backward_count 861520 real_backward_count 72942   8.467%\n",
      "lif layer 2 self.abs_max_v: 3586.0\n",
      "lif layer 2 self.abs_max_v: 3644.5\n",
      "lif layer 2 self.abs_max_v: 3654.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.000974/  1.305943, val:  80.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   2  Sparsity: 66.3390%\n",
      "layer   3  Sparsity: 66.3220%\n",
      "total_backward_count 871310 real_backward_count 73403   8.424%\n",
      "lif layer 2 self.abs_max_v: 3683.5\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.027390/  1.251812, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0758%\n",
      "layer   2  Sparsity: 66.2534%\n",
      "layer   3  Sparsity: 66.3046%\n",
      "total_backward_count 881100 real_backward_count 73904   8.388%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.007354/  1.282781, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.17 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0583%\n",
      "layer   2  Sparsity: 66.3616%\n",
      "layer   3  Sparsity: 66.3242%\n",
      "total_backward_count 890890 real_backward_count 74380   8.349%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.008275/  1.269536, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.14 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0564%\n",
      "layer   2  Sparsity: 66.8098%\n",
      "layer   3  Sparsity: 66.4061%\n",
      "total_backward_count 900680 real_backward_count 74844   8.310%\n",
      "lif layer 2 self.abs_max_v: 3686.0\n",
      "lif layer 2 self.abs_max_v: 3725.5\n",
      "lif layer 2 self.abs_max_v: 3741.0\n",
      "lif layer 2 self.abs_max_v: 3881.5\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  0.997975/  1.255448, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.14 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0481%\n",
      "layer   2  Sparsity: 66.8326%\n",
      "layer   3  Sparsity: 66.3566%\n",
      "total_backward_count 910470 real_backward_count 75283   8.269%\n",
      "lif layer 2 self.abs_max_v: 3990.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  0.997010/  1.280342, val:  85.00%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1290%\n",
      "layer   2  Sparsity: 66.5352%\n",
      "layer   3  Sparsity: 66.2613%\n",
      "total_backward_count 920260 real_backward_count 75777   8.234%\n",
      "lif layer 2 self.abs_max_v: 4034.5\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.022779/  1.255062, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1079%\n",
      "layer   2  Sparsity: 66.3848%\n",
      "layer   3  Sparsity: 66.6703%\n",
      "total_backward_count 930050 real_backward_count 76272   8.201%\n",
      "fc layer 3 self.abs_max_out: 900.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  0.994803/  1.270991, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0622%\n",
      "layer   2  Sparsity: 66.3386%\n",
      "layer   3  Sparsity: 66.6529%\n",
      "total_backward_count 939840 real_backward_count 76756   8.167%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  0.988932/  1.249829, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 66.4601%\n",
      "layer   3  Sparsity: 66.7192%\n",
      "total_backward_count 949630 real_backward_count 77239   8.134%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  0.979252/  1.269158, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.50 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0971%\n",
      "layer   2  Sparsity: 66.5790%\n",
      "layer   3  Sparsity: 66.4132%\n",
      "total_backward_count 959420 real_backward_count 77665   8.095%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  0.986577/  1.241673, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0833%\n",
      "layer   2  Sparsity: 66.3446%\n",
      "layer   3  Sparsity: 66.2137%\n",
      "total_backward_count 969210 real_backward_count 78099   8.058%\n",
      "fc layer 1 self.abs_max_out: 4405.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.006475/  1.255874, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0372%\n",
      "layer   2  Sparsity: 66.3402%\n",
      "layer   3  Sparsity: 65.8674%\n",
      "total_backward_count 979000 real_backward_count 78573   8.026%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  0.991833/  1.254354, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.51 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   2  Sparsity: 66.4237%\n",
      "layer   3  Sparsity: 66.2588%\n",
      "total_backward_count 988790 real_backward_count 79046   7.994%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  0.989474/  1.266129, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0779%\n",
      "layer   2  Sparsity: 66.3587%\n",
      "layer   3  Sparsity: 66.4516%\n",
      "total_backward_count 998580 real_backward_count 79444   7.956%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  0.986433/  1.261781, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   2  Sparsity: 66.7221%\n",
      "layer   3  Sparsity: 66.6376%\n",
      "total_backward_count 1008370 real_backward_count 79872   7.921%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  0.981062/  1.253021, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.38 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0406%\n",
      "layer   2  Sparsity: 66.3574%\n",
      "layer   3  Sparsity: 66.5659%\n",
      "total_backward_count 1018160 real_backward_count 80257   7.883%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  0.982620/  1.239356, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.70 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   2  Sparsity: 66.3991%\n",
      "layer   3  Sparsity: 66.7470%\n",
      "total_backward_count 1027950 real_backward_count 80681   7.849%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  0.961892/  1.241637, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.82 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   2  Sparsity: 66.4761%\n",
      "layer   3  Sparsity: 66.9945%\n",
      "total_backward_count 1037740 real_backward_count 81062   7.811%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  0.962489/  1.244962, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.57 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   2  Sparsity: 66.5438%\n",
      "layer   3  Sparsity: 66.7411%\n",
      "total_backward_count 1047530 real_backward_count 81447   7.775%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  0.967441/  1.246208, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   2  Sparsity: 66.4847%\n",
      "layer   3  Sparsity: 66.6977%\n",
      "total_backward_count 1057320 real_backward_count 81839   7.740%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  0.962195/  1.217799, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0904%\n",
      "layer   2  Sparsity: 66.5655%\n",
      "layer   3  Sparsity: 66.9403%\n",
      "total_backward_count 1067110 real_backward_count 82231   7.706%\n",
      "fc layer 3 self.abs_max_out: 913.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  0.960434/  1.216135, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0620%\n",
      "layer   2  Sparsity: 66.2808%\n",
      "layer   3  Sparsity: 66.5265%\n",
      "total_backward_count 1076900 real_backward_count 82606   7.671%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  0.961724/  1.230784, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   2  Sparsity: 66.2673%\n",
      "layer   3  Sparsity: 66.6774%\n",
      "total_backward_count 1086690 real_backward_count 83039   7.641%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  0.962075/  1.252170, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.51 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0472%\n",
      "layer   2  Sparsity: 66.2128%\n",
      "layer   3  Sparsity: 66.9133%\n",
      "total_backward_count 1096480 real_backward_count 83412   7.607%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  0.958331/  1.233756, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   2  Sparsity: 66.2296%\n",
      "layer   3  Sparsity: 66.7834%\n",
      "total_backward_count 1106270 real_backward_count 83820   7.577%\n",
      "lif layer 1 self.abs_max_v: 6915.0\n",
      "lif layer 1 self.abs_max_v: 7134.5\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  0.940811/  1.242981, val:  77.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.72 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   2  Sparsity: 66.2598%\n",
      "layer   3  Sparsity: 67.1351%\n",
      "total_backward_count 1116060 real_backward_count 84217   7.546%\n",
      "fc layer 2 self.abs_max_out: 2319.0\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  0.950312/  1.209742, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   2  Sparsity: 66.5573%\n",
      "layer   3  Sparsity: 66.7473%\n",
      "total_backward_count 1125850 real_backward_count 84608   7.515%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  0.934674/  1.168152, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0887%\n",
      "layer   2  Sparsity: 66.4320%\n",
      "layer   3  Sparsity: 66.8326%\n",
      "total_backward_count 1135640 real_backward_count 84989   7.484%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  0.919501/  1.196516, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0909%\n",
      "layer   2  Sparsity: 66.1317%\n",
      "layer   3  Sparsity: 66.9392%\n",
      "total_backward_count 1145430 real_backward_count 85367   7.453%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  0.938890/  1.228674, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.15 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0961%\n",
      "layer   2  Sparsity: 66.2672%\n",
      "layer   3  Sparsity: 66.6003%\n",
      "total_backward_count 1155220 real_backward_count 85762   7.424%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  0.943052/  1.246628, val:  72.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.53 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   2  Sparsity: 66.4843%\n",
      "layer   3  Sparsity: 66.8109%\n",
      "total_backward_count 1165010 real_backward_count 86193   7.398%\n",
      "lif layer 1 self.abs_max_v: 7233.0\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  0.935256/  1.234750, val:  81.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 66.5909%\n",
      "layer   3  Sparsity: 66.3592%\n",
      "total_backward_count 1174800 real_backward_count 86578   7.370%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  0.935615/  1.189269, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.92 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   2  Sparsity: 66.3603%\n",
      "layer   3  Sparsity: 66.5674%\n",
      "total_backward_count 1184590 real_backward_count 86967   7.342%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  0.921049/  1.174657, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.56 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   2  Sparsity: 66.1950%\n",
      "layer   3  Sparsity: 66.6512%\n",
      "total_backward_count 1194380 real_backward_count 87360   7.314%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  0.909366/  1.195751, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.08 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0629%\n",
      "layer   2  Sparsity: 66.1351%\n",
      "layer   3  Sparsity: 66.7562%\n",
      "total_backward_count 1204170 real_backward_count 87723   7.285%\n",
      "fc layer 1 self.abs_max_out: 4475.0\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  0.899649/  1.185438, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.36 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0556%\n",
      "layer   2  Sparsity: 66.4548%\n",
      "layer   3  Sparsity: 66.9185%\n",
      "total_backward_count 1213960 real_backward_count 88062   7.254%\n",
      "lif layer 1 self.abs_max_v: 7428.0\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  0.899853/  1.237401, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.64 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0740%\n",
      "layer   2  Sparsity: 66.6122%\n",
      "layer   3  Sparsity: 66.8975%\n",
      "total_backward_count 1223750 real_backward_count 88436   7.227%\n",
      "fc layer 3 self.abs_max_out: 916.0\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  0.914893/  1.221544, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.14 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0506%\n",
      "layer   2  Sparsity: 66.7189%\n",
      "layer   3  Sparsity: 67.2712%\n",
      "total_backward_count 1233540 real_backward_count 88783   7.197%\n",
      "fc layer 1 self.abs_max_out: 4478.0\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  0.913152/  1.187550, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0887%\n",
      "layer   2  Sparsity: 66.7239%\n",
      "layer   3  Sparsity: 67.4729%\n",
      "total_backward_count 1243330 real_backward_count 89105   7.167%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  0.919287/  1.204401, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.57 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   2  Sparsity: 66.4084%\n",
      "layer   3  Sparsity: 67.4721%\n",
      "total_backward_count 1253120 real_backward_count 89427   7.136%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  0.915308/  1.163853, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1010%\n",
      "layer   2  Sparsity: 66.4428%\n",
      "layer   3  Sparsity: 67.2558%\n",
      "total_backward_count 1262910 real_backward_count 89797   7.110%\n",
      "fc layer 3 self.abs_max_out: 929.0\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  0.896462/  1.167663, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0236%\n",
      "layer   2  Sparsity: 66.0562%\n",
      "layer   3  Sparsity: 67.1396%\n",
      "total_backward_count 1272700 real_backward_count 90135   7.082%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  0.896548/  1.175079, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0977%\n",
      "layer   2  Sparsity: 66.0648%\n",
      "layer   3  Sparsity: 67.0580%\n",
      "total_backward_count 1282490 real_backward_count 90460   7.053%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  0.890230/  1.192078, val:  80.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0828%\n",
      "layer   2  Sparsity: 66.0324%\n",
      "layer   3  Sparsity: 66.8692%\n",
      "total_backward_count 1292280 real_backward_count 90806   7.027%\n",
      "fc layer 3 self.abs_max_out: 936.0\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  0.896978/  1.206420, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.83 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 66.1499%\n",
      "layer   3  Sparsity: 67.2023%\n",
      "total_backward_count 1302070 real_backward_count 91165   7.002%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  0.900375/  1.139380, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.19 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   2  Sparsity: 66.2572%\n",
      "layer   3  Sparsity: 67.2133%\n",
      "total_backward_count 1311860 real_backward_count 91487   6.974%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  0.900322/  1.160310, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.05 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 66.2741%\n",
      "layer   3  Sparsity: 67.2146%\n",
      "total_backward_count 1321650 real_backward_count 91822   6.948%\n",
      "fc layer 2 self.abs_max_out: 2361.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  0.890722/  1.182153, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.14 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 66.3527%\n",
      "layer   3  Sparsity: 67.8187%\n",
      "total_backward_count 1331440 real_backward_count 92167   6.922%\n",
      "lif layer 1 self.abs_max_v: 7484.0\n",
      "fc layer 3 self.abs_max_out: 949.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  0.907671/  1.188152, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.92 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1114%\n",
      "layer   2  Sparsity: 66.1291%\n",
      "layer   3  Sparsity: 67.4366%\n",
      "total_backward_count 1341230 real_backward_count 92514   6.898%\n",
      "lif layer 1 self.abs_max_v: 7687.0\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  0.883905/  1.195959, val:  79.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.02 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0851%\n",
      "layer   2  Sparsity: 66.1152%\n",
      "layer   3  Sparsity: 67.5825%\n",
      "total_backward_count 1351020 real_backward_count 92808   6.869%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  0.887112/  1.161139, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   2  Sparsity: 66.2992%\n",
      "layer   3  Sparsity: 67.7677%\n",
      "total_backward_count 1360810 real_backward_count 93130   6.844%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  0.879955/  1.185133, val:  79.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.22 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1071%\n",
      "layer   2  Sparsity: 66.2457%\n",
      "layer   3  Sparsity: 67.4361%\n",
      "total_backward_count 1370600 real_backward_count 93455   6.819%\n",
      "fc layer 1 self.abs_max_out: 4547.0\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  0.863955/  1.141982, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0691%\n",
      "layer   2  Sparsity: 66.3583%\n",
      "layer   3  Sparsity: 67.0938%\n",
      "total_backward_count 1380390 real_backward_count 93763   6.793%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  0.859242/  1.138010, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0549%\n",
      "layer   2  Sparsity: 66.4313%\n",
      "layer   3  Sparsity: 67.5711%\n",
      "total_backward_count 1390180 real_backward_count 94103   6.769%\n",
      "fc layer 1 self.abs_max_out: 4563.0\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  0.854897/  1.141811, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.70 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.1351%\n",
      "layer   2  Sparsity: 66.2726%\n",
      "layer   3  Sparsity: 67.2934%\n",
      "total_backward_count 1399970 real_backward_count 94447   6.746%\n",
      "lif layer 1 self.abs_max_v: 7688.0\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  0.851169/  1.135139, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 66.3738%\n",
      "layer   3  Sparsity: 67.4062%\n",
      "total_backward_count 1409760 real_backward_count 94746   6.721%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  0.857865/  1.138022, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0486%\n",
      "layer   2  Sparsity: 66.0801%\n",
      "layer   3  Sparsity: 67.4792%\n",
      "total_backward_count 1419550 real_backward_count 95051   6.696%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  0.864591/  1.119203, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0746%\n",
      "layer   2  Sparsity: 66.0657%\n",
      "layer   3  Sparsity: 67.0154%\n",
      "total_backward_count 1429340 real_backward_count 95368   6.672%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  0.852026/  1.159473, val:  81.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0694%\n",
      "layer   2  Sparsity: 66.2653%\n",
      "layer   3  Sparsity: 67.2452%\n",
      "total_backward_count 1439130 real_backward_count 95674   6.648%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  0.851496/  1.155255, val:  83.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.31 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   2  Sparsity: 66.5788%\n",
      "layer   3  Sparsity: 67.3264%\n",
      "total_backward_count 1448920 real_backward_count 95991   6.625%\n",
      "fc layer 2 self.abs_max_out: 2420.0\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  0.856781/  1.143768, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.53 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 66.5360%\n",
      "layer   3  Sparsity: 67.3423%\n",
      "total_backward_count 1458710 real_backward_count 96280   6.600%\n",
      "fc layer 1 self.abs_max_out: 4602.0\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  0.852891/  1.167948, val:  82.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.00 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0660%\n",
      "layer   2  Sparsity: 66.4798%\n",
      "layer   3  Sparsity: 67.7904%\n",
      "total_backward_count 1468500 real_backward_count 96575   6.576%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  0.858978/  1.153802, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   2  Sparsity: 66.5591%\n",
      "layer   3  Sparsity: 67.7501%\n",
      "total_backward_count 1478290 real_backward_count 96909   6.555%\n",
      "lif layer 1 self.abs_max_v: 7712.0\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  0.858071/  1.157807, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   2  Sparsity: 66.5385%\n",
      "layer   3  Sparsity: 67.5225%\n",
      "total_backward_count 1488080 real_backward_count 97176   6.530%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  0.846112/  1.139829, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1030%\n",
      "layer   2  Sparsity: 66.5409%\n",
      "layer   3  Sparsity: 67.3605%\n",
      "total_backward_count 1497870 real_backward_count 97466   6.507%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  0.843199/  1.137821, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.18 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   2  Sparsity: 66.7726%\n",
      "layer   3  Sparsity: 67.8450%\n",
      "total_backward_count 1507660 real_backward_count 97757   6.484%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  0.828008/  1.112907, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.46 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0492%\n",
      "layer   2  Sparsity: 66.7011%\n",
      "layer   3  Sparsity: 68.2089%\n",
      "total_backward_count 1517450 real_backward_count 98015   6.459%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  0.837097/  1.123904, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0362%\n",
      "layer   2  Sparsity: 66.2703%\n",
      "layer   3  Sparsity: 67.8308%\n",
      "total_backward_count 1527240 real_backward_count 98290   6.436%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  0.838894/  1.149246, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.49 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0710%\n",
      "layer   2  Sparsity: 66.1929%\n",
      "layer   3  Sparsity: 67.8095%\n",
      "total_backward_count 1537030 real_backward_count 98555   6.412%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  0.843974/  1.104420, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.17 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0241%\n",
      "layer   2  Sparsity: 66.1352%\n",
      "layer   3  Sparsity: 68.2267%\n",
      "total_backward_count 1546820 real_backward_count 98849   6.390%\n",
      "fc layer 3 self.abs_max_out: 950.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  0.829646/  1.104213, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0536%\n",
      "layer   2  Sparsity: 66.1467%\n",
      "layer   3  Sparsity: 68.0477%\n",
      "total_backward_count 1556610 real_backward_count 99120   6.368%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  0.834921/  1.116749, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.27 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   2  Sparsity: 66.2267%\n",
      "layer   3  Sparsity: 67.6609%\n",
      "total_backward_count 1566400 real_backward_count 99415   6.347%\n",
      "fc layer 2 self.abs_max_out: 2426.0\n",
      "fc layer 3 self.abs_max_out: 956.0\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  0.847274/  1.112683, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.38 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0718%\n",
      "layer   2  Sparsity: 65.9651%\n",
      "layer   3  Sparsity: 67.7527%\n",
      "total_backward_count 1576190 real_backward_count 99718   6.327%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  0.839581/  1.150818, val:  83.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.53 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   2  Sparsity: 65.9114%\n",
      "layer   3  Sparsity: 67.1535%\n",
      "total_backward_count 1585980 real_backward_count 100045   6.308%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  0.824843/  1.126661, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   2  Sparsity: 66.2334%\n",
      "layer   3  Sparsity: 67.3571%\n",
      "total_backward_count 1595770 real_backward_count 100344   6.288%\n",
      "fc layer 2 self.abs_max_out: 2438.0\n",
      "fc layer 2 self.abs_max_out: 2442.0\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  0.828725/  1.094400, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.29 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   2  Sparsity: 66.3159%\n",
      "layer   3  Sparsity: 67.6796%\n",
      "total_backward_count 1605560 real_backward_count 100607   6.266%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  0.821612/  1.094347, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0891%\n",
      "layer   2  Sparsity: 66.4193%\n",
      "layer   3  Sparsity: 67.9336%\n",
      "total_backward_count 1615350 real_backward_count 100852   6.243%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  0.808803/  1.122611, val:  83.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   2  Sparsity: 66.3179%\n",
      "layer   3  Sparsity: 68.1680%\n",
      "total_backward_count 1625140 real_backward_count 101129   6.223%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  0.834685/  1.128320, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.60 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   2  Sparsity: 66.0153%\n",
      "layer   3  Sparsity: 67.5146%\n",
      "total_backward_count 1634930 real_backward_count 101397   6.202%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  0.837940/  1.110771, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 65.8909%\n",
      "layer   3  Sparsity: 67.3399%\n",
      "total_backward_count 1644720 real_backward_count 101639   6.180%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  0.821805/  1.105615, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.57 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0243%\n",
      "layer   2  Sparsity: 66.1124%\n",
      "layer   3  Sparsity: 67.2982%\n",
      "total_backward_count 1654510 real_backward_count 101922   6.160%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  0.818868/  1.113696, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   2  Sparsity: 66.1199%\n",
      "layer   3  Sparsity: 67.3494%\n",
      "total_backward_count 1664300 real_backward_count 102199   6.141%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  0.832974/  1.099363, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.68 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0828%\n",
      "layer   2  Sparsity: 66.1911%\n",
      "layer   3  Sparsity: 67.4653%\n",
      "total_backward_count 1674090 real_backward_count 102474   6.121%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  0.833010/  1.105326, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 65.9900%\n",
      "layer   3  Sparsity: 67.4733%\n",
      "total_backward_count 1683880 real_backward_count 102723   6.100%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  0.824752/  1.120418, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1067%\n",
      "layer   2  Sparsity: 66.1284%\n",
      "layer   3  Sparsity: 67.6324%\n",
      "total_backward_count 1693670 real_backward_count 102988   6.081%\n",
      "fc layer 1 self.abs_max_out: 4619.0\n",
      "lif layer 1 self.abs_max_v: 7838.0\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  0.826947/  1.122949, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.46 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1166%\n",
      "layer   2  Sparsity: 66.0242%\n",
      "layer   3  Sparsity: 68.2906%\n",
      "total_backward_count 1703460 real_backward_count 103233   6.060%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  0.840297/  1.123713, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   2  Sparsity: 66.2065%\n",
      "layer   3  Sparsity: 68.3075%\n",
      "total_backward_count 1713250 real_backward_count 103522   6.042%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  0.834138/  1.120934, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.08 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   2  Sparsity: 66.2298%\n",
      "layer   3  Sparsity: 68.2637%\n",
      "total_backward_count 1723040 real_backward_count 103752   6.021%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  0.843702/  1.113225, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1134%\n",
      "layer   2  Sparsity: 66.2757%\n",
      "layer   3  Sparsity: 68.2130%\n",
      "total_backward_count 1732830 real_backward_count 104002   6.002%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  0.847875/  1.116313, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1029%\n",
      "layer   2  Sparsity: 66.3049%\n",
      "layer   3  Sparsity: 68.0853%\n",
      "total_backward_count 1742620 real_backward_count 104243   5.982%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  0.839914/  1.120502, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.53 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   2  Sparsity: 66.1757%\n",
      "layer   3  Sparsity: 67.7501%\n",
      "total_backward_count 1752410 real_backward_count 104487   5.962%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  0.846993/  1.126201, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   2  Sparsity: 66.0918%\n",
      "layer   3  Sparsity: 67.9865%\n",
      "total_backward_count 1762200 real_backward_count 104738   5.944%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  0.835177/  1.105167, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.08 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0861%\n",
      "layer   2  Sparsity: 65.9642%\n",
      "layer   3  Sparsity: 68.1845%\n",
      "total_backward_count 1771990 real_backward_count 104976   5.924%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  0.822095/  1.119115, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1054%\n",
      "layer   2  Sparsity: 66.2666%\n",
      "layer   3  Sparsity: 68.3517%\n",
      "total_backward_count 1781780 real_backward_count 105193   5.904%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  0.830974/  1.114741, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.12 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   2  Sparsity: 66.3002%\n",
      "layer   3  Sparsity: 68.3085%\n",
      "total_backward_count 1791570 real_backward_count 105437   5.885%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  0.822810/  1.099801, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.11 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   2  Sparsity: 66.4336%\n",
      "layer   3  Sparsity: 68.2637%\n",
      "total_backward_count 1801360 real_backward_count 105703   5.868%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  0.839665/  1.131016, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.47 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   2  Sparsity: 66.3315%\n",
      "layer   3  Sparsity: 68.5539%\n",
      "total_backward_count 1811150 real_backward_count 105957   5.850%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  0.826463/  1.112295, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.04 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   2  Sparsity: 66.2859%\n",
      "layer   3  Sparsity: 68.2905%\n",
      "total_backward_count 1820940 real_backward_count 106144   5.829%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  0.821141/  1.107958, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.31 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0985%\n",
      "layer   2  Sparsity: 66.1738%\n",
      "layer   3  Sparsity: 68.6815%\n",
      "total_backward_count 1830730 real_backward_count 106376   5.811%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  0.824884/  1.128586, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.14 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0280%\n",
      "layer   2  Sparsity: 66.0637%\n",
      "layer   3  Sparsity: 68.6778%\n",
      "total_backward_count 1840520 real_backward_count 106606   5.792%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  0.830877/  1.123531, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.84 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   2  Sparsity: 66.3162%\n",
      "layer   3  Sparsity: 68.4179%\n",
      "total_backward_count 1850310 real_backward_count 106832   5.774%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  0.822768/  1.100822, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.91 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0665%\n",
      "layer   2  Sparsity: 66.3690%\n",
      "layer   3  Sparsity: 68.2851%\n",
      "total_backward_count 1860100 real_backward_count 107069   5.756%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  0.820516/  1.109829, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0648%\n",
      "layer   2  Sparsity: 66.1779%\n",
      "layer   3  Sparsity: 68.5129%\n",
      "total_backward_count 1869890 real_backward_count 107277   5.737%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  0.813034/  1.102131, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1136%\n",
      "layer   2  Sparsity: 66.3493%\n",
      "layer   3  Sparsity: 68.3349%\n",
      "total_backward_count 1879680 real_backward_count 107512   5.720%\n",
      "fc layer 2 self.abs_max_out: 2486.0\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  0.805508/  1.072513, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.42 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0530%\n",
      "layer   2  Sparsity: 66.6661%\n",
      "layer   3  Sparsity: 68.2327%\n",
      "total_backward_count 1889470 real_backward_count 107748   5.703%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  0.795441/  1.073571, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0910%\n",
      "layer   2  Sparsity: 66.4762%\n",
      "layer   3  Sparsity: 68.3379%\n",
      "total_backward_count 1899260 real_backward_count 107982   5.685%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  0.800566/  1.082103, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   2  Sparsity: 66.0854%\n",
      "layer   3  Sparsity: 68.4695%\n",
      "total_backward_count 1909050 real_backward_count 108198   5.668%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  0.805474/  1.072767, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0600%\n",
      "layer   2  Sparsity: 66.2029%\n",
      "layer   3  Sparsity: 67.8313%\n",
      "total_backward_count 1918840 real_backward_count 108398   5.649%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  0.806925/  1.083047, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   2  Sparsity: 66.1880%\n",
      "layer   3  Sparsity: 67.9842%\n",
      "total_backward_count 1928630 real_backward_count 108629   5.632%\n",
      "fc layer 2 self.abs_max_out: 2492.0\n",
      "fc layer 1 self.abs_max_out: 4648.0\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  0.812708/  1.099161, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.40 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0496%\n",
      "layer   2  Sparsity: 65.9488%\n",
      "layer   3  Sparsity: 68.3796%\n",
      "total_backward_count 1938420 real_backward_count 108888   5.617%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  0.807034/  1.100125, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.31 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1386%\n",
      "layer   2  Sparsity: 66.2895%\n",
      "layer   3  Sparsity: 68.3014%\n",
      "total_backward_count 1948210 real_backward_count 109113   5.601%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  0.797587/  1.090204, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.87 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0538%\n",
      "layer   2  Sparsity: 66.3315%\n",
      "layer   3  Sparsity: 67.9589%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a644720c8e48aab835badf047180b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÜ‚ñá‚ñá‚ñÑ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÜ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÜ‚ñá‚ñá‚ñÑ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.79759</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.87917</td></tr><tr><td>val_loss</td><td>1.0902</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lively-sweep-4</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a17jwskc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a17jwskc</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_082914-a17jwskc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2zkqg4x0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 10766\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_123253-2zkqg4x0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2zkqg4x0' target=\"_blank\">dark-sweep-5</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2zkqg4x0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2zkqg4x0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251118_123302_642', 'my_seed': 10766, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 295.0\n",
      "lif layer 1 self.abs_max_v: 295.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 232.0\n",
      "lif layer 2 self.abs_max_v: 232.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 104.0\n",
      "fc layer 1 self.abs_max_out: 309.0\n",
      "lif layer 1 self.abs_max_v: 422.5\n",
      "fc layer 2 self.abs_max_out: 280.0\n",
      "lif layer 2 self.abs_max_v: 299.5\n",
      "fc layer 1 self.abs_max_out: 312.0\n",
      "lif layer 1 self.abs_max_v: 463.5\n",
      "lif layer 2 self.abs_max_v: 328.5\n",
      "fc layer 3 self.abs_max_out: 158.0\n",
      "lif layer 1 self.abs_max_v: 531.0\n",
      "fc layer 2 self.abs_max_out: 288.0\n",
      "lif layer 2 self.abs_max_v: 381.5\n",
      "fc layer 2 self.abs_max_out: 320.0\n",
      "lif layer 2 self.abs_max_v: 451.0\n",
      "fc layer 1 self.abs_max_out: 317.0\n",
      "fc layer 2 self.abs_max_out: 342.0\n",
      "fc layer 2 self.abs_max_out: 346.0\n",
      "lif layer 1 self.abs_max_v: 573.5\n",
      "lif layer 1 self.abs_max_v: 574.0\n",
      "lif layer 2 self.abs_max_v: 485.0\n",
      "fc layer 1 self.abs_max_out: 335.0\n",
      "lif layer 2 self.abs_max_v: 488.5\n",
      "fc layer 1 self.abs_max_out: 420.0\n",
      "fc layer 2 self.abs_max_out: 347.0\n",
      "lif layer 2 self.abs_max_v: 519.5\n",
      "fc layer 3 self.abs_max_out: 168.0\n",
      "fc layer 2 self.abs_max_out: 348.0\n",
      "fc layer 2 self.abs_max_out: 373.0\n",
      "lif layer 2 self.abs_max_v: 525.0\n",
      "fc layer 2 self.abs_max_out: 406.0\n",
      "fc layer 1 self.abs_max_out: 483.0\n",
      "fc layer 2 self.abs_max_out: 411.0\n",
      "fc layer 1 self.abs_max_out: 576.0\n",
      "lif layer 1 self.abs_max_v: 576.0\n",
      "lif layer 2 self.abs_max_v: 531.5\n",
      "lif layer 2 self.abs_max_v: 574.0\n",
      "lif layer 2 self.abs_max_v: 607.5\n",
      "fc layer 2 self.abs_max_out: 444.0\n",
      "lif layer 2 self.abs_max_v: 609.5\n",
      "fc layer 3 self.abs_max_out: 187.0\n",
      "fc layer 3 self.abs_max_out: 214.0\n",
      "fc layer 3 self.abs_max_out: 244.0\n",
      "fc layer 2 self.abs_max_out: 457.0\n",
      "lif layer 2 self.abs_max_v: 668.5\n",
      "fc layer 2 self.abs_max_out: 468.0\n",
      "lif layer 2 self.abs_max_v: 705.5\n",
      "fc layer 1 self.abs_max_out: 701.0\n",
      "lif layer 1 self.abs_max_v: 701.0\n",
      "fc layer 2 self.abs_max_out: 542.0\n",
      "fc layer 2 self.abs_max_out: 545.0\n",
      "fc layer 1 self.abs_max_out: 742.0\n",
      "lif layer 1 self.abs_max_v: 742.0\n",
      "fc layer 1 self.abs_max_out: 748.0\n",
      "lif layer 1 self.abs_max_v: 748.0\n",
      "lif layer 1 self.abs_max_v: 802.5\n",
      "fc layer 1 self.abs_max_out: 769.0\n",
      "lif layer 2 self.abs_max_v: 750.5\n",
      "lif layer 2 self.abs_max_v: 786.5\n",
      "fc layer 2 self.abs_max_out: 572.0\n",
      "lif layer 2 self.abs_max_v: 924.0\n",
      "fc layer 2 self.abs_max_out: 584.0\n",
      "fc layer 1 self.abs_max_out: 792.0\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "lif layer 2 self.abs_max_v: 957.0\n",
      "fc layer 2 self.abs_max_out: 587.0\n",
      "fc layer 2 self.abs_max_out: 668.0\n",
      "fc layer 3 self.abs_max_out: 339.0\n",
      "lif layer 1 self.abs_max_v: 926.0\n",
      "fc layer 1 self.abs_max_out: 871.0\n",
      "fc layer 1 self.abs_max_out: 920.0\n",
      "lif layer 2 self.abs_max_v: 1028.0\n",
      "fc layer 2 self.abs_max_out: 715.0\n",
      "fc layer 2 self.abs_max_out: 820.0\n",
      "lif layer 1 self.abs_max_v: 1030.5\n",
      "lif layer 2 self.abs_max_v: 1094.5\n",
      "lif layer 2 self.abs_max_v: 1134.5\n",
      "lif layer 1 self.abs_max_v: 1069.0\n",
      "fc layer 3 self.abs_max_out: 347.0\n",
      "fc layer 1 self.abs_max_out: 1031.0\n",
      "lif layer 2 self.abs_max_v: 1140.5\n",
      "lif layer 2 self.abs_max_v: 1166.5\n",
      "fc layer 3 self.abs_max_out: 356.0\n",
      "lif layer 1 self.abs_max_v: 1151.5\n",
      "fc layer 2 self.abs_max_out: 847.0\n",
      "lif layer 1 self.abs_max_v: 1320.0\n",
      "lif layer 1 self.abs_max_v: 1321.0\n",
      "fc layer 1 self.abs_max_out: 1042.0\n",
      "lif layer 1 self.abs_max_v: 1331.0\n",
      "lif layer 2 self.abs_max_v: 1186.0\n",
      "lif layer 2 self.abs_max_v: 1191.5\n",
      "fc layer 1 self.abs_max_out: 1193.0\n",
      "fc layer 1 self.abs_max_out: 1262.0\n",
      "lif layer 2 self.abs_max_v: 1199.5\n",
      "lif layer 2 self.abs_max_v: 1202.0\n",
      "lif layer 2 self.abs_max_v: 1217.5\n",
      "lif layer 1 self.abs_max_v: 1384.0\n",
      "fc layer 2 self.abs_max_out: 853.0\n",
      "fc layer 3 self.abs_max_out: 376.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 3 self.abs_max_out: 387.0\n",
      "fc layer 3 self.abs_max_out: 401.0\n",
      "fc layer 2 self.abs_max_out: 900.0\n",
      "lif layer 1 self.abs_max_v: 1386.5\n",
      "fc layer 2 self.abs_max_out: 915.0\n",
      "fc layer 2 self.abs_max_out: 925.0\n",
      "lif layer 2 self.abs_max_v: 1219.0\n",
      "lif layer 2 self.abs_max_v: 1225.5\n",
      "lif layer 2 self.abs_max_v: 1276.0\n",
      "lif layer 2 self.abs_max_v: 1298.0\n",
      "lif layer 2 self.abs_max_v: 1337.0\n",
      "lif layer 2 self.abs_max_v: 1367.5\n",
      "lif layer 1 self.abs_max_v: 1454.5\n",
      "fc layer 2 self.abs_max_out: 950.0\n",
      "lif layer 1 self.abs_max_v: 1466.5\n",
      "lif layer 1 self.abs_max_v: 1494.5\n",
      "lif layer 1 self.abs_max_v: 1496.5\n",
      "lif layer 1 self.abs_max_v: 1527.5\n",
      "fc layer 2 self.abs_max_out: 1123.0\n",
      "lif layer 2 self.abs_max_v: 1377.5\n",
      "fc layer 1 self.abs_max_out: 1284.0\n",
      "fc layer 3 self.abs_max_out: 409.0\n",
      "fc layer 3 self.abs_max_out: 443.0\n",
      "fc layer 3 self.abs_max_out: 452.0\n",
      "fc layer 3 self.abs_max_out: 462.0\n",
      "fc layer 3 self.abs_max_out: 476.0\n",
      "lif layer 1 self.abs_max_v: 1554.5\n",
      "lif layer 1 self.abs_max_v: 1710.5\n",
      "fc layer 1 self.abs_max_out: 1408.0\n",
      "lif layer 2 self.abs_max_v: 1440.5\n",
      "lif layer 2 self.abs_max_v: 1441.5\n",
      "lif layer 2 self.abs_max_v: 1473.5\n",
      "lif layer 2 self.abs_max_v: 1570.0\n",
      "lif layer 1 self.abs_max_v: 1711.5\n",
      "lif layer 1 self.abs_max_v: 1874.5\n",
      "lif layer 1 self.abs_max_v: 1933.5\n",
      "lif layer 1 self.abs_max_v: 2092.0\n",
      "lif layer 1 self.abs_max_v: 2232.0\n",
      "fc layer 1 self.abs_max_out: 1480.0\n",
      "lif layer 1 self.abs_max_v: 2294.5\n",
      "fc layer 1 self.abs_max_out: 1494.0\n",
      "lif layer 1 self.abs_max_v: 2532.5\n",
      "fc layer 1 self.abs_max_out: 1502.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.731961/  1.951202, val:  44.17%, val_best:  44.17%, tr:  96.22%, tr_best:  96.22%, epoch time: 72.40 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0915%\n",
      "layer   2  Sparsity: 73.9165%\n",
      "layer   3  Sparsity: 71.4530%\n",
      "total_backward_count 9790 real_backward_count 2140  21.859%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 1612.0\n",
      "lif layer 2 self.abs_max_v: 1665.5\n",
      "fc layer 1 self.abs_max_out: 1518.0\n",
      "lif layer 2 self.abs_max_v: 1678.5\n",
      "lif layer 2 self.abs_max_v: 1778.0\n",
      "lif layer 2 self.abs_max_v: 1861.0\n",
      "fc layer 1 self.abs_max_out: 1520.0\n",
      "fc layer 1 self.abs_max_out: 1593.0\n",
      "fc layer 2 self.abs_max_out: 1134.0\n",
      "fc layer 1 self.abs_max_out: 1622.0\n",
      "lif layer 1 self.abs_max_v: 2629.0\n",
      "fc layer 2 self.abs_max_out: 1156.0\n",
      "fc layer 2 self.abs_max_out: 1187.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.648954/  1.892054, val:  37.92%, val_best:  44.17%, tr:  99.08%, tr_best:  99.08%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 74.4214%\n",
      "layer   3  Sparsity: 70.0647%\n",
      "total_backward_count 19580 real_backward_count 3762  19.213%\n",
      "fc layer 2 self.abs_max_out: 1199.0\n",
      "fc layer 2 self.abs_max_out: 1219.0\n",
      "lif layer 1 self.abs_max_v: 2746.0\n",
      "lif layer 1 self.abs_max_v: 2796.0\n",
      "fc layer 1 self.abs_max_out: 1689.0\n",
      "fc layer 1 self.abs_max_out: 1716.0\n",
      "lif layer 1 self.abs_max_v: 2932.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.621977/  1.858930, val:  39.17%, val_best:  44.17%, tr:  99.49%, tr_best:  99.49%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   2  Sparsity: 75.6933%\n",
      "layer   3  Sparsity: 70.4742%\n",
      "total_backward_count 29370 real_backward_count 5284  17.991%\n",
      "fc layer 1 self.abs_max_out: 1723.0\n",
      "fc layer 1 self.abs_max_out: 1734.0\n",
      "lif layer 1 self.abs_max_v: 3022.5\n",
      "fc layer 2 self.abs_max_out: 1276.0\n",
      "lif layer 1 self.abs_max_v: 3076.5\n",
      "fc layer 2 self.abs_max_out: 1300.0\n",
      "lif layer 2 self.abs_max_v: 1870.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.604344/  1.864300, val:  45.83%, val_best:  45.83%, tr:  99.69%, tr_best:  99.69%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0957%\n",
      "layer   2  Sparsity: 75.5855%\n",
      "layer   3  Sparsity: 70.0500%\n",
      "total_backward_count 39160 real_backward_count 6677  17.051%\n",
      "fc layer 1 self.abs_max_out: 1802.0\n",
      "fc layer 1 self.abs_max_out: 1819.0\n",
      "fc layer 2 self.abs_max_out: 1361.0\n",
      "fc layer 1 self.abs_max_out: 1860.0\n",
      "fc layer 1 self.abs_max_out: 1941.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.615906/  1.850829, val:  45.42%, val_best:  45.83%, tr:  99.49%, tr_best:  99.69%, epoch time: 72.25 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   2  Sparsity: 75.7077%\n",
      "layer   3  Sparsity: 69.7406%\n",
      "total_backward_count 48950 real_backward_count 8117  16.582%\n",
      "fc layer 1 self.abs_max_out: 2040.0\n",
      "fc layer 1 self.abs_max_out: 2056.0\n",
      "lif layer 1 self.abs_max_v: 3392.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.578376/  1.838359, val:  44.58%, val_best:  45.83%, tr:  99.59%, tr_best:  99.69%, epoch time: 72.71 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   2  Sparsity: 75.6817%\n",
      "layer   3  Sparsity: 70.2188%\n",
      "total_backward_count 58740 real_backward_count 9448  16.084%\n",
      "fc layer 1 self.abs_max_out: 2064.0\n",
      "fc layer 3 self.abs_max_out: 492.0\n",
      "fc layer 3 self.abs_max_out: 531.0\n",
      "fc layer 1 self.abs_max_out: 2081.0\n",
      "fc layer 2 self.abs_max_out: 1362.0\n",
      "lif layer 1 self.abs_max_v: 3437.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.589745/  1.827780, val:  55.83%, val_best:  55.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.30 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1059%\n",
      "layer   2  Sparsity: 75.9991%\n",
      "layer   3  Sparsity: 70.9165%\n",
      "total_backward_count 68530 real_backward_count 10835  15.811%\n",
      "fc layer 2 self.abs_max_out: 1369.0\n",
      "fc layer 2 self.abs_max_out: 1382.0\n",
      "fc layer 1 self.abs_max_out: 2141.0\n",
      "fc layer 1 self.abs_max_out: 2155.0\n",
      "lif layer 1 self.abs_max_v: 3604.5\n",
      "lif layer 1 self.abs_max_v: 3695.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.569206/  1.785339, val:  52.50%, val_best:  55.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   2  Sparsity: 74.3838%\n",
      "layer   3  Sparsity: 70.4224%\n",
      "total_backward_count 78320 real_backward_count 12132  15.490%\n",
      "fc layer 2 self.abs_max_out: 1426.0\n",
      "fc layer 1 self.abs_max_out: 2301.0\n",
      "fc layer 1 self.abs_max_out: 2304.0\n",
      "lif layer 1 self.abs_max_v: 3868.5\n",
      "lif layer 1 self.abs_max_v: 3893.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.555948/  1.809598, val:  47.50%, val_best:  55.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1060%\n",
      "layer   2  Sparsity: 73.5616%\n",
      "layer   3  Sparsity: 70.6507%\n",
      "total_backward_count 88110 real_backward_count 13366  15.170%\n",
      "fc layer 2 self.abs_max_out: 1471.0\n",
      "lif layer 2 self.abs_max_v: 1905.5\n",
      "lif layer 2 self.abs_max_v: 2006.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.551360/  1.794593, val:  50.00%, val_best:  55.83%, tr:  99.59%, tr_best:  99.90%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   2  Sparsity: 73.8818%\n",
      "layer   3  Sparsity: 70.1842%\n",
      "total_backward_count 97900 real_backward_count 14664  14.979%\n",
      "fc layer 1 self.abs_max_out: 2365.0\n",
      "fc layer 1 self.abs_max_out: 2378.0\n",
      "lif layer 1 self.abs_max_v: 3982.0\n",
      "lif layer 1 self.abs_max_v: 4015.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.535409/  1.761618, val:  60.83%, val_best:  60.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.72 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1209%\n",
      "layer   2  Sparsity: 73.5554%\n",
      "layer   3  Sparsity: 70.1160%\n",
      "total_backward_count 107690 real_backward_count 15849  14.717%\n",
      "fc layer 2 self.abs_max_out: 1546.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.530828/  1.799488, val:  57.08%, val_best:  60.83%, tr:  99.49%, tr_best:  99.90%, epoch time: 72.49 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0659%\n",
      "layer   2  Sparsity: 73.9653%\n",
      "layer   3  Sparsity: 70.7813%\n",
      "total_backward_count 117480 real_backward_count 17019  14.487%\n",
      "lif layer 2 self.abs_max_v: 2046.5\n",
      "fc layer 1 self.abs_max_out: 2547.0\n",
      "fc layer 1 self.abs_max_out: 2639.0\n",
      "lif layer 1 self.abs_max_v: 4383.5\n",
      "lif layer 1 self.abs_max_v: 4429.0\n",
      "lif layer 1 self.abs_max_v: 4582.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.516732/  1.764713, val:  49.17%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 72.16 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0513%\n",
      "layer   2  Sparsity: 73.7344%\n",
      "layer   3  Sparsity: 70.2847%\n",
      "total_backward_count 127270 real_backward_count 18220  14.316%\n",
      "lif layer 1 self.abs_max_v: 4639.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.501839/  1.721825, val:  60.83%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0934%\n",
      "layer   2  Sparsity: 73.5446%\n",
      "layer   3  Sparsity: 70.8446%\n",
      "total_backward_count 137060 real_backward_count 19422  14.170%\n",
      "lif layer 2 self.abs_max_v: 2062.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.496728/  1.701009, val:  58.75%, val_best:  60.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   2  Sparsity: 72.7479%\n",
      "layer   3  Sparsity: 70.0876%\n",
      "total_backward_count 146850 real_backward_count 20580  14.014%\n",
      "lif layer 2 self.abs_max_v: 2288.0\n",
      "fc layer 1 self.abs_max_out: 2682.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.464606/  1.747241, val:  57.92%, val_best:  60.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0401%\n",
      "layer   2  Sparsity: 72.7964%\n",
      "layer   3  Sparsity: 69.8588%\n",
      "total_backward_count 156640 real_backward_count 21727  13.871%\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.477956/  1.725410, val:  56.67%, val_best:  60.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0909%\n",
      "layer   2  Sparsity: 72.7314%\n",
      "layer   3  Sparsity: 69.4549%\n",
      "total_backward_count 166430 real_backward_count 22897  13.758%\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.464479/  1.725615, val:  63.75%, val_best:  63.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 71.74 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 72.3090%\n",
      "layer   3  Sparsity: 69.5417%\n",
      "total_backward_count 176220 real_backward_count 24010  13.625%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.470823/  1.713999, val:  71.67%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 71.61 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 72.4104%\n",
      "layer   3  Sparsity: 69.7102%\n",
      "total_backward_count 186010 real_backward_count 25097  13.492%\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.468499/  1.665249, val:  74.17%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0953%\n",
      "layer   2  Sparsity: 71.7853%\n",
      "layer   3  Sparsity: 69.6454%\n",
      "total_backward_count 195800 real_backward_count 26166  13.364%\n",
      "fc layer 3 self.abs_max_out: 566.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.448815/  1.705434, val:  66.25%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 71.67 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   2  Sparsity: 71.0324%\n",
      "layer   3  Sparsity: 69.7613%\n",
      "total_backward_count 205590 real_backward_count 27231  13.245%\n",
      "fc layer 1 self.abs_max_out: 2778.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.437606/  1.678991, val:  54.58%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0249%\n",
      "layer   2  Sparsity: 71.8937%\n",
      "layer   3  Sparsity: 69.9311%\n",
      "total_backward_count 215380 real_backward_count 28312  13.145%\n",
      "fc layer 2 self.abs_max_out: 1627.0\n",
      "lif layer 2 self.abs_max_v: 2291.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.447341/  1.643427, val:  62.08%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.02 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   2  Sparsity: 71.9037%\n",
      "layer   3  Sparsity: 70.1207%\n",
      "total_backward_count 225170 real_backward_count 29355  13.037%\n",
      "lif layer 2 self.abs_max_v: 2402.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.425511/  1.672762, val:  64.17%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.79 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0304%\n",
      "layer   2  Sparsity: 72.4993%\n",
      "layer   3  Sparsity: 70.5713%\n",
      "total_backward_count 234960 real_backward_count 30414  12.944%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.430128/  1.712754, val:  55.42%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0543%\n",
      "layer   2  Sparsity: 71.5368%\n",
      "layer   3  Sparsity: 71.3026%\n",
      "total_backward_count 244750 real_backward_count 31500  12.870%\n",
      "lif layer 1 self.abs_max_v: 4655.0\n",
      "lif layer 1 self.abs_max_v: 4759.5\n",
      "fc layer 1 self.abs_max_out: 2821.0\n",
      "lif layer 2 self.abs_max_v: 2428.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.423086/  1.654700, val:  52.50%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   2  Sparsity: 71.0857%\n",
      "layer   3  Sparsity: 71.5426%\n",
      "total_backward_count 254540 real_backward_count 32511  12.772%\n",
      "lif layer 2 self.abs_max_v: 2483.0\n",
      "fc layer 1 self.abs_max_out: 2851.0\n",
      "fc layer 1 self.abs_max_out: 2938.0\n",
      "lif layer 2 self.abs_max_v: 2500.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.421792/  1.638462, val:  67.08%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.67 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   2  Sparsity: 71.1489%\n",
      "layer   3  Sparsity: 71.9912%\n",
      "total_backward_count 264330 real_backward_count 33497  12.672%\n",
      "fc layer 1 self.abs_max_out: 2975.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.423104/  1.689701, val:  59.17%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.64 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0483%\n",
      "layer   2  Sparsity: 70.7577%\n",
      "layer   3  Sparsity: 71.4269%\n",
      "total_backward_count 274120 real_backward_count 34506  12.588%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.434127/  1.647517, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   2  Sparsity: 71.3606%\n",
      "layer   3  Sparsity: 71.0452%\n",
      "total_backward_count 283910 real_backward_count 35440  12.483%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.422318/  1.616748, val:  66.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   2  Sparsity: 71.3216%\n",
      "layer   3  Sparsity: 70.3294%\n",
      "total_backward_count 293700 real_backward_count 36410  12.397%\n",
      "fc layer 2 self.abs_max_out: 1630.0\n",
      "fc layer 1 self.abs_max_out: 3049.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.375149/  1.604990, val:  70.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   2  Sparsity: 71.2782%\n",
      "layer   3  Sparsity: 70.2209%\n",
      "total_backward_count 303490 real_backward_count 37408  12.326%\n",
      "lif layer 2 self.abs_max_v: 2527.5\n",
      "fc layer 2 self.abs_max_out: 1649.0\n",
      "fc layer 1 self.abs_max_out: 3085.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.368338/  1.598175, val:  72.08%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.44 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0712%\n",
      "layer   2  Sparsity: 71.0563%\n",
      "layer   3  Sparsity: 70.3531%\n",
      "total_backward_count 313280 real_backward_count 38393  12.255%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.367113/  1.611205, val:  69.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.49 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0691%\n",
      "layer   2  Sparsity: 71.0931%\n",
      "layer   3  Sparsity: 71.0756%\n",
      "total_backward_count 323070 real_backward_count 39347  12.179%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.366992/  1.598677, val:  69.17%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0830%\n",
      "layer   2  Sparsity: 70.9539%\n",
      "layer   3  Sparsity: 70.8623%\n",
      "total_backward_count 332860 real_backward_count 40304  12.108%\n",
      "fc layer 3 self.abs_max_out: 569.0\n",
      "fc layer 2 self.abs_max_out: 1652.0\n",
      "fc layer 3 self.abs_max_out: 575.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.356809/  1.613646, val:  75.42%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 70.6075%\n",
      "layer   3  Sparsity: 70.5244%\n",
      "total_backward_count 342650 real_backward_count 41197  12.023%\n",
      "lif layer 1 self.abs_max_v: 4826.5\n",
      "lif layer 1 self.abs_max_v: 4956.5\n",
      "lif layer 2 self.abs_max_v: 2537.0\n",
      "lif layer 2 self.abs_max_v: 2545.5\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.351840/  1.574259, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0492%\n",
      "layer   2  Sparsity: 70.7898%\n",
      "layer   3  Sparsity: 70.7003%\n",
      "total_backward_count 352440 real_backward_count 42089  11.942%\n",
      "fc layer 2 self.abs_max_out: 1654.0\n",
      "lif layer 2 self.abs_max_v: 2558.0\n",
      "lif layer 2 self.abs_max_v: 2571.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.341760/  1.612027, val:  57.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.08 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0235%\n",
      "layer   2  Sparsity: 70.5686%\n",
      "layer   3  Sparsity: 70.8879%\n",
      "total_backward_count 362230 real_backward_count 42956  11.859%\n",
      "lif layer 2 self.abs_max_v: 2616.5\n",
      "fc layer 2 self.abs_max_out: 1661.0\n",
      "fc layer 1 self.abs_max_out: 3152.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.343302/  1.583598, val:  69.17%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   2  Sparsity: 70.8088%\n",
      "layer   3  Sparsity: 70.8503%\n",
      "total_backward_count 372020 real_backward_count 43818  11.778%\n",
      "fc layer 1 self.abs_max_out: 3165.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.348180/  1.555411, val:  67.08%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.27 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0936%\n",
      "layer   2  Sparsity: 70.6189%\n",
      "layer   3  Sparsity: 70.8291%\n",
      "total_backward_count 381810 real_backward_count 44721  11.713%\n",
      "lif layer 1 self.abs_max_v: 5011.5\n",
      "fc layer 1 self.abs_max_out: 3178.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.296993/  1.582829, val:  60.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.16 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   2  Sparsity: 70.8622%\n",
      "layer   3  Sparsity: 70.4468%\n",
      "total_backward_count 391600 real_backward_count 45599  11.644%\n",
      "lif layer 1 self.abs_max_v: 5034.5\n",
      "lif layer 1 self.abs_max_v: 5130.5\n",
      "fc layer 2 self.abs_max_out: 1675.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.309855/  1.556387, val:  75.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0836%\n",
      "layer   2  Sparsity: 70.7500%\n",
      "layer   3  Sparsity: 70.9069%\n",
      "total_backward_count 401390 real_backward_count 46497  11.584%\n",
      "fc layer 3 self.abs_max_out: 579.0\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "fc layer 2 self.abs_max_out: 1715.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.314546/  1.537372, val:  74.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.19 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 70.2742%\n",
      "layer   3  Sparsity: 71.3003%\n",
      "total_backward_count 411180 real_backward_count 47361  11.518%\n",
      "fc layer 1 self.abs_max_out: 3196.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.313378/  1.539503, val:  67.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0367%\n",
      "layer   2  Sparsity: 70.5581%\n",
      "layer   3  Sparsity: 71.3033%\n",
      "total_backward_count 420970 real_backward_count 48184  11.446%\n",
      "fc layer 1 self.abs_max_out: 3213.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.330525/  1.528393, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.68 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   2  Sparsity: 70.4899%\n",
      "layer   3  Sparsity: 72.3218%\n",
      "total_backward_count 430760 real_backward_count 48962  11.366%\n",
      "lif layer 2 self.abs_max_v: 2650.5\n",
      "fc layer 2 self.abs_max_out: 1722.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.309846/  1.529239, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.61 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0205%\n",
      "layer   2  Sparsity: 70.8616%\n",
      "layer   3  Sparsity: 72.4169%\n",
      "total_backward_count 440550 real_backward_count 49768  11.297%\n",
      "lif layer 2 self.abs_max_v: 2734.0\n",
      "lif layer 2 self.abs_max_v: 2745.0\n",
      "fc layer 2 self.abs_max_out: 1748.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.298777/  1.551989, val:  74.58%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0887%\n",
      "layer   2  Sparsity: 70.8057%\n",
      "layer   3  Sparsity: 71.9509%\n",
      "total_backward_count 450340 real_backward_count 50559  11.227%\n",
      "fc layer 3 self.abs_max_out: 591.0\n",
      "fc layer 2 self.abs_max_out: 1770.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.305388/  1.519727, val:  74.17%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.72 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1107%\n",
      "layer   2  Sparsity: 70.7509%\n",
      "layer   3  Sparsity: 72.2182%\n",
      "total_backward_count 460130 real_backward_count 51387  11.168%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.291119/  1.504330, val:  77.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0560%\n",
      "layer   2  Sparsity: 70.7938%\n",
      "layer   3  Sparsity: 72.1514%\n",
      "total_backward_count 469920 real_backward_count 52169  11.102%\n",
      "fc layer 2 self.abs_max_out: 1779.0\n",
      "fc layer 2 self.abs_max_out: 1828.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.303163/  1.518360, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   2  Sparsity: 70.5893%\n",
      "layer   3  Sparsity: 71.8834%\n",
      "total_backward_count 479710 real_backward_count 52933  11.034%\n",
      "fc layer 3 self.abs_max_out: 592.0\n",
      "fc layer 3 self.abs_max_out: 614.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.274063/  1.499513, val:  68.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.63 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1033%\n",
      "layer   2  Sparsity: 70.4301%\n",
      "layer   3  Sparsity: 71.7875%\n",
      "total_backward_count 489500 real_backward_count 53718  10.974%\n",
      "fc layer 2 self.abs_max_out: 1857.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.266140/  1.524837, val:  76.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.1109%\n",
      "layer   2  Sparsity: 70.1858%\n",
      "layer   3  Sparsity: 71.6328%\n",
      "total_backward_count 499290 real_backward_count 54500  10.916%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.296872/  1.512401, val:  74.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.72 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1110%\n",
      "layer   2  Sparsity: 70.4800%\n",
      "layer   3  Sparsity: 72.0652%\n",
      "total_backward_count 509080 real_backward_count 55250  10.853%\n",
      "lif layer 2 self.abs_max_v: 2757.0\n",
      "lif layer 1 self.abs_max_v: 5262.0\n",
      "lif layer 1 self.abs_max_v: 5390.0\n",
      "lif layer 2 self.abs_max_v: 2890.5\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.285137/  1.479423, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.58 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0543%\n",
      "layer   2  Sparsity: 70.3317%\n",
      "layer   3  Sparsity: 71.4393%\n",
      "total_backward_count 518870 real_backward_count 55943  10.782%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.262263/  1.515893, val:  78.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 70.8812%\n",
      "layer   3  Sparsity: 71.3418%\n",
      "total_backward_count 528660 real_backward_count 56694  10.724%\n",
      "lif layer 2 self.abs_max_v: 2947.0\n",
      "lif layer 2 self.abs_max_v: 2983.5\n",
      "lif layer 2 self.abs_max_v: 2987.0\n",
      "lif layer 2 self.abs_max_v: 3009.5\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.275027/  1.498047, val:  81.67%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.17 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0928%\n",
      "layer   2  Sparsity: 70.4726%\n",
      "layer   3  Sparsity: 71.7152%\n",
      "total_backward_count 538450 real_backward_count 57417  10.663%\n",
      "fc layer 2 self.abs_max_out: 1877.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.274454/  1.469952, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0650%\n",
      "layer   2  Sparsity: 70.2651%\n",
      "layer   3  Sparsity: 71.8450%\n",
      "total_backward_count 548240 real_backward_count 58104  10.598%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.246713/  1.469392, val:  70.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0165%\n",
      "layer   2  Sparsity: 70.5015%\n",
      "layer   3  Sparsity: 71.0863%\n",
      "total_backward_count 558030 real_backward_count 58800  10.537%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.231958/  1.468672, val:  77.50%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0457%\n",
      "layer   2  Sparsity: 70.4361%\n",
      "layer   3  Sparsity: 70.9914%\n",
      "total_backward_count 567820 real_backward_count 59518  10.482%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.249250/  1.464914, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.10 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1296%\n",
      "layer   2  Sparsity: 69.9050%\n",
      "layer   3  Sparsity: 70.7422%\n",
      "total_backward_count 577610 real_backward_count 60208  10.424%\n",
      "fc layer 2 self.abs_max_out: 1886.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.214175/  1.495382, val:  76.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.56 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0982%\n",
      "layer   2  Sparsity: 69.8949%\n",
      "layer   3  Sparsity: 70.4826%\n",
      "total_backward_count 587400 real_backward_count 60846  10.359%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.217052/  1.437544, val:  74.17%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.38 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   2  Sparsity: 69.8548%\n",
      "layer   3  Sparsity: 70.3130%\n",
      "total_backward_count 597190 real_backward_count 61521  10.302%\n",
      "fc layer 2 self.abs_max_out: 1916.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.198706/  1.449736, val:  74.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.20 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0355%\n",
      "layer   2  Sparsity: 70.2296%\n",
      "layer   3  Sparsity: 70.9404%\n",
      "total_backward_count 606980 real_backward_count 62192  10.246%\n",
      "fc layer 3 self.abs_max_out: 648.0\n",
      "fc layer 2 self.abs_max_out: 1938.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.217053/  1.494047, val:  70.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.58 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0809%\n",
      "layer   2  Sparsity: 70.2149%\n",
      "layer   3  Sparsity: 70.4238%\n",
      "total_backward_count 616770 real_backward_count 62815  10.185%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.192201/  1.438715, val:  78.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.17 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0992%\n",
      "layer   2  Sparsity: 70.4140%\n",
      "layer   3  Sparsity: 70.6681%\n",
      "total_backward_count 626560 real_backward_count 63413  10.121%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.197319/  1.435776, val:  68.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.50 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0632%\n",
      "layer   2  Sparsity: 70.4824%\n",
      "layer   3  Sparsity: 70.9934%\n",
      "total_backward_count 636350 real_backward_count 64104  10.074%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.193668/  1.427169, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.68 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0662%\n",
      "layer   2  Sparsity: 70.3242%\n",
      "layer   3  Sparsity: 70.1574%\n",
      "total_backward_count 646140 real_backward_count 64742  10.020%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.187035/  1.400009, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   2  Sparsity: 70.1658%\n",
      "layer   3  Sparsity: 69.8689%\n",
      "total_backward_count 655930 real_backward_count 65392   9.969%\n",
      "fc layer 2 self.abs_max_out: 1963.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.195689/  1.458109, val:  76.25%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.72 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   2  Sparsity: 70.2952%\n",
      "layer   3  Sparsity: 70.0773%\n",
      "total_backward_count 665720 real_backward_count 66014   9.916%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.194471/  1.438953, val:  74.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   2  Sparsity: 70.4827%\n",
      "layer   3  Sparsity: 70.7311%\n",
      "total_backward_count 675510 real_backward_count 66628   9.863%\n",
      "lif layer 1 self.abs_max_v: 5491.0\n",
      "lif layer 1 self.abs_max_v: 5573.5\n",
      "fc layer 2 self.abs_max_out: 1981.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.193312/  1.423628, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   2  Sparsity: 70.2515%\n",
      "layer   3  Sparsity: 70.7296%\n",
      "total_backward_count 685300 real_backward_count 67206   9.807%\n",
      "fc layer 2 self.abs_max_out: 2026.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.199653/  1.422624, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.68 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0836%\n",
      "layer   2  Sparsity: 70.1477%\n",
      "layer   3  Sparsity: 70.4553%\n",
      "total_backward_count 695090 real_backward_count 67780   9.751%\n",
      "fc layer 3 self.abs_max_out: 659.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.190175/  1.443171, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0988%\n",
      "layer   2  Sparsity: 70.0530%\n",
      "layer   3  Sparsity: 70.7010%\n",
      "total_backward_count 704880 real_backward_count 68415   9.706%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.178156/  1.415024, val:  82.08%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1182%\n",
      "layer   2  Sparsity: 70.0648%\n",
      "layer   3  Sparsity: 70.6522%\n",
      "total_backward_count 714670 real_backward_count 68995   9.654%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.173617/  1.399888, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.61 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   2  Sparsity: 70.1355%\n",
      "layer   3  Sparsity: 70.2929%\n",
      "total_backward_count 724460 real_backward_count 69588   9.605%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.157126/  1.415785, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   2  Sparsity: 69.8071%\n",
      "layer   3  Sparsity: 70.4159%\n",
      "total_backward_count 734250 real_backward_count 70197   9.560%\n",
      "fc layer 3 self.abs_max_out: 660.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.174513/  1.441121, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0443%\n",
      "layer   2  Sparsity: 69.4934%\n",
      "layer   3  Sparsity: 70.6029%\n",
      "total_backward_count 744040 real_backward_count 70767   9.511%\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.168197/  1.408207, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.52 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   2  Sparsity: 69.4831%\n",
      "layer   3  Sparsity: 70.1386%\n",
      "total_backward_count 753830 real_backward_count 71367   9.467%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.166244/  1.425471, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0917%\n",
      "layer   2  Sparsity: 70.0157%\n",
      "layer   3  Sparsity: 70.4300%\n",
      "total_backward_count 763620 real_backward_count 71923   9.419%\n",
      "lif layer 1 self.abs_max_v: 5588.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.172450/  1.384940, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.15 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0626%\n",
      "layer   2  Sparsity: 70.0331%\n",
      "layer   3  Sparsity: 70.3358%\n",
      "total_backward_count 773410 real_backward_count 72508   9.375%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.161783/  1.412378, val:  76.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.79 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   2  Sparsity: 70.1195%\n",
      "layer   3  Sparsity: 70.3291%\n",
      "total_backward_count 783200 real_backward_count 73068   9.329%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.168619/  1.380752, val:  86.67%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.62 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   2  Sparsity: 69.9833%\n",
      "layer   3  Sparsity: 70.1411%\n",
      "total_backward_count 792990 real_backward_count 73626   9.285%\n",
      "lif layer 1 self.abs_max_v: 5801.0\n",
      "lif layer 1 self.abs_max_v: 5894.5\n",
      "fc layer 3 self.abs_max_out: 676.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.170954/  1.409538, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.13 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1148%\n",
      "layer   2  Sparsity: 69.8224%\n",
      "layer   3  Sparsity: 70.3509%\n",
      "total_backward_count 802780 real_backward_count 74215   9.245%\n",
      "fc layer 3 self.abs_max_out: 708.0\n",
      "fc layer 2 self.abs_max_out: 2140.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.163815/  1.404228, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.31 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   2  Sparsity: 69.9056%\n",
      "layer   3  Sparsity: 70.1627%\n",
      "total_backward_count 812570 real_backward_count 74777   9.203%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.157582/  1.399588, val:  83.75%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.51 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 69.8637%\n",
      "layer   3  Sparsity: 70.0221%\n",
      "total_backward_count 822360 real_backward_count 75307   9.157%\n",
      "lif layer 1 self.abs_max_v: 5986.5\n",
      "fc layer 1 self.abs_max_out: 3306.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.152690/  1.385068, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0569%\n",
      "layer   2  Sparsity: 69.8083%\n",
      "layer   3  Sparsity: 70.4519%\n",
      "total_backward_count 832150 real_backward_count 75851   9.115%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.160452/  1.406483, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   2  Sparsity: 69.5781%\n",
      "layer   3  Sparsity: 70.6640%\n",
      "total_backward_count 841940 real_backward_count 76391   9.073%\n",
      "fc layer 2 self.abs_max_out: 2216.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.169993/  1.386657, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.64 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0932%\n",
      "layer   2  Sparsity: 69.4640%\n",
      "layer   3  Sparsity: 70.5988%\n",
      "total_backward_count 851730 real_backward_count 76881   9.026%\n",
      "fc layer 2 self.abs_max_out: 2314.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.158349/  1.383347, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0596%\n",
      "layer   2  Sparsity: 69.2149%\n",
      "layer   3  Sparsity: 70.6229%\n",
      "total_backward_count 861520 real_backward_count 77416   8.986%\n",
      "fc layer 3 self.abs_max_out: 731.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.148476/  1.393074, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.56 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.1032%\n",
      "layer   2  Sparsity: 69.3714%\n",
      "layer   3  Sparsity: 70.7795%\n",
      "total_backward_count 871310 real_backward_count 77934   8.944%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.154911/  1.404251, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.47 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1035%\n",
      "layer   2  Sparsity: 69.5681%\n",
      "layer   3  Sparsity: 70.9895%\n",
      "total_backward_count 881100 real_backward_count 78452   8.904%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.133393/  1.351588, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1112%\n",
      "layer   2  Sparsity: 69.8061%\n",
      "layer   3  Sparsity: 70.8741%\n",
      "total_backward_count 890890 real_backward_count 78933   8.860%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.122221/  1.351765, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   2  Sparsity: 69.8213%\n",
      "layer   3  Sparsity: 70.5366%\n",
      "total_backward_count 900680 real_backward_count 79456   8.822%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.116216/  1.387168, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0731%\n",
      "layer   2  Sparsity: 69.5113%\n",
      "layer   3  Sparsity: 70.7036%\n",
      "total_backward_count 910470 real_backward_count 79955   8.782%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.107050/  1.334472, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0686%\n",
      "layer   2  Sparsity: 69.6601%\n",
      "layer   3  Sparsity: 71.6069%\n",
      "total_backward_count 920260 real_backward_count 80425   8.739%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.113234/  1.383242, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.09 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   2  Sparsity: 69.7704%\n",
      "layer   3  Sparsity: 71.4013%\n",
      "total_backward_count 930050 real_backward_count 80950   8.704%\n",
      "lif layer 1 self.abs_max_v: 6037.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.103444/  1.357872, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.14 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1004%\n",
      "layer   2  Sparsity: 69.7189%\n",
      "layer   3  Sparsity: 71.2315%\n",
      "total_backward_count 939840 real_backward_count 81422   8.663%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.092290/  1.339071, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.94 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0241%\n",
      "layer   2  Sparsity: 69.6247%\n",
      "layer   3  Sparsity: 71.4151%\n",
      "total_backward_count 949630 real_backward_count 81880   8.622%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.079699/  1.325842, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.84 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0502%\n",
      "layer   2  Sparsity: 69.7074%\n",
      "layer   3  Sparsity: 71.1547%\n",
      "total_backward_count 959420 real_backward_count 82333   8.582%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.094512/  1.350452, val:  76.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.18 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0688%\n",
      "layer   2  Sparsity: 69.5123%\n",
      "layer   3  Sparsity: 71.1767%\n",
      "total_backward_count 969210 real_backward_count 82773   8.540%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.099203/  1.327380, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0954%\n",
      "layer   2  Sparsity: 69.4600%\n",
      "layer   3  Sparsity: 70.9656%\n",
      "total_backward_count 979000 real_backward_count 83252   8.504%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.080660/  1.329580, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.68 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0516%\n",
      "layer   2  Sparsity: 69.5285%\n",
      "layer   3  Sparsity: 71.0909%\n",
      "total_backward_count 988790 real_backward_count 83748   8.470%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.089916/  1.309127, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.37 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0863%\n",
      "layer   2  Sparsity: 69.3365%\n",
      "layer   3  Sparsity: 71.2642%\n",
      "total_backward_count 998580 real_backward_count 84221   8.434%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.076278/  1.321643, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.57 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   2  Sparsity: 69.1461%\n",
      "layer   3  Sparsity: 70.8842%\n",
      "total_backward_count 1008370 real_backward_count 84682   8.398%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.071471/  1.309597, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0704%\n",
      "layer   2  Sparsity: 69.1769%\n",
      "layer   3  Sparsity: 71.2272%\n",
      "total_backward_count 1018160 real_backward_count 85128   8.361%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.069786/  1.355839, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0436%\n",
      "layer   2  Sparsity: 69.1708%\n",
      "layer   3  Sparsity: 70.9694%\n",
      "total_backward_count 1027950 real_backward_count 85553   8.323%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.088181/  1.329059, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0936%\n",
      "layer   2  Sparsity: 69.5427%\n",
      "layer   3  Sparsity: 70.7819%\n",
      "total_backward_count 1037740 real_backward_count 85996   8.287%\n",
      "fc layer 1 self.abs_max_out: 3507.0\n",
      "fc layer 1 self.abs_max_out: 3896.0\n",
      "lif layer 1 self.abs_max_v: 6483.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.091876/  1.312571, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.23 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0897%\n",
      "layer   2  Sparsity: 69.7258%\n",
      "layer   3  Sparsity: 71.1387%\n",
      "total_backward_count 1047530 real_backward_count 86456   8.253%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.075841/  1.329365, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0758%\n",
      "layer   2  Sparsity: 69.7176%\n",
      "layer   3  Sparsity: 70.9057%\n",
      "total_backward_count 1057320 real_backward_count 86911   8.220%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.073016/  1.337638, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.29 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0936%\n",
      "layer   2  Sparsity: 69.5053%\n",
      "layer   3  Sparsity: 71.0208%\n",
      "total_backward_count 1067110 real_backward_count 87348   8.185%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.085008/  1.322930, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 69.4746%\n",
      "layer   3  Sparsity: 71.1827%\n",
      "total_backward_count 1076900 real_backward_count 87801   8.153%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.078307/  1.375936, val:  77.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.57 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0977%\n",
      "layer   2  Sparsity: 69.5388%\n",
      "layer   3  Sparsity: 71.3632%\n",
      "total_backward_count 1086690 real_backward_count 88285   8.124%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.083438/  1.315272, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.78 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0466%\n",
      "layer   2  Sparsity: 69.8899%\n",
      "layer   3  Sparsity: 70.8981%\n",
      "total_backward_count 1096480 real_backward_count 88749   8.094%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.077616/  1.310730, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.39 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0892%\n",
      "layer   2  Sparsity: 70.0135%\n",
      "layer   3  Sparsity: 70.8473%\n",
      "total_backward_count 1106270 real_backward_count 89171   8.061%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.076851/  1.307934, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.78 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0545%\n",
      "layer   2  Sparsity: 69.7108%\n",
      "layer   3  Sparsity: 71.1161%\n",
      "total_backward_count 1116060 real_backward_count 89555   8.024%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.060006/  1.300933, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.42 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 69.8037%\n",
      "layer   3  Sparsity: 70.9436%\n",
      "total_backward_count 1125850 real_backward_count 89955   7.990%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.067481/  1.315358, val:  81.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.64 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.1043%\n",
      "layer   2  Sparsity: 69.8632%\n",
      "layer   3  Sparsity: 70.9737%\n",
      "total_backward_count 1135640 real_backward_count 90402   7.960%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.054949/  1.300982, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.71 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0925%\n",
      "layer   2  Sparsity: 69.6057%\n",
      "layer   3  Sparsity: 71.1037%\n",
      "total_backward_count 1145430 real_backward_count 90836   7.930%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.054898/  1.322652, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.81 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 69.6252%\n",
      "layer   3  Sparsity: 71.0818%\n",
      "total_backward_count 1155220 real_backward_count 91248   7.899%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.046684/  1.305704, val:  77.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.59 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   2  Sparsity: 69.5341%\n",
      "layer   3  Sparsity: 70.8084%\n",
      "total_backward_count 1165010 real_backward_count 91671   7.869%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.050598/  1.284756, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.61 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   2  Sparsity: 69.4685%\n",
      "layer   3  Sparsity: 70.6507%\n",
      "total_backward_count 1174800 real_backward_count 92068   7.837%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.040489/  1.293663, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.52 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0833%\n",
      "layer   2  Sparsity: 69.4701%\n",
      "layer   3  Sparsity: 70.7668%\n",
      "total_backward_count 1184590 real_backward_count 92507   7.809%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.048256/  1.294290, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.94 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0359%\n",
      "layer   2  Sparsity: 69.4527%\n",
      "layer   3  Sparsity: 70.8730%\n",
      "total_backward_count 1194380 real_backward_count 92895   7.778%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.042253/  1.276393, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.20 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   2  Sparsity: 69.5680%\n",
      "layer   3  Sparsity: 70.3274%\n",
      "total_backward_count 1204170 real_backward_count 93248   7.744%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.029688/  1.283051, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.84 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1410%\n",
      "layer   2  Sparsity: 69.7278%\n",
      "layer   3  Sparsity: 70.4339%\n",
      "total_backward_count 1213960 real_backward_count 93696   7.718%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.029011/  1.287668, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   2  Sparsity: 69.5888%\n",
      "layer   3  Sparsity: 70.7765%\n",
      "total_backward_count 1223750 real_backward_count 94116   7.691%\n",
      "fc layer 1 self.abs_max_out: 3951.0\n",
      "lif layer 1 self.abs_max_v: 6629.5\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.031164/  1.267134, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.97 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1113%\n",
      "layer   2  Sparsity: 69.5987%\n",
      "layer   3  Sparsity: 70.5334%\n",
      "total_backward_count 1233540 real_backward_count 94505   7.661%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.017583/  1.304838, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.72 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   2  Sparsity: 69.3112%\n",
      "layer   3  Sparsity: 70.5016%\n",
      "total_backward_count 1243330 real_backward_count 94875   7.631%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.032104/  1.281394, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1094%\n",
      "layer   2  Sparsity: 69.3318%\n",
      "layer   3  Sparsity: 70.5716%\n",
      "total_backward_count 1253120 real_backward_count 95239   7.600%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.026877/  1.274889, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.98 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0872%\n",
      "layer   2  Sparsity: 69.1218%\n",
      "layer   3  Sparsity: 70.1910%\n",
      "total_backward_count 1262910 real_backward_count 95588   7.569%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.025956/  1.287870, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.94 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0830%\n",
      "layer   2  Sparsity: 69.1094%\n",
      "layer   3  Sparsity: 70.3075%\n",
      "total_backward_count 1272700 real_backward_count 95944   7.539%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.029374/  1.282599, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.06 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0195%\n",
      "layer   2  Sparsity: 69.2978%\n",
      "layer   3  Sparsity: 70.0063%\n",
      "total_backward_count 1282490 real_backward_count 96310   7.510%\n",
      "fc layer 1 self.abs_max_out: 4042.0\n",
      "lif layer 1 self.abs_max_v: 6790.0\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.029471/  1.291668, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0600%\n",
      "layer   2  Sparsity: 69.4991%\n",
      "layer   3  Sparsity: 70.5384%\n",
      "total_backward_count 1292280 real_backward_count 96690   7.482%\n",
      "fc layer 1 self.abs_max_out: 4166.0\n",
      "lif layer 1 self.abs_max_v: 7034.0\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.008821/  1.256048, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.71 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   2  Sparsity: 69.6334%\n",
      "layer   3  Sparsity: 70.6373%\n",
      "total_backward_count 1302070 real_backward_count 97026   7.452%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  0.987755/  1.270279, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.51 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   2  Sparsity: 69.5450%\n",
      "layer   3  Sparsity: 70.8428%\n",
      "total_backward_count 1311860 real_backward_count 97376   7.423%\n",
      "fc layer 1 self.abs_max_out: 4268.0\n",
      "lif layer 1 self.abs_max_v: 7223.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.007292/  1.275411, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.71 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1007%\n",
      "layer   2  Sparsity: 69.4557%\n",
      "layer   3  Sparsity: 70.7052%\n",
      "total_backward_count 1321650 real_backward_count 97712   7.393%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.012521/  1.275370, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.63 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   2  Sparsity: 69.3162%\n",
      "layer   3  Sparsity: 70.5880%\n",
      "total_backward_count 1331440 real_backward_count 98076   7.366%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.020364/  1.275063, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.81 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   2  Sparsity: 69.2994%\n",
      "layer   3  Sparsity: 70.5721%\n",
      "total_backward_count 1341230 real_backward_count 98447   7.340%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.013224/  1.270258, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.00 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 68.9863%\n",
      "layer   3  Sparsity: 70.5796%\n",
      "total_backward_count 1351020 real_backward_count 98790   7.312%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.009654/  1.250893, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.23 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   2  Sparsity: 68.9198%\n",
      "layer   3  Sparsity: 70.6243%\n",
      "total_backward_count 1360810 real_backward_count 99130   7.285%\n",
      "fc layer 3 self.abs_max_out: 737.0\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.000827/  1.260243, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.85 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   2  Sparsity: 69.1768%\n",
      "layer   3  Sparsity: 70.4307%\n",
      "total_backward_count 1370600 real_backward_count 99513   7.261%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  0.987750/  1.269519, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   2  Sparsity: 69.2105%\n",
      "layer   3  Sparsity: 70.7126%\n",
      "total_backward_count 1380390 real_backward_count 99843   7.233%\n",
      "fc layer 3 self.abs_max_out: 774.0\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  0.987046/  1.269340, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0737%\n",
      "layer   2  Sparsity: 69.4655%\n",
      "layer   3  Sparsity: 70.7834%\n",
      "total_backward_count 1390180 real_backward_count 100191   7.207%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  0.983969/  1.250732, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0928%\n",
      "layer   2  Sparsity: 69.6151%\n",
      "layer   3  Sparsity: 70.6893%\n",
      "total_backward_count 1399970 real_backward_count 100504   7.179%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  0.989334/  1.232806, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.05 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 69.7791%\n",
      "layer   3  Sparsity: 70.7220%\n",
      "total_backward_count 1409760 real_backward_count 100868   7.155%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  0.985919/  1.264223, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.01 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   2  Sparsity: 69.1655%\n",
      "layer   3  Sparsity: 70.9085%\n",
      "total_backward_count 1419550 real_backward_count 101215   7.130%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.003578/  1.271702, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.90 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0617%\n",
      "layer   2  Sparsity: 69.3281%\n",
      "layer   3  Sparsity: 71.0476%\n",
      "total_backward_count 1429340 real_backward_count 101541   7.104%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  0.994706/  1.271304, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.57 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0428%\n",
      "layer   2  Sparsity: 69.1612%\n",
      "layer   3  Sparsity: 71.0281%\n",
      "total_backward_count 1439130 real_backward_count 101931   7.083%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  0.998540/  1.264948, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.31 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0548%\n",
      "layer   2  Sparsity: 69.0891%\n",
      "layer   3  Sparsity: 71.1838%\n",
      "total_backward_count 1448920 real_backward_count 102249   7.057%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  0.981599/  1.247269, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   2  Sparsity: 69.1305%\n",
      "layer   3  Sparsity: 70.9806%\n",
      "total_backward_count 1458710 real_backward_count 102585   7.033%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  0.985455/  1.275426, val:  82.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.07 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0633%\n",
      "layer   2  Sparsity: 69.3099%\n",
      "layer   3  Sparsity: 71.1422%\n",
      "total_backward_count 1468500 real_backward_count 102931   7.009%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  0.973972/  1.226992, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.19 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0824%\n",
      "layer   2  Sparsity: 69.2127%\n",
      "layer   3  Sparsity: 70.9509%\n",
      "total_backward_count 1478290 real_backward_count 103255   6.985%\n",
      "lif layer 2 self.abs_max_v: 3060.0\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  0.971133/  1.236400, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.09 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   2  Sparsity: 69.1601%\n",
      "layer   3  Sparsity: 71.4920%\n",
      "total_backward_count 1488080 real_backward_count 103548   6.958%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  0.979133/  1.249183, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.78 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0591%\n",
      "layer   2  Sparsity: 69.2905%\n",
      "layer   3  Sparsity: 71.8336%\n",
      "total_backward_count 1497870 real_backward_count 103854   6.933%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  0.992947/  1.290789, val:  76.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.64 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0659%\n",
      "layer   2  Sparsity: 69.1835%\n",
      "layer   3  Sparsity: 71.8516%\n",
      "total_backward_count 1507660 real_backward_count 104158   6.909%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  0.993729/  1.238203, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0567%\n",
      "layer   2  Sparsity: 68.9669%\n",
      "layer   3  Sparsity: 71.7288%\n",
      "total_backward_count 1517450 real_backward_count 104481   6.885%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  0.980869/  1.231412, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.15 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   2  Sparsity: 69.0989%\n",
      "layer   3  Sparsity: 72.0035%\n",
      "total_backward_count 1527240 real_backward_count 104762   6.860%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  0.984365/  1.255068, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.29 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 69.2620%\n",
      "layer   3  Sparsity: 72.0680%\n",
      "total_backward_count 1537030 real_backward_count 105097   6.838%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  0.986408/  1.256385, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.21 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0685%\n",
      "layer   2  Sparsity: 69.1882%\n",
      "layer   3  Sparsity: 72.4032%\n",
      "total_backward_count 1546820 real_backward_count 105417   6.815%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  0.971080/  1.245795, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.49 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0916%\n",
      "layer   2  Sparsity: 69.2149%\n",
      "layer   3  Sparsity: 72.6164%\n",
      "total_backward_count 1556610 real_backward_count 105725   6.792%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  0.966834/  1.229344, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.86 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0691%\n",
      "layer   2  Sparsity: 69.0310%\n",
      "layer   3  Sparsity: 72.6975%\n",
      "total_backward_count 1566400 real_backward_count 106031   6.769%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  0.972011/  1.256108, val:  83.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.17 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   2  Sparsity: 69.0082%\n",
      "layer   3  Sparsity: 72.7728%\n",
      "total_backward_count 1576190 real_backward_count 106283   6.743%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  0.984658/  1.230038, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.61 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   2  Sparsity: 69.0548%\n",
      "layer   3  Sparsity: 72.4168%\n",
      "total_backward_count 1585980 real_backward_count 106557   6.719%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  0.970675/  1.237889, val:  82.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0275%\n",
      "layer   2  Sparsity: 69.3060%\n",
      "layer   3  Sparsity: 72.7734%\n",
      "total_backward_count 1595770 real_backward_count 106834   6.695%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  0.964354/  1.203774, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 69.4406%\n",
      "layer   3  Sparsity: 72.5717%\n",
      "total_backward_count 1605560 real_backward_count 107090   6.670%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  0.975328/  1.275620, val:  82.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.24 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 69.3363%\n",
      "layer   3  Sparsity: 72.5539%\n",
      "total_backward_count 1615350 real_backward_count 107347   6.645%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  0.986719/  1.234362, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1065%\n",
      "layer   2  Sparsity: 69.3380%\n",
      "layer   3  Sparsity: 72.4471%\n",
      "total_backward_count 1625140 real_backward_count 107650   6.624%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  0.973809/  1.255964, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.04 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1153%\n",
      "layer   2  Sparsity: 69.3287%\n",
      "layer   3  Sparsity: 72.1758%\n",
      "total_backward_count 1634930 real_backward_count 107909   6.600%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  0.973105/  1.230326, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1041%\n",
      "layer   2  Sparsity: 69.3276%\n",
      "layer   3  Sparsity: 72.0483%\n",
      "total_backward_count 1644720 real_backward_count 108181   6.577%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  0.962172/  1.246312, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0938%\n",
      "layer   2  Sparsity: 69.3200%\n",
      "layer   3  Sparsity: 72.1895%\n",
      "total_backward_count 1654510 real_backward_count 108503   6.558%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  0.969203/  1.230142, val:  89.17%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.79 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0974%\n",
      "layer   2  Sparsity: 69.3763%\n",
      "layer   3  Sparsity: 72.2768%\n",
      "total_backward_count 1664300 real_backward_count 108801   6.537%\n",
      "fc layer 2 self.abs_max_out: 2319.0\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  0.967191/  1.223595, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.51 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   2  Sparsity: 69.3523%\n",
      "layer   3  Sparsity: 72.1974%\n",
      "total_backward_count 1674090 real_backward_count 109075   6.515%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  0.964259/  1.218076, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.80 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 69.4538%\n",
      "layer   3  Sparsity: 72.4244%\n",
      "total_backward_count 1683880 real_backward_count 109318   6.492%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  0.953809/  1.198790, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.10 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   2  Sparsity: 69.1884%\n",
      "layer   3  Sparsity: 72.3269%\n",
      "total_backward_count 1693670 real_backward_count 109601   6.471%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  0.944883/  1.221228, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0546%\n",
      "layer   2  Sparsity: 69.5190%\n",
      "layer   3  Sparsity: 72.4402%\n",
      "total_backward_count 1703460 real_backward_count 109873   6.450%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  0.958874/  1.220122, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.34 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0957%\n",
      "layer   2  Sparsity: 69.5584%\n",
      "layer   3  Sparsity: 72.4886%\n",
      "total_backward_count 1713250 real_backward_count 110117   6.427%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  0.968747/  1.217448, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.53 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   2  Sparsity: 69.1759%\n",
      "layer   3  Sparsity: 72.4436%\n",
      "total_backward_count 1723040 real_backward_count 110402   6.407%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  0.953387/  1.219812, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.64 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0326%\n",
      "layer   2  Sparsity: 68.9401%\n",
      "layer   3  Sparsity: 72.8671%\n",
      "total_backward_count 1732830 real_backward_count 110662   6.386%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  0.953831/  1.216803, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.05 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0638%\n",
      "layer   2  Sparsity: 68.9267%\n",
      "layer   3  Sparsity: 73.0246%\n",
      "total_backward_count 1742620 real_backward_count 110881   6.363%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  0.956892/  1.220276, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   2  Sparsity: 69.3175%\n",
      "layer   3  Sparsity: 73.1706%\n",
      "total_backward_count 1752410 real_backward_count 111126   6.341%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  0.956149/  1.224638, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.23 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   2  Sparsity: 69.2858%\n",
      "layer   3  Sparsity: 73.0363%\n",
      "total_backward_count 1762200 real_backward_count 111360   6.319%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  0.950842/  1.199838, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.1060%\n",
      "layer   2  Sparsity: 69.2549%\n",
      "layer   3  Sparsity: 72.7587%\n",
      "total_backward_count 1771990 real_backward_count 111581   6.297%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  0.946640/  1.205602, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.13 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   2  Sparsity: 69.2112%\n",
      "layer   3  Sparsity: 72.9148%\n",
      "total_backward_count 1781780 real_backward_count 111839   6.277%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  0.936460/  1.225770, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.30 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   2  Sparsity: 69.3606%\n",
      "layer   3  Sparsity: 73.1895%\n",
      "total_backward_count 1791570 real_backward_count 112073   6.256%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  0.938372/  1.214141, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.69 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   2  Sparsity: 69.4191%\n",
      "layer   3  Sparsity: 72.8397%\n",
      "total_backward_count 1801360 real_backward_count 112317   6.235%\n",
      "fc layer 3 self.abs_max_out: 785.0\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  0.954369/  1.219620, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0913%\n",
      "layer   2  Sparsity: 69.5309%\n",
      "layer   3  Sparsity: 73.4555%\n",
      "total_backward_count 1811150 real_backward_count 112574   6.216%\n",
      "fc layer 3 self.abs_max_out: 793.0\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  0.945002/  1.218809, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.22 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   2  Sparsity: 69.5095%\n",
      "layer   3  Sparsity: 73.2061%\n",
      "total_backward_count 1820940 real_backward_count 112832   6.196%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  0.940430/  1.219181, val:  85.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.70 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0969%\n",
      "layer   2  Sparsity: 69.5189%\n",
      "layer   3  Sparsity: 73.1511%\n",
      "total_backward_count 1830730 real_backward_count 113083   6.177%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  0.940289/  1.211835, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.92 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0723%\n",
      "layer   2  Sparsity: 69.6391%\n",
      "layer   3  Sparsity: 72.8052%\n",
      "total_backward_count 1840520 real_backward_count 113321   6.157%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  0.954724/  1.217366, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 69.7277%\n",
      "layer   3  Sparsity: 73.1006%\n",
      "total_backward_count 1850310 real_backward_count 113545   6.137%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  0.945849/  1.204197, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.15 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0543%\n",
      "layer   2  Sparsity: 69.4995%\n",
      "layer   3  Sparsity: 72.8125%\n",
      "total_backward_count 1860100 real_backward_count 113807   6.118%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  0.938760/  1.225084, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.02 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0495%\n",
      "layer   2  Sparsity: 69.4096%\n",
      "layer   3  Sparsity: 72.9504%\n",
      "total_backward_count 1869890 real_backward_count 114073   6.101%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  0.934546/  1.217711, val:  86.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.97 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0494%\n",
      "layer   2  Sparsity: 69.5649%\n",
      "layer   3  Sparsity: 73.0348%\n",
      "total_backward_count 1879680 real_backward_count 114309   6.081%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  0.925030/  1.219216, val:  85.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.96 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0384%\n",
      "layer   2  Sparsity: 69.6089%\n",
      "layer   3  Sparsity: 72.9685%\n",
      "total_backward_count 1889470 real_backward_count 114508   6.060%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  0.939983/  1.204307, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.17 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   2  Sparsity: 69.6856%\n",
      "layer   3  Sparsity: 73.1711%\n",
      "total_backward_count 1899260 real_backward_count 114750   6.042%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  0.930435/  1.210303, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.29 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1262%\n",
      "layer   2  Sparsity: 69.5359%\n",
      "layer   3  Sparsity: 73.0457%\n",
      "total_backward_count 1909050 real_backward_count 114942   6.021%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  0.933230/  1.205046, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.95 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0732%\n",
      "layer   2  Sparsity: 69.3505%\n",
      "layer   3  Sparsity: 73.2212%\n",
      "total_backward_count 1918840 real_backward_count 115162   6.002%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  0.929125/  1.185860, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.24 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0907%\n",
      "layer   2  Sparsity: 69.4425%\n",
      "layer   3  Sparsity: 73.2661%\n",
      "total_backward_count 1928630 real_backward_count 115360   5.981%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  0.935202/  1.201210, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.57 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0736%\n",
      "layer   2  Sparsity: 69.5815%\n",
      "layer   3  Sparsity: 72.7941%\n",
      "total_backward_count 1938420 real_backward_count 115592   5.963%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  0.935172/  1.205128, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.21 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0823%\n",
      "layer   2  Sparsity: 69.5999%\n",
      "layer   3  Sparsity: 72.4897%\n",
      "total_backward_count 1948210 real_backward_count 115811   5.944%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  0.934641/  1.207330, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.25 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1170%\n",
      "layer   2  Sparsity: 69.4348%\n",
      "layer   3  Sparsity: 72.4829%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd323b350cd452faf8a8315be58fabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÑ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.93464</td></tr><tr><td>val_acc_best</td><td>0.91667</td></tr><tr><td>val_acc_now</td><td>0.87083</td></tr><tr><td>val_loss</td><td>1.20733</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-sweep-5</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2zkqg4x0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2zkqg4x0</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_123253-2zkqg4x0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ddny9yod with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 10074\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_163450-ddny9yod</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ddny9yod' target=\"_blank\">breezy-sweep-6</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/27lqvb5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ddny9yod' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ddny9yod</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251118_163459_058', 'my_seed': 10074, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 299.0\n",
      "lif layer 1 self.abs_max_v: 299.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 283.0\n",
      "lif layer 2 self.abs_max_v: 283.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 83.0\n",
      "lif layer 1 self.abs_max_v: 358.0\n",
      "lif layer 2 self.abs_max_v: 310.0\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "fc layer 2 self.abs_max_out: 329.0\n",
      "lif layer 2 self.abs_max_v: 386.0\n",
      "fc layer 3 self.abs_max_out: 158.0\n",
      "fc layer 2 self.abs_max_out: 337.0\n",
      "lif layer 2 self.abs_max_v: 465.0\n",
      "fc layer 1 self.abs_max_out: 355.0\n",
      "lif layer 1 self.abs_max_v: 426.5\n",
      "fc layer 2 self.abs_max_out: 464.0\n",
      "lif layer 2 self.abs_max_v: 566.5\n",
      "fc layer 1 self.abs_max_out: 387.0\n",
      "lif layer 1 self.abs_max_v: 532.5\n",
      "lif layer 2 self.abs_max_v: 629.5\n",
      "lif layer 1 self.abs_max_v: 594.5\n",
      "fc layer 3 self.abs_max_out: 159.0\n",
      "fc layer 1 self.abs_max_out: 452.0\n",
      "fc layer 1 self.abs_max_out: 462.0\n",
      "fc layer 3 self.abs_max_out: 170.0\n",
      "fc layer 2 self.abs_max_out: 478.0\n",
      "fc layer 2 self.abs_max_out: 490.0\n",
      "lif layer 2 self.abs_max_v: 666.0\n",
      "fc layer 1 self.abs_max_out: 504.0\n",
      "lif layer 2 self.abs_max_v: 669.0\n",
      "fc layer 3 self.abs_max_out: 183.0\n",
      "fc layer 2 self.abs_max_out: 535.0\n",
      "fc layer 1 self.abs_max_out: 547.0\n",
      "lif layer 2 self.abs_max_v: 729.5\n",
      "lif layer 1 self.abs_max_v: 618.5\n",
      "fc layer 1 self.abs_max_out: 667.0\n",
      "lif layer 1 self.abs_max_v: 667.0\n",
      "fc layer 2 self.abs_max_out: 581.0\n",
      "lif layer 2 self.abs_max_v: 906.0\n",
      "lif layer 1 self.abs_max_v: 732.5\n",
      "lif layer 2 self.abs_max_v: 952.0\n",
      "fc layer 3 self.abs_max_out: 193.0\n",
      "lif layer 1 self.abs_max_v: 807.5\n",
      "lif layer 2 self.abs_max_v: 975.0\n",
      "fc layer 3 self.abs_max_out: 225.0\n",
      "fc layer 1 self.abs_max_out: 773.0\n",
      "fc layer 3 self.abs_max_out: 250.0\n",
      "fc layer 2 self.abs_max_out: 631.0\n",
      "fc layer 1 self.abs_max_out: 979.0\n",
      "lif layer 1 self.abs_max_v: 979.0\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "fc layer 2 self.abs_max_out: 651.0\n",
      "fc layer 2 self.abs_max_out: 665.0\n",
      "fc layer 2 self.abs_max_out: 750.0\n",
      "fc layer 3 self.abs_max_out: 261.0\n",
      "fc layer 3 self.abs_max_out: 268.0\n",
      "fc layer 3 self.abs_max_out: 270.0\n",
      "fc layer 3 self.abs_max_out: 327.0\n",
      "fc layer 2 self.abs_max_out: 765.0\n",
      "lif layer 1 self.abs_max_v: 1016.5\n",
      "lif layer 2 self.abs_max_v: 981.5\n",
      "fc layer 2 self.abs_max_out: 777.0\n",
      "fc layer 2 self.abs_max_out: 857.0\n",
      "fc layer 2 self.abs_max_out: 936.0\n",
      "fc layer 1 self.abs_max_out: 1008.0\n",
      "lif layer 1 self.abs_max_v: 1060.5\n",
      "fc layer 1 self.abs_max_out: 1093.0\n",
      "lif layer 1 self.abs_max_v: 1093.0\n",
      "fc layer 1 self.abs_max_out: 1121.0\n",
      "lif layer 1 self.abs_max_v: 1121.0\n",
      "lif layer 2 self.abs_max_v: 982.5\n",
      "fc layer 1 self.abs_max_out: 1130.0\n",
      "lif layer 1 self.abs_max_v: 1130.0\n",
      "lif layer 2 self.abs_max_v: 1001.0\n",
      "fc layer 1 self.abs_max_out: 1133.0\n",
      "lif layer 1 self.abs_max_v: 1133.0\n",
      "lif layer 2 self.abs_max_v: 1059.5\n",
      "fc layer 1 self.abs_max_out: 1179.0\n",
      "lif layer 1 self.abs_max_v: 1179.0\n",
      "lif layer 1 self.abs_max_v: 1212.5\n",
      "lif layer 1 self.abs_max_v: 1329.5\n",
      "fc layer 3 self.abs_max_out: 329.0\n",
      "fc layer 2 self.abs_max_out: 939.0\n",
      "lif layer 1 self.abs_max_v: 1392.0\n",
      "lif layer 1 self.abs_max_v: 1537.0\n",
      "fc layer 2 self.abs_max_out: 1003.0\n",
      "lif layer 2 self.abs_max_v: 1113.0\n",
      "lif layer 2 self.abs_max_v: 1135.5\n",
      "lif layer 2 self.abs_max_v: 1156.5\n",
      "lif layer 2 self.abs_max_v: 1169.0\n",
      "fc layer 1 self.abs_max_out: 1199.0\n",
      "lif layer 1 self.abs_max_v: 1552.0\n",
      "lif layer 1 self.abs_max_v: 1625.5\n",
      "lif layer 1 self.abs_max_v: 1661.0\n",
      "lif layer 1 self.abs_max_v: 1724.0\n",
      "lif layer 2 self.abs_max_v: 1212.0\n",
      "fc layer 1 self.abs_max_out: 1259.0\n",
      "lif layer 1 self.abs_max_v: 1751.0\n",
      "lif layer 1 self.abs_max_v: 1836.5\n",
      "lif layer 2 self.abs_max_v: 1242.5\n",
      "fc layer 3 self.abs_max_out: 341.0\n",
      "fc layer 1 self.abs_max_out: 1408.0\n",
      "lif layer 2 self.abs_max_v: 1249.0\n",
      "fc layer 2 self.abs_max_out: 1095.0\n",
      "lif layer 2 self.abs_max_v: 1266.0\n",
      "lif layer 1 self.abs_max_v: 1890.5\n",
      "lif layer 1 self.abs_max_v: 1894.5\n",
      "lif layer 1 self.abs_max_v: 1922.5\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "lif layer 1 self.abs_max_v: 1923.0\n",
      "lif layer 1 self.abs_max_v: 1992.0\n",
      "lif layer 1 self.abs_max_v: 2012.0\n",
      "lif layer 2 self.abs_max_v: 1305.5\n",
      "fc layer 3 self.abs_max_out: 376.0\n",
      "lif layer 2 self.abs_max_v: 1323.5\n",
      "lif layer 2 self.abs_max_v: 1384.5\n",
      "fc layer 1 self.abs_max_out: 1463.0\n",
      "lif layer 2 self.abs_max_v: 1414.0\n",
      "lif layer 1 self.abs_max_v: 2077.0\n",
      "fc layer 1 self.abs_max_out: 1571.0\n",
      "lif layer 1 self.abs_max_v: 2514.0\n",
      "lif layer 1 self.abs_max_v: 2713.0\n",
      "lif layer 2 self.abs_max_v: 1437.5\n",
      "lif layer 2 self.abs_max_v: 1489.0\n",
      "lif layer 2 self.abs_max_v: 1489.5\n",
      "fc layer 2 self.abs_max_out: 1101.0\n",
      "fc layer 2 self.abs_max_out: 1112.0\n",
      "fc layer 1 self.abs_max_out: 1605.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.782376/  1.971215, val:  30.00%, val_best:  30.00%, tr:  97.14%, tr_best:  97.14%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0444%\n",
      "layer   2  Sparsity: 77.2923%\n",
      "layer   3  Sparsity: 72.3281%\n",
      "total_backward_count 9790 real_backward_count 2252  23.003%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 1635.0\n",
      "fc layer 2 self.abs_max_out: 1140.0\n",
      "fc layer 3 self.abs_max_out: 379.0\n",
      "lif layer 2 self.abs_max_v: 1544.0\n",
      "fc layer 3 self.abs_max_out: 384.0\n",
      "fc layer 2 self.abs_max_out: 1269.0\n",
      "fc layer 1 self.abs_max_out: 1693.0\n",
      "lif layer 1 self.abs_max_v: 2986.0\n",
      "fc layer 3 self.abs_max_out: 385.0\n",
      "fc layer 3 self.abs_max_out: 389.0\n",
      "fc layer 3 self.abs_max_out: 406.0\n",
      "fc layer 3 self.abs_max_out: 419.0\n",
      "fc layer 3 self.abs_max_out: 422.0\n",
      "lif layer 2 self.abs_max_v: 1585.5\n",
      "fc layer 3 self.abs_max_out: 433.0\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "fc layer 2 self.abs_max_out: 1311.0\n",
      "fc layer 1 self.abs_max_out: 1707.0\n",
      "fc layer 1 self.abs_max_out: 1755.0\n",
      "fc layer 1 self.abs_max_out: 1897.0\n",
      "fc layer 1 self.abs_max_out: 1934.0\n",
      "lif layer 1 self.abs_max_v: 3276.0\n",
      "lif layer 1 self.abs_max_v: 3561.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.654542/  1.929696, val:  33.33%, val_best:  33.33%, tr:  99.28%, tr_best:  99.28%, epoch time: 71.90 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0499%\n",
      "layer   2  Sparsity: 77.4198%\n",
      "layer   3  Sparsity: 68.5866%\n",
      "total_backward_count 19580 real_backward_count 3829  19.556%\n",
      "lif layer 2 self.abs_max_v: 1589.5\n",
      "lif layer 2 self.abs_max_v: 1637.0\n",
      "lif layer 2 self.abs_max_v: 1679.0\n",
      "lif layer 2 self.abs_max_v: 1787.5\n",
      "lif layer 2 self.abs_max_v: 1796.5\n",
      "fc layer 3 self.abs_max_out: 466.0\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "fc layer 3 self.abs_max_out: 482.0\n",
      "fc layer 1 self.abs_max_out: 2065.0\n",
      "fc layer 1 self.abs_max_out: 2076.0\n",
      "lif layer 1 self.abs_max_v: 3835.5\n",
      "fc layer 2 self.abs_max_out: 1313.0\n",
      "fc layer 2 self.abs_max_out: 1342.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.608837/  1.911446, val:  29.58%, val_best:  33.33%, tr:  99.18%, tr_best:  99.28%, epoch time: 71.88 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   2  Sparsity: 76.9317%\n",
      "layer   3  Sparsity: 67.2582%\n",
      "total_backward_count 29370 real_backward_count 5278  17.971%\n",
      "fc layer 2 self.abs_max_out: 1361.0\n",
      "fc layer 3 self.abs_max_out: 484.0\n",
      "fc layer 3 self.abs_max_out: 491.0\n",
      "fc layer 1 self.abs_max_out: 2107.0\n",
      "fc layer 1 self.abs_max_out: 2114.0\n",
      "fc layer 1 self.abs_max_out: 2117.0\n",
      "lif layer 1 self.abs_max_v: 3934.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.587956/  1.877309, val:  50.42%, val_best:  50.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 71.53 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0935%\n",
      "layer   2  Sparsity: 77.1607%\n",
      "layer   3  Sparsity: 66.6213%\n",
      "total_backward_count 39160 real_backward_count 6668  17.028%\n",
      "fc layer 2 self.abs_max_out: 1362.0\n",
      "fc layer 2 self.abs_max_out: 1390.0\n",
      "fc layer 1 self.abs_max_out: 2215.0\n",
      "fc layer 1 self.abs_max_out: 2315.0\n",
      "lif layer 1 self.abs_max_v: 4058.0\n",
      "lif layer 1 self.abs_max_v: 4115.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.584726/  1.864495, val:  45.00%, val_best:  50.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0475%\n",
      "layer   2  Sparsity: 76.4042%\n",
      "layer   3  Sparsity: 66.5654%\n",
      "total_backward_count 48950 real_backward_count 7970  16.282%\n",
      "fc layer 2 self.abs_max_out: 1408.0\n",
      "fc layer 3 self.abs_max_out: 509.0\n",
      "lif layer 1 self.abs_max_v: 4192.0\n",
      "lif layer 1 self.abs_max_v: 4193.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.543706/  1.825379, val:  55.83%, val_best:  55.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 71.66 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   2  Sparsity: 76.9191%\n",
      "layer   3  Sparsity: 66.2050%\n",
      "total_backward_count 58740 real_backward_count 9279  15.797%\n",
      "fc layer 3 self.abs_max_out: 510.0\n",
      "fc layer 2 self.abs_max_out: 1440.0\n",
      "lif layer 2 self.abs_max_v: 1829.0\n",
      "lif layer 2 self.abs_max_v: 1889.5\n",
      "fc layer 3 self.abs_max_out: 525.0\n",
      "fc layer 1 self.abs_max_out: 2353.0\n",
      "fc layer 1 self.abs_max_out: 2417.0\n",
      "lif layer 1 self.abs_max_v: 4322.5\n",
      "fc layer 2 self.abs_max_out: 1470.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.531952/  1.809435, val:  50.42%, val_best:  55.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0497%\n",
      "layer   2  Sparsity: 76.1774%\n",
      "layer   3  Sparsity: 66.4094%\n",
      "total_backward_count 68530 real_backward_count 10572  15.427%\n",
      "lif layer 2 self.abs_max_v: 1910.5\n",
      "fc layer 1 self.abs_max_out: 2420.0\n",
      "fc layer 1 self.abs_max_out: 2505.0\n",
      "lif layer 1 self.abs_max_v: 4446.5\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.506742/  1.812306, val:  37.50%, val_best:  55.83%, tr:  99.59%, tr_best:  99.90%, epoch time: 71.92 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   2  Sparsity: 76.2618%\n",
      "layer   3  Sparsity: 66.9158%\n",
      "total_backward_count 78320 real_backward_count 11762  15.018%\n",
      "lif layer 2 self.abs_max_v: 2018.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.487758/  1.773399, val:  45.83%, val_best:  55.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 72.20 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   2  Sparsity: 75.5891%\n",
      "layer   3  Sparsity: 66.7397%\n",
      "total_backward_count 88110 real_backward_count 12965  14.715%\n",
      "lif layer 1 self.abs_max_v: 4538.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.478007/  1.721355, val:  64.58%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.56 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0366%\n",
      "layer   2  Sparsity: 76.0318%\n",
      "layer   3  Sparsity: 66.9856%\n",
      "total_backward_count 97900 real_backward_count 14183  14.487%\n",
      "fc layer 3 self.abs_max_out: 528.0\n",
      "fc layer 1 self.abs_max_out: 2533.0\n",
      "lif layer 2 self.abs_max_v: 2112.5\n",
      "lif layer 2 self.abs_max_v: 2205.5\n",
      "fc layer 1 self.abs_max_out: 2565.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.479747/  1.726557, val:  52.92%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.52 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   2  Sparsity: 75.5932%\n",
      "layer   3  Sparsity: 68.1537%\n",
      "total_backward_count 107690 real_backward_count 15419  14.318%\n",
      "lif layer 2 self.abs_max_v: 2251.5\n",
      "lif layer 2 self.abs_max_v: 2256.5\n",
      "fc layer 2 self.abs_max_out: 1497.0\n",
      "fc layer 2 self.abs_max_out: 1513.0\n",
      "lif layer 1 self.abs_max_v: 4665.5\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.476169/  1.730558, val:  59.17%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 71.87 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   2  Sparsity: 75.7345%\n",
      "layer   3  Sparsity: 68.8546%\n",
      "total_backward_count 117480 real_backward_count 16589  14.121%\n",
      "fc layer 1 self.abs_max_out: 2615.0\n",
      "lif layer 1 self.abs_max_v: 4779.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.480679/  1.725661, val:  55.83%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.11 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   2  Sparsity: 75.6660%\n",
      "layer   3  Sparsity: 67.5881%\n",
      "total_backward_count 127270 real_backward_count 17785  13.974%\n",
      "fc layer 1 self.abs_max_out: 2678.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.459789/  1.699972, val:  57.08%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.67 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0397%\n",
      "layer   2  Sparsity: 75.2402%\n",
      "layer   3  Sparsity: 67.0422%\n",
      "total_backward_count 137060 real_backward_count 18960  13.833%\n",
      "fc layer 3 self.abs_max_out: 552.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.427395/  1.711142, val:  56.25%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.20 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   2  Sparsity: 75.4114%\n",
      "layer   3  Sparsity: 66.9247%\n",
      "total_backward_count 146850 real_backward_count 20074  13.670%\n",
      "fc layer 3 self.abs_max_out: 571.0\n",
      "fc layer 3 self.abs_max_out: 572.0\n",
      "fc layer 2 self.abs_max_out: 1520.0\n",
      "fc layer 1 self.abs_max_out: 2756.0\n",
      "lif layer 1 self.abs_max_v: 4885.5\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.419668/  1.631543, val:  69.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.10 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0522%\n",
      "layer   2  Sparsity: 75.1096%\n",
      "layer   3  Sparsity: 66.5491%\n",
      "total_backward_count 156640 real_backward_count 21222  13.548%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'random', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        # \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.25]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/512]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [-8]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = '27lqvb5e'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
