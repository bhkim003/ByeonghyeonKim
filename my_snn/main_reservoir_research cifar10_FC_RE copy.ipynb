{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                  FC_RESERVOIR=False):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        if self.FC_RESERVOIR == True:\n",
    "            self.reservoir = nn.Linear(in_features=self.in_channel*self.in_spike_size*self.in_spike_size, out_features=self.in_channel*self.in_spike_size*self.in_spike_size, bias=True)\n",
    "        else:\n",
    "            self.reservoir = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                            kernel_size=self.receptive_size, \n",
    "                                            stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.reservoir.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.reservoir.bias, 0)\n",
    "\n",
    "        # membrane potential 초기화\n",
    "        self.v = torch.full((self.in_channel, self.in_spike_size, self.in_spike_size), fill_value=self.v_init, requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # depthwise conv reservoir: pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            # fc conv reservoir: pre_spike[t] [batch_size, in_channel*in_spike_size*in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "                \n",
    "            recurrent_current = self.reservoir(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# class RESERVOIR(nn.Module):\n",
    "#     def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "#                   FC_RESERVOIR=False):\n",
    "#         super(RESERVOIR, self).__init__()\n",
    "#         self.TIME_STEP = TIME_STEP\n",
    "#         self.in_spike_size = in_spike_size\n",
    "#         self.in_channel = in_channel\n",
    "#         self.receptive_size = receptive_size #3\n",
    "#         self.v_init = v_init\n",
    "#         self.v_decay = v_decay\n",
    "#         self.v_threshold = v_threshold\n",
    "#         self.v_reset = v_reset\n",
    "#         self.hard_reset = hard_reset\n",
    "#         self.pre_spike_weight = pre_spike_weight\n",
    "#         self.FC_RESERVOIR = FC_RESERVOIR\n",
    "#         self.BATCH_SIZE = 256\n",
    "\n",
    "#         self.out_channel = 1\n",
    "\n",
    "#         # 파라미터 \n",
    "#         if self.FC_RESERVOIR == True:\n",
    "#             self.reservoir = nn.Linear(in_features=self.in_channel*self.in_spike_size*self.in_spike_size, out_features=self.in_channel*self.in_spike_size*self.in_spike_size, bias=True)\n",
    "#         else:\n",
    "#             self.reservoir = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "#                                             kernel_size=self.receptive_size, \n",
    "#                                             stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "#         self.reservoir = self.reservoir\n",
    "#         # kaiming 초기화\n",
    "#         nn.init.kaiming_normal_(self.reservoir.weight, mode='fan_out', nonlinearity='relu')\n",
    "#         nn.init.constant_(self.reservoir.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "#         if self.FC_RESERVOIR == True:\n",
    "#             spike_size = (self.BATCH_SIZE, self.in_channel * self.in_spike_size * self.in_spike_size)\n",
    "#         else: \n",
    "#             spike_size = (self.BATCH_SIZE, self.in_channel, self.in_spike_size, self.in_spike_size)\n",
    "\n",
    "#         device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "#         # membrane potential 초기화 (이거를 forward안에서 하면 forward 할 때마다 delay 됨!!!)\n",
    "#         self.v = torch.full(spike_size, fill_value=self.v_init, requires_grad=False).to(device)\n",
    "\n",
    "#         # postspike 초기화 (이거를 forward안에서 하면 forward 할 때마다 delay 됨!!!)\n",
    "#         self.post_spike = torch.full(spike_size, fill_value=0.0, requires_grad=False).to(device)\n",
    "\n",
    "\n",
    "#     def forward(self, pre_spike):    \n",
    "#         # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "#         # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "#         # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "#         # timestep 안 맞으면 종료\n",
    "#         assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "#         self.BATCH_SIZE\n",
    "#         current_spike_batch_size = pre_spike.size(1)\n",
    "#         # self.v = self.v[:current_spike_batch_size]\n",
    "#         # self.post_spike = self.post_spike[:current_spike_batch_size]\n",
    "\n",
    "#         output = []\n",
    "#         for t in range(self.TIME_STEP):\n",
    "#             step_start_time = time.time()\n",
    "            \n",
    "#             # depthwise conv reservoir: pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "#             # fc conv reservoir: pre_spike[t] [batch_size, in_channel*in_spike_size*in_spike_size]\n",
    "#             input_current = self.pre_spike_weight * pre_spike[t]\n",
    "            \n",
    "#             pre_spike_time = time.time()\n",
    "#             # print(f\"Pre-spike processing time for step {t}: {pre_spike_time - step_start_time:.6f} seconds\")\n",
    "            \n",
    "#             recurrent_current = self.reservoir(self.post_spike)\n",
    "            \n",
    "#             recurrent_time = time.time()\n",
    "#             # print(f\"Recurrent processing time for step {t}: {recurrent_time - pre_spike_time:.6f} seconds\")\n",
    "            \n",
    "#             current = input_current + recurrent_current\n",
    "#             # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "\n",
    "#             # decay and itegrate\n",
    "#             self.v = self.v * self.v_decay + current\n",
    "            \n",
    "#             integrate_time = time.time()\n",
    "#             # print(f\"Integrate processing time for step {t}: {integrate_time - recurrent_time:.6f} seconds\")\n",
    "\n",
    "#             # post spike\n",
    "#             self.post_spike = (self.v >= self.v_threshold).float()\n",
    "\n",
    "#             post_spike_time = time.time()\n",
    "#             # print(f\"Post-spike processing time for step {t}: {post_spike_time - integrate_time:.6f} seconds\")\n",
    "\n",
    "#             output.append(self.post_spike)\n",
    "            \n",
    "#             # reset\n",
    "#             if self.hard_reset: # hard reset\n",
    "#                 self.v = (1 - self.post_spike) * self.v + self.post_spike * self.v_reset \n",
    "#             else: # soft reset\n",
    "#                 self.v = self.v - self.post_spike * self.v_threshold\n",
    "                \n",
    "#             self.v = self.v.detach()\n",
    "#             self.post_spike = self.post_spike.detach()\n",
    "\n",
    "\n",
    "#             reset_time = time.time()\n",
    "#             # print(f\"Reset processing time for step {t}: {reset_time - post_spike_time:.6f} seconds\")\n",
    "\n",
    "#         output = torch.stack(output, dim=0)\n",
    "        \n",
    "#         end_time = time.time()\n",
    "        \n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                 no_reservoir = False, FC_RESERVOIR=False):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.no_reservoir = no_reservoir\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight,\n",
    "                                       FC_RESERVOIR=FC_RESERVOIR)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.TIME_STEP == x.size(1), f\"Time step mismatch: {x.size(1)} vs {self.TIME_STEP}\"\n",
    "        start = time.time()\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        if (self.FC_RESERVOIR == True):\n",
    "            x = x.reshape(x.size(0), x.size(1), -1)\n",
    "\n",
    "        \n",
    "        if self.no_reservoir == False:\n",
    "            with torch.no_grad():\n",
    "                x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        \n",
    "        ## 11번째 클래스 배제 ########################################################################\n",
    "        exclude_class = 10\n",
    "        if dvs_duration > 0:\n",
    "            train_file_name = f'modules/dvs_gesture_class_index/train_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            test_file_name = f'modules/dvs_gesture_class_index/test_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            if (os.path.isfile(train_file_name) and os.path.isfile(test_file_name)):\n",
    "                print('\\ndvsgestrue 10 class indices exist. we want to exclude the 11th class\\n')\n",
    "                with open(train_file_name, 'rb') as f:\n",
    "                    train_indices = pickle.load(f)\n",
    "                with open(test_file_name, 'rb') as f:\n",
    "                    test_indices = pickle.load(f)\n",
    "            else:\n",
    "                print('\\ndvsgestrue 10 class indices doesn\\'t exist. we want to exclude the 11th class\\n')\n",
    "                train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "                test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "                with open(train_file_name, 'wb') as f:\n",
    "                    pickle.dump(train_indices, f)\n",
    "                with open(test_file_name, 'wb') as f:\n",
    "                    pickle.dump(test_indices, f)\n",
    "        else:\n",
    "            train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "            test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "        ################################################################################################\n",
    "\n",
    "        # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "        # mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "        print(i, '/', len(train_loader), time.time())\n",
    "        \n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        # 데이터 로딩 및 전처리\n",
    "        load_start_time = time.time()\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        load_end_time = time.time()\n",
    "        # print(f\"Data loading and preprocessing time: {load_end_time - load_start_time:.6f} seconds\")\n",
    "        \n",
    "        preprocess_start_time = time.time()\n",
    "        if which_data == 'n_tidigits':\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif which_data == 'heidelberg':\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            \n",
    "        if which_data in ['DVS_CIFAR10', 'DVS_GESTURE', 'DVS_CIFAR10_2', 'NMNIST', 'N_CALTECH101', 'n_tidigits', 'heidelberg']:\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding:\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else:\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        preprocess_end_time = time.time()\n",
    "        # print(f\"Preprocessing time: {preprocess_end_time - preprocess_start_time:.6f} seconds\")\n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "        # 모델 포워드 패스\n",
    "        forward_start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        forward_end_time = time.time()\n",
    "        # print(f\"Model forward pass time: {forward_end_time - forward_start_time:.6f} seconds\")\n",
    "        \n",
    "        # 손실 계산 및 백워드 패스\n",
    "        backward_start_time = time.time()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        backward_end_time = time.time()\n",
    "        # print(f\"Loss calculation and backward pass time: {backward_end_time - backward_start_time:.6f} seconds\")\n",
    "        \n",
    "        # 옵티마이저 스텝\n",
    "        optimizer_start_time = time.time()\n",
    "        optimizer.step()\n",
    "        optimizer_end_time = time.time()\n",
    "        print(f\"Optimizer step time: {optimizer_end_time - optimizer_start_time:.6f} seconds\")\n",
    "        \n",
    "        # 손실 및 정확도 계산\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "        \n",
    "        batch_end_time = time.time()\n",
    "        # print(f\"Total batch processing time: {batch_end_time - batch_start_time:.6f} seconds\")\n",
    "        \n",
    "    tr_accuracy = 100 * correct / total\n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    # print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # if rate_coding == True:\n",
    "            #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            # else:\n",
    "            #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, \n",
    "                            no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 87uc23yv\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/87uc23yv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ytpqho1e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.48265186299710905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.08364004008987933\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 7.061709310053014\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: CIFAR10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_214245-ytpqho1e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/ytpqho1e' target=\"_blank\">playful-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/87uc23yv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/87uc23yv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/87uc23yv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/87uc23yv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/ytpqho1e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/ytpqho1e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "RESERVOIR_NET(\n",
      "  (reservoir): RESERVOIR(\n",
      "    (reservoir): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "0 / 196 1721997786.1027758\n",
      "Optimizer step time: 0.000452 seconds\n",
      "1 / 196 1721997789.4012861\n",
      "Optimizer step time: 0.000520 seconds\n",
      "2 / 196 1721997790.51707\n",
      "Optimizer step time: 0.000317 seconds\n",
      "3 / 196 1721997791.6281228\n",
      "Optimizer step time: 0.000233 seconds\n",
      "4 / 196 1721997792.7215464\n",
      "Optimizer step time: 0.000278 seconds\n",
      "5 / 196 1721997793.9390116\n",
      "Optimizer step time: 0.000246 seconds\n",
      "6 / 196 1721997794.9565222\n",
      "Optimizer step time: 0.000459 seconds\n",
      "7 / 196 1721997795.8824046\n",
      "Optimizer step time: 0.000260 seconds\n",
      "8 / 196 1721997796.902342\n",
      "Optimizer step time: 0.000223 seconds\n",
      "9 / 196 1721997798.136958\n",
      "Optimizer step time: 0.000233 seconds\n",
      "10 / 196 1721997799.2521088\n",
      "Optimizer step time: 0.000398 seconds\n",
      "11 / 196 1721997800.47128\n",
      "Optimizer step time: 0.000235 seconds\n",
      "12 / 196 1721997801.4934487\n",
      "Optimizer step time: 0.000259 seconds\n",
      "13 / 196 1721997802.7037723\n",
      "Optimizer step time: 0.000261 seconds\n",
      "14 / 196 1721997803.8093312\n",
      "Optimizer step time: 0.000255 seconds\n",
      "15 / 196 1721997805.1028094\n",
      "Optimizer step time: 0.000406 seconds\n",
      "16 / 196 1721997806.340647\n",
      "Optimizer step time: 0.000438 seconds\n",
      "17 / 196 1721997807.5369258\n",
      "Optimizer step time: 0.000211 seconds\n",
      "18 / 196 1721997808.642492\n",
      "Optimizer step time: 0.000218 seconds\n",
      "19 / 196 1721997809.7554624\n",
      "Optimizer step time: 0.000307 seconds\n",
      "20 / 196 1721997810.8760836\n",
      "Optimizer step time: 0.000417 seconds\n",
      "21 / 196 1721997812.129597\n",
      "Optimizer step time: 0.000275 seconds\n",
      "22 / 196 1721997813.3393128\n",
      "Optimizer step time: 0.000299 seconds\n",
      "23 / 196 1721997814.4940999\n",
      "Optimizer step time: 0.000240 seconds\n",
      "24 / 196 1721997815.6280806\n",
      "Optimizer step time: 0.000250 seconds\n",
      "25 / 196 1721997816.7121637\n",
      "Optimizer step time: 0.000273 seconds\n",
      "26 / 196 1721997817.8401213\n",
      "Optimizer step time: 0.000231 seconds\n",
      "27 / 196 1721997819.0849469\n",
      "Optimizer step time: 0.000260 seconds\n",
      "28 / 196 1721997820.2724116\n",
      "Optimizer step time: 0.000287 seconds\n",
      "29 / 196 1721997821.3822615\n",
      "Optimizer step time: 0.000223 seconds\n",
      "30 / 196 1721997822.6201813\n",
      "Optimizer step time: 0.000878 seconds\n",
      "31 / 196 1721997823.836142\n",
      "Optimizer step time: 0.000313 seconds\n",
      "32 / 196 1721997825.0654805\n",
      "Optimizer step time: 0.000235 seconds\n",
      "33 / 196 1721997826.2422752\n",
      "Optimizer step time: 0.000235 seconds\n",
      "34 / 196 1721997827.4153008\n",
      "Optimizer step time: 0.000242 seconds\n",
      "35 / 196 1721997828.5751305\n",
      "Optimizer step time: 0.000265 seconds\n",
      "36 / 196 1721997829.7185044\n",
      "Optimizer step time: 0.000446 seconds\n",
      "37 / 196 1721997830.856578\n",
      "Optimizer step time: 0.000227 seconds\n",
      "38 / 196 1721997832.01073\n",
      "Optimizer step time: 0.000223 seconds\n",
      "39 / 196 1721997833.1772594\n",
      "Optimizer step time: 0.000226 seconds\n",
      "40 / 196 1721997834.3885865\n",
      "Optimizer step time: 0.000710 seconds\n",
      "41 / 196 1721997835.6972673\n",
      "Optimizer step time: 0.000256 seconds\n",
      "42 / 196 1721997836.9081485\n",
      "Optimizer step time: 0.000242 seconds\n",
      "43 / 196 1721997838.1519928\n",
      "Optimizer step time: 0.000258 seconds\n",
      "44 / 196 1721997839.4210577\n",
      "Optimizer step time: 0.000230 seconds\n",
      "45 / 196 1721997840.56195\n",
      "Optimizer step time: 0.000314 seconds\n",
      "46 / 196 1721997841.8370242\n",
      "Optimizer step time: 0.000255 seconds\n",
      "47 / 196 1721997842.9856734\n",
      "Optimizer step time: 0.000264 seconds\n",
      "48 / 196 1721997844.1326163\n",
      "Optimizer step time: 0.000262 seconds\n",
      "49 / 196 1721997845.2690678\n",
      "Optimizer step time: 0.000473 seconds\n",
      "50 / 196 1721997846.4136689\n",
      "Optimizer step time: 0.000386 seconds\n",
      "51 / 196 1721997847.4938135\n",
      "Optimizer step time: 0.000222 seconds\n",
      "52 / 196 1721997848.7211983\n",
      "Optimizer step time: 0.000219 seconds\n",
      "53 / 196 1721997850.0400043\n",
      "Optimizer step time: 0.000236 seconds\n",
      "54 / 196 1721997851.202492\n",
      "Optimizer step time: 0.000407 seconds\n",
      "55 / 196 1721997852.3902607\n",
      "Optimizer step time: 0.000272 seconds\n",
      "56 / 196 1721997853.5666776\n",
      "Optimizer step time: 0.000394 seconds\n",
      "57 / 196 1721997854.8962276\n",
      "Optimizer step time: 0.000277 seconds\n",
      "58 / 196 1721997856.085835\n",
      "Optimizer step time: 0.000211 seconds\n",
      "59 / 196 1721997857.26809\n",
      "Optimizer step time: 0.000253 seconds\n",
      "60 / 196 1721997858.3545208\n",
      "Optimizer step time: 0.000410 seconds\n",
      "61 / 196 1721997859.4801314\n",
      "Optimizer step time: 0.000253 seconds\n",
      "62 / 196 1721997860.733368\n",
      "Optimizer step time: 0.000288 seconds\n",
      "63 / 196 1721997862.0229208\n",
      "Optimizer step time: 0.000269 seconds\n",
      "64 / 196 1721997863.1604338\n",
      "Optimizer step time: 0.000214 seconds\n",
      "65 / 196 1721997864.3640258\n",
      "Optimizer step time: 0.000200 seconds\n",
      "66 / 196 1721997865.4695704\n",
      "Optimizer step time: 0.000274 seconds\n",
      "67 / 196 1721997866.6024024\n",
      "Optimizer step time: 0.000390 seconds\n",
      "68 / 196 1721997867.8321786\n",
      "Optimizer step time: 0.000314 seconds\n",
      "69 / 196 1721997868.944039\n",
      "Optimizer step time: 0.000286 seconds\n",
      "70 / 196 1721997870.1007648\n",
      "Optimizer step time: 0.000377 seconds\n",
      "71 / 196 1721997871.3121428\n",
      "Optimizer step time: 0.000282 seconds\n",
      "72 / 196 1721997872.5168269\n",
      "Optimizer step time: 0.000240 seconds\n",
      "73 / 196 1721997873.6716795\n",
      "Optimizer step time: 0.000388 seconds\n",
      "74 / 196 1721997874.7881203\n",
      "Optimizer step time: 0.000306 seconds\n",
      "75 / 196 1721997875.867852\n",
      "Optimizer step time: 0.000268 seconds\n",
      "76 / 196 1721997877.0961354\n",
      "Optimizer step time: 0.000249 seconds\n",
      "77 / 196 1721997878.36925\n",
      "Optimizer step time: 0.000273 seconds\n",
      "78 / 196 1721997879.5631719\n",
      "Optimizer step time: 0.000262 seconds\n",
      "79 / 196 1721997880.7439773\n",
      "Optimizer step time: 0.000271 seconds\n",
      "80 / 196 1721997881.969029\n",
      "Optimizer step time: 0.000255 seconds\n",
      "81 / 196 1721997883.1932054\n",
      "Optimizer step time: 0.000336 seconds\n",
      "82 / 196 1721997884.334308\n",
      "Optimizer step time: 0.000254 seconds\n",
      "83 / 196 1721997885.496281\n",
      "Optimizer step time: 0.000316 seconds\n",
      "84 / 196 1721997886.7454226\n",
      "Optimizer step time: 0.000268 seconds\n",
      "85 / 196 1721997887.8682935\n",
      "Optimizer step time: 0.000260 seconds\n",
      "86 / 196 1721997889.0804248\n",
      "Optimizer step time: 0.000244 seconds\n",
      "87 / 196 1721997890.1789513\n",
      "Optimizer step time: 0.000228 seconds\n",
      "88 / 196 1721997891.3024578\n",
      "Optimizer step time: 0.000253 seconds\n",
      "89 / 196 1721997892.456628\n",
      "Optimizer step time: 0.000255 seconds\n",
      "90 / 196 1721997893.6634247\n",
      "Optimizer step time: 0.000425 seconds\n",
      "91 / 196 1721997894.9530885\n",
      "Optimizer step time: 0.000189 seconds\n",
      "92 / 196 1721997896.192554\n",
      "Optimizer step time: 0.000234 seconds\n",
      "93 / 196 1721997897.4820056\n",
      "Optimizer step time: 0.000490 seconds\n",
      "94 / 196 1721997898.6316166\n",
      "Optimizer step time: 0.000251 seconds\n",
      "95 / 196 1721997899.7761009\n",
      "Optimizer step time: 0.000193 seconds\n",
      "96 / 196 1721997901.032326\n",
      "Optimizer step time: 0.000266 seconds\n",
      "97 / 196 1721997902.2002518\n",
      "Optimizer step time: 0.000428 seconds\n",
      "98 / 196 1721997903.436181\n",
      "Optimizer step time: 0.000305 seconds\n",
      "99 / 196 1721997904.6121268\n",
      "Optimizer step time: 0.001030 seconds\n",
      "100 / 196 1721997905.8527637\n",
      "Optimizer step time: 0.000485 seconds\n",
      "101 / 196 1721997907.0485494\n",
      "Optimizer step time: 0.000257 seconds\n",
      "102 / 196 1721997908.2018292\n",
      "Optimizer step time: 0.000266 seconds\n",
      "103 / 196 1721997909.3296216\n",
      "Optimizer step time: 0.000385 seconds\n",
      "104 / 196 1721997910.625661\n",
      "Optimizer step time: 0.000385 seconds\n",
      "105 / 196 1721997911.8478308\n",
      "Optimizer step time: 0.000279 seconds\n",
      "106 / 196 1721997913.067206\n",
      "Optimizer step time: 0.000265 seconds\n",
      "107 / 196 1721997914.196633\n",
      "Optimizer step time: 0.000250 seconds\n",
      "108 / 196 1721997915.354407\n",
      "Optimizer step time: 0.000229 seconds\n",
      "109 / 196 1721997916.5241199\n",
      "Optimizer step time: 0.000378 seconds\n",
      "110 / 196 1721997917.7041676\n",
      "Optimizer step time: 0.000958 seconds\n",
      "111 / 196 1721997918.8186438\n",
      "Optimizer step time: 0.000240 seconds\n",
      "112 / 196 1721997919.9252706\n",
      "Optimizer step time: 0.000357 seconds\n",
      "113 / 196 1721997921.0741737\n",
      "Optimizer step time: 0.000209 seconds\n",
      "114 / 196 1721997922.3267272\n",
      "Optimizer step time: 0.000269 seconds\n",
      "115 / 196 1721997923.5503864\n",
      "Optimizer step time: 0.000196 seconds\n",
      "116 / 196 1721997924.6663973\n",
      "Optimizer step time: 0.000255 seconds\n",
      "117 / 196 1721997925.850985\n",
      "Optimizer step time: 0.000564 seconds\n",
      "118 / 196 1721997927.055326\n",
      "Optimizer step time: 0.000202 seconds\n",
      "119 / 196 1721997928.3005157\n",
      "Optimizer step time: 0.000372 seconds\n",
      "120 / 196 1721997929.6116061\n",
      "Optimizer step time: 0.000306 seconds\n",
      "121 / 196 1721997930.7861273\n",
      "Optimizer step time: 0.000241 seconds\n",
      "122 / 196 1721997932.052035\n",
      "Optimizer step time: 0.000260 seconds\n",
      "123 / 196 1721997933.2272792\n",
      "Optimizer step time: 0.000403 seconds\n",
      "124 / 196 1721997934.4896846\n",
      "Optimizer step time: 0.000308 seconds\n",
      "125 / 196 1721997935.6160696\n",
      "Optimizer step time: 0.000226 seconds\n",
      "126 / 196 1721997936.7303302\n",
      "Optimizer step time: 0.000209 seconds\n",
      "127 / 196 1721997937.8758798\n",
      "Optimizer step time: 0.000214 seconds\n",
      "128 / 196 1721997939.0803666\n",
      "Optimizer step time: 0.000224 seconds\n",
      "129 / 196 1721997940.2349088\n",
      "Optimizer step time: 0.000340 seconds\n",
      "130 / 196 1721997941.385215\n",
      "Optimizer step time: 0.000239 seconds\n",
      "131 / 196 1721997942.5039597\n",
      "Optimizer step time: 0.000431 seconds\n",
      "132 / 196 1721997943.736696\n",
      "Optimizer step time: 0.000288 seconds\n",
      "133 / 196 1721997944.9190574\n",
      "Optimizer step time: 0.000351 seconds\n",
      "134 / 196 1721997946.1681898\n",
      "Optimizer step time: 0.001740 seconds\n",
      "135 / 196 1721997947.4256425\n",
      "Optimizer step time: 0.000290 seconds\n",
      "136 / 196 1721997948.4946592\n",
      "Optimizer step time: 0.000235 seconds\n",
      "137 / 196 1721997949.752124\n",
      "Optimizer step time: 0.000245 seconds\n",
      "138 / 196 1721997950.9092307\n",
      "Optimizer step time: 0.004994 seconds\n",
      "139 / 196 1721997952.024192\n",
      "Optimizer step time: 0.000416 seconds\n",
      "140 / 196 1721997953.121853\n",
      "Optimizer step time: 0.000291 seconds\n",
      "141 / 196 1721997954.1687186\n",
      "Optimizer step time: 0.000215 seconds\n",
      "142 / 196 1721997955.1143394\n",
      "Optimizer step time: 0.000206 seconds\n",
      "143 / 196 1721997956.1060734\n",
      "Optimizer step time: 0.000258 seconds\n",
      "144 / 196 1721997957.125317\n",
      "Optimizer step time: 0.000255 seconds\n",
      "145 / 196 1721997958.2354705\n",
      "Optimizer step time: 0.000229 seconds\n",
      "146 / 196 1721997959.2871752\n",
      "Optimizer step time: 0.001225 seconds\n",
      "147 / 196 1721997960.2823188\n",
      "Optimizer step time: 0.000259 seconds\n",
      "148 / 196 1721997961.264296\n",
      "Optimizer step time: 0.000242 seconds\n",
      "149 / 196 1721997962.2745225\n",
      "Optimizer step time: 0.000284 seconds\n",
      "150 / 196 1721997963.0871987\n",
      "Optimizer step time: 0.000318 seconds\n",
      "151 / 196 1721997964.1193452\n",
      "Optimizer step time: 0.000416 seconds\n",
      "152 / 196 1721997965.096522\n",
      "Optimizer step time: 0.000224 seconds\n",
      "153 / 196 1721997966.1129327\n",
      "Optimizer step time: 0.000265 seconds\n",
      "154 / 196 1721997967.1331055\n",
      "Optimizer step time: 0.000256 seconds\n",
      "155 / 196 1721997968.2213202\n",
      "Optimizer step time: 0.000277 seconds\n",
      "156 / 196 1721997969.2244241\n",
      "Optimizer step time: 0.000242 seconds\n",
      "157 / 196 1721997970.2908185\n",
      "Optimizer step time: 0.000213 seconds\n",
      "158 / 196 1721997971.2015023\n",
      "Optimizer step time: 0.000208 seconds\n",
      "159 / 196 1721997972.0208282\n",
      "Optimizer step time: 0.000201 seconds\n",
      "160 / 196 1721997972.9481277\n",
      "Optimizer step time: 0.000250 seconds\n",
      "161 / 196 1721997973.9444964\n",
      "Optimizer step time: 0.000244 seconds\n",
      "162 / 196 1721997974.9763727\n",
      "Optimizer step time: 0.000216 seconds\n",
      "163 / 196 1721997976.0391026\n",
      "Optimizer step time: 0.000245 seconds\n",
      "164 / 196 1721997977.0587294\n",
      "Optimizer step time: 0.000224 seconds\n",
      "165 / 196 1721997977.9737296\n",
      "Optimizer step time: 0.000236 seconds\n",
      "166 / 196 1721997978.9374013\n",
      "Optimizer step time: 0.000452 seconds\n",
      "167 / 196 1721997979.9122026\n",
      "Optimizer step time: 0.000219 seconds\n",
      "168 / 196 1721997980.7342267\n",
      "Optimizer step time: 0.000260 seconds\n",
      "169 / 196 1721997981.6721852\n",
      "Optimizer step time: 0.000238 seconds\n",
      "170 / 196 1721997982.6518896\n",
      "Optimizer step time: 0.000222 seconds\n",
      "171 / 196 1721997983.635487\n",
      "Optimizer step time: 0.000217 seconds\n",
      "172 / 196 1721997984.6704755\n",
      "Optimizer step time: 0.000319 seconds\n",
      "173 / 196 1721997985.585798\n",
      "Optimizer step time: 0.000242 seconds\n",
      "174 / 196 1721997986.5913055\n",
      "Optimizer step time: 0.000230 seconds\n",
      "175 / 196 1721997987.553057\n",
      "Optimizer step time: 0.000200 seconds\n",
      "176 / 196 1721997988.5596097\n",
      "Optimizer step time: 0.000252 seconds\n",
      "177 / 196 1721997989.5564466\n",
      "Optimizer step time: 0.000243 seconds\n",
      "178 / 196 1721997990.421158\n",
      "Optimizer step time: 0.000386 seconds\n",
      "179 / 196 1721997991.4417012\n",
      "Optimizer step time: 0.000349 seconds\n",
      "180 / 196 1721997992.46202\n",
      "Optimizer step time: 0.000212 seconds\n",
      "181 / 196 1721997993.412918\n",
      "Optimizer step time: 0.000206 seconds\n",
      "182 / 196 1721997994.3739598\n",
      "Optimizer step time: 0.000218 seconds\n",
      "183 / 196 1721997995.3509889\n",
      "Optimizer step time: 0.000481 seconds\n",
      "184 / 196 1721997996.355054\n",
      "Optimizer step time: 0.000313 seconds\n",
      "185 / 196 1721997997.3313465\n",
      "Optimizer step time: 0.000184 seconds\n",
      "186 / 196 1721997998.3093517\n",
      "Optimizer step time: 0.000239 seconds\n",
      "187 / 196 1721997999.1785498\n",
      "Optimizer step time: 0.000248 seconds\n",
      "188 / 196 1721998000.1093183\n",
      "Optimizer step time: 0.000203 seconds\n",
      "189 / 196 1721998001.1227403\n",
      "Optimizer step time: 0.000307 seconds\n",
      "190 / 196 1721998002.1567078\n",
      "Optimizer step time: 0.000261 seconds\n",
      "191 / 196 1721998003.1459336\n",
      "Optimizer step time: 0.000229 seconds\n",
      "192 / 196 1721998004.1036355\n",
      "Optimizer step time: 0.000273 seconds\n",
      "193 / 196 1721998005.0429237\n",
      "Optimizer step time: 0.000204 seconds\n",
      "194 / 196 1721998006.102558\n",
      "Optimizer step time: 0.000262 seconds\n",
      "195 / 196 1721998007.136068\n",
      "Optimizer step time: 0.000278 seconds\n",
      "Test loss: 35.069, Val Accuracy: 22.77%\n",
      "Epoch 2\n",
      "0 / 196 1721998049.167242\n",
      "Optimizer step time: 0.000406 seconds\n",
      "1 / 196 1721998050.3776817\n",
      "Optimizer step time: 0.000407 seconds\n",
      "2 / 196 1721998051.4802268\n",
      "Optimizer step time: 0.000253 seconds\n",
      "3 / 196 1721998052.4340262\n",
      "Optimizer step time: 0.000244 seconds\n",
      "4 / 196 1721998053.3353865\n",
      "Optimizer step time: 0.000343 seconds\n",
      "5 / 196 1721998054.340092\n",
      "Optimizer step time: 0.000256 seconds\n",
      "6 / 196 1721998055.376068\n",
      "Optimizer step time: 0.000256 seconds\n",
      "7 / 196 1721998056.3245523\n",
      "Optimizer step time: 0.000230 seconds\n",
      "8 / 196 1721998057.2149398\n",
      "Optimizer step time: 0.000322 seconds\n",
      "9 / 196 1721998058.2792494\n",
      "Optimizer step time: 0.000233 seconds\n",
      "10 / 196 1721998059.3112736\n",
      "Optimizer step time: 0.000205 seconds\n",
      "11 / 196 1721998060.3507903\n",
      "Optimizer step time: 0.000375 seconds\n",
      "12 / 196 1721998061.3527756\n",
      "Optimizer step time: 0.000260 seconds\n",
      "13 / 196 1721998062.317003\n",
      "Optimizer step time: 0.000196 seconds\n",
      "14 / 196 1721998063.3712397\n",
      "Optimizer step time: 0.000242 seconds\n",
      "15 / 196 1721998064.2944937\n",
      "Optimizer step time: 0.000202 seconds\n",
      "16 / 196 1721998065.3345058\n",
      "Optimizer step time: 0.000239 seconds\n",
      "17 / 196 1721998066.1937878\n",
      "Optimizer step time: 0.000242 seconds\n",
      "18 / 196 1721998067.1843147\n",
      "Optimizer step time: 0.000245 seconds\n",
      "19 / 196 1721998068.095611\n",
      "Optimizer step time: 0.000237 seconds\n",
      "20 / 196 1721998069.1386096\n",
      "Optimizer step time: 0.000213 seconds\n",
      "21 / 196 1721998070.1884842\n",
      "Optimizer step time: 0.000522 seconds\n",
      "22 / 196 1721998071.2762387\n",
      "Optimizer step time: 0.000222 seconds\n",
      "23 / 196 1721998072.2039232\n",
      "Optimizer step time: 0.000209 seconds\n",
      "24 / 196 1721998073.2601702\n",
      "Optimizer step time: 0.000376 seconds\n",
      "25 / 196 1721998074.2176743\n",
      "Optimizer step time: 0.000250 seconds\n",
      "26 / 196 1721998075.1103754\n",
      "Optimizer step time: 0.000255 seconds\n",
      "27 / 196 1721998075.9907608\n",
      "Optimizer step time: 0.000260 seconds\n",
      "28 / 196 1721998077.0665994\n",
      "Optimizer step time: 0.000244 seconds\n",
      "29 / 196 1721998078.150718\n",
      "Optimizer step time: 0.000245 seconds\n",
      "30 / 196 1721998079.1288626\n",
      "Optimizer step time: 0.000200 seconds\n",
      "31 / 196 1721998080.0826929\n",
      "Optimizer step time: 0.000207 seconds\n",
      "32 / 196 1721998081.047277\n",
      "Optimizer step time: 0.000230 seconds\n",
      "33 / 196 1721998082.0791717\n",
      "Optimizer step time: 0.000541 seconds\n",
      "34 / 196 1721998083.1308155\n",
      "Optimizer step time: 0.000318 seconds\n",
      "35 / 196 1721998084.0757153\n",
      "Optimizer step time: 0.000237 seconds\n",
      "36 / 196 1721998085.112149\n",
      "Optimizer step time: 0.000255 seconds\n",
      "37 / 196 1721998086.096186\n",
      "Optimizer step time: 0.000247 seconds\n",
      "38 / 196 1721998087.1519403\n",
      "Optimizer step time: 0.000341 seconds\n",
      "39 / 196 1721998088.15353\n",
      "Optimizer step time: 0.000258 seconds\n",
      "40 / 196 1721998089.22145\n",
      "Optimizer step time: 0.000299 seconds\n",
      "41 / 196 1721998090.256812\n",
      "Optimizer step time: 0.000216 seconds\n",
      "42 / 196 1721998091.2802458\n",
      "Optimizer step time: 0.000251 seconds\n",
      "43 / 196 1721998092.3880975\n",
      "Optimizer step time: 0.000265 seconds\n",
      "44 / 196 1721998093.4225366\n",
      "Optimizer step time: 0.000246 seconds\n",
      "45 / 196 1721998094.524272\n",
      "Optimizer step time: 0.000281 seconds\n",
      "46 / 196 1721998095.6965544\n",
      "Optimizer step time: 0.000249 seconds\n",
      "47 / 196 1721998096.8062162\n",
      "Optimizer step time: 0.000410 seconds\n",
      "48 / 196 1721998098.0001383\n",
      "Optimizer step time: 0.000248 seconds\n",
      "49 / 196 1721998099.1629195\n",
      "Optimizer step time: 0.000309 seconds\n",
      "50 / 196 1721998100.4401195\n",
      "Optimizer step time: 0.000269 seconds\n",
      "51 / 196 1721998101.5999055\n",
      "Optimizer step time: 0.000382 seconds\n",
      "52 / 196 1721998102.8110447\n",
      "Optimizer step time: 0.000205 seconds\n",
      "53 / 196 1721998103.9681754\n",
      "Optimizer step time: 0.000287 seconds\n",
      "54 / 196 1721998105.2079341\n",
      "Optimizer step time: 0.000407 seconds\n",
      "55 / 196 1721998106.387072\n",
      "Optimizer step time: 0.000357 seconds\n",
      "56 / 196 1721998107.5159414\n",
      "Optimizer step time: 0.000193 seconds\n",
      "57 / 196 1721998108.7268767\n",
      "Optimizer step time: 0.000482 seconds\n",
      "58 / 196 1721998109.8884118\n",
      "Optimizer step time: 0.000202 seconds\n",
      "59 / 196 1721998111.0551047\n",
      "Optimizer step time: 0.000227 seconds\n",
      "60 / 196 1721998112.1400504\n",
      "Optimizer step time: 0.000211 seconds\n",
      "61 / 196 1721998113.1533794\n",
      "Optimizer step time: 0.000384 seconds\n",
      "62 / 196 1721998114.0797086\n",
      "Optimizer step time: 0.000248 seconds\n",
      "63 / 196 1721998115.0257223\n",
      "Optimizer step time: 0.000929 seconds\n",
      "64 / 196 1721998115.8719692\n",
      "Optimizer step time: 0.000253 seconds\n",
      "65 / 196 1721998116.9293678\n",
      "Optimizer step time: 0.000221 seconds\n",
      "66 / 196 1721998117.9494874\n",
      "Optimizer step time: 0.000245 seconds\n",
      "67 / 196 1721998119.042754\n",
      "Optimizer step time: 0.000224 seconds\n",
      "68 / 196 1721998120.1343355\n",
      "Optimizer step time: 0.000297 seconds\n",
      "69 / 196 1721998121.1414213\n",
      "Optimizer step time: 0.000752 seconds\n",
      "70 / 196 1721998122.0041215\n",
      "Optimizer step time: 0.000244 seconds\n",
      "71 / 196 1721998122.9795089\n",
      "Optimizer step time: 0.000201 seconds\n",
      "72 / 196 1721998123.9492495\n",
      "Optimizer step time: 0.000425 seconds\n",
      "73 / 196 1721998124.874203\n",
      "Optimizer step time: 0.000239 seconds\n",
      "74 / 196 1721998125.8335547\n",
      "Optimizer step time: 0.000456 seconds\n",
      "75 / 196 1721998126.9579494\n",
      "Optimizer step time: 0.001219 seconds\n",
      "76 / 196 1721998127.9455388\n",
      "Optimizer step time: 0.000224 seconds\n",
      "77 / 196 1721998128.9647748\n",
      "Optimizer step time: 0.000245 seconds\n",
      "78 / 196 1721998129.9470718\n",
      "Optimizer step time: 0.000225 seconds\n",
      "79 / 196 1721998131.0348897\n",
      "Optimizer step time: 0.000251 seconds\n",
      "80 / 196 1721998132.1296098\n",
      "Optimizer step time: 0.000242 seconds\n",
      "81 / 196 1721998133.014206\n",
      "Optimizer step time: 0.000234 seconds\n",
      "82 / 196 1721998134.027536\n",
      "Optimizer step time: 0.000252 seconds\n",
      "83 / 196 1721998135.0480952\n",
      "Optimizer step time: 0.000458 seconds\n",
      "84 / 196 1721998136.1695487\n",
      "Optimizer step time: 0.000295 seconds\n",
      "85 / 196 1721998137.223847\n",
      "Optimizer step time: 0.000350 seconds\n",
      "86 / 196 1721998138.324109\n",
      "Optimizer step time: 0.000221 seconds\n",
      "87 / 196 1721998139.3334262\n",
      "Optimizer step time: 0.000218 seconds\n",
      "88 / 196 1721998140.437484\n",
      "Optimizer step time: 0.000281 seconds\n",
      "89 / 196 1721998141.557045\n",
      "Optimizer step time: 0.000536 seconds\n",
      "90 / 196 1721998142.576947\n",
      "Optimizer step time: 0.000221 seconds\n",
      "91 / 196 1721998143.6734436\n",
      "Optimizer step time: 0.000192 seconds\n",
      "92 / 196 1721998144.7201416\n",
      "Optimizer step time: 0.000233 seconds\n",
      "93 / 196 1721998145.6777015\n",
      "Optimizer step time: 0.000219 seconds\n",
      "94 / 196 1721998146.7307608\n",
      "Optimizer step time: 0.000223 seconds\n",
      "95 / 196 1721998147.798386\n",
      "Optimizer step time: 0.000196 seconds\n",
      "96 / 196 1721998148.7281485\n",
      "Optimizer step time: 0.000212 seconds\n",
      "97 / 196 1721998149.718182\n",
      "Optimizer step time: 0.000217 seconds\n",
      "98 / 196 1721998150.6906447\n",
      "Optimizer step time: 0.000429 seconds\n",
      "99 / 196 1721998151.647276\n",
      "Optimizer step time: 0.000260 seconds\n",
      "100 / 196 1721998152.7323346\n",
      "Optimizer step time: 0.000273 seconds\n",
      "101 / 196 1721998153.7510197\n",
      "Optimizer step time: 0.000207 seconds\n",
      "102 / 196 1721998154.6093106\n",
      "Optimizer step time: 0.000223 seconds\n",
      "103 / 196 1721998155.540727\n",
      "Optimizer step time: 0.000202 seconds\n",
      "104 / 196 1721998156.4387977\n",
      "Optimizer step time: 0.000202 seconds\n",
      "105 / 196 1721998157.5127213\n",
      "Optimizer step time: 0.000215 seconds\n",
      "106 / 196 1721998158.509721\n",
      "Optimizer step time: 0.000206 seconds\n",
      "107 / 196 1721998159.5713565\n",
      "Optimizer step time: 0.000227 seconds\n",
      "108 / 196 1721998160.573203\n",
      "Optimizer step time: 0.000267 seconds\n",
      "109 / 196 1721998161.6173828\n",
      "Optimizer step time: 0.000237 seconds\n",
      "110 / 196 1721998162.648103\n",
      "Optimizer step time: 0.012091 seconds\n",
      "111 / 196 1721998163.5841236\n",
      "Optimizer step time: 0.000452 seconds\n",
      "112 / 196 1721998164.6107082\n",
      "Optimizer step time: 0.000389 seconds\n",
      "113 / 196 1721998165.5407865\n",
      "Optimizer step time: 0.000277 seconds\n",
      "114 / 196 1721998166.495972\n",
      "Optimizer step time: 0.000194 seconds\n",
      "115 / 196 1721998167.5455794\n",
      "Optimizer step time: 0.000277 seconds\n",
      "116 / 196 1721998168.5790696\n",
      "Optimizer step time: 0.000228 seconds\n",
      "117 / 196 1721998169.531095\n",
      "Optimizer step time: 0.000208 seconds\n",
      "118 / 196 1721998170.5280938\n",
      "Optimizer step time: 0.000233 seconds\n",
      "119 / 196 1721998171.5467377\n",
      "Optimizer step time: 0.000216 seconds\n",
      "120 / 196 1721998172.512107\n",
      "Optimizer step time: 0.000241 seconds\n",
      "121 / 196 1721998173.498018\n",
      "Optimizer step time: 0.000264 seconds\n",
      "122 / 196 1721998174.4447927\n",
      "Optimizer step time: 0.000205 seconds\n",
      "123 / 196 1721998175.452279\n",
      "Optimizer step time: 0.000256 seconds\n",
      "124 / 196 1721998176.4244027\n",
      "Optimizer step time: 0.000320 seconds\n",
      "125 / 196 1721998177.5107155\n",
      "Optimizer step time: 0.000239 seconds\n",
      "126 / 196 1721998178.452693\n",
      "Optimizer step time: 0.000264 seconds\n",
      "127 / 196 1721998179.3662992\n",
      "Optimizer step time: 0.000225 seconds\n",
      "128 / 196 1721998180.3101609\n",
      "Optimizer step time: 0.000451 seconds\n",
      "129 / 196 1721998181.1891565\n",
      "Optimizer step time: 0.000255 seconds\n",
      "130 / 196 1721998182.1358159\n",
      "Optimizer step time: 0.000249 seconds\n",
      "131 / 196 1721998183.056113\n",
      "Optimizer step time: 0.000283 seconds\n",
      "132 / 196 1721998184.104101\n",
      "Optimizer step time: 0.000258 seconds\n",
      "133 / 196 1721998184.9991665\n",
      "Optimizer step time: 0.000247 seconds\n",
      "134 / 196 1721998186.0001109\n",
      "Optimizer step time: 0.000212 seconds\n",
      "135 / 196 1721998187.0564382\n",
      "Optimizer step time: 0.000353 seconds\n",
      "136 / 196 1721998187.9280567\n",
      "Optimizer step time: 0.000200 seconds\n",
      "137 / 196 1721998188.9051669\n",
      "Optimizer step time: 0.000241 seconds\n",
      "138 / 196 1721998189.904823\n",
      "Optimizer step time: 0.000196 seconds\n",
      "139 / 196 1721998190.909954\n",
      "Optimizer step time: 0.000313 seconds\n",
      "140 / 196 1721998191.8456845\n",
      "Optimizer step time: 0.000267 seconds\n",
      "141 / 196 1721998192.9241922\n",
      "Optimizer step time: 0.000220 seconds\n",
      "142 / 196 1721998193.885302\n",
      "Optimizer step time: 0.000813 seconds\n",
      "143 / 196 1721998194.9255865\n",
      "Optimizer step time: 0.000379 seconds\n",
      "144 / 196 1721998195.985736\n",
      "Optimizer step time: 0.000208 seconds\n",
      "145 / 196 1721998197.0321481\n",
      "Optimizer step time: 0.000204 seconds\n",
      "146 / 196 1721998198.0905309\n",
      "Optimizer step time: 0.000396 seconds\n",
      "147 / 196 1721998198.962141\n",
      "Optimizer step time: 0.000258 seconds\n",
      "148 / 196 1721998199.8744109\n",
      "Optimizer step time: 0.000210 seconds\n",
      "149 / 196 1721998200.9108832\n",
      "Optimizer step time: 0.000240 seconds\n",
      "150 / 196 1721998201.8364487\n",
      "Optimizer step time: 0.000195 seconds\n",
      "151 / 196 1721998202.7658517\n",
      "Optimizer step time: 0.000247 seconds\n",
      "152 / 196 1721998203.8693688\n",
      "Optimizer step time: 0.000242 seconds\n",
      "153 / 196 1721998204.8057308\n",
      "Optimizer step time: 0.000240 seconds\n",
      "154 / 196 1721998205.7821994\n",
      "Optimizer step time: 0.000224 seconds\n",
      "155 / 196 1721998206.7241075\n",
      "Optimizer step time: 0.000374 seconds\n",
      "156 / 196 1721998207.7123437\n",
      "Optimizer step time: 0.000229 seconds\n",
      "157 / 196 1721998208.6698625\n",
      "Optimizer step time: 0.000253 seconds\n",
      "158 / 196 1721998209.535272\n",
      "Optimizer step time: 0.000266 seconds\n",
      "159 / 196 1721998210.420122\n",
      "Optimizer step time: 0.000211 seconds\n",
      "160 / 196 1721998211.2814105\n",
      "Optimizer step time: 0.000419 seconds\n",
      "161 / 196 1721998212.1964123\n",
      "Optimizer step time: 0.000199 seconds\n",
      "162 / 196 1721998213.3001394\n",
      "Optimizer step time: 0.000234 seconds\n",
      "163 / 196 1721998214.1866565\n",
      "Optimizer step time: 0.000268 seconds\n",
      "164 / 196 1721998215.3121095\n",
      "Optimizer step time: 0.000282 seconds\n",
      "165 / 196 1721998216.293627\n",
      "Optimizer step time: 0.000223 seconds\n",
      "166 / 196 1721998217.43839\n",
      "Optimizer step time: 0.000390 seconds\n",
      "167 / 196 1721998218.5712044\n",
      "Optimizer step time: 0.000215 seconds\n",
      "168 / 196 1721998219.573461\n",
      "Optimizer step time: 0.000202 seconds\n",
      "169 / 196 1721998220.519133\n",
      "Optimizer step time: 0.000274 seconds\n",
      "170 / 196 1721998221.4365158\n",
      "Optimizer step time: 0.000257 seconds\n",
      "171 / 196 1721998222.732142\n",
      "Optimizer step time: 0.000409 seconds\n",
      "172 / 196 1721998223.8096566\n",
      "Optimizer step time: 0.000242 seconds\n",
      "173 / 196 1721998224.831014\n",
      "Optimizer step time: 0.000311 seconds\n",
      "174 / 196 1721998225.7942166\n",
      "Optimizer step time: 0.000267 seconds\n",
      "175 / 196 1721998226.8874223\n",
      "Optimizer step time: 0.000231 seconds\n",
      "176 / 196 1721998227.9177773\n",
      "Optimizer step time: 0.000364 seconds\n",
      "177 / 196 1721998228.9243383\n",
      "Optimizer step time: 0.000383 seconds\n",
      "178 / 196 1721998229.9413116\n",
      "Optimizer step time: 0.000293 seconds\n",
      "179 / 196 1721998231.015395\n",
      "Optimizer step time: 0.000442 seconds\n",
      "180 / 196 1721998231.94401\n",
      "Optimizer step time: 0.000246 seconds\n",
      "181 / 196 1721998232.8682756\n",
      "Optimizer step time: 0.000202 seconds\n",
      "182 / 196 1721998233.8934076\n",
      "Optimizer step time: 0.000402 seconds\n",
      "183 / 196 1721998234.9602711\n",
      "Optimizer step time: 0.000260 seconds\n",
      "184 / 196 1721998236.0115747\n",
      "Optimizer step time: 0.000319 seconds\n",
      "185 / 196 1721998236.9881296\n",
      "Optimizer step time: 0.000398 seconds\n",
      "186 / 196 1721998237.906442\n",
      "Optimizer step time: 0.000247 seconds\n",
      "187 / 196 1721998239.040663\n",
      "Optimizer step time: 0.007840 seconds\n",
      "188 / 196 1721998240.073398\n",
      "Optimizer step time: 0.000243 seconds\n",
      "189 / 196 1721998241.100626\n",
      "Optimizer step time: 0.000225 seconds\n",
      "190 / 196 1721998242.1153638\n",
      "Optimizer step time: 0.000222 seconds\n",
      "191 / 196 1721998243.2234342\n",
      "Optimizer step time: 0.000202 seconds\n",
      "192 / 196 1721998244.1775208\n",
      "Optimizer step time: 0.000436 seconds\n",
      "193 / 196 1721998245.1001925\n",
      "Optimizer step time: 0.000262 seconds\n",
      "194 / 196 1721998246.109093\n",
      "Optimizer step time: 0.000419 seconds\n",
      "195 / 196 1721998247.105508\n",
      "Optimizer step time: 0.000228 seconds\n",
      "Test loss: 18.643, Val Accuracy: 27.07%\n",
      "Epoch 3\n",
      "0 / 196 1721998288.1602635\n",
      "Optimizer step time: 0.000414 seconds\n",
      "1 / 196 1721998289.233681\n",
      "Optimizer step time: 0.000291 seconds\n",
      "2 / 196 1721998290.1907918\n",
      "Optimizer step time: 0.000211 seconds\n",
      "3 / 196 1721998291.2941756\n",
      "Optimizer step time: 0.000921 seconds\n",
      "4 / 196 1721998292.2172534\n",
      "Optimizer step time: 0.000275 seconds\n",
      "5 / 196 1721998293.2204103\n",
      "Optimizer step time: 0.000237 seconds\n",
      "6 / 196 1721998294.0594249\n",
      "Optimizer step time: 0.000242 seconds\n",
      "7 / 196 1721998295.194711\n",
      "Optimizer step time: 0.000234 seconds\n",
      "8 / 196 1721998296.0635953\n",
      "Optimizer step time: 0.000950 seconds\n",
      "9 / 196 1721998297.0494316\n",
      "Optimizer step time: 0.000274 seconds\n",
      "10 / 196 1721998298.0322392\n",
      "Optimizer step time: 0.000914 seconds\n",
      "11 / 196 1721998299.0521507\n",
      "Optimizer step time: 0.000246 seconds\n",
      "12 / 196 1721998300.022345\n",
      "Optimizer step time: 0.000286 seconds\n",
      "13 / 196 1721998300.9970765\n",
      "Optimizer step time: 0.000209 seconds\n",
      "14 / 196 1721998301.9582677\n",
      "Optimizer step time: 0.000193 seconds\n",
      "15 / 196 1721998302.8811138\n",
      "Optimizer step time: 0.000362 seconds\n",
      "16 / 196 1721998303.9974172\n",
      "Optimizer step time: 0.000272 seconds\n",
      "17 / 196 1721998304.9367328\n",
      "Optimizer step time: 0.000245 seconds\n",
      "18 / 196 1721998305.9715855\n",
      "Optimizer step time: 0.000257 seconds\n",
      "19 / 196 1721998306.9612362\n",
      "Optimizer step time: 0.000662 seconds\n",
      "20 / 196 1721998307.9404933\n",
      "Optimizer step time: 0.000220 seconds\n",
      "21 / 196 1721998308.922814\n",
      "Optimizer step time: 0.000211 seconds\n",
      "22 / 196 1721998309.8332198\n",
      "Optimizer step time: 0.000215 seconds\n",
      "23 / 196 1721998310.8674605\n",
      "Optimizer step time: 0.000224 seconds\n",
      "24 / 196 1721998311.6936903\n",
      "Optimizer step time: 0.000261 seconds\n",
      "25 / 196 1721998312.751484\n",
      "Optimizer step time: 0.000324 seconds\n",
      "26 / 196 1721998313.8421235\n",
      "Optimizer step time: 0.000249 seconds\n",
      "27 / 196 1721998314.925688\n",
      "Optimizer step time: 0.000193 seconds\n",
      "28 / 196 1721998315.856437\n",
      "Optimizer step time: 0.000312 seconds\n",
      "29 / 196 1721998316.9643657\n",
      "Optimizer step time: 0.000248 seconds\n",
      "30 / 196 1721998318.0316021\n",
      "Optimizer step time: 0.000408 seconds\n",
      "31 / 196 1721998318.9855103\n",
      "Optimizer step time: 0.000237 seconds\n",
      "32 / 196 1721998319.9644814\n",
      "Optimizer step time: 0.000329 seconds\n",
      "33 / 196 1721998320.9494364\n",
      "Optimizer step time: 0.000193 seconds\n",
      "34 / 196 1721998322.0331926\n",
      "Optimizer step time: 0.000204 seconds\n",
      "35 / 196 1721998322.9775648\n",
      "Optimizer step time: 0.000267 seconds\n",
      "36 / 196 1721998324.0788636\n",
      "Optimizer step time: 0.000211 seconds\n",
      "37 / 196 1721998325.1050634\n",
      "Optimizer step time: 0.000285 seconds\n",
      "38 / 196 1721998326.070902\n",
      "Optimizer step time: 0.000343 seconds\n",
      "39 / 196 1721998327.0481696\n",
      "Optimizer step time: 0.000307 seconds\n",
      "40 / 196 1721998328.143173\n",
      "Optimizer step time: 0.000215 seconds\n",
      "41 / 196 1721998329.2123241\n",
      "Optimizer step time: 0.000219 seconds\n",
      "42 / 196 1721998330.2227924\n",
      "Optimizer step time: 0.000194 seconds\n",
      "43 / 196 1721998331.1994402\n",
      "Optimizer step time: 0.000226 seconds\n",
      "44 / 196 1721998332.323719\n",
      "Optimizer step time: 0.000221 seconds\n",
      "45 / 196 1721998333.4382744\n",
      "Optimizer step time: 0.000470 seconds\n",
      "46 / 196 1721998334.5149965\n",
      "Optimizer step time: 0.000284 seconds\n",
      "47 / 196 1721998335.678311\n",
      "Optimizer step time: 0.000239 seconds\n",
      "48 / 196 1721998336.8416796\n",
      "Optimizer step time: 0.000445 seconds\n",
      "49 / 196 1721998338.013985\n",
      "Optimizer step time: 0.000295 seconds\n",
      "50 / 196 1721998339.1269486\n",
      "Optimizer step time: 0.000609 seconds\n",
      "51 / 196 1721998340.1833012\n",
      "Optimizer step time: 0.000305 seconds\n",
      "52 / 196 1721998341.4622533\n",
      "Optimizer step time: 0.000202 seconds\n",
      "53 / 196 1721998342.6134956\n",
      "Optimizer step time: 0.000388 seconds\n",
      "54 / 196 1721998343.7703524\n",
      "Optimizer step time: 0.000196 seconds\n",
      "55 / 196 1721998344.9868982\n",
      "Optimizer step time: 0.000219 seconds\n",
      "56 / 196 1721998346.1412299\n",
      "Optimizer step time: 0.000324 seconds\n",
      "57 / 196 1721998347.2649372\n",
      "Optimizer step time: 0.000257 seconds\n",
      "58 / 196 1721998348.3975224\n",
      "Optimizer step time: 0.000266 seconds\n",
      "59 / 196 1721998349.5428672\n",
      "Optimizer step time: 0.001762 seconds\n",
      "60 / 196 1721998350.8058703\n",
      "Optimizer step time: 0.000283 seconds\n",
      "61 / 196 1721998351.948123\n",
      "Optimizer step time: 0.000227 seconds\n",
      "62 / 196 1721998353.108703\n",
      "Optimizer step time: 0.000288 seconds\n",
      "63 / 196 1721998354.2962315\n",
      "Optimizer step time: 0.000304 seconds\n",
      "64 / 196 1721998355.5028563\n",
      "Optimizer step time: 0.000238 seconds\n",
      "65 / 196 1721998356.7668462\n",
      "Optimizer step time: 0.000211 seconds\n",
      "66 / 196 1721998358.0225675\n",
      "Optimizer step time: 0.000928 seconds\n",
      "67 / 196 1721998359.1814165\n",
      "Optimizer step time: 0.000246 seconds\n",
      "68 / 196 1721998360.2841442\n",
      "Optimizer step time: 0.000264 seconds\n",
      "69 / 196 1721998361.3336918\n",
      "Optimizer step time: 0.000257 seconds\n",
      "70 / 196 1721998362.5515778\n",
      "Optimizer step time: 0.000248 seconds\n",
      "71 / 196 1721998363.6491733\n",
      "Optimizer step time: 0.000213 seconds\n",
      "72 / 196 1721998364.8001072\n",
      "Optimizer step time: 0.000190 seconds\n",
      "73 / 196 1721998365.9255197\n",
      "Optimizer step time: 0.000395 seconds\n",
      "74 / 196 1721998367.2091746\n",
      "Optimizer step time: 0.000321 seconds\n",
      "75 / 196 1721998368.3611164\n",
      "Optimizer step time: 0.000214 seconds\n",
      "76 / 196 1721998369.5472455\n",
      "Optimizer step time: 0.000260 seconds\n",
      "77 / 196 1721998370.6682067\n",
      "Optimizer step time: 0.000206 seconds\n",
      "78 / 196 1721998371.8881514\n",
      "Optimizer step time: 0.000315 seconds\n",
      "79 / 196 1721998373.112486\n",
      "Optimizer step time: 0.000279 seconds\n",
      "80 / 196 1721998374.413035\n",
      "Optimizer step time: 0.000272 seconds\n",
      "81 / 196 1721998375.5895715\n",
      "Optimizer step time: 0.000203 seconds\n",
      "82 / 196 1721998376.739116\n",
      "Optimizer step time: 0.000196 seconds\n",
      "83 / 196 1721998377.8646004\n",
      "Optimizer step time: 0.000348 seconds\n",
      "84 / 196 1721998379.0295174\n",
      "Optimizer step time: 0.000209 seconds\n",
      "85 / 196 1721998380.1373932\n",
      "Optimizer step time: 0.000426 seconds\n",
      "86 / 196 1721998381.3136528\n",
      "Optimizer step time: 0.000277 seconds\n",
      "87 / 196 1721998382.5136802\n",
      "Optimizer step time: 0.000279 seconds\n",
      "88 / 196 1721998383.8299062\n",
      "Optimizer step time: 0.000241 seconds\n",
      "89 / 196 1721998385.0692725\n",
      "Optimizer step time: 0.000348 seconds\n",
      "90 / 196 1721998386.300472\n",
      "Optimizer step time: 0.000561 seconds\n",
      "91 / 196 1721998387.509256\n",
      "Optimizer step time: 0.000199 seconds\n",
      "92 / 196 1721998388.636453\n",
      "Optimizer step time: 0.000242 seconds\n",
      "93 / 196 1721998389.8150342\n",
      "Optimizer step time: 0.000294 seconds\n",
      "94 / 196 1721998390.9529705\n",
      "Optimizer step time: 0.000283 seconds\n",
      "95 / 196 1721998392.1404922\n",
      "Optimizer step time: 0.000211 seconds\n",
      "96 / 196 1721998393.373663\n",
      "Optimizer step time: 0.000304 seconds\n",
      "97 / 196 1721998394.5772004\n",
      "Optimizer step time: 0.000253 seconds\n",
      "98 / 196 1721998395.7569613\n",
      "Optimizer step time: 0.000244 seconds\n",
      "99 / 196 1721998396.9581113\n",
      "Optimizer step time: 0.000233 seconds\n",
      "100 / 196 1721998398.100117\n",
      "Optimizer step time: 0.000231 seconds\n",
      "101 / 196 1721998399.2196527\n",
      "Optimizer step time: 0.000242 seconds\n",
      "102 / 196 1721998400.3031163\n",
      "Optimizer step time: 0.000229 seconds\n",
      "103 / 196 1721998401.6403663\n",
      "Optimizer step time: 0.000441 seconds\n",
      "104 / 196 1721998402.8281236\n",
      "Optimizer step time: 0.000254 seconds\n",
      "105 / 196 1721998403.9841607\n",
      "Optimizer step time: 0.000251 seconds\n",
      "106 / 196 1721998405.121444\n",
      "Optimizer step time: 0.000269 seconds\n",
      "107 / 196 1721998406.2572465\n",
      "Optimizer step time: 0.000264 seconds\n",
      "108 / 196 1721998407.3705332\n",
      "Optimizer step time: 0.000225 seconds\n",
      "109 / 196 1721998408.5250373\n",
      "Optimizer step time: 0.000315 seconds\n",
      "110 / 196 1721998409.7583592\n",
      "Optimizer step time: 0.000250 seconds\n",
      "111 / 196 1721998411.036628\n",
      "Optimizer step time: 0.000262 seconds\n",
      "112 / 196 1721998412.2840881\n",
      "Optimizer step time: 0.000250 seconds\n",
      "113 / 196 1721998413.3858423\n",
      "Optimizer step time: 0.000212 seconds\n",
      "114 / 196 1721998414.5077035\n",
      "Optimizer step time: 0.000397 seconds\n",
      "115 / 196 1721998415.7055373\n",
      "Optimizer step time: 0.000250 seconds\n",
      "116 / 196 1721998416.8696642\n",
      "Optimizer step time: 0.000308 seconds\n",
      "117 / 196 1721998418.0655704\n",
      "Optimizer step time: 0.000262 seconds\n",
      "118 / 196 1721998419.2442806\n",
      "Optimizer step time: 0.000237 seconds\n",
      "119 / 196 1721998420.493357\n",
      "Optimizer step time: 0.000199 seconds\n",
      "120 / 196 1721998421.6934493\n",
      "Optimizer step time: 0.000238 seconds\n",
      "121 / 196 1721998422.9061852\n",
      "Optimizer step time: 0.000253 seconds\n",
      "122 / 196 1721998424.094355\n",
      "Optimizer step time: 0.000244 seconds\n",
      "123 / 196 1721998425.3237915\n",
      "Optimizer step time: 0.000217 seconds\n",
      "124 / 196 1721998426.4590728\n",
      "Optimizer step time: 0.000407 seconds\n",
      "125 / 196 1721998427.583092\n",
      "Optimizer step time: 0.000330 seconds\n",
      "126 / 196 1721998428.7856932\n",
      "Optimizer step time: 0.000293 seconds\n",
      "127 / 196 1721998430.063894\n",
      "Optimizer step time: 0.000211 seconds\n",
      "128 / 196 1721998431.2139504\n",
      "Optimizer step time: 0.000262 seconds\n",
      "129 / 196 1721998432.3553782\n",
      "Optimizer step time: 0.000402 seconds\n",
      "130 / 196 1721998433.5495267\n",
      "Optimizer step time: 0.000266 seconds\n",
      "131 / 196 1721998434.6482337\n",
      "Optimizer step time: 0.000284 seconds\n",
      "132 / 196 1721998435.7890406\n",
      "Optimizer step time: 0.003631 seconds\n",
      "133 / 196 1721998437.016709\n",
      "Optimizer step time: 0.000249 seconds\n",
      "134 / 196 1721998438.1562011\n",
      "Optimizer step time: 0.000268 seconds\n",
      "135 / 196 1721998439.3812754\n",
      "Optimizer step time: 0.000246 seconds\n",
      "136 / 196 1721998440.5153375\n",
      "Optimizer step time: 0.000213 seconds\n",
      "137 / 196 1721998441.6618795\n",
      "Optimizer step time: 0.000721 seconds\n",
      "138 / 196 1721998442.8090076\n",
      "Optimizer step time: 0.000359 seconds\n",
      "139 / 196 1721998443.9401708\n",
      "Optimizer step time: 0.000221 seconds\n",
      "140 / 196 1721998445.1001124\n",
      "Optimizer step time: 0.000263 seconds\n",
      "141 / 196 1721998446.179788\n",
      "Optimizer step time: 0.000250 seconds\n",
      "142 / 196 1721998447.3304384\n",
      "Optimizer step time: 0.000209 seconds\n",
      "143 / 196 1721998448.370253\n",
      "Optimizer step time: 0.000272 seconds\n",
      "144 / 196 1721998449.5812962\n",
      "Optimizer step time: 0.000363 seconds\n",
      "145 / 196 1721998450.7692814\n",
      "Optimizer step time: 0.000205 seconds\n",
      "146 / 196 1721998451.9521744\n",
      "Optimizer step time: 0.000240 seconds\n",
      "147 / 196 1721998453.080505\n",
      "Optimizer step time: 0.000202 seconds\n",
      "148 / 196 1721998454.2252398\n",
      "Optimizer step time: 0.000262 seconds\n",
      "149 / 196 1721998455.4000947\n",
      "Optimizer step time: 0.000344 seconds\n",
      "150 / 196 1721998456.5736992\n",
      "Optimizer step time: 0.000314 seconds\n",
      "151 / 196 1721998457.8361547\n",
      "Optimizer step time: 0.000220 seconds\n",
      "152 / 196 1721998459.0495663\n",
      "Optimizer step time: 0.000263 seconds\n",
      "153 / 196 1721998460.253185\n",
      "Optimizer step time: 0.000224 seconds\n",
      "154 / 196 1721998461.4270437\n",
      "Optimizer step time: 0.000231 seconds\n",
      "155 / 196 1721998462.6655428\n",
      "Optimizer step time: 0.000242 seconds\n",
      "156 / 196 1721998463.808346\n",
      "Optimizer step time: 0.000188 seconds\n",
      "157 / 196 1721998464.940258\n",
      "Optimizer step time: 0.008159 seconds\n",
      "158 / 196 1721998466.014584\n",
      "Optimizer step time: 0.000262 seconds\n",
      "159 / 196 1721998467.2142076\n",
      "Optimizer step time: 0.000425 seconds\n",
      "160 / 196 1721998468.3466568\n",
      "Optimizer step time: 0.000225 seconds\n",
      "161 / 196 1721998469.5318468\n",
      "Optimizer step time: 0.000260 seconds\n",
      "162 / 196 1721998470.653096\n",
      "Optimizer step time: 0.000274 seconds\n",
      "163 / 196 1721998471.7520359\n",
      "Optimizer step time: 0.000226 seconds\n",
      "164 / 196 1721998472.8869276\n",
      "Optimizer step time: 0.000230 seconds\n",
      "165 / 196 1721998474.013366\n",
      "Optimizer step time: 0.000307 seconds\n",
      "166 / 196 1721998475.0737045\n",
      "Optimizer step time: 0.000273 seconds\n",
      "167 / 196 1721998476.2282813\n",
      "Optimizer step time: 0.000228 seconds\n",
      "168 / 196 1721998477.3961246\n",
      "Optimizer step time: 0.000210 seconds\n",
      "169 / 196 1721998478.5370982\n",
      "Optimizer step time: 0.000266 seconds\n",
      "170 / 196 1721998479.64162\n",
      "Optimizer step time: 0.012431 seconds\n",
      "171 / 196 1721998480.793273\n",
      "Optimizer step time: 0.000234 seconds\n",
      "172 / 196 1721998481.945349\n",
      "Optimizer step time: 0.000262 seconds\n",
      "173 / 196 1721998483.0532172\n",
      "Optimizer step time: 0.000209 seconds\n",
      "174 / 196 1721998484.2532248\n",
      "Optimizer step time: 0.000226 seconds\n",
      "175 / 196 1721998485.4194772\n",
      "Optimizer step time: 0.000371 seconds\n",
      "176 / 196 1721998486.5348303\n",
      "Optimizer step time: 0.000212 seconds\n",
      "177 / 196 1721998487.6669497\n",
      "Optimizer step time: 0.000210 seconds\n",
      "178 / 196 1721998488.8091166\n",
      "Optimizer step time: 0.000190 seconds\n",
      "179 / 196 1721998489.9533982\n",
      "Optimizer step time: 0.000254 seconds\n",
      "180 / 196 1721998491.1741138\n",
      "Optimizer step time: 0.000297 seconds\n",
      "181 / 196 1721998492.3723276\n",
      "Optimizer step time: 0.000240 seconds\n",
      "182 / 196 1721998493.5400662\n",
      "Optimizer step time: 0.000203 seconds\n",
      "183 / 196 1721998494.677453\n",
      "Optimizer step time: 0.000223 seconds\n",
      "184 / 196 1721998495.8780966\n",
      "Optimizer step time: 0.000274 seconds\n",
      "185 / 196 1721998497.076207\n",
      "Optimizer step time: 0.000353 seconds\n",
      "186 / 196 1721998498.2758284\n",
      "Optimizer step time: 0.000202 seconds\n",
      "187 / 196 1721998499.4724298\n",
      "Optimizer step time: 0.000206 seconds\n",
      "188 / 196 1721998500.6829128\n",
      "Optimizer step time: 0.000257 seconds\n",
      "189 / 196 1721998501.8403049\n",
      "Optimizer step time: 0.000219 seconds\n",
      "190 / 196 1721998503.1294198\n",
      "Optimizer step time: 0.000202 seconds\n",
      "191 / 196 1721998504.2699363\n",
      "Optimizer step time: 0.000256 seconds\n",
      "192 / 196 1721998505.3916903\n",
      "Optimizer step time: 0.000283 seconds\n",
      "193 / 196 1721998506.643364\n",
      "Optimizer step time: 0.000284 seconds\n",
      "194 / 196 1721998507.8390913\n",
      "Optimizer step time: 0.000325 seconds\n",
      "195 / 196 1721998508.891522\n",
      "Optimizer step time: 0.000222 seconds\n",
      "Test loss: 14.012, Val Accuracy: 27.71%\n",
      "Epoch 4\n",
      "0 / 196 1721998554.4005992\n",
      "Optimizer step time: 0.000363 seconds\n",
      "1 / 196 1721998555.4090908\n",
      "Optimizer step time: 0.000209 seconds\n",
      "2 / 196 1721998556.406978\n",
      "Optimizer step time: 0.000242 seconds\n",
      "3 / 196 1721998557.3770325\n",
      "Optimizer step time: 0.000198 seconds\n",
      "4 / 196 1721998558.4405735\n",
      "Optimizer step time: 0.000206 seconds\n",
      "5 / 196 1721998559.303948\n",
      "Optimizer step time: 0.000269 seconds\n",
      "6 / 196 1721998560.2076256\n",
      "Optimizer step time: 0.000368 seconds\n",
      "7 / 196 1721998561.241278\n",
      "Optimizer step time: 0.000201 seconds\n",
      "8 / 196 1721998562.237252\n",
      "Optimizer step time: 0.000210 seconds\n",
      "9 / 196 1721998563.1905754\n",
      "Optimizer step time: 0.000262 seconds\n",
      "10 / 196 1721998564.225565\n",
      "Optimizer step time: 0.000271 seconds\n",
      "11 / 196 1721998565.2842963\n",
      "Optimizer step time: 0.000307 seconds\n",
      "12 / 196 1721998566.3968487\n",
      "Optimizer step time: 0.000215 seconds\n",
      "13 / 196 1721998567.3973355\n",
      "Optimizer step time: 0.000206 seconds\n",
      "14 / 196 1721998568.476397\n",
      "Optimizer step time: 0.000202 seconds\n",
      "15 / 196 1721998569.4851828\n",
      "Optimizer step time: 0.000289 seconds\n",
      "16 / 196 1721998570.4428518\n",
      "Optimizer step time: 0.000254 seconds\n",
      "17 / 196 1721998571.4383137\n",
      "Optimizer step time: 0.000247 seconds\n",
      "18 / 196 1721998572.3883836\n",
      "Optimizer step time: 0.000291 seconds\n",
      "19 / 196 1721998573.3467064\n",
      "Optimizer step time: 0.000258 seconds\n",
      "20 / 196 1721998574.3572574\n",
      "Optimizer step time: 0.000221 seconds\n",
      "21 / 196 1721998575.3030746\n",
      "Optimizer step time: 0.000437 seconds\n",
      "22 / 196 1721998576.2741563\n",
      "Optimizer step time: 0.000233 seconds\n",
      "23 / 196 1721998577.3353999\n",
      "Optimizer step time: 0.000215 seconds\n",
      "24 / 196 1721998578.3000538\n",
      "Optimizer step time: 0.000224 seconds\n",
      "25 / 196 1721998579.3434682\n",
      "Optimizer step time: 0.000260 seconds\n",
      "26 / 196 1721998580.2991118\n",
      "Optimizer step time: 0.000215 seconds\n",
      "27 / 196 1721998581.2711823\n",
      "Optimizer step time: 0.000258 seconds\n",
      "28 / 196 1721998582.2367136\n",
      "Optimizer step time: 0.000208 seconds\n",
      "29 / 196 1721998583.2243688\n",
      "Optimizer step time: 0.000216 seconds\n",
      "30 / 196 1721998584.1270006\n",
      "Optimizer step time: 0.000244 seconds\n",
      "31 / 196 1721998585.1520789\n",
      "Optimizer step time: 0.000243 seconds\n",
      "32 / 196 1721998586.1402593\n",
      "Optimizer step time: 0.000195 seconds\n",
      "33 / 196 1721998587.1241345\n",
      "Optimizer step time: 0.000251 seconds\n",
      "34 / 196 1721998588.2172587\n",
      "Optimizer step time: 0.000269 seconds\n",
      "35 / 196 1721998589.1713617\n",
      "Optimizer step time: 0.000535 seconds\n",
      "36 / 196 1721998590.2088175\n",
      "Optimizer step time: 0.000321 seconds\n",
      "37 / 196 1721998591.2881358\n",
      "Optimizer step time: 0.000251 seconds\n",
      "38 / 196 1721998592.2482197\n",
      "Optimizer step time: 0.000246 seconds\n",
      "39 / 196 1721998593.1242564\n",
      "Optimizer step time: 0.000204 seconds\n",
      "40 / 196 1721998594.1589093\n",
      "Optimizer step time: 0.000275 seconds\n",
      "41 / 196 1721998595.1032078\n",
      "Optimizer step time: 0.000247 seconds\n",
      "42 / 196 1721998596.0363429\n",
      "Optimizer step time: 0.001096 seconds\n",
      "43 / 196 1721998597.1201282\n",
      "Optimizer step time: 0.000198 seconds\n",
      "44 / 196 1721998598.117467\n",
      "Optimizer step time: 0.000246 seconds\n",
      "45 / 196 1721998599.305394\n",
      "Optimizer step time: 0.000406 seconds\n",
      "46 / 196 1721998600.4208584\n",
      "Optimizer step time: 0.000230 seconds\n",
      "47 / 196 1721998601.6320982\n",
      "Optimizer step time: 0.000227 seconds\n",
      "48 / 196 1721998602.9159634\n",
      "Optimizer step time: 0.000219 seconds\n",
      "49 / 196 1721998604.17088\n",
      "Optimizer step time: 0.000376 seconds\n",
      "50 / 196 1721998605.304853\n",
      "Optimizer step time: 0.000252 seconds\n",
      "51 / 196 1721998606.529534\n",
      "Optimizer step time: 0.000244 seconds\n",
      "52 / 196 1721998607.716277\n",
      "Optimizer step time: 0.000248 seconds\n",
      "53 / 196 1721998608.952082\n",
      "Optimizer step time: 0.007762 seconds\n",
      "54 / 196 1721998610.1065469\n",
      "Optimizer step time: 0.000310 seconds\n",
      "55 / 196 1721998611.3086252\n",
      "Optimizer step time: 0.000194 seconds\n",
      "56 / 196 1721998612.5174541\n",
      "Optimizer step time: 0.000199 seconds\n",
      "57 / 196 1721998613.652265\n",
      "Optimizer step time: 0.000244 seconds\n",
      "58 / 196 1721998614.7953098\n",
      "Optimizer step time: 0.000253 seconds\n",
      "59 / 196 1721998615.972226\n",
      "Optimizer step time: 0.000403 seconds\n",
      "60 / 196 1721998617.1750138\n",
      "Optimizer step time: 0.000222 seconds\n",
      "61 / 196 1721998618.371425\n",
      "Optimizer step time: 0.000226 seconds\n",
      "62 / 196 1721998619.635464\n",
      "Optimizer step time: 0.000387 seconds\n",
      "63 / 196 1721998620.745463\n",
      "Optimizer step time: 0.000277 seconds\n",
      "64 / 196 1721998621.9390845\n",
      "Optimizer step time: 0.000257 seconds\n",
      "65 / 196 1721998623.1724422\n",
      "Optimizer step time: 0.000263 seconds\n",
      "66 / 196 1721998624.3991823\n",
      "Optimizer step time: 0.000247 seconds\n",
      "67 / 196 1721998625.584105\n",
      "Optimizer step time: 0.000443 seconds\n",
      "68 / 196 1721998626.8204792\n",
      "Optimizer step time: 0.000281 seconds\n",
      "69 / 196 1721998627.9331899\n",
      "Optimizer step time: 0.000204 seconds\n",
      "70 / 196 1721998629.1403751\n",
      "Optimizer step time: 0.000222 seconds\n",
      "71 / 196 1721998630.2710605\n",
      "Optimizer step time: 0.000374 seconds\n",
      "72 / 196 1721998631.562808\n",
      "Optimizer step time: 0.000198 seconds\n",
      "73 / 196 1721998632.7158227\n",
      "Optimizer step time: 0.000314 seconds\n",
      "74 / 196 1721998633.8603153\n",
      "Optimizer step time: 0.000253 seconds\n",
      "75 / 196 1721998635.1125448\n",
      "Optimizer step time: 0.000212 seconds\n",
      "76 / 196 1721998636.3851998\n",
      "Optimizer step time: 0.000363 seconds\n",
      "77 / 196 1721998637.6273935\n",
      "Optimizer step time: 0.000261 seconds\n",
      "78 / 196 1721998638.901275\n",
      "Optimizer step time: 0.000232 seconds\n",
      "79 / 196 1721998640.128102\n",
      "Optimizer step time: 0.000237 seconds\n",
      "80 / 196 1721998641.2694507\n",
      "Optimizer step time: 0.000246 seconds\n",
      "81 / 196 1721998642.5148103\n",
      "Optimizer step time: 0.000206 seconds\n",
      "82 / 196 1721998643.6763642\n",
      "Optimizer step time: 0.000242 seconds\n",
      "83 / 196 1721998644.736574\n",
      "Optimizer step time: 0.000233 seconds\n",
      "84 / 196 1721998645.961505\n",
      "Optimizer step time: 0.000422 seconds\n",
      "85 / 196 1721998647.1579425\n",
      "Optimizer step time: 0.000201 seconds\n",
      "86 / 196 1721998648.3189754\n",
      "Optimizer step time: 0.000236 seconds\n",
      "87 / 196 1721998649.48519\n",
      "Optimizer step time: 0.000269 seconds\n",
      "88 / 196 1721998650.7000966\n",
      "Optimizer step time: 0.000209 seconds\n",
      "89 / 196 1721998651.853168\n",
      "Optimizer step time: 0.000233 seconds\n",
      "90 / 196 1721998652.9787054\n",
      "Optimizer step time: 0.000200 seconds\n",
      "91 / 196 1721998654.0886908\n",
      "Optimizer step time: 0.000242 seconds\n",
      "92 / 196 1721998655.1402504\n",
      "Optimizer step time: 0.000205 seconds\n",
      "93 / 196 1721998656.3696804\n",
      "Optimizer step time: 0.000863 seconds\n",
      "94 / 196 1721998657.4920914\n",
      "Optimizer step time: 0.000355 seconds\n",
      "95 / 196 1721998658.556002\n",
      "Optimizer step time: 0.000216 seconds\n",
      "96 / 196 1721998659.7613993\n",
      "Optimizer step time: 0.000300 seconds\n",
      "97 / 196 1721998660.915626\n",
      "Optimizer step time: 0.000255 seconds\n",
      "98 / 196 1721998662.0769475\n",
      "Optimizer step time: 0.000237 seconds\n",
      "99 / 196 1721998663.2640886\n",
      "Optimizer step time: 0.000240 seconds\n",
      "100 / 196 1721998664.4203026\n",
      "Optimizer step time: 0.000266 seconds\n",
      "101 / 196 1721998665.5166864\n",
      "Optimizer step time: 0.000251 seconds\n",
      "102 / 196 1721998666.6718419\n",
      "Optimizer step time: 0.000371 seconds\n",
      "103 / 196 1721998667.7674782\n",
      "Optimizer step time: 0.000194 seconds\n",
      "104 / 196 1721998668.8982863\n",
      "Optimizer step time: 0.000236 seconds\n",
      "105 / 196 1721998670.1884284\n",
      "Optimizer step time: 0.000248 seconds\n",
      "106 / 196 1721998671.365642\n",
      "Optimizer step time: 0.000338 seconds\n",
      "107 / 196 1721998672.5200999\n",
      "Optimizer step time: 0.000271 seconds\n",
      "108 / 196 1721998673.7712443\n",
      "Optimizer step time: 0.000217 seconds\n",
      "109 / 196 1721998675.0001934\n",
      "Optimizer step time: 0.000214 seconds\n",
      "110 / 196 1721998676.3326747\n",
      "Optimizer step time: 0.000546 seconds\n",
      "111 / 196 1721998677.4150548\n",
      "Optimizer step time: 0.000539 seconds\n",
      "112 / 196 1721998678.560924\n",
      "Optimizer step time: 0.000261 seconds\n",
      "113 / 196 1721998679.6943495\n",
      "Optimizer step time: 0.000212 seconds\n",
      "114 / 196 1721998680.953195\n",
      "Optimizer step time: 0.000660 seconds\n",
      "115 / 196 1721998682.1463196\n",
      "Optimizer step time: 0.000300 seconds\n",
      "116 / 196 1721998683.2471561\n",
      "Optimizer step time: 0.000229 seconds\n",
      "117 / 196 1721998684.4568567\n",
      "Optimizer step time: 0.000242 seconds\n",
      "118 / 196 1721998685.5601728\n",
      "Optimizer step time: 0.000203 seconds\n",
      "119 / 196 1721998686.8836288\n",
      "Optimizer step time: 0.000230 seconds\n",
      "120 / 196 1721998688.1139965\n",
      "Optimizer step time: 0.000297 seconds\n",
      "121 / 196 1721998689.2492547\n",
      "Optimizer step time: 0.000229 seconds\n",
      "122 / 196 1721998690.4095528\n",
      "Optimizer step time: 0.000205 seconds\n",
      "123 / 196 1721998691.555281\n",
      "Optimizer step time: 0.000204 seconds\n",
      "124 / 196 1721998692.7899306\n",
      "Optimizer step time: 0.000240 seconds\n",
      "125 / 196 1721998693.9565685\n",
      "Optimizer step time: 0.000385 seconds\n",
      "126 / 196 1721998695.2452507\n",
      "Optimizer step time: 0.000225 seconds\n",
      "127 / 196 1721998696.422148\n",
      "Optimizer step time: 0.000310 seconds\n",
      "128 / 196 1721998697.5989025\n",
      "Optimizer step time: 0.000268 seconds\n",
      "129 / 196 1721998698.7288678\n",
      "Optimizer step time: 0.000292 seconds\n",
      "130 / 196 1721998699.9446738\n",
      "Optimizer step time: 0.000242 seconds\n",
      "131 / 196 1721998701.091293\n",
      "Optimizer step time: 0.000257 seconds\n",
      "132 / 196 1721998702.175358\n",
      "Optimizer step time: 0.000264 seconds\n",
      "133 / 196 1721998703.300722\n",
      "Optimizer step time: 0.000309 seconds\n",
      "134 / 196 1721998704.5680716\n",
      "Optimizer step time: 0.000399 seconds\n",
      "135 / 196 1721998705.7336137\n",
      "Optimizer step time: 0.000303 seconds\n",
      "136 / 196 1721998706.921257\n",
      "Optimizer step time: 0.000611 seconds\n",
      "137 / 196 1721998708.0880709\n",
      "Optimizer step time: 0.000272 seconds\n",
      "138 / 196 1721998709.2544096\n",
      "Optimizer step time: 0.000243 seconds\n",
      "139 / 196 1721998710.3880882\n",
      "Optimizer step time: 0.000234 seconds\n",
      "140 / 196 1721998711.543874\n",
      "Optimizer step time: 0.000235 seconds\n",
      "141 / 196 1721998712.660137\n",
      "Optimizer step time: 0.000238 seconds\n",
      "142 / 196 1721998713.8053536\n",
      "Optimizer step time: 0.000202 seconds\n",
      "143 / 196 1721998714.9788146\n",
      "Optimizer step time: 0.000204 seconds\n",
      "144 / 196 1721998716.0165098\n",
      "Optimizer step time: 0.000251 seconds\n",
      "145 / 196 1721998717.109215\n",
      "Optimizer step time: 0.000441 seconds\n",
      "146 / 196 1721998718.3763642\n",
      "Optimizer step time: 0.000247 seconds\n",
      "147 / 196 1721998719.497251\n",
      "Optimizer step time: 0.000249 seconds\n",
      "148 / 196 1721998720.565267\n",
      "Optimizer step time: 0.000233 seconds\n",
      "149 / 196 1721998721.7825549\n",
      "Optimizer step time: 0.000309 seconds\n",
      "150 / 196 1721998722.9283807\n",
      "Optimizer step time: 0.000218 seconds\n",
      "151 / 196 1721998724.105444\n",
      "Optimizer step time: 0.000391 seconds\n",
      "152 / 196 1721998725.351487\n",
      "Optimizer step time: 0.000254 seconds\n",
      "153 / 196 1721998726.5226011\n",
      "Optimizer step time: 0.000296 seconds\n",
      "154 / 196 1721998727.7401013\n",
      "Optimizer step time: 0.000314 seconds\n",
      "155 / 196 1721998728.8401253\n",
      "Optimizer step time: 0.000216 seconds\n",
      "156 / 196 1721998729.9601126\n",
      "Optimizer step time: 0.000358 seconds\n",
      "157 / 196 1721998731.0199623\n",
      "Optimizer step time: 0.000271 seconds\n",
      "158 / 196 1721998732.2010868\n",
      "Optimizer step time: 0.000247 seconds\n",
      "159 / 196 1721998733.3581495\n",
      "Optimizer step time: 0.000244 seconds\n",
      "160 / 196 1721998734.551725\n",
      "Optimizer step time: 0.000237 seconds\n",
      "161 / 196 1721998735.7508512\n",
      "Optimizer step time: 0.000228 seconds\n",
      "162 / 196 1721998737.0040963\n",
      "Optimizer step time: 0.000400 seconds\n",
      "163 / 196 1721998738.205754\n",
      "Optimizer step time: 0.000257 seconds\n",
      "164 / 196 1721998739.3465884\n",
      "Optimizer step time: 0.000292 seconds\n",
      "165 / 196 1721998740.48413\n",
      "Optimizer step time: 0.000250 seconds\n",
      "166 / 196 1721998741.7640803\n",
      "Optimizer step time: 0.000251 seconds\n",
      "167 / 196 1721998742.9520912\n",
      "Optimizer step time: 0.000450 seconds\n",
      "168 / 196 1721998744.192132\n",
      "Optimizer step time: 0.000262 seconds\n",
      "169 / 196 1721998745.3353047\n",
      "Optimizer step time: 0.000283 seconds\n",
      "170 / 196 1721998746.448123\n",
      "Optimizer step time: 0.000220 seconds\n",
      "171 / 196 1721998747.672087\n",
      "Optimizer step time: 0.000206 seconds\n",
      "172 / 196 1721998748.8955965\n",
      "Optimizer step time: 0.000249 seconds\n",
      "173 / 196 1721998750.112138\n",
      "Optimizer step time: 0.000299 seconds\n",
      "174 / 196 1721998751.3728662\n",
      "Optimizer step time: 0.000260 seconds\n",
      "175 / 196 1721998752.627519\n",
      "Optimizer step time: 0.000220 seconds\n",
      "176 / 196 1721998753.7204669\n",
      "Optimizer step time: 0.000280 seconds\n",
      "177 / 196 1721998754.9583068\n",
      "Optimizer step time: 0.000376 seconds\n",
      "178 / 196 1721998756.2018378\n",
      "Optimizer step time: 0.000470 seconds\n",
      "179 / 196 1721998757.392131\n",
      "Optimizer step time: 0.000255 seconds\n",
      "180 / 196 1721998758.5605218\n",
      "Optimizer step time: 0.000324 seconds\n",
      "181 / 196 1721998759.7080827\n",
      "Optimizer step time: 0.000243 seconds\n",
      "182 / 196 1721998760.9523003\n",
      "Optimizer step time: 0.000428 seconds\n",
      "183 / 196 1721998762.2743664\n",
      "Optimizer step time: 0.000223 seconds\n",
      "184 / 196 1721998763.4776118\n",
      "Optimizer step time: 0.000330 seconds\n",
      "185 / 196 1721998764.5827749\n",
      "Optimizer step time: 0.000280 seconds\n",
      "186 / 196 1721998765.6463647\n",
      "Optimizer step time: 0.000348 seconds\n",
      "187 / 196 1721998766.8240988\n",
      "Optimizer step time: 0.000521 seconds\n",
      "188 / 196 1721998767.9649866\n",
      "Optimizer step time: 0.000214 seconds\n",
      "189 / 196 1721998769.141787\n",
      "Optimizer step time: 0.000245 seconds\n",
      "190 / 196 1721998770.4075549\n",
      "Optimizer step time: 0.000294 seconds\n",
      "191 / 196 1721998771.6024103\n",
      "Optimizer step time: 0.000247 seconds\n",
      "192 / 196 1721998772.7956352\n",
      "Optimizer step time: 0.000195 seconds\n",
      "193 / 196 1721998773.8929958\n",
      "Optimizer step time: 0.000224 seconds\n",
      "194 / 196 1721998775.072526\n",
      "Optimizer step time: 0.000253 seconds\n",
      "195 / 196 1721998776.1364138\n",
      "Optimizer step time: 0.000229 seconds\n",
      "Test loss: 14.871, Val Accuracy: 27.00%\n",
      "Epoch 5\n",
      "0 / 196 1721998825.6732914\n",
      "Optimizer step time: 0.000489 seconds\n",
      "1 / 196 1721998826.8487308\n",
      "Optimizer step time: 0.000250 seconds\n",
      "2 / 196 1721998827.9643993\n"
     ]
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '2', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.batch_size\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    FC_RESERVOIR  =  wandb.config.FC_RESERVOIR\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'CIFAR10' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': f'{which_data_hyper} fc_reservoir',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"min\": 0.00001, \"max\": 0.1},\n",
    "        \"batch_size\": {\"values\": [256]},\n",
    "        \"time_step\": {\"values\": [8]},\n",
    "        \"decay\": {\"min\": 0.25, \"max\": 1.0},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [20]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [32]},\n",
    "        \"dvs_duration\": {\"values\": [1000000]},\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [False]},\n",
    "        \"FC_RESERVOIR\": {\"values\": [True]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
